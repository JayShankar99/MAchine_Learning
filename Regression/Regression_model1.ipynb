{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77</td>\n",
       "      <td>79.775152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>23.177279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>25.609262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>17.857388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>41.849864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x          y\n",
       "0  77  79.775152\n",
       "1  21  23.177279\n",
       "2  22  25.609262\n",
       "3  20  17.857388\n",
       "4  36  41.849864"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking care of Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before taking care of the Missing data fist we have to look for the missing data i.e. we will check wahat percentage of data we are missing and based on that we will make our dicision.\n",
    "\n",
    "so we can youse the \".isnull()\" or \".isna()\" methods for the perpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x    0\n",
       "y    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"y\"].fillna(int(dataset[\"y\"].mean()),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105.5918375"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['y'].unique().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into Training set and Test set.\n",
    "\n",
    "Now, before splitting the dataset into taining set and testing set. Make sure to seperate the independent and dependent veriable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(columns=[\"y\"],axis=1)\n",
    "y = dataset[\"x\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      x\n",
       "183  91\n",
       "38   28\n",
       "24   97\n",
       "142  73\n",
       "141  97\n",
       "..   ..\n",
       "106   1\n",
       "14   36\n",
       "92   63\n",
       "179  72\n",
       "102  19\n",
       "\n",
       "[159 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x\n",
       "82   56\n",
       "15   15\n",
       "111  16\n",
       "177  32\n",
       "76   81\n",
       "163  16\n",
       "68   55\n",
       "67   50\n",
       "120  64\n",
       "173  56\n",
       "176  58\n",
       "148  35\n",
       "65   18\n",
       "30   99\n",
       "86   82\n",
       "85   39\n",
       "55    0\n",
       "60   13\n",
       "90   67\n",
       "159   7\n",
       "16   65\n",
       "124  41\n",
       "96   21\n",
       "172  54\n",
       "66   41\n",
       "189  46\n",
       "147  60\n",
       "9     5\n",
       "18   87\n",
       "128  89\n",
       "190  92\n",
       "45   37\n",
       "192  77\n",
       "164  49\n",
       "101  82\n",
       "69   13\n",
       "126  78\n",
       "123  12\n",
       "75   28\n",
       "78   52"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models\n",
    "\n",
    "1. Simple Linear Regression\n",
    "2. Multiple Linear Regression\n",
    "3. Polynomial Regression\n",
    "4. Support Vector for Regression (SVR)\n",
    "5. Decision Tree Regression\n",
    "6. Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predecting the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualising the training set result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_train, y_train, color = 'red')\n",
    "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
    "plt.title(\"X vs Y\")\n",
    "plt.xlabel(\"X values\")\n",
    "plt.ylabel(\"Y values\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualising the testing set result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_test, y_test, color = 'red')\n",
    "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
    "plt.title(\"X vs Y\")\n",
    "plt.xlabel(\"X values\")\n",
    "plt.ylabel(\"Y values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1: runfile('C:/Users/Jay/.spyder-py3/temp.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 2/1: runfile('C:/Users/Jay/.spyder-py3/temp.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 2/2: runfile('C:/Users/Jay/.spyder-py3/temp.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 2/3: runfile('C:/Users/Jay/.spyder-py3/temp.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 2/4: runfile('C:/Users/Jay/.spyder-py3/temp.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 2/5: runfile('C:/Users/Jay/.spyder-py3/temp.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 2/6: runfile('C:/Users/Jay/.spyder-py3/trial1.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 2/7: runfile('C:/Users/Jay/.spyder-py3/trial1.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 2/8: runfile('C:/Users/Jay/.spyder-py3/trial1.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 2/9: runfile('C:/Users/Jay/.spyder-py3/trial1.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 3/1: runfile('C:/Users/Jay/.spyder-py3/trial1.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 3/2: runfile('C:/Users/Jay/.spyder-py3/trial1.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 3/3: k = temp.factorial(6)\n",
      " 3/4: runfile('C:/Users/Jay/.spyder-py3/trial2.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 3/5: runfile('C:/Users/Jay/.spyder-py3/trial2.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 3/6: runfile('C:/Users/Jay/.spyder-py3/trial2.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 3/7: runfile('C:/Users/Jay/.spyder-py3/trial2.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 3/8: clear\n",
      " 3/9: runfile('C:/Users/Jay/.spyder-py3/all_call.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      "3/10: clear\n",
      "3/11: runfile('C:/Users/Jay/.spyder-py3/all_call.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      "3/12: clear\n",
      "3/13: runfile('C:/Users/Jay/.spyder-py3/all_call.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      "3/14: runfile('C:/Users/Jay/.spyder-py3/all_call.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      "3/15: clear\n",
      "3/16: runfile('C:/Users/Jay/.spyder-py3/all_call.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      "3/17: clear\n",
      " 6/1:\n",
      "a =20\n",
      "b= 30\n",
      "print(a+b)\n",
      " 7/1: runfile('C:/Users/Jay/.spyder-py3/all_call.py', wdir='C:/Users/Jay/.spyder-py3')\n",
      " 8/1: runcell(0, 'C:/Users/Jay/demo.py')\n",
      " 8/2: runfile('C:/Users/Jay/demo.py', wdir='C:/Users/Jay')\n",
      " 8/3: runfile('C:/Users/Jay/demo.py', wdir='C:/Users/Jay')\n",
      " 9/1: arr = [1,0,0,1,0,1]\n",
      " 9/2:\n",
      "arr = [1,0,0,1,0,1]\n",
      "j=0\n",
      "for i in range(len(arr)):\n",
      "    if arr[i]<1:\n",
      "        temp = arr[i]\n",
      "        arr[i] = arr[j]\n",
      "        arr[j] = temp\n",
      "        \n",
      "arr\n",
      " 9/3:\n",
      "arr = [1,0,0,1,0,1]\n",
      "j=0\n",
      "for i in range(len(arr)):\n",
      "    if arr[i]<1:\n",
      "        temp = arr[i]\n",
      "        arr[i] = arr[j]\n",
      "        arr[j] = temp\n",
      "        j += 1\n",
      "        \n",
      "arr\n",
      " 9/4:\n",
      "arr = [0,1,0,0,1,0,1]\n",
      "j=0\n",
      "for i in range(len(arr)):\n",
      "    if arr[i]<1:\n",
      "        temp = arr[i]\n",
      "        arr[i] = arr[j]\n",
      "        arr[j] = temp\n",
      "        j += 1\n",
      "        \n",
      "arr\n",
      " 9/5:\n",
      "arr = [0,1,0,0,1,0,1,0,1,0,0]\n",
      "j=0\n",
      "for i in range(len(arr)):\n",
      "    if arr[i]<1:\n",
      "        temp = arr[i]\n",
      "        arr[i] = arr[j]\n",
      "        arr[j] = temp\n",
      "        j += 1\n",
      "        \n",
      "arr\n",
      " 9/6:\n",
      "arr = [0,1,0,0,1,0,1,0,1,0,0]\n",
      "j=0\n",
      "for i in range(len(arr)):\n",
      "    if arr[i]<1:\n",
      "        temp = arr[i]\n",
      "        arr[i] = arr[j]\n",
      "        arr[j] = temp\n",
      "        j += 1\n",
      "        \n",
      "arr\n",
      "10/1:\n",
      "import matplotlib.pyplot as plt\n",
      "x1 = [1,2,3,4,5]\n",
      "x2 = [1,2,3,4,5]\n",
      "\n",
      "y1 = [2,4,1,4,-1] \n",
      "y2 = [5,-2,-4,7,8]\n",
      "\n",
      "plt.subplot(2,1,1)\n",
      "\n",
      "plt.plot(x1, y1)\n",
      "plt.title(\"Subplot 1\")\n",
      "plt.xlabel(\"x1\")\n",
      "plt.ylabel(\"y1\")\n",
      "\n",
      "plt.subplot(2,1,2)\n",
      "plt.plot(x2, y2)\n",
      "plt.title(\"Subplot 2\")\n",
      "plt.xlabel(\"x2\")\n",
      "plt.ylabel(\"y2\")\n",
      "\n",
      "plt.show()\n",
      "10/2:\n",
      "import matplotlib.pyplot as plt\n",
      "x1 = [1,2,3,4,5]\n",
      "x2 = [1,2,3,4,5]\n",
      "\n",
      "y1 = [2,4,1,4,-1] \n",
      "y2 = [5,-2,-4,7,8]\n",
      "\n",
      "plt.subplot(2,1,1)\n",
      "\n",
      "plt.plot(x1, y1)\n",
      "plt.title(\"Subplot 1\")\n",
      "plt.xlabel(\"x1\")\n",
      "plt.ylabel(\"y1\")\n",
      "\n",
      "plt.subplot(2,1,2)\n",
      "plt.plot(x2, y2)\n",
      "plt.title(\"Subplot 2\")\n",
      "plt.xlabel(\"x2\")\n",
      "plt.ylabel(\"y2\")\n",
      "\n",
      "plt.show()\n",
      "10/3:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "x1 = [1,2,3,4,5]\n",
      "x2 = [1,2,3,4,5]\n",
      "\n",
      "y1 = [2,4,1,4,-1] \n",
      "y2 = [5,-2,-4,7,8]\n",
      "\n",
      "plt.subplot(1,1,1)\n",
      "\n",
      "plt.plot(x1, y1)\n",
      "plt.title(\"Subplot 1\")\n",
      "plt.xlabel(\"x1\")\n",
      "plt.ylabel(\"y1\")\n",
      "\n",
      "plt.subplot(1,2,2)\n",
      "plt.plot(x2, y2)\n",
      "plt.title(\"Subplot 2\")\n",
      "plt.xlabel(\"x2\")\n",
      "plt.ylabel(\"y2\")\n",
      "\n",
      "plt.show()\n",
      "10/4:\n",
      "import matplotlib.pyplot as plt\n",
      "x1 = [1,2,3,4,5]\n",
      "x2 = [1,2,3,4,5]\n",
      "\n",
      "y1 = [2,4,1,4,-1] \n",
      "y2 = [5,-2,-4,7,8]\n",
      "\n",
      "plt.subplot(1,2,1)\n",
      "\n",
      "plt.plot(x1, y1)\n",
      "plt.title(\"Subplot 1\")\n",
      "plt.xlabel(\"x1\")\n",
      "plt.ylabel(\"y1\")\n",
      "\n",
      "plt.subplot(1,2,2)\n",
      "plt.plot(x2, y2)\n",
      "plt.title(\"Subplot 2\")\n",
      "plt.xlabel(\"x2\")\n",
      "plt.ylabel(\"y2\")\n",
      "\n",
      "plt.show()\n",
      "10/5:\n",
      "import matplotlib.pyplot as plt\n",
      "x1 = [1,2,3,4,5]\n",
      "x2 = [1,2,3,4,5]\n",
      "\n",
      "y1 = [2,4,1,4,-1] \n",
      "y2 = [5,-2,-4,7,8]\n",
      "\n",
      "plt.subplot(1,2,1)\n",
      "\n",
      "plt.plot(x1, y1)\n",
      "plt.title(\"Subplot 1\")\n",
      "plt.xlabel(\"x1\")\n",
      "plt.ylabel(\"y1\")\n",
      "\n",
      "plt.subplot(1,2,1)\n",
      "plt.plot(x2, y2)\n",
      "plt.title(\"Subplot 2\")\n",
      "plt.xlabel(\"x2\")\n",
      "plt.ylabel(\"y2\")\n",
      "\n",
      "plt.show()\n",
      "10/6:\n",
      "import matplotlib.pyplot as plt\n",
      "x1 = [1,2,3,4,5]\n",
      "x2 = [1,2,3,4,5]\n",
      "\n",
      "y1 = [2,4,1,4,-1] \n",
      "y2 = [5,-2,-4,7,8]\n",
      "\n",
      "plt.subplot(1,1,1)\n",
      "\n",
      "plt.plot(x1, y1)\n",
      "plt.title(\"Subplot 1\")\n",
      "plt.xlabel(\"x1\")\n",
      "plt.ylabel(\"y1\")\n",
      "\n",
      "plt.subplot(1,2,1)\n",
      "plt.plot(x2, y2)\n",
      "plt.title(\"Subplot 2\")\n",
      "plt.xlabel(\"x2\")\n",
      "plt.ylabel(\"y2\")\n",
      "\n",
      "plt.show()\n",
      "10/7:\n",
      "import matplotlib.pyplot as plt\n",
      "x1 = [1,2,3,4,5]\n",
      "x2 = [1,2,3,4,5]\n",
      "\n",
      "y1 = [2,4,1,4,-1] \n",
      "y2 = [5,-2,-4,7,8]\n",
      "\n",
      "plt.subplot(1,1,1)\n",
      "\n",
      "plt.plot(x1, y1)\n",
      "plt.title(\"Subplot 1\")\n",
      "plt.xlabel(\"x1\")\n",
      "plt.ylabel(\"y1\")\n",
      "\n",
      "plt.subplot(1,2,2)\n",
      "plt.plot(x2, y2)\n",
      "plt.title(\"Subplot 2\")\n",
      "plt.xlabel(\"x2\")\n",
      "plt.ylabel(\"y2\")\n",
      "\n",
      "plt.show()\n",
      "10/8:\n",
      "import matplotlib.pyplot as plt\n",
      "x1 = [1,2,3,4,5]\n",
      "x2 = [1,2,3,4,5]\n",
      "\n",
      "y1 = [2,4,1,4,-1] \n",
      "y2 = [5,-2,-4,7,8]\n",
      "\n",
      "plt.subplot(1,2,1)\n",
      "\n",
      "plt.plot(x1, y1)\n",
      "plt.title(\"Subplot 1\")\n",
      "plt.xlabel(\"x1\")\n",
      "plt.ylabel(\"y1\")\n",
      "\n",
      "plt.subplot(1,2,2)\n",
      "plt.plot(x2, y2)\n",
      "plt.title(\"Subplot 2\")\n",
      "plt.xlabel(\"x2\")\n",
      "plt.ylabel(\"y2\")\n",
      "\n",
      "plt.show()\n",
      "11/1:\n",
      "for i in range(11):\n",
      "    if i==0 or i==5 or i==10:\n",
      "        print(\"+----+----+\")\n",
      "    else:\n",
      "        print(\"|    |    |\")\n",
      "11/2:\n",
      "for i in range(11):\n",
      "    if i==0 or i==5 or i==10:\n",
      "        print(\"+----+----+\")\n",
      "    else:\n",
      "        print(\"|   |  |\")\n",
      "11/3:\n",
      "for i in range(11):\n",
      "    if i==0 or i==5 or i==10:\n",
      "        print(\"+----+----+\")\n",
      "    else:\n",
      "        print(\"|   |  |\")\n",
      "11/4:\n",
      "for i in range(11):\n",
      "    if i==0 or i==5 or i==10:\n",
      "        print(\"+----+----+\")\n",
      "    else:\n",
      "        print(\"|  |  |\")\n",
      "11/5:\n",
      "for i in range(11):\n",
      "    if i==0 or i==5 or i==10:\n",
      "        print(\"+----+----+\")\n",
      "    else:\n",
      "        print(\"|  |   |\")\n",
      "12/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x = np.arange(0,2*np.pi,0.01)\n",
      "\n",
      "y1 = np.sin(x)\n",
      "y2 = np.cos(x)\n",
      "\n",
      "\n",
      "plt.plot(x,y1,label='Sin')\n",
      "plt.plot(x,y2,label='Cos')\n",
      "plt.xlabel(\"X-axis\")\n",
      "plt.ylabel(\"Y-axis\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "plt.title(\"Sin and Cos graph\")\n",
      "plt.show()\n",
      "12/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x = np.arange(0,2*np.pi,0.01)\n",
      "\n",
      "y1 = np.sin(x)\n",
      "y2 = np.cos(x)\n",
      "\n",
      "\n",
      "plt.plot(x,y1,label='Sin')\n",
      "plt.plot(x,y2,label='Cos')\n",
      "plt.xlabel(\"X-axis\")\n",
      "plt.ylabel(\"Y-axis\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "plt.title(\"Sin and Cos graph\")\n",
      "plt.show()\n",
      "12/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x = np.arange(0,2*np.pi,0.01)\n",
      "\n",
      "y1 = np.sin(x)\n",
      "y2 = np.cos(x)\n",
      "y3 = np.tan(x)\n",
      "\n",
      "\n",
      "plt.plot(x,y1,label='Sin')\n",
      "plt.plot(x,y2,label='Cos')\n",
      "plt.show(x,y3,label='Tan')\n",
      "plt.xlabel(\"X-axis\")\n",
      "plt.ylabel(\"Y-axis\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "plt.title(\"Sin and Cos graph\")\n",
      "plt.show()\n",
      "12/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x = np.arange(0,2*np.pi,0.01)\n",
      "\n",
      "y1 = np.sin(x)\n",
      "y2 = np.cos(x)\n",
      "y3 = np.tan(x)\n",
      "\n",
      "\n",
      "plt.plot(x,y1,label='Sin')\n",
      "plt.plot(x,y2,label='Cos')\n",
      "plt.plot(x,y3,label='Tan')\n",
      "plt.xlabel(\"X-axis\")\n",
      "plt.ylabel(\"Y-axis\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "plt.title(\"Sin and Cos graph\")\n",
      "plt.show()\n",
      "13/1: runfile('F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)/Reading_CSV_file.py', wdir='F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)')\n",
      "13/2: runfile('F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)/Reading_CSV_file.py', wdir='F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)')\n",
      "13/3: runfile('F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)/Reading_CSV_file.py', wdir='F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)')\n",
      "13/4: runfile('F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)/Reading_CSV_file.py', wdir='F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)')\n",
      "13/5: runfile('F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)/Reading_CSV_file.py', wdir='F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)')\n",
      "13/6: runfile('F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)/Reading_CSV_file.py', wdir='F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)')\n",
      "14/1:\n",
      "import pandas as pd\n",
      "\n",
      "pokemon = pd.read_csv(\"F:\\\\Pythn prgramming practice\\\\Machine Learning (ML)\\\\Data_Reading(Panda)\\\\pokemon1.csv\",\n",
      "    usecols=[\"Name\"],squeeze=True)\n",
      "\n",
      "#   squeeze -> basicaly this argument converts the data into a series.\n",
      "\n",
      "#print(pokemon.head())\n",
      "print(pokemon.sort_values())\n",
      "\n",
      "#   sort_values() -> This method sort the values in ass\n",
      "#print(pokemon.sort_values().head())\n",
      "14/2:\n",
      "import pandas as pd\n",
      "\n",
      "pokemon = pd.read_csv(\"F:\\\\Pythn prgramming practice\\\\Machine Learning (ML)\\\\Data_Reading(Panda)\\\\pokemon1.csv\",\n",
      "    usecols=[\"Name\"],squeeze=True)\n",
      "\n",
      "#   squeeze -> basicaly this argument converts the data into a series.\n",
      "\n",
      "#print(pokemon.head())\n",
      "pokemon.sort_values()\n",
      "\n",
      "#   sort_values() -> This method sort the values in ass\n",
      "#print(pokemon.sort_values().head())\n",
      "15/1: x = \"$121,121\"\n",
      "15/2:\n",
      "x = \"$121,121\"\n",
      "\n",
      "print(int(x))\n",
      "15/3:\n",
      "x = \"$121,121\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "print(y)\n",
      "15/4:\n",
      "x = \"$121,121\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$,\")\n",
      "print(y)\n",
      "15/5:\n",
      "x = \"$121,121\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = y.split(\",\")\n",
      "print(y)\n",
      "15/6:\n",
      "x = \"$121,121\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = y[1].split(\",\")\n",
      "print(y)\n",
      "15/7:\n",
      "x = \"$121,121\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = \",\".join(y[1])\n",
      "print(y)\n",
      "15/8:\n",
      "x = \"$121,121\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = \",\".join(y)\n",
      "print(y)\n",
      "15/9:\n",
      "x = \"$121,121\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = \"\".join(y[1])\n",
      "print(y)\n",
      "15/10:\n",
      "x = \"$121,121\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = \",\".join(y[1])\n",
      "print(y)\n",
      "15/11:\n",
      "x = \"$121,121\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = \"\".join(y[1])\n",
      "print(y)\n",
      "15/12:\n",
      "x = \"$121,121\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "print(type(y))\n",
      "y = \"\".join(y[1])\n",
      "print(y)\n",
      "15/13:\n",
      "x = \"$121,121,545\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = y.split(y[1])\n",
      "print(type(y))\n",
      "y = \"\".join(y[1])\n",
      "print(y)\n",
      "15/14:\n",
      "x = \"$121,121,545\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = y[1].split(\",\")\n",
      "print(type(y))\n",
      "y = \"\".join(y[1])\n",
      "print(y)\n",
      "15/15:\n",
      "x = \"$121,121,545\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = y[1].split(\",\")\n",
      "print(type(y))\n",
      "y = \"\".join(y)\n",
      "print(y)\n",
      "15/16:\n",
      "x = \"$121,121,545\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = y[1].split(\",\")\n",
      "print(type(y))\n",
      "y = \"\".join(y)\n",
      "print(type(y))\n",
      "15/17:\n",
      "x = \"$121,121,545\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = y[1].split(\",\")\n",
      "print(type(y))\n",
      "y = \"\".join(y)\n",
      "print(type(int(y)))\n",
      "15/18:\n",
      "x = \"$121,121,545\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = y[1].split(\",\")\n",
      "print(type(y))\n",
      "y = int(\"\".join(y))\n",
      "print(type(y))\n",
      "15/19:\n",
      "x = \"$121,121,545\"\n",
      "\n",
      "y = str()\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = y[1].split(\",\")\n",
      "\n",
      "y = int(\"\".join(y))\n",
      "print(type(y))\n",
      "15/20:\n",
      "x = \"$121,121,545\"\n",
      "\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = y[1].split(\",\")\n",
      "\n",
      "y = int(\"\".join(y))\n",
      "print(type(y))\n",
      "15/21:\n",
      "x = \"$121,121,545\"\n",
      "\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = y[1].split(\",\")\n",
      "\n",
      "y = int(\"\".join(y))\n",
      "y = y*2\n",
      "print(y)\n",
      "print(type(y))\n",
      "15/22:\n",
      "x = \"$121,121,545\"\n",
      "\n",
      "\n",
      "y = x.split(\"$\")\n",
      "y = y[1].split(\",\")\n",
      "\n",
      "y = int(\"\".join(y))\n",
      "print(type(y))\n",
      "16/1:\n",
      "days = int(input(\"Enter number of days\"))\n",
      "year = days//365\n",
      "days =days%365\n",
      "month = days//30\n",
      "week = days//7\n",
      "days = days%7\n",
      "print(year,\" Year\",month,\" Months\",week,\" Week\",days,\" Days\")\n",
      "16/2:\n",
      "days = int(input(\"Enter number of days\"))\n",
      "year = days//365\n",
      "days =days%365\n",
      "month = days//30\n",
      "week = days//7\n",
      "days = days%7\n",
      "print(year,\" Year\",month,\" Months\",week,\" Week\",days,\" Days\")\n",
      "16/3: 500//365\n",
      "16/4: 500%365\n",
      "16/5: 500%365\n",
      "16/6: 500%365\n",
      "16/7:\n",
      "days = int(input(\"Enter number of days\"))\n",
      "year = days//365\n",
      "days = days%365\n",
      "month = days//30\n",
      "week = days//7\n",
      "days = days%7\n",
      "print(year,\" Year\",month,\" Months\",week,\" Week\",days,\" Days\")\n",
      "16/8: 500%365\n",
      "16/9:\n",
      "days = int(input(\"Enter number of days\"))\n",
      "year = days//365\n",
      "days = days%365\n",
      "month = days//30\n",
      "week = days//7\n",
      "days = days%7\n",
      "print(year,\" Year\",month,\" Months\",week,\" Week\",days,\" Days\")\n",
      "16/10:\n",
      "days = int(input(\"Enter number of days\"))\n",
      "year = days//365\n",
      "days = days%365\n",
      "month = days//30\n",
      "week = days//7\n",
      "days = days%7\n",
      "print(year,\" Year\",month,\" Months\",week,\" Week\",days,\" Days\")\n",
      "16/11: 500%365\n",
      "16/12: 5%5\n",
      "16/13: 5/5\n",
      "16/14: 4/5\n",
      "16/15: 6/5\n",
      "16/16:\n",
      "if 5/5 >5//5:\n",
      "    true\n",
      "else:\n",
      "    false\n",
      "16/17:\n",
      "if 5/5 >5//5:\n",
      "    print(\"true\")\n",
      "else:\n",
      "    print(\"false\")\n",
      "16/18:\n",
      "if 6/5 >6//5:\n",
      "    print(\"true\")\n",
      "else:\n",
      "    print(\"false\")\n",
      "16/19:\n",
      "import math\n",
      "\n",
      "math.abs(-12)\n",
      "16/20:\n",
      "import math\n",
      "\n",
      "abs(-12)\n",
      "16/21:\n",
      "import math\n",
      "\n",
      "math.floor(9/2)\n",
      "16/22: import pandas as pd\n",
      "16/23:\n",
      "import pandas as pd\n",
      "\n",
      "pokemon = pd.read_csv(\"F:\\\\Pythn prgramming practice\\\\Machine Learning (ML)\\\\Data_Reading(Panda)\\\\pokemon.csv\")\n",
      "\n",
      "pokemon.info\n",
      "16/24:\n",
      "import pandas as pd\n",
      "\n",
      "pokemon = pd.read_csv(\"F:\\\\Pythn prgramming practice\\\\Machine Learning (ML)\\\\Data_Reading(Panda)\\\\pokemon.csv\")\n",
      "\n",
      "pokemon.info()\n",
      "16/25: pokemon.get_dtype_counts()\n",
      "16/26:\n",
      "a=[1,2,3]\n",
      "a.insert(1,5)\n",
      "a\n",
      "16/27:\n",
      "a=[]\n",
      "a.insert(1,5)\n",
      "a\n",
      "16/28: hash(2)\n",
      "16/29: hash(1,2)\n",
      "16/30: hash(2./)\n",
      "16/31: hash(2,/)\n",
      "16/32:\n",
      "a=2\n",
      "b=3\n",
      "c=\"**\"\n",
      "acb\n",
      "16/33:\n",
      "a=2\n",
      "b=3\n",
      "c=\"**\"\n",
      "a*c*b\n",
      "16/34:\n",
      "a=2\n",
      "b=3\n",
      "c=\"**\"\n",
      "a**b\n",
      "16/35:\n",
      "l = [[10,2,5],[7,1,0],[9,9,9],[1,23,12],[6,5,9]]\n",
      "l.sort()\n",
      "16/36:\n",
      "l = [[10,2,5],[7,1,0],[9,9,9],[1,23,12],[6,5,9]]\n",
      "l.sort()\n",
      "l\n",
      "16/37:\n",
      "l = [[10,2,5],[7,1,0],[9,9,9],[1,23,12],[6,5,9]]\n",
      "l.sort(key = 1)\n",
      "l\n",
      "16/38:\n",
      "l = [[10,2,5],[7,1,0],[9,9,9],[1,23,12],[6,5,9]]\n",
      "l.sort()\n",
      "l\n",
      "16/39: SUBSTR(SQUARE ANS ALWAYS WORK HARD,14,6)\n",
      "18/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "18/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "18/3:\n",
      "arr = [[1,2,3],[4,5,6]]\n",
      "max(arr)\n",
      "18/4:\n",
      "arr = [[1,2,3],[4,5,6]]\n",
      "max(arr[1][])\n",
      "18/5:\n",
      "arr = [[1,2,3],[4,5,6]]\n",
      "max(arr[1])\n",
      "19/1: data = pd.read_csv(\"employ.csv\")\n",
      "19/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "19/3: data = pd.read_csv(\"employ.csv\")\n",
      "19/4:\n",
      "data = pd.read_csv(\"employ.csv\")\n",
      "data.head()\n",
      "19/5:\n",
      "real_x = data.iloc[:0]values\n",
      "real_x\n",
      "19/6:\n",
      "real_x = data.iloc[:,0]values\n",
      "real_x\n",
      "19/7:\n",
      "real_x = data.iloc[:,0]values\n",
      "real_x\n",
      "19/8:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "19/9:\n",
      "real_x = data.iloc[:,0]values\n",
      "real_x\n",
      "19/10:\n",
      "real_x = data.iloc[:,0].values\n",
      "real_x\n",
      "19/11:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "19/12:\n",
      "real_x = data.iloc[:,0].values\n",
      "real_y = data.iloc[:,1].values.reshape()\n",
      "real_y\n",
      "19/13:\n",
      "real_x = data.iloc[:,0].values\n",
      "real_y = data.iloc[:,1].values.reshape\n",
      "real_y\n",
      "19/14:\n",
      "real_x = data.iloc[:,0].values\n",
      "real_y = data.iloc[:,1].values.reshape\n",
      "real_y\n",
      "19/15: training_x,testing_x,training_y,testing_y = train_test_split(real_x, real_y, test_size = 0.3, random_state = 0)\n",
      "19/16:\n",
      "real_x = data.iloc[:,0].values\n",
      "real_y = data.iloc[:,1].values\n",
      "real_y\n",
      "19/17: training_x,testing_x,training_y,testing_y = train_test_split(real_x, real_y, test_size = 0.3, random_state = 0)\n",
      "19/18:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "19/19:\n",
      "lin = LinearRegression()\n",
      "lin.fit(training_x,training_y)\n",
      "19/20:\n",
      "real_x = data.iloc[:,0].values\n",
      "real_y = data.iloc[:,1].values\n",
      "real_x = real_x.reshape(-1,1)\n",
      "real_y = real_y.reshape(-1,1)\n",
      "real_y\n",
      "19/21:\n",
      "real_x = data.iloc[:,0].values\n",
      "real_y = data.iloc[:,1].values\n",
      "real_x = real_x.reshape(-1,1)\n",
      "real_y = real_y.reshape(1,1)\n",
      "real_y\n",
      "19/22:\n",
      "real_x = data.iloc[:,0].values\n",
      "real_y = data.iloc[:,1].values\n",
      "real_x = real_x.reshape(-1,1)\n",
      "real_y = real_y.reshape(0,1)\n",
      "real_y\n",
      "19/23:\n",
      "real_x = data.iloc[:,0].values\n",
      "real_y = data.iloc[:,1].values\n",
      "real_x = real_x.reshape(-1,1)\n",
      "real_y = real_y.reshape(2,1)\n",
      "real_y\n",
      "19/24:\n",
      "real_x = data.iloc[:,0].values\n",
      "real_y = data.iloc[:,1].values\n",
      "real_x = real_x.reshape(-1,1)\n",
      "real_y = real_y.reshape(-2,1)\n",
      "real_y\n",
      "19/25:\n",
      "real_x = data.iloc[:,0].values\n",
      "real_y = data.iloc[:,1].values\n",
      "real_x = real_x.reshape(-1,1)\n",
      "real_y = real_y.reshape(-3,1)\n",
      "real_y\n",
      "19/26:\n",
      "real_x = data.iloc[:,0].values\n",
      "real_y = data.iloc[:,1].values\n",
      "real_x = real_x.reshape(-1,1)\n",
      "real_y = real_y.reshape(-1,1)\n",
      "real_y\n",
      "19/27:\n",
      "lin = LinearRegression()\n",
      "lin.fit(training_x,training_y)\n",
      "19/28:\n",
      "lin = LinearRegression()\n",
      "lin.fit(training_x,training_y)\n",
      "19/29:\n",
      "real_x = data.iloc[:,0].values\n",
      "real_y = data.iloc[:,1].values\n",
      "real_x = real_x.reshape(-1,1)\n",
      "real_y = real_y.reshape(-1,1)\n",
      "19/30:\n",
      "lin = LinearRegression()\n",
      "lin.fit(training_x,training_y)\n",
      "19/31: training_x,testing_x,training_y,testing_y = train_test_split(real_x, real_y, test_size = 0.3, random_state = 0)\n",
      "19/32:\n",
      "lin = LinearRegression()\n",
      "lin.fit(training_x,training_y)\n",
      "19/33: predict_y = lin.predict(testing_x)\n",
      "19/34: testing_y[3]\n",
      "19/35: testing_y[3]\n",
      "19/36: testing_y[0]\n",
      "19/37: testing_y[1]\n",
      "19/38:\n",
      "data = pd.read_csv(\"employ.csv\")\n",
      "data.head(10)\n",
      "19/39: testing_y[2]\n",
      "19/40: testing_y[3]\n",
      "19/41: testing_y[2]\n",
      "19/42: testing_y[-1]\n",
      "19/43: testing_y[-2]\n",
      "19/44: testing_y[2]\n",
      "19/45:\n",
      "training_x,testing_x,training_y,testing_y = train_test_split(real_x, real_y, test_size = 0.3, random_state = 0)\n",
      "testing_y\n",
      "19/46: testing_y[2]\n",
      "19/47:\n",
      "predict_y = lin.predict(testing_x)\n",
      "predict_y\n",
      "19/48: predict_y[2]\n",
      "19/49: predict_y(39891)\n",
      "19/50: predict_y[(39891)]\n",
      "19/51: predict_y[39891]\n",
      "19/52: predict_y[2]\n",
      "21/1: ##PRICE PREDICTOR\n",
      "21/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "21/3:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "21/4:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "21/5: arr = np.array([i for i in range(11)])\n",
      "21/6:\n",
      "arr = np.array([i for i in range(11)])\n",
      "arr\n",
      "21/7:\n",
      "arr = np.array([i for i in range(11)])\n",
      "arr1 = np.array([2*i for i in range(11)])\n",
      "21/8:\n",
      "arr = np.array([i for i in range(11)])\n",
      "arr1 = np.array([2*i for i in range(11)])\n",
      "arr\n",
      "arr1\n",
      "21/9: arr\n",
      "21/10:\n",
      "a = np.sum(y*x)\n",
      "a\n",
      "21/11:\n",
      "a = np.sum(arr*arr1)\n",
      "a\n",
      "21/12: arr\n",
      "21/13:\n",
      "a = np.sum(arr*arr1)\n",
      "n = np.size(arr)\n",
      "n\n",
      "21/14:\n",
      "predict = -0.0586206896552 + 1.45747126437*x\n",
      "predict\n",
      "21/15:\n",
      "predict = -0.0586206896552 + 1.45747126437*arr\n",
      "predict\n",
      "21/16:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "22/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "22/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "22/3: data = pd.read_csv(\"Book1.csv\")\n",
      "22/4:\n",
      "data = pd.read_csv(\"Book1.csv\")\n",
      "data.head(5)\n",
      "22/5: arr = np.array(data.PovPct)\n",
      "22/6:\n",
      "arr = np.array(data.PovPct)\n",
      "arr\n",
      "22/7: arr1 = np.array(Brth15to17)\n",
      "22/8: arr1 = np.array(data.Brth15to17)\n",
      "22/9:\n",
      "arr1 = np.array(data.Brth15to17)\n",
      "arr1\n",
      "22/10:\n",
      "plt.scatter(arr,arr1,c=\"m\",label = \"Scatter plot\")\n",
      "plt.xlabel(\"PovPct\")\n",
      "plt.ylabel(\"Brth15to17\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "22/11:\n",
      "# READING DATA\n",
      "data = pd.read_csv(\"Book1.csv\")\n",
      "print(data.shape)\n",
      "data.head(5)\n",
      "22/12:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "22/13:\n",
      "# COLLECTING X, Y\n",
      "x = np.array(data.PovPct)\n",
      "y = np.array(data.Brth15to17)\n",
      "22/14:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "22/15:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "22/16:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "    \n",
      "numerator\n",
      "22/17:\n",
      "# COLLECTING X, Y\n",
      "x = np.array(data.PovPct)\n",
      "y = np.array(data.Brth15to17)\n",
      "\n",
      "x\n",
      "22/18:\n",
      "# COLLECTING X, Y\n",
      "x = np.array(data.PovPct)\n",
      "y = np.array(data.Brth15to17)\n",
      "\n",
      "print(x)\n",
      "y\n",
      "22/19:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "print(n)\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "    \n",
      "numerator\n",
      "22/20:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "print(n)\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n-1):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "    \n",
      "numerator\n",
      "22/21:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "print(n)\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n-1):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "    \n",
      "numerator\n",
      "22/22:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "print(n-1)\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n-1):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "    \n",
      "numerator\n",
      "22/23:\n",
      "# COLLECTING X, Y\n",
      "x = data[\"PovPct\"].values\n",
      "y = data[\"Brth15to17\"].values\n",
      "\n",
      "print(x)\n",
      "y\n",
      "22/24:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "print(n-1)\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n-1):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "    \n",
      "numerator\n",
      "22/25:\n",
      "# READING DATA\n",
      "data = pd.read_csv(\"Book1.csv\")\n",
      "print(data.shape)\n",
      "data.head(5)\n",
      "22/26:\n",
      "# COLLECTING X, Y\n",
      "x = data[\"PovPct\"].values\n",
      "y = data[\"Brth15to17\"].values\n",
      "\n",
      "print(x)\n",
      "y\n",
      "22/27:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "print(n-1)\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n-1):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "    \n",
      "numerator\n",
      "22/28:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "print(mean_x,mean_y)\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "print(n-1)\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n-1):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "    \n",
      "numerator\n",
      "22/29:\n",
      "# COLLECTING X, Y\n",
      "x = data[\"PovPct\"].values[:-2]\n",
      "y = data[\"Brth15to17\"].values[:-2]\n",
      "\n",
      "print(x)\n",
      "y\n",
      "22/30:\n",
      "# COLLECTING X, Y\n",
      "x = data[\"PovPct\"].values[:-1]\n",
      "y = data[\"Brth15to17\"].values[:-2]\n",
      "\n",
      "print(x)\n",
      "y\n",
      "22/31:\n",
      "# COLLECTING X, Y\n",
      "x = data[\"PovPct\"].values[:-1]\n",
      "y = data[\"Brth15to17\"].values[:-1]\n",
      "\n",
      "print(x)\n",
      "y\n",
      "22/32:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "print(mean_x,mean_y)\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "print(n-1)\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n-1):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "    \n",
      "numerator\n",
      "22/33:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "print(n-1)\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n-1):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "22/34:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "22/35:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "    \n",
      "m = numerator//denominator\n",
      "c = mean_y - m*mean_x\n",
      "\n",
      "print(m,c)\n",
      "22/36:\n",
      "# Mean of X, Y\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "\n",
      "\n",
      "# Total numbers of values\n",
      "n = len(x)\n",
      "\n",
      "\n",
      "# Using the formula to calculate m and c\n",
      "numerator = 0\n",
      "denominator = 0\n",
      "\n",
      "for i in range(n):\n",
      "    numerator += (x[i]-mean_x)*(y[i]-mean_y)\n",
      "    denominator += (x[i]-mean_x)**2\n",
      "    \n",
      "m = numerator/denominator\n",
      "c = mean_y - m*mean_x\n",
      "\n",
      "print(m,c)\n",
      "22/37:\n",
      "Y = m*x + c\n",
      "\n",
      "plt.plot(x,Y,c=\"g\",label = \"Regration linr\")\n",
      "\n",
      "plt.scatter(arr,arr1,c=\"m\",label = \"Scatter plot\")\n",
      "plt.xlabel(\"PovPct\")\n",
      "plt.ylabel(\"Brth15to17\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "22/38:\n",
      "# Calculating the R-squared value\n",
      "\n",
      "ss_t = 0       # Total sum of squares(y-mean_y)**2\n",
      "ss_r = 0       # Total sum of reseduals(y-predicted - mean_y)**2\n",
      "\n",
      "for i in range(n):\n",
      "    y_predict = m*x[i] + c\n",
      "    ss_t += (Y[i] - mean_y)**2\n",
      "    ss_r += (Y[i] - y_predict)**2\n",
      "r2 = 1 - (ss_r/ss_t)\n",
      "print(r2)\n",
      "22/39:\n",
      "# Calculating the R-squared value\n",
      "\n",
      "ss_t = 0       # Total sum of squares(y-mean_y)**2\n",
      "ss_r = 0       # Total sum of reseduals(y-predicted - mean_y)**2\n",
      "\n",
      "for i in range(n):\n",
      "    y_predict = m*x[i] + c\n",
      "    ss_t += (Y[i] - mean_y)**2\n",
      "    ss_r += (Y[i] - y_predict)**2\n",
      "r2 =(ss_r/ss_t)\n",
      "print(r2)\n",
      "22/40:\n",
      "# Calculating the R-squared value\n",
      "\n",
      "ss_t = 0       # Total sum of squares(y-mean_y)**2\n",
      "ss_r = 0       # Total sum of reseduals(y-predicted - mean_y)**2\n",
      "\n",
      "for i in range(n):\n",
      "    y_predict = m*x[i] + c\n",
      "    ss_t += (Y[i] - mean_y)**2\n",
      "    ss_r += (Y[i] - y_predict)**2\n",
      "print(ss_t,ss_r)\n",
      "r2 = (ss_r/ss_t)\n",
      "print(r2)\n",
      "22/41:\n",
      "x = x.reshape((m,1))\n",
      "x\n",
      "22/42:\n",
      "#x = x.reshape((m,1))\n",
      "x\n",
      "22/43:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "22/44:\n",
      "#x = x.reshape((m,1))\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "Y_predict = reg.predict(x)\n",
      "\n",
      "mse = mean_squared_error(x,Y_predict)\n",
      "rsme = np.sqrt(mse)\n",
      "r2 _score = reg.score(x,y)\n",
      "r2_score\n",
      "22/45:\n",
      "#x = x.reshape((m,1))\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "Y_predict = reg.predict(x)\n",
      "\n",
      "mse = mean_squared_error(x,Y_predict)\n",
      "rsme = np.sqrt(mse)\n",
      "r2_score = reg.score(x,y)\n",
      "r2_score\n",
      "22/46:\n",
      "x = x.reshape((-1,1))\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "Y_predict = reg.predict(x)\n",
      "\n",
      "mse = mean_squared_error(x,Y_predict)\n",
      "rsme = np.sqrt(mse)\n",
      "r2_score = reg.score(x,y)\n",
      "r2_score\n",
      "22/47:\n",
      "x = x.reshape((1,-1))\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "Y_predict = reg.predict(x)\n",
      "\n",
      "mse = mean_squared_error(x,Y_predict)\n",
      "rsme = np.sqrt(mse)\n",
      "r2_score = reg.score(x,y)\n",
      "r2_score\n",
      "22/48:\n",
      "x = x.reshape(1,-1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "Y_predict = reg.predict(x)\n",
      "\n",
      "mse = mean_squared_error(x,Y_predict)\n",
      "rsme = np.sqrt(mse)\n",
      "r2_score = reg.score(x,y)\n",
      "r2_score\n",
      "22/49:\n",
      "x = x.reshape(-1,-1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "Y_predict = reg.predict(x)\n",
      "\n",
      "mse = mean_squared_error(x,Y_predict)\n",
      "rsme = np.sqrt(mse)\n",
      "r2_score = reg.score(x,y)\n",
      "r2_score\n",
      "22/50:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "Y_predict = reg.predict(x)\n",
      "\n",
      "mse = mean_squared_error(x,Y_predict)\n",
      "rsme = np.sqrt(mse)\n",
      "r2_score = reg.score(x,y)\n",
      "r2_score\n",
      "22/51:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "22/52:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "22/53:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "reg.predict(20.1)\n",
      "22/54:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "reg.predict([20.1])\n",
      "22/55:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "reg.predict(31.5)\n",
      "22/56:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "reg.predict[20.1]\n",
      "22/57:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "reg.predict[[20.1]\n",
      "22/58:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "reg.predict[[20.1]]\n",
      "22/59:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "reg.predict([20.1])\n",
      "22/60:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "reg.predict([[20.1]])\n",
      "22/61:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "p = reg.predict(data.PovPct)\n",
      "data[\"predicted\"] = p\n",
      "data.head(10)\n",
      "22/62:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "p = reg.predict(data[\"PovPct\"].values[:-1])\n",
      "data[\"predicted\"] = p\n",
      "data.head(10)\n",
      "22/63:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "p = reg.predict(data[[\"PovPct\"]].values[:-1])\n",
      "data[\"predicted\"] = p\n",
      "data.head(10)\n",
      "22/64:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "p = reg.predict(data[[\"PovPct\"]].values)\n",
      "data[\"predicted\"] = p\n",
      "data.head(10)\n",
      "22/65:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "p = reg.predict(data[[\"PovPct\"]].values[:-1])\n",
      "data[\"predicted\"] = p\n",
      "data.head(10)\n",
      "22/66:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "p = reg.predict(data[[\"PovPct\"]].values[:-2])\n",
      "data[\"predicted\"] = p\n",
      "data.head(10)\n",
      "22/67:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "p = reg.predict(data[[\"PovPct\"]].values!=nan)\n",
      "data[\"predicted\"] = p\n",
      "data.head(10)\n",
      "22/68:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "p = reg.predict(data[[\"PovPct\"]].values!=\"nan\")\n",
      "data[\"predicted\"] = p\n",
      "data.head(10)\n",
      "22/69:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "p = reg.predict((data[[\"PovPct\"]].values)!=\"nan\")\n",
      "data[\"predicted\"] = p\n",
      "data.head(10)\n",
      "22/70:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "p = reg.predict(x)\n",
      "data[\"predicted\"] = p\n",
      "data.head(10)\n",
      "22/71:\n",
      "# COLLECTING X, Y\n",
      "x = data[\"PovPct\"].values[:-1]\n",
      "y = data[\"Brth15to17\"].values[:-1]\n",
      "\n",
      "print(x)\n",
      "y\n",
      "22/72:\n",
      "# READING DATA\n",
      "data = pd.read_csv(\"Book1.csv\")\n",
      "print(data.shape)\n",
      "data.head(5)\n",
      "22/73:\n",
      "# COLLECTING X, Y\n",
      "x = data[\"PovPct\"].values\n",
      "y = data[\"Brth15to17\"].values\n",
      "\n",
      "print(x)\n",
      "y\n",
      "22/74:\n",
      "# COLLECTING X, Y\n",
      "x = data[\"PovPct\"].values\n",
      "y = data[\"Brth15to17\"].values\n",
      "\n",
      "print(x)\n",
      "y\n",
      "22/75:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "p = reg.predict(x)\n",
      "data[\"predicted\"] = p\n",
      "data.head(10)\n",
      "22/76:\n",
      "# READING DATA\n",
      "data = pd.read_csv(\"Book1.csv\")\n",
      "print(data.shape)\n",
      "data.head(5)\n",
      "22/77:\n",
      "# COLLECTING X, Y\n",
      "x = data[\"PovPct\"].values\n",
      "y = data[\"Brth15to17\"].values\n",
      "\n",
      "print(x)\n",
      "y\n",
      "22/78:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "p = reg.predict(x)\n",
      "data[\"predicted\"] = p\n",
      "data.head(10)\n",
      "22/79:\n",
      "x = x.reshape(-1,1)\n",
      "reg = LinearRegression()\n",
      "reg = reg.fit(x,y)\n",
      "reg.coef_\n",
      "reg.intercept_\n",
      "p = reg.predict(x)\n",
      "data[\"predicted\"] = p\n",
      "data.head(10)\n",
      "22/80:\n",
      "# Calculating the R-squared value\n",
      "\n",
      "ss_t = 0       # Total sum of squares(y-mean_y)**2\n",
      "ss_r = 0       # Total sum of reseduals(y-predicted - mean_y)**2\n",
      "\n",
      "for i in range(n):\n",
      "    y_predict = m*x[i] + c\n",
      "    ss_t += (Y[i] - mean_y)**2\n",
      "    ss_r += (Y[i] - y_predict)**2\n",
      "\n",
      "r2 = (ss_r/ss_t)\n",
      "print(r2)\n",
      "24/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "24/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "24/3:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "24/4:\n",
      "aar = [1,2,3,4,5,6,7]\n",
      "aar.reshape(1,-1)\n",
      "arr\n",
      "24/5:\n",
      "aar = [1,2,3,4,5,6,7]\n",
      "aar.reshape(-1,1)\n",
      "arr\n",
      "24/6:\n",
      "aar = [1,2,3,4,5,6,7]\n",
      "arr = np.array(aar)\n",
      "arr\n",
      "24/7:\n",
      "aar = [1,2,3,4,5,6,7]\n",
      "arr = np.array(aar)\n",
      "arr\n",
      "arr.reshape(1,-1)\n",
      "arr\n",
      "24/8:\n",
      "aar = [1,2,3,4,5,6,7]\n",
      "arr = np.array(aar)\n",
      "arr\n",
      "arr.reshape(-1,-1)\n",
      "arr\n",
      "24/9:\n",
      "aar = [1,2,3,4,5,6,7]\n",
      "arr = np.array(aar)\n",
      "arr\n",
      "arr.reshape(-1,1)\n",
      "arr\n",
      "24/10:\n",
      "aar = [1,2,3,4,5,6,7]\n",
      "arr = np.array(aar)\n",
      "arr\n",
      "arr.reshape(-1,1)\n",
      "arr\n",
      "24/11:\n",
      "aar = [1,2,3,4,5,6,7]\n",
      "arr = np.array(aar)\n",
      "arr\n",
      "arr.reshape(-1,1)\n",
      "arr\n",
      "24/12:\n",
      "aar = [1,2,3,4,5,6,7]\n",
      "arr = np.array(aar)\n",
      "arr\n",
      "arr.reshape(1,1)\n",
      "arr\n",
      "24/13:\n",
      "aar = [1,2,3,4,5,6,7]\n",
      "arr = np.array(aar)\n",
      "arr\n",
      "arr.reshape(1,-1)\n",
      "arr\n",
      "24/14:\n",
      "aar = [1,2,3,4,5,6,7]\n",
      "arr = np.array(aar)\n",
      "arr\n",
      "arr.reshape(1,-1)\n",
      "arr\n",
      "24/15:\n",
      "data = pd.read_csv(\"Foodtruck.csv\")\n",
      "data.head(10)\n",
      "24/16: data.describe()\n",
      "24/17: data.shape\n",
      "24/18:\n",
      "data = pd.read_csv(\"Foodtruck.csv\")\n",
      "data.head(5)\n",
      "24/19:\n",
      "p = data[\"Population\"].values\n",
      "p\n",
      "24/20:\n",
      "p = data[\"Population\"].values\n",
      "p = p.reshape(1,-1)\n",
      "p\n",
      "24/21:\n",
      "p = data[\"Population\"].values\n",
      "p = p.reshape(1,-1)\n",
      "p\n",
      "24/22:\n",
      "p = data[\"Population\"].values\n",
      "p = p.reshape(-1,1)\n",
      "p\n",
      "24/23:\n",
      "p = data[\"Population\"].values.reshape(-1,1)\n",
      "p\n",
      "24/24:\n",
      "x = data[\"population\"].values.reshape(-1,1)\n",
      "y = data[\"Profit\"].values.reshape(-1,1)\n",
      "y\n",
      "24/25:\n",
      "x = data[\"Population\"].values.reshape(-1,1)\n",
      "y = data[\"Profit\"].values.reshape(-1,1)\n",
      "y\n",
      "24/26:\n",
      "x = data[\"Population\"].values.reshape(-1,1)\n",
      "y = data[\"Profit\"].values.reshape(-1,1)\n",
      "x\n",
      "24/27:\n",
      "x = data[\"Population\"].values.reshape(-1,1)\n",
      "y = data[\"Profit\"].values.reshape(-1,1)\n",
      "24/28:\n",
      "plt.scatter(x,y,c=\"g\",marker=\"o\",label = \"datapoints\")\n",
      "plt.xlabel(\"Population\")\n",
      "plt.ylabel(\"Profit\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "24/29:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.matrics import mean_squared_error\n",
      "24/30:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "24/31:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "24/32: training_x,testing_x,training_y,testing_y = train_test_split(x, y, test_size = 0.3, random_state = 0)\n",
      "24/33:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.model_selection import train_test_split\n",
      "24/34: training_x,testing_x,training_y,testing_y = train_test_split(x, y, test_size = 0.3, random_state = 0)\n",
      "24/35:\n",
      "reg = LinearRegression()\n",
      "reg.fit(training_x,training_y)\n",
      "24/36: y_predict = model.predict(testing_x)\n",
      "24/37: y_predict = reg.predict(testing_x)\n",
      "24/38: y_predict = reg.predict(testing_x)\n",
      "24/39:\n",
      "y_predict = reg.predict(testing_x)\n",
      "y_predict\n",
      "24/40: y_predict = reg.predict(testing_x)\n",
      "24/41: testing_y[3]\n",
      "24/42: y_predict[3]\n",
      "24/43:\n",
      "y_predict = reg.predict(testing_x)\n",
      "mse = mean_squared_error(testing_y,y_predicted)\n",
      "mse\n",
      "24/44:\n",
      "y_predict = reg.predict(testing_x)\n",
      "mse = mean_squared_error(testing_y,y_predict)\n",
      "mse\n",
      "24/45:\n",
      "y_predict = reg.predict(testing_x)\n",
      "mse = mean_squared_error(testing_y,y_predict)\n",
      "print(\"Mean Squared error = \",mse)\n",
      "print(\"Coefficient = \",reg.coef)\n",
      "print(\"Intercept = \",reg.intercept)\n",
      "24/46:\n",
      "y_predict = reg.predict(testing_x)\n",
      "mse = mean_squared_error(testing_y,y_predict)\n",
      "print(\"Mean Squared error = \",mse)\n",
      "print(\"Coefficient = \",reg.coef_)\n",
      "print(\"Intercept = \",reg.intercept_)\n",
      "24/47:\n",
      "plt.scatter(testing_x,testing_y,c=\"g\",marker=\"o\",label = \"datapoints\")\n",
      "plt.xlabel(\"Population\")\n",
      "plt.ylabel(\"Profit\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "24/48:\n",
      "plt.scatter(testing_x,testing_y,c=\"g\",marker=\"o\",label = \"datapoints\")\n",
      "plt.plot(testing_x,y_predict,c=\"m\",label = \"regration\")\n",
      "plt.xlabel(\"Population\")\n",
      "plt.ylabel(\"Profit\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "24/49:\n",
      "plt.scatter(testing_x,testing_y,c=\"g\",marker=\"o\",label = \"datapoints\")\n",
      "plt.scatter(testing_x,y_predict,c=\"m\",label = \"regration\")\n",
      "plt.xlabel(\"Population\")\n",
      "plt.ylabel(\"Profit\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "24/50:\n",
      "plt.scatter(testing_x,testing_y,c=\"g\",marker=\"o\",label = \"datapoints\")\n",
      "plt.plot(testing_x,y_predict,c=\"m\",label = \"regration\")\n",
      "plt.scatter(testing_x,y_predict,c=\"r\")\n",
      "plt.xlabel(\"Population\")\n",
      "plt.ylabel(\"Profit\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "25/1:\n",
      "arr[] = {4,0,2,1,3}\n",
      "for i in range(0,len(arr)):\n",
      "    arr = arr[arr[i]]\n",
      "    \n",
      "arr\n",
      "25/2:\n",
      "arr = {4,0,2,1,3}\n",
      "for i in range(0,len(arr)):\n",
      "    arr = arr[arr[i]]\n",
      "    \n",
      "arr\n",
      "25/3:\n",
      "arr = [4,0,2,1,3]\n",
      "\n",
      "for i in range(0,len(arr)):\n",
      "    arr = arr[arr[i]]\n",
      "    \n",
      "arr\n",
      "25/4:\n",
      "arr = [4,0,2,1,3]\n",
      "\n",
      "for i in range(0,len(arr)):\n",
      "    arr[i] = arr[arr[i]]\n",
      "    \n",
      "arr\n",
      "25/5:\n",
      "arr = [4,0,2,1,3]\n",
      "arr1\n",
      "\n",
      "for i in range(0,len(arr)):\n",
      "    arr1.append(arr[arr[i]])\n",
      "    \n",
      "arr1\n",
      "25/6:\n",
      "arr = [4,0,2,1,3]\n",
      "arr1=[]\n",
      "\n",
      "for i in range(0,len(arr)):\n",
      "    arr1.append(arr[arr[i]])\n",
      "    \n",
      "arr1\n",
      "25/7:\n",
      "arr = [4,0,2,1,3]\n",
      "arr1=[]\n",
      "\n",
      "for i in range(0,len(arr)):\n",
      "    arr1.append(arr[arr[i]])\n",
      "    \n",
      "arr = arr1\n",
      "arr\n",
      "25/8:\n",
      "arr = [4,0,2,1,3]\n",
      "arr1=[]\n",
      "\n",
      "for i in range(0,len(arr)):\n",
      "    arr1.append(arr[arr[i]])\n",
      "    \n",
      "arr = arr1\n",
      "arr\n",
      "25/9:\n",
      "arr = [1,0]\n",
      "arr1=[]\n",
      "\n",
      "for i in range(0,len(arr)):\n",
      "    arr1.append(arr[arr[i]])\n",
      "    \n",
      "arr = arr1\n",
      "arr\n",
      "25/10:\n",
      "arr = [1,0]\n",
      "arr1=[]\n",
      "\n",
      "for i in range(0,len(arr)):\n",
      "    arr1.append(arr[arr[i]])\n",
      "    \n",
      "arr = arr1\n",
      "arr\n",
      "25/11: arr = [int(x) for x in input().strip().split()]\n",
      "25/12: arr\n",
      "25/13:\n",
      "arr\n",
      "arr1=[]\n",
      "\n",
      "for i in range(0,len(arr)):\n",
      "    arr1.append(arr[arr[i]])\n",
      "    \n",
      "arr = arr1\n",
      "arr\n",
      "25/14:\n",
      "arr\n",
      "arr1=[]\n",
      "\n",
      "for i in range(len(arr)):\n",
      "    arr1.append(arr[arr[i]])\n",
      "    \n",
      "arr = arr1\n",
      "arr\n",
      "25/15:\n",
      "arr\n",
      "arr1=[]\n",
      "\n",
      "for i in range(len(arr)):\n",
      "    arr1.append(arr[arr[i]])\n",
      "    \n",
      "arr = arr1\n",
      "arr\n",
      "25/16:\n",
      "arr\n",
      "arr1=[]\n",
      "\n",
      "for i in range(len(arr)):\n",
      "    arr1.append(arr[arr[i]])\n",
      "    \n",
      "arr = arr1\n",
      "arr\n",
      "26/1:\n",
      "arr = [1,0]\n",
      "arr1=[]\n",
      "\n",
      "for i in range(0,len(arr)):\n",
      "    arr1.append(arr[arr[i]])\n",
      "    \n",
      "arr = arr1\n",
      "arr\n",
      "26/2: arr = [int(x) for x in input().strip().split()]\n",
      "26/3:\n",
      "arr\n",
      "arr1=[]\n",
      "\n",
      "for i in range(len(arr)):\n",
      "    arr1.append(arr[arr[i]])\n",
      "    \n",
      "arr = arr1\n",
      "arr\n",
      "26/4:\n",
      "def arrange(arr,n):\n",
      "    m=[]\n",
      "    for i in range(n):\n",
      "        m.append(arr[arr[i]])\n",
      "    arr=m\n",
      "t=int(input())\n",
      "while(t>0):\n",
      "    n=int(input())\n",
      "    arr=[int(x) for x in input().strip().split()]\n",
      "    arrange(arr,n)\n",
      "    for i in arr:\n",
      "        print(i,end=\" \")\n",
      "    print()\n",
      "    t=-1\n",
      "26/5:\n",
      "def arrange(arr,n):\n",
      "    m=[]\n",
      "    for i in range(n):\n",
      "        m.append(arr[arr[i]])\n",
      "    arr=m\n",
      "\n",
      "t=int(input())\n",
      "while(t>0):\n",
      "    n=int(input())\n",
      "    arr=[int(x) for x in input().strip().split()]\n",
      "    arrange(arr,n)\n",
      "    for i in arr:\n",
      "        print(i,end=\" \")\n",
      "    print()\n",
      "    t=-1\n",
      "26/6:\n",
      "def arrange(arr,n):\n",
      "    m=[]\n",
      "    for i in range(n):\n",
      "        m.append(arr[arr[i]])\n",
      "    arr=m\n",
      "    return arr\n",
      "\n",
      "t=int(input())\n",
      "while(t>0):\n",
      "    n=int(input())\n",
      "    arr=[int(x) for x in input().strip().split()]\n",
      "    arrange(arr,n)\n",
      "    for i in arr:\n",
      "        print(i,end=\" \")\n",
      "    print()\n",
      "    t=-1\n",
      "26/7:\n",
      "def arrange(arr,n):\n",
      "    m=[]\n",
      "    for i in range(n):\n",
      "        m.append(arr[arr[i]])\n",
      "    arr[:]=m[:]\n",
      "    return arr\n",
      "\n",
      "t=int(input())\n",
      "while(t>0):\n",
      "    n=int(input())\n",
      "    arr=[int(x) for x in input().strip().split()]\n",
      "    arrange(arr,n)\n",
      "    for i in arr:\n",
      "        print(i,end=\" \")\n",
      "    print()\n",
      "    t=-1\n",
      "26/8:\n",
      "def arrange(arr,n):\n",
      "    m=[]\n",
      "    for i in range(n):\n",
      "        m.append(arr[arr[i]])\n",
      "    arr[:]=m[:]\n",
      "  \n",
      "\n",
      "t=int(input())\n",
      "while(t>0):\n",
      "    n=int(input())\n",
      "    arr=[int(x) for x in input().strip().split()]\n",
      "    arrange(arr,n)\n",
      "    for i in arr:\n",
      "        print(i,end=\" \")\n",
      "    print()\n",
      "    t=-1\n",
      "26/9:\n",
      "arr = [1,0]\n",
      "arr1=[]\n",
      "\n",
      "for i in range(0,len(arr)):\n",
      "    arr.append(arr[arr[i]])\n",
      "    \n",
      "\n",
      "arr\n",
      "26/10:\n",
      "arr = [1,2]\n",
      "arr1 = [2,1]\n",
      "arr = arr1\n",
      "arr\n",
      "27/1:\n",
      "arr = [1,0]\n",
      "arr1=[]\n",
      "\n",
      "for i in range(0,len(arr)):\n",
      "    arr.append(arr[arr[i]])\n",
      "    \n",
      "\n",
      "arr\n",
      "27/2:\n",
      "arr = [1,0]\n",
      "arr1=[]\n",
      "\n",
      "for i in range(0,len(arr)):\n",
      "    arr.append(arr[arr[i]])\n",
      "    \n",
      "\n",
      "arr[:] = arr[n:]\n",
      "arr\n",
      "27/3:\n",
      "arr = [1,0]\n",
      "arr1=[]\n",
      "\n",
      "for i in range(0,len(arr)):\n",
      "    arr.append(arr[arr[i]])\n",
      "    \n",
      "\n",
      "arr[:] = arr[2:]\n",
      "arr\n",
      "30/1:\n",
      "arr = [1,2,3,3]\n",
      "arr.counts()\n",
      "30/2:\n",
      "arr = [1,2,3,3]\n",
      "arr.count()\n",
      "30/3:\n",
      "arr = [1,2,3,3]\n",
      "count(arr)\n",
      "30/4:\n",
      "arr = [1,2,3,3]\n",
      "arr.count(3)\n",
      "30/5:\n",
      "arr = [\"1\",\"2\",\"3\",\"3\"]\n",
      "arr.type()\n",
      "30/6:\n",
      "arr = [\"1\",\"2\",\"3\",\"3\"]\n",
      "type(arr)\n",
      "30/7:\n",
      "arr = [\"1\",\"2\",\"3\",\"3\"]\n",
      "type(arr[i])\n",
      "30/8:\n",
      "arr = [\"1\",\"2\",\"3\",\"3\"]\n",
      "type(arr[0])\n",
      "30/9:\n",
      "arr = [\"1\",\"2\",\"3\",\"3\"]\n",
      "arr = int(arr)\n",
      "type(arr[0])\n",
      "30/10:\n",
      "arr = [\"1\",\"2\",\"3\",\"3\"]\n",
      "arr[:] = int(arr{:})\n",
      "type(arr[0])\n",
      "30/11:\n",
      "arr = [\"1\",\"2\",\"3\",\"3\"]\n",
      "arr[:] = int(arr[:])\n",
      "type(arr[0])\n",
      "30/12:\n",
      "arr = [\"1\",\"2\",\"3\",\"3\"]\n",
      "arr = int(arr[i] for i in range(len(arr)))\n",
      "type(arr[0])\n",
      "30/13:\n",
      "arr = [\"1\",\"2\",\"3\",\"3\"]\n",
      "arr = int(arr[i] for i in range(len(arr)))\n",
      "type(arr[0])\n",
      "30/14:\n",
      "arr = [\"1\",\"2\",\"3\",\"3\"]\n",
      "arr = int(arr[i] for i in range(len(4)))\n",
      "type(arr[0])\n",
      "30/15:\n",
      "arr = [\"1\",\"2\",\"3\",\"3\"]\n",
      "arr = int(i for i in range(len(4)))\n",
      "type(arr[0])\n",
      "30/16:\n",
      "arr = [\"1\",\"2\",\"3\",\"3\"]\n",
      "arr = int(i for i in arr)\n",
      "type(arr[0])\n",
      "30/17:\n",
      "arr = [\"1\",\"2\",\"3\",\"3\"]\n",
      "arr = (int(i) for i in arr)\n",
      "type(arr[0])\n",
      "30/18:\n",
      "arr = [\"1\",\"2\",\"3\",\"3\"]\n",
      "arr[] = (int(i) for i in arr)\n",
      "type(arr[0])\n",
      "30/19:\n",
      "arr = [0]*5\n",
      "arr\n",
      "30/20:\n",
      "arr = [\"1\",\"3\",\"2\"]\n",
      "sorted(arr)\n",
      "arr\n",
      "30/21:\n",
      "arr = [\"1\",\"3\",\"2\"]\n",
      "sorted(arr)\n",
      "arr\n",
      "30/22:\n",
      "arr = [\"1\",\"3\",\"2\"]\n",
      "sorted(arr)\n",
      "arr\n",
      "30/23:\n",
      "arr = [\"1\",\"3\",\"2\"]\n",
      "max(arr)\n",
      "30/24:\n",
      "i=1,j=0\n",
      "    \n",
      "while(j!=10):\n",
      "    if((i%2==0) or (i%3==0) or (i%5==0) or (i=1)):\n",
      "        j+=1\n",
      "        i+=1\n",
      "i\n",
      "31/1: import pandas as pd\n",
      "31/2: data = pd.read_csv(\"climate.csv\")\n",
      "31/3: data = pd.read_csv(\"climate.csv\")\n",
      "31/4: data = pd.read_csv(\"climate.csv\")\n",
      "31/5:\n",
      "data = pd.read_csv(\"climate.csv\")\n",
      "data.head()\n",
      "31/6:\n",
      "data = pd.read_csv(\"climate.csv\")\n",
      "data.head()\n",
      "31/7:\n",
      "data = pd.read_csv(\"climate.csv\")\n",
      "data.head()\n",
      "31/8:\n",
      "data = pd.read_csv(\"climate.csv\")\n",
      "data.head()\n",
      "31/9: import pandas as pd\n",
      "31/10:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "data.head()\n",
      "31/11:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "data.head()\n",
      "data.info()\n",
      "31/12:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "data.head()\n",
      "data.describe()\n",
      "31/13: data.hist(bins=50, figsize=(20,15))\n",
      "31/14: data.hist(bins=10, figsize=(20,15))\n",
      "31/15: data.hist(bins=20, figsize=(20,15))\n",
      "31/16: data.hist(bins=5, figsize=(20,15))\n",
      "31/17: data.hist(bins=15, figsize=(20,15))\n",
      "31/18: data.bar()\n",
      "31/19: data.plot()\n",
      "34/1: import pandas as pd\n",
      "34/2: housing = pd.read_csv(\"data.csv\")\n",
      "34/3: housing.head()\n",
      "34/4: housing.info()\n",
      "34/5: housing['CHAS'].value_counts()\n",
      "34/6: housing.describe()\n",
      "34/7: %matplotlib inline\n",
      "34/8:\n",
      "# # For plotting histogram\n",
      "# import matplotlib.pyplot as plt\n",
      "# housing.hist(bins=50, figsize=(20, 15))\n",
      "34/9:\n",
      "# For learning purpose\n",
      "import numpy as np\n",
      "def split_train_test(data, test_ratio):\n",
      "    np.random.seed(42)\n",
      "    shuffled = np.random.permutation(len(data))\n",
      "    print(shuffled)\n",
      "    test_set_size = int(len(data) * test_ratio)\n",
      "    test_indices = shuffled[:test_set_size]\n",
      "    train_indices = shuffled[test_set_size:] \n",
      "    return data.iloc[train_indices], data.iloc[test_indices]\n",
      "34/10: # train_set, test_set = split_train_test(housing, 0.2)\n",
      "34/11: # print(f\"Rows in train set: {len(train_set)}\\nRows in test set: {len(test_set)}\\n\")\n",
      "34/12:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train_set, test_set  = train_test_split(housing, test_size=0.2, random_state=42)\n",
      "print(f\"Rows in train set: {len(train_set)}\\nRows in test set: {len(test_set)}\\n\")\n",
      "34/13:\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
      "for train_index, test_index in split.split(housing, housing['CHAS']):\n",
      "    strat_train_set = housing.loc[train_index]\n",
      "    strat_test_set = housing.loc[test_index]\n",
      "34/14: strat_test_set['CHAS'].value_counts()\n",
      "34/15: strat_train_set['CHAS'].value_counts()\n",
      "34/16: # 95/7\n",
      "34/17: # 376/28\n",
      "34/18: housing = strat_train_set.copy()\n",
      "34/19:\n",
      "corr_matrix = housing.corr()\n",
      "corr_matrix['MEDV'].sort_values(ascending=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/20:\n",
      "# from pandas.plotting import scatter_matrix\n",
      "# attributes = [\"MEDV\", \"RM\", \"ZN\", \"LSTAT\"]\n",
      "# scatter_matrix(housing[attributes], figsize = (12,8))\n",
      "34/21: housing.plot(kind=\"scatter\", x=\"RM\", y=\"MEDV\", alpha=0.8)\n",
      "34/22: housing[\"TAXRM\"] = housing['TAX']/housing['RM']\n",
      "34/23: housing.head()\n",
      "34/24:\n",
      "corr_matrix = housing.corr()\n",
      "corr_matrix['MEDV'].sort_values(ascending=False)\n",
      "34/25: housing.plot(kind=\"scatter\", x=\"TAXRM\", y=\"MEDV\", alpha=0.8)\n",
      "34/26:\n",
      "housing = strat_train_set.drop(\"MEDV\", axis=1)\n",
      "housing_labels = strat_train_set[\"MEDV\"].copy()\n",
      "34/27:\n",
      "# To take care of missing attributes, you have three options:\n",
      "#     1. Get rid of the missing data points\n",
      "#     2. Get rid of the whole attribute\n",
      "#     3. Set the value to some value(0, mean or median)\n",
      "34/28:\n",
      "a = housing.dropna(subset=[\"RM\"]) #Option 1\n",
      "a.shape\n",
      "# Note that the original housing dataframe will remain unchanged\n",
      "34/29:\n",
      "housing.drop(\"RM\", axis=1).shape # Option 2\n",
      "# Note that there is no RM column and also note that the original housing dataframe will remain unchanged\n",
      "34/30: median = housing[\"RM\"].median() # Compute median for Option 3\n",
      "34/31:\n",
      "housing[\"RM\"].fillna(median) # Option 3\n",
      "# Note that the original housing dataframe will remain unchanged\n",
      "34/32: housing.shape\n",
      "34/33: housing.describe() # before we started filling missing attributes\n",
      "34/34:\n",
      "from sklearn.impute import SimpleImputer\n",
      "imputer = SimpleImputer(strategy=\"median\")\n",
      "imputer.fit(housing)\n",
      "34/35: imputer.statistics_\n",
      "34/36: X = imputer.transform(housing)\n",
      "34/37: housing_tr = pd.DataFrame(X, columns=housing.columns)\n",
      "34/38: housing_tr.describe()\n",
      "34/39:\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "my_pipeline = Pipeline([\n",
      "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
      "    #     ..... add as many as you want in your pipeline\n",
      "    ('std_scaler', StandardScaler()),\n",
      "])\n",
      "34/40: housing_num_tr = my_pipeline.fit_transform(housing)\n",
      "34/41: housing_num_tr.shape\n",
      "34/42:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "# model = LinearRegression()\n",
      "# model = DecisionTreeRegressor()\n",
      "model = RandomForestRegressor()\n",
      "model.fit(housing_num_tr, housing_labels)\n",
      "34/43: some_data = housing.iloc[:5]\n",
      "34/44: some_labels = housing_labels.iloc[:5]\n",
      "34/45: prepared_data = my_pipeline.transform(some_data)\n",
      "34/46: model.predict(prepared_data)\n",
      "34/47: list(some_labels)\n",
      "34/48:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "housing_predictions = model.predict(housing_num_tr)\n",
      "mse = mean_squared_error(housing_labels, housing_predictions)\n",
      "rmse = np.sqrt(mse)\n",
      "34/49: rmse\n",
      "34/50:\n",
      "# 1 2 3 4 5 6 7 8 9 10\n",
      "from sklearn.model_selection import cross_val_score\n",
      "scores = cross_val_score(model, housing_num_tr, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
      "rmse_scores = np.sqrt(-scores)\n",
      "34/51: rmse_scores\n",
      "34/52:\n",
      "def print_scores(scores):\n",
      "    print(\"Scores:\", scores)\n",
      "    print(\"Mean: \", scores.mean())\n",
      "    print(\"Standard deviation: \", scores.std())\n",
      "34/53: print_scores(rmse_scores)\n",
      "34/54:\n",
      "from joblib import dump, load\n",
      "dump(model, 'Dragon.joblib')\n",
      "34/55:\n",
      "X_test = strat_test_set.drop(\"MEDV\", axis=1)\n",
      "Y_test = strat_test_set[\"MEDV\"].copy()\n",
      "X_test_prepared = my_pipeline.transform(X_test)\n",
      "final_predictions = model.predict(X_test_prepared)\n",
      "final_mse = mean_squared_error(Y_test, final_predictions)\n",
      "final_rmse = np.sqrt(final_mse)\n",
      "# print(final_predictions, list(Y_test))\n",
      "34/56: final_rmse\n",
      "34/57: prepared_data[0]\n",
      "34/58:\n",
      "from joblib import dump, load\n",
      "import numpy as np\n",
      "model = load('Dragon.joblib') \n",
      "features = np.array([[-5.43942006, 4.12628155, -1.6165014, -0.67288841, -1.42262747,\n",
      "       -11.44443979304, -49.31238772,  7.61111401, -26.0016879 , -0.5778192 ,\n",
      "       -0.97491834,  0.41164221, -66.86091034]])\n",
      "model.predict(features)\n",
      "36/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "36/2: data = pd.read_csv(\"Bihar.csv\")\n",
      "36/3:\n",
      "data = pd.read_csv(\"Bihar.csv\")\n",
      "data.describe()\n",
      "36/4: data.hist(bins=10, figsize=(20,15))\n",
      "36/5: data.hist(bins=10, figsize=(1,15))\n",
      "36/6: data.hist(bins=10, figsize=(10,15))\n",
      "36/7: data.hist(bins=10, figsize=(100,15))\n",
      "36/8: data.hist(bins=10, figsize=(50,15))\n",
      "36/9: data.hist(bins=10, figsize=(20,15))\n",
      "36/10: data.hist(bins=5, figsize=(20,15))\n",
      "36/11: data.hist(xlabel=data[\"Year\"],bins=5, figsize=(20,15))\n",
      "36/12: data.hist(xlabel=data.[\"Year\"],bins=5, figsize=(20,15))\n",
      "36/13: data.hist(xlabelsize=data.[\"Year\"],bins=5, figsize=(20,15))\n",
      "36/14: data.hist(xlabelsize=data.\"Year\",bins=5, figsize=(20,15))\n",
      "36/15: data.hist(xlabelsize=data[\"YEAR\"],bins=5, figsize=(20,15))\n",
      "36/16: plt.plot(data)\n",
      "36/17: plt.plot(data.[\"YEAR\"])\n",
      "36/18: plt.plot(data.\"YEAR\")\n",
      "36/19: plt.plot(data[\"YEAR\"])\n",
      "36/20: plt.plot(data[\"YEAR\"],data[\"JAN\"])\n",
      "36/21: plt.plot(data[\"YEAR\"],data[\"JAN\",\"FEB\"])\n",
      "36/22: plt.plot(data[\"YEAR\"],data[\"JAN\"],data[\"FEB\"])\n",
      "36/23: plt.plot(data[\"YEAR\"],data[\"JAN\"])\n",
      "36/24: data.labels()\n",
      "36/25: data.label()\n",
      "36/26: data.attributes()\n",
      "36/27:\n",
      "\n",
      "plt.plot(data[\"YEAR\"],data[[\"JAN\"],[\"FEB\"]])\n",
      "36/28:\n",
      "\n",
      "plt.plot(data[\"YEAR\"],data[[\"JAN\",\"FEB\"]])\n",
      "36/29:\n",
      "\n",
      "plt.plot(data[\"YEAR\"],data[[\"JAN\",\"FEB\",\"MAR\"]])\n",
      "36/30: data.info()\n",
      "36/31:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "36/32: train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)\n",
      "36/33:\n",
      "train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)\n",
      "train_set\n",
      "36/34:\n",
      "train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)\n",
      "len(train_set)\n",
      "36/35:\n",
      "train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)\n",
      "print(len(train_set), len(test_set))\n",
      "36/36:\n",
      "train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)\n",
      "print(len(train_set), len(test_set))\n",
      "36/37:\n",
      "corelation = data.corr()\n",
      "corelation[\"YEAR\"].sort_values(ascending=False)\n",
      "36/38:\n",
      "corelation = data.corr()\n",
      "corelation[\"YEAR\"].sort_values(ascending=False)\n",
      "36/39:\n",
      "attributes = [\"YEAR\", \"ANNUAL\", \"Jan-Feb\", \"Mar-May\", \"Jun-Sep\", \"Oct-Dec\"]\n",
      "scatter_matrix(data[attributes], figsize = (12,8))\n",
      "38/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "38/2:\n",
      "data = pd.read_csv(\"Bihar.csv\")\n",
      "data.describe()\n",
      "38/3:\n",
      "\n",
      "plt.plot(data[\"YEAR\"],data[[\"JAN\",\"FEB\",\"MAR\"]])\n",
      "38/4: data.info()\n",
      "38/5:\n",
      "train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)\n",
      "print(len(train_set), len(test_set))\n",
      "38/6:\n",
      "corelation = data.corr()\n",
      "corelation[\"YEAR\"].sort_values(ascending=False)\n",
      "38/7:\n",
      "attributes = [\"YEAR\", \"ANNUAL\", \"Jan-Feb\", \"Mar-May\", \"Jun-Sep\", \"Oct-Dec\"]\n",
      "scatter_matrix(data[attributes], figsize = (12,8))\n",
      "38/8:\n",
      "#attributes = [\"YEAR\", \"ANNUAL\", \"Jan-Feb\", \"Mar-May\", \"Jun-Sep\", \"Oct-Dec\"]\n",
      "#scatter_matrix(data[attributes], figsize = (12,8))\n",
      "38/9: data.plot(kind=\"scatter\", x=\"YEAR\", y=\"ANNUAL\", alpha=0.8)\n",
      "38/10: data.plot(kind=\"scatter\", x=\"YEAR\", y=\"June-Sep\", alpha=0.8)\n",
      "38/11: data.plot(kind=\"scatter\", x=\"YEAR\", y=\"Jun-Sep\", alpha=0.8)\n",
      "38/12: data.plot(kind=\"scatter\", x=\"YEAR\", y=\"Jun-Sep\", alpha=0.1)\n",
      "38/13: data.plot(kind=\"scatter\", x=\"YEAR\", y=\"Jun-Sep\", alpha=0.8)\n",
      "38/14:\n",
      "attributes = [\"YEAR\", \"ANNUAL\", \"Jan-Feb\", \"Mar-May\", \"Jun-Sep\", \"Oct-Dec\"]\n",
      "scatter_matrix(data[attributes], figsize = (12,8))\n",
      "39/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "# model = LinearRegression()\n",
      "# model = DecisionTreeRegressor()\n",
      "model = RandomForestRegressor()\n",
      "model.fit(housing_num_tr, housing_labels)\n",
      "40/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "# model = LinearRegression()\n",
      "# model = DecisionTreeRegressor()\n",
      "40/2:\n",
      "data = pd.read_csv(\"Bihar.csv\")\n",
      "data.describe()\n",
      "40/3:\n",
      "\n",
      "plt.plot(data[\"YEAR\"],data[[\"JAN\",\"FEB\",\"MAR\"]])\n",
      "40/4: data.info()\n",
      "40/5:\n",
      "train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)\n",
      "print(len(train_set), len(test_set))\n",
      "40/6:\n",
      "corelation = data.corr()\n",
      "corelation[\"YEAR\"].sort_values(ascending=False)\n",
      "40/7:\n",
      "#attributes = [\"YEAR\", \"ANNUAL\", \"Jan-Feb\", \"Mar-May\", \"Jun-Sep\", \"Oct-Dec\"]\n",
      "#scatter_matrix(data[attributes], figsize = (12,8))\n",
      "40/8: data.plot(kind=\"scatter\", x=\"YEAR\", y=\"Jun-Sep\", alpha=0.8)\n",
      "40/9:\n",
      "data = pd.read_csv(\"Bihar.csv\")\n",
      "data.describe()\n",
      "data_labels\n",
      "40/10:\n",
      "data = pd.read_csv(\"Bihar.csv\")\n",
      "data.describe()\n",
      "data_labels()\n",
      "41/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "# model = LinearRegression()\n",
      "# model = DecisionTreeRegressor()\n",
      "41/2:\n",
      "data = pd.read_csv(\"Bihar.csv\")\n",
      "data.describe()\n",
      "41/3:\n",
      "\n",
      "plt.plot(data[\"YEAR\"],data[[\"JAN\",\"FEB\",\"MAR\"]])\n",
      "41/4: data.info()\n",
      "41/5:\n",
      "train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)\n",
      "print(len(train_set), len(test_set))\n",
      "41/6:\n",
      "corelation = data.corr()\n",
      "corelation[\"YEAR\"].sort_values(ascending=False)\n",
      "41/7:\n",
      "#attributes = [\"YEAR\", \"ANNUAL\", \"Jan-Feb\", \"Mar-May\", \"Jun-Sep\", \"Oct-Dec\"]\n",
      "#scatter_matrix(data[attributes], figsize = (12,8))\n",
      "41/8: data.plot(kind=\"scatter\", x=\"YEAR\", y=\"Jun-Sep\", alpha=0.8)\n",
      "41/9:\n",
      "\n",
      "plt.plot(data[\"YEAR\"],data[[\"JAN\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]])\n",
      "41/10:\n",
      "\n",
      "plt.plot(data[\"YEAR\"],data[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]])\n",
      "41/11:\n",
      "corelation = data.corr()\n",
      "corelation[\"ANNUAL\"].sort_values(ascending=False)\n",
      "41/12:\n",
      "data = pd.read_csv(\"Bihar.csv\")\n",
      "data.head(10)\n",
      "42/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "42/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "data.head()\n",
      "data.describe()\n",
      "42/3: data.hist(bins=15, figsize=(20,15))\n",
      "42/4: data.plot(data.[\"YEAR\"])\n",
      "42/5:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "data.head()\n",
      "data.info()\n",
      "43/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "43/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "43/3:\n",
      "data = data.fillna(np.median(data))\n",
      "data.describe()\n",
      "43/4:\n",
      "data = data.fillna(np.mean(data))\n",
      "data.describe()\n",
      "43/5:\n",
      "print(data.isnull().sum())\n",
      "data = data.fillna(np.mean(data))\n",
      "data.describe()\n",
      "44/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "44/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "44/3:\n",
      "print(data.isnull().sum())\n",
      "data = data.fillna(np.mean(data))\n",
      "data.describe()\n",
      "44/4: #data.hist(bins=15, figsize=(20,15))\n",
      "44/5: data.plot(data.[\"YEAR\"])\n",
      "45/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "45/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "45/3:\n",
      "print(data.isnull().sum())\n",
      "data = data.fillna(np.mean(data))\n",
      "data.describe()\n",
      "45/4: #data.hist(bins=15, figsize=(20,15))\n",
      "45/5: data.plot(data.[\"YEAR\"])\n",
      "45/6:\n",
      "print(data.isnull().sum())\n",
      "data = data.fillna(np.median(data))\n",
      "data.describe()\n",
      "46/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "46/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "46/3:\n",
      "print(data.isnull().sum())\n",
      "data = data.fillna(np.median(data))\n",
      "data.describe()\n",
      "47/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "47/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "47/3:\n",
      "print(data.isnull().sum())\n",
      "median = data[\"JAN\"].median()\n",
      "data[\"JAN\"].fillna(median)\n",
      "print(data.isnull().sum)\n",
      "47/4: #data.hist(bins=15, figsize=(20,15))\n",
      "47/5: data.plot(data.[\"YEAR\"])\n",
      "48/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "48/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "48/3:\n",
      "print(data.isnull().sum())\n",
      "median = data[\"JAN\"].median()\n",
      "data[\"JAN\"].fillna(median)\n",
      "print(data.isnull().sum())\n",
      "48/4: #data.hist(bins=15, figsize=(20,15))\n",
      "48/5: data.plot(data.[\"YEAR\"])\n",
      "49/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "49/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "49/3:\n",
      "print(data.isnull().sum())\n",
      "median = data[\"JAN\"].median()\n",
      "print(median)\n",
      "data[\"JAN\"].fillna(median)\n",
      "print(data.isnull().sum())\n",
      "49/4: #data.hist(bins=15, figsize=(20,15))\n",
      "49/5: data.plot(data.[\"YEAR\"])\n",
      "50/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "50/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "50/3:\n",
      "print(data.isnull().sum())\n",
      "median = data[\"JAN\"].median()\n",
      "print(median)\n",
      "data = data[\"JAN\"].fillna(median)\n",
      "print(data.isnull().sum())\n",
      "50/4: #data.hist(bins=15, figsize=(20,15))\n",
      "50/5: data.plot(data.[\"YEAR\"])\n",
      "51/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.impute import SimpleImputer\n",
      "51/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "51/3:\n",
      "print(data.isnull().sum())\n",
      "impuret = SimpleImputer(strategy = \"median\")\n",
      "imputer.fit(data)\n",
      "\n",
      "print(data.isnull().sum())\n",
      "52/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.impute import SimpleImputer\n",
      "52/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "52/3:\n",
      "print(data.isnull().sum())\n",
      "imputer = SimpleImputer(strategy = \"median\")\n",
      "imputer.fit(data)\n",
      "\n",
      "print(data.isnull().sum())\n",
      "52/4:\n",
      "print(data.isnull().sum())\n",
      "imputer = SimpleImputer(strategy = \"median\")data[1:]\n",
      "imputer.fit(data)\n",
      "\n",
      "print(data.isnull().sum())\n",
      "52/5:\n",
      "print(data.isnull().sum())\n",
      "imputer = SimpleImputer(strategy = \"median\").data[1:]\n",
      "imputer.fit(data)\n",
      "\n",
      "print(data.isnull().sum())\n",
      "52/6:\n",
      "print(data.isnull().sum())\n",
      "imputer = SimpleImputer(strategy = \"median\")data[1:]\n",
      "imputer.fit(data)\n",
      "\n",
      "print(data.isnull().sum())\n",
      "52/7:\n",
      "print(data.isnull().sum())\n",
      "imputer = SimpleImputer(strategy = \"median\")\n",
      "imputer.fit(data)\n",
      "\n",
      "print(data.isnull().sum())\n",
      "52/8:\n",
      "print(data.isnull().sum())\n",
      "imputer = SimpleImputer(strategy = \"mean\")\n",
      "imputer.fit(data)\n",
      "\n",
      "print(data.isnull().sum())\n",
      "52/9:\n",
      "print(data.isnull().sum())\n",
      "imputer = SimpleImputer(strategy = \"mean\",axis=1)\n",
      "imputer.fit(data)\n",
      "\n",
      "print(data.isnull().sum())\n",
      "52/10:\n",
      "print(data.isnull().sum())\n",
      "imputer = SimpleImputer(strategy = \"mean\",axis=1)\n",
      "imputer.fit(data[\"JAN\"])\n",
      "\n",
      "print(data.isnull().sum())\n",
      "52/11:\n",
      "print(data.isnull().sum())\n",
      "imputer = SimpleImputer(strategy = \"mean\")\n",
      "imputer.fit(data[\"JAN\"])\n",
      "\n",
      "print(data.isnull().sum())\n",
      "52/12:\n",
      "print(data.isnull().sum())\n",
      "imputer = SimpleImputer(strategy = \"mean\")\n",
      "imputer.fit(data[[\"JAN\"]])\n",
      "\n",
      "print(data.isnull().sum())\n",
      "52/13:\n",
      "print(data.isnull().sum())\n",
      "imputer = SimpleImputer(strategy = \"mean\")\n",
      "imputer.fit(data[[\"JAN\"]])\n",
      "\n",
      "print(data.isnull().sum())\n",
      "52/14:\n",
      "print(data.isnull().sum())\n",
      "imputer = SimpleImputer(strategy = \"mean\")\n",
      "imputer.fit(data)\n",
      "\n",
      "print(data.isnull().sum())\n",
      "52/15:\n",
      "print(data.isnull().sum())\n",
      "data = data.fillna.mean(data)\n",
      "print(data.isnull().sum())\n",
      "52/16:\n",
      "print(data.isnull().sum())\n",
      "data = data.fillna(np.mean(data))\n",
      "print(data.isnull().sum())\n",
      "52/17: data = data.fillna(np.mean(data))\n",
      "52/18: #data.plot(data.[\"YEAR\"])\n",
      "52/19: y = data[\"YEAR\"]\n",
      "52/20:\n",
      "y = data[\"YEAR\"]\n",
      "x = data[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "tain_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)\n",
      "print(train_x,train_y,test_x,test_y)\n",
      "52/21:\n",
      "y = data[\"YEAR\"]\n",
      "x = data[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)\n",
      "print(train_x,train_y,test_x,test_y)\n",
      "52/22:\n",
      "y = data[\"YEAR\"]\n",
      "x = data[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)\n",
      "print(len(train_x),len(train_y),len(test_x),len(test_y))\n",
      "52/23:\n",
      "model = LinearRegression()\n",
      "model.fit(train_x,train_y)\n",
      "52/24:\n",
      "some_data = data.iloc[:5]\n",
      "some_data\n",
      "52/25:\n",
      "some_data = data.iloc[:5]\n",
      "some_labels = train_y.iloc[:5]\n",
      "some_labels\n",
      "52/26:\n",
      "y = data[\"ANNUAL\"]\n",
      "x = data[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)\n",
      "#print(len(train_x),len(train_y),len(test_x),len(test_y))\n",
      "52/27:\n",
      "model = LinearRegression()\n",
      "model.fit(train_x,train_y)\n",
      "52/28:\n",
      "some_data = data.iloc[:5]\n",
      "some_labels = train_y.iloc[:5]\n",
      "some_labels\n",
      "52/29: predicted_data = model.predict(some_data)\n",
      "52/30:\n",
      "some_data = train_x.iloc[:5]\n",
      "some_labels = train_y.iloc[:5]\n",
      "52/31: predicted_data = model.predict(some_data)\n",
      "52/32:\n",
      "predicted_data = model.predict(some_data)\n",
      "predicted_data\n",
      "52/33: list(some_labels)\n",
      "52/34:\n",
      "predicted_anual = model.predict(train_x)\n",
      "mse = mean_squared_error(train_y, predicted_anual)\n",
      "rmse = np.sqrt(mse)\n",
      "print(mse,rmse)\n",
      "53/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.impute import SimpleImputer\n",
      "53/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "53/3: data = data.fillna(np.mean(data))\n",
      "53/4: #data.hist(bins=15, figsize=(20,15))\n",
      "53/5: #data.plot(data.[\"YEAR\"])\n",
      "53/6:\n",
      "y = data[\"ANNUAL\"]\n",
      "x = data[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)\n",
      "#print(len(train_x),len(train_y),len(test_x),len(test_y))\n",
      "53/7:\n",
      "model = LinearRegression()\n",
      "model.fit(train_x,train_y)\n",
      "53/8:\n",
      "some_data = train_x.iloc[:5]\n",
      "some_labels = train_y.iloc[:5]\n",
      "53/9:\n",
      "predicted_data = model.predict(some_data)\n",
      "predicted_data\n",
      "53/10: list(some_labels)\n",
      "53/11:\n",
      "predicted_anual = model.predict(train_x)\n",
      "mse = mean_squared_error(train_y, predicted_anual)\n",
      "rmse = np.sqrt(mse)\n",
      "print(mse,rmse)\n",
      "54/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.impute import SimpleImputer\n",
      "54/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "54/3: data = data.fillna(np.mean(data))\n",
      "54/4: #data.hist(bins=15, figsize=(20,15))\n",
      "54/5: #data.plot(data.[\"YEAR\"])\n",
      "54/6:\n",
      "y = data[\"ANNUAL\"]\n",
      "x = data[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)\n",
      "#print(len(train_x),len(train_y),len(test_x),len(test_y))\n",
      "54/7:\n",
      "model = LinearRegression()\n",
      "model.fit(train_x,train_y)\n",
      "54/8:\n",
      "predicted_anual = model.predict(train_x)\n",
      "mse = mean_squared_error(train_y, predicted_anual)\n",
      "rmse = np.sqrt(mse)\n",
      "print(mse,rmse)\n",
      "54/9:\n",
      "predicted_anual = model.predict(test_x)\n",
      "mse = mean_squared_error(test_y, predicted_anual)\n",
      "rmse = np.sqrt(mse)\n",
      "print(mse,rmse)\n",
      "mae = mean_absolute_error(test_y,predict_anual)\n",
      "print(mae)\n",
      "54/10:\n",
      "predicted_anual = model.predict(test_x)\n",
      "mse = mean_squared_error(test_y, predicted_anual)\n",
      "rmse = np.sqrt(mse)\n",
      "print(mse,rmse)\n",
      "mae = mean_absolute_error(test_y,predicted_anual)\n",
      "print(mae)\n",
      "54/11:\n",
      "predicted_anual = model.predict(test_x)\n",
      "mse = mean_squared_error(test_y, predicted_anual)\n",
      "rmse = np.sqrt(mse)\n",
      "print(mse,rmse)\n",
      "mae = mean_absolute_error(test_y,predicted_anual)\n",
      "print(mae)\n",
      "r2 = r2_score(test_y,predicted_anual)\n",
      "print(r2)\n",
      "55/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.impute import SimpleImputer\n",
      "55/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "55/3: data = data.fillna(np.mean(data))\n",
      "55/4: #data.hist(bins=15, figsize=(20,15))\n",
      "55/5: #data.plot(data.[\"YEAR\"])\n",
      "55/6:\n",
      "y = data[\"ANNUAL\"]\n",
      "x = data[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)\n",
      "#print(len(train_x),len(train_y),len(test_x),len(test_y))\n",
      "55/7:\n",
      "model = LinearRegression()\n",
      "model.fit(train_x,train_y)\n",
      "55/8:\n",
      "predicted_anual = model.predict(test_x)\n",
      "mse = mean_squared_error(test_y, predicted_anual)\n",
      "rmse = np.sqrt(mse)\n",
      "print(mse,rmse)\n",
      "mae = mean_absolute_error(test_y,predicted_anual)\n",
      "print(mae)\n",
      "r2 = r2_score(test_y,predicted_anual)\n",
      "print(r2)\n",
      "56/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.impute import SimpleImputer\n",
      "56/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "56/3: data = data.fillna(np.mean(data))\n",
      "56/4: #data.hist(bins=15, figsize=(20,15))\n",
      "56/5: #data.plot(data.[\"YEAR\"])\n",
      "56/6:\n",
      "y = data[\"ANNUAL\"]\n",
      "x = data[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)\n",
      "#print(len(train_x),len(train_y),len(test_x),len(test_y))\n",
      "56/7:\n",
      "model = DecisionTreeRegressor()\n",
      "model.fit(train_x,train_y)\n",
      "56/8:\n",
      "predicted_anual = model.predict(test_x)\n",
      "mse = mean_squared_error(test_y, predicted_anual)\n",
      "rmse = np.sqrt(mse)\n",
      "print(mse,rmse)\n",
      "mae = mean_absolute_error(test_y,predicted_anual)\n",
      "print(mae)\n",
      "r2 = r2_score(test_y,predicted_anual)\n",
      "print(r2)\n",
      "56/9:\n",
      "# 1843.6557087291785 42.9378121092491\n",
      "# 10.520464918764377\n",
      "# 0.9978930199054238\n",
      "57/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.impute import SimpleImputer\n",
      "57/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "57/3: data = data.fillna(np.mean(data))\n",
      "57/4: #data.hist(bins=15, figsize=(20,15))\n",
      "57/5: #data.plot(data.[\"YEAR\"])\n",
      "57/6:\n",
      "y = data[\"ANNUAL\"]\n",
      "x = data[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)\n",
      "#print(len(train_x),len(train_y),len(test_x),len(test_y))\n",
      "57/7:\n",
      "model = RandomForestRegressor()\n",
      "model.fit(train_x,train_y)\n",
      "57/8:\n",
      "predicted_anual = model.predict(test_x)\n",
      "mse = mean_squared_error(test_y, predicted_anual)\n",
      "rmse = np.sqrt(mse)\n",
      "print(mse,rmse)\n",
      "mae = mean_absolute_error(test_y,predicted_anual)\n",
      "print(mae)\n",
      "r2 = r2_score(test_y,predicted_anual)\n",
      "print(r2)\n",
      "57/9:\n",
      "# 1843.6557087291785 42.9378121092491\n",
      "# 10.520464918764377\n",
      "# 0.9978930199054238\n",
      "\n",
      "# 8353.710373708016 91.39863441927355\n",
      "# 56.11648325302063\n",
      "# 0.9904531516432695\n",
      "57/10: plt.scatter(predicted_anual,test_y)\n",
      "57/11: plt.plot(predicted_anual,test_y)\n",
      "58/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.impute import SimpleImputer\n",
      "58/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "58/3: data = data.fillna(np.mean(data))\n",
      "58/4: #data.hist(bins=15, figsize=(20,15))\n",
      "58/5: #data.plot(data.[\"YEAR\"])\n",
      "58/6:\n",
      "y = data[\"ANNUAL\"]\n",
      "x = data[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)\n",
      "#print(len(train_x),len(train_y),len(test_x),len(test_y))\n",
      "58/7:\n",
      "model = LinearRegression()\n",
      "model.fit(train_x,train_y)\n",
      "58/8:\n",
      "predicted_anual = model.predict(test_x)\n",
      "mse = mean_squared_error(test_y, predicted_anual)\n",
      "rmse = np.sqrt(mse)\n",
      "print(mse,rmse)\n",
      "mae = mean_absolute_error(test_y,predicted_anual)\n",
      "print(mae)\n",
      "r2 = r2_score(test_y,predicted_anual)\n",
      "print(r2)\n",
      "58/9:\n",
      "# 1843.6557087291785 42.9378121092491\n",
      "# 10.520464918764377\n",
      "# 0.9978930199054238\n",
      "\n",
      "# 8353.710373708016 91.39863441927355\n",
      "# 56.11648325302063\n",
      "# 0.9904531516432695\n",
      "\n",
      "# 4668.890150852565 68.32927740619364\n",
      "# 34.12003601668758\n",
      "# 0.9946642648271947\n",
      "58/10: plt.scatter(predicted_anual,test_y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/11: plt.plot(predicted_anual,test_y)\n",
      "59/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.impute import SimpleImputer\n",
      "59/2:\n",
      "data = pd.read_csv(\"rainfall in india 1901-2015.csv\")\n",
      "print(\"\\nDiscription\\n\")\n",
      "print(data.describe())\n",
      "data.info()\n",
      "59/3: data = data.fillna(np.mean(data))\n",
      "59/4: #data.hist(bins=15, figsize=(20,15))\n",
      "59/5: #data.plot(data.[\"YEAR\"])\n",
      "59/6:\n",
      "y = data[\"ANNUAL\"]\n",
      "x = data[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)\n",
      "#print(len(train_x),len(train_y),len(test_x),len(test_y))\n",
      "59/7:\n",
      "model = DecisionTreeRegressor()\n",
      "model.fit(train_x,train_y)\n",
      "59/8:\n",
      "predicted_anual = model.predict(test_x)\n",
      "mse = mean_squared_error(test_y, predicted_anual)\n",
      "rmse = np.sqrt(mse)\n",
      "print(mse,rmse)\n",
      "mae = mean_absolute_error(test_y,predicted_anual)\n",
      "print(mae)\n",
      "r2 = r2_score(test_y,predicted_anual)\n",
      "print(r2)\n",
      "59/9:\n",
      "# 1843.6557087291785 42.9378121092491\n",
      "# 10.520464918764377\n",
      "# 0.9978930199054238\n",
      "\n",
      "# 8353.710373708016 91.39863441927355\n",
      "# 56.11648325302063\n",
      "# 0.9904531516432695\n",
      "\n",
      "# 4668.890150852565 68.32927740619364\n",
      "# 34.12003601668758\n",
      "# 0.9946642648271947\n",
      "59/10: plt.scatter(predicted_anual,test_y)\n",
      "59/11: plt.plot(predicted_anual,test_y)\n",
      "41/13:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "60/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "60/2:\n",
      "data = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data.head()\n",
      "60/3:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "60/4:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "60/5:\n",
      "x = data[\"IQ1\"]\n",
      "x\n",
      "60/6:\n",
      "x = data[\"IQ1\"].values()\n",
      "x\n",
      "60/7:\n",
      "x = data[\"iQ1\"].values()\n",
      "x\n",
      "60/8:\n",
      "x = data[\"IQ1\"].values()\n",
      "x\n",
      "60/9:\n",
      "x = data.[\"IQ1\"]\n",
      "x\n",
      "60/10:\n",
      "x = data[\"IQ1\"]\n",
      "x\n",
      "60/11:\n",
      "x = data[\"IQ1\"].value\n",
      "x\n",
      "60/12:\n",
      "x = data[\"IQ1\"].values\n",
      "x\n",
      "60/13:\n",
      "x = data[\"IQ1\"].values()\n",
      "x\n",
      "60/14:\n",
      "x = data[\"IQ1\"].value()\n",
      "x\n",
      "61/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "61/2:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "61/3:\n",
      "x = data[\"IQ1\"].value()\n",
      "x\n",
      "62/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "62/2:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "62/3:\n",
      "x = data[\"IQ1\"].value()\n",
      "x\n",
      "62/4:\n",
      "x = data[\"Q1\"].value()\n",
      "x\n",
      "62/5:\n",
      "x = data[\"Q1\"]\n",
      "x\n",
      "62/6:\n",
      "x = data[\"Q1\"]\n",
      "x\n",
      "62/7:\n",
      "x = data[\"Q1\"]\n",
      "x\n",
      "62/8:\n",
      "x = data[\"Q1\"]\n",
      "x\n",
      "62/9:\n",
      "x = data[\"Q1\"]\n",
      "x\n",
      "62/10:\n",
      "x = data[\"Q1\"]\n",
      "x\n",
      "62/11:\n",
      "x = data[\"Q1\"]\n",
      "x\n",
      "62/12:\n",
      "x = data[\"Q1\"]\n",
      "x\n",
      "62/13:\n",
      "x = data[\"Q1\"]\n",
      "x\n",
      "62/14: x = data[\"STUDENT ID\"]\n",
      "62/15:\n",
      "x = data[\"STUDENT ID\"]\n",
      "x\n",
      "62/16: x = data[\"Q1\"]\n",
      "62/17: x = data[\"q1\"]\n",
      "62/18: x = data[\"Q 1\"]\n",
      "62/19: x = data[\"Q 2\"]\n",
      "62/20: x = data[\"Q3\"]\n",
      "62/21: x = data[\"IQ1 \"]\n",
      "62/22: x = data[\"IQ 1\"]\n",
      "62/23: x = data[\"IQ 1\"]\n",
      "63/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "63/2:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "63/3: x = data[\"IQ 1\"]\n",
      "63/4: x = data[\"Q 1\"]\n",
      "63/5: x = data[\"Q 1\"]\n",
      "64/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "64/2:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "64/3: x = data[\"Q 1\"]\n",
      "64/4: x = data[\"Q 2\"]\n",
      "64/5:\n",
      "x = data[\"Q 2\"]\n",
      "x\n",
      "64/6:\n",
      "x = data[\"Q 2\"]\n",
      "y = data[\"Q 3\"]\n",
      "64/7:\n",
      "x = data[\"Q 2\"]\n",
      "y = data[\"Q 4\"]\n",
      "65/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "65/2:\n",
      "data = pd.read_csv(\"practice1.csv\")\n",
      "data.head()\n",
      "66/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "66/2:\n",
      "data = pd.read_csv(\"practice1.csv\")\n",
      "data.head()\n",
      "66/3:\n",
      "x = data[\"Q 2\"]\n",
      "y = data[\"Q 4\"]\n",
      "66/4:\n",
      "x = data[\"Q 2\"]\n",
      "y = data[\"Q 5\"]\n",
      "67/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "67/2:\n",
      "data = pd.read_csv(\"practice1.csv\")\n",
      "data.head()\n",
      "67/3:\n",
      "x = data[\"iq1\"]\n",
      "y = data[\"iq2\"]\n",
      "67/4:\n",
      "x = data.iloc[\"iq1\"]\n",
      "y = data[\"iq2\"]\n",
      "67/5:\n",
      "x = data.iloc[1]\n",
      "y = data[\"iq2\"]\n",
      "67/6:\n",
      "x = data.iloc[1]\n",
      "x\n",
      "67/7:\n",
      "x = data.iloc[:1]\n",
      "x\n",
      "68/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "68/2:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "68/3:\n",
      "x = data[\"iq1\"]\n",
      "y = data[\"iq2\"]\n",
      "68/4:\n",
      "x = data.iloc[:,[1]]\n",
      "x\n",
      "68/5:\n",
      "x = data.iloc[:,[1]]\n",
      "y = data.iloc[:,[2]]\n",
      "print(x,y)\n",
      "68/6:\n",
      "x = data.iloc[:,[1]]\n",
      "y = data.iloc[:,[2]]\n",
      "print(x)\n",
      "y\n",
      "68/7:\n",
      "x = data.iloc[:,[1]]\n",
      "y = data.iloc[:,[2]]\n",
      "68/8:\n",
      "x = data.iloc[:,[1]]\n",
      "y = data.iloc[:,[2]]\n",
      "68/9:\n",
      "x = data.iloc[:,[1]]\n",
      "y = data.iloc[:,[2]]\n",
      "68/10:\n",
      "x = data.iloc[:,[1]]\n",
      "y = data.iloc[:,[2]]\n",
      "68/11: train_x,test_x,train_y,test_y =\n",
      "68/12: plt.plot(x,y)\n",
      "68/13: plt.scatter(x,y)\n",
      "68/14: plt.scatter(iq1,iq2)\n",
      "68/15:\n",
      "iq1 = data.iloc[:,[1]]\n",
      "iq2 = data.iloc[:,[2]]\n",
      "68/16: plt.scatter(iq1,iq2)\n",
      "68/17: plt.scatter(iq1,iq2,linewidth=200)\n",
      "68/18: plt.scatter(iq1,iq2,linewidth=20)\n",
      "68/19: plt.scatter(iq1,iq2,linewidth=2)\n",
      "68/20: plt.scatter(iq1,iq2,figsize(200,200))\n",
      "68/21: plt.scatter(iq1,iq2)\n",
      "68/22:\n",
      "model = LinearRegression()\n",
      "model.fit(iq1,iq2)\n",
      "68/23:\n",
      "train_x,test_x,train_y,test_y = (iq1,iq2,test_size=0.2,random_state=42)\n",
      "\n",
      "model = LinearRegression()\n",
      "model.fit(train_x,train_y)\n",
      "68/24:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "68/25:\n",
      "train_x,test_x,train_y,test_y = train_test_split(iq1,iq2,test_size=0.2,random_state=42)\n",
      "\n",
      "model = LinearRegression()\n",
      "model.fit(train_x,train_y)\n",
      "68/26: predict_y = model.predict(test_x)\n",
      "68/27:\n",
      "predict_y = model.predict(test_x)\n",
      "predict_y\n",
      "68/28:\n",
      "some_x = test_x.iloc[:10]\n",
      "some_y = test_yiloc[:10]\n",
      "\n",
      "predict_y = model.predict(some_x)\n",
      "predict_y\n",
      "68/29:\n",
      "some_x = test_x.iloc[:10]\n",
      "some_y = test_y.iloc[:10]\n",
      "\n",
      "predict_y = model.predict(some_x)\n",
      "predict_y\n",
      "68/30:\n",
      "some_x = test_x.iloc[:10]\n",
      "some_y = test_y.iloc[:10]\n",
      "\n",
      "predict_y = model.predict(some_x)\n",
      "predict_y\n",
      "68/31: some_y\n",
      "68/32:\n",
      "plt.scatter(iq1,iq2,color=\"m\")\n",
      "plt.plot(iq1,iq2,color=\"b\")\n",
      "68/33:\n",
      "plt.scatter(iq1,iq2,color=\"m\")\n",
      "plt.plot(iq1,predict_y,color=\"g\")\n",
      "68/34:\n",
      "plt.scatter(iq1,iq2,color=\"m\")\n",
      "plt.plot(y_test,predict_y,color=\"g\")\n",
      "68/35:\n",
      "plt.scatter(iq1,iq2,color=\"m\")\n",
      "plt.plot(x_test,predict_y,color=\"g\")\n",
      "68/36:\n",
      "plt.scatter(iq1,iq2,color=\"m\")\n",
      "plt.scatter(x_test,predict_y,color=\"g\")\n",
      "68/37:\n",
      "plt.scatter(iq1,iq2,color=\"m\")\n",
      "plt.scatter(test_x,predict_y,color=\"g\")\n",
      "68/38:\n",
      "#plt.scatter(test_x,iq2,color=\"m\")\n",
      "plt.scatter(test_y,predict_y,color=\"g\")\n",
      "68/39:\n",
      "#plt.scatter(test_x,iq2,color=\"m\")\n",
      "plt.scatter(test_y,predict_y,color=\"g\")\n",
      "68/40:\n",
      "#plt.scatter(test_x,iq2,color=\"m\")\n",
      "plt.scatter(test_x,predict_y,color=\"g\")\n",
      "68/41:\n",
      "model = LinearRegression()\n",
      "x = x.fit(x,y)\n",
      "y_predict = model.predict(x)\n",
      "69/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "69/2:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "69/3:\n",
      "x = data.iloc[:,[1]]\n",
      "y = data.iloc[:,[2]]\n",
      "69/4:\n",
      "model = LinearRegression()\n",
      "x = x.fit(x,y)\n",
      "y_predict = model.predict(x)\n",
      "69/5:\n",
      "model = LinearRegression()\n",
      "x = model.fit(x,y)\n",
      "y_predict = model.predict(x)\n",
      "69/6:\n",
      "x = data.iloc[:,[1]].reshape(-1,1)\n",
      "y = data.iloc[:,[2]].reshape(-1,1)\n",
      "69/7:\n",
      "x = data.iloc[:,[1]]\n",
      "x = x.reshape(-1,1)\n",
      "y = data.iloc[:,[2]]\n",
      "y = y.reshape(-1,1)\n",
      "69/8:\n",
      "x = data.iloc[:,[1]]\n",
      "\n",
      "y = data.iloc[:,[2]]\n",
      "x\n",
      "69/9:\n",
      "x = data.iloc[:,[1]]\n",
      "\n",
      "y = data.iloc[:,[2]]\n",
      "x = np.reshape(x)\n",
      "69/10:\n",
      "x = data.iloc[:,[1]]\n",
      "\n",
      "y = data.iloc[:,[2]]\n",
      "x = x.reshape(1,-1)\n",
      "69/11:\n",
      "x = data.iloc[:,[1]]\n",
      "\n",
      "y = data.iloc[:,[2]]\n",
      "x = x.reshape(-1,-1)\n",
      "70/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "70/2:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "70/3:\n",
      "x = data.iloc[:,[1]]\n",
      "\n",
      "y = data.iloc[:,[2]]\n",
      "x = x.reshape(-1,-1)\n",
      "70/4:\n",
      "x = data.iloc[[:,[1]]]\n",
      "\n",
      "y = data.iloc[:,[2]]\n",
      "x = x.reshape(-1,-1)\n",
      "70/5:\n",
      "x = data.iloc[:,[1]]\n",
      "\n",
      "y = data.iloc[:,[2]]\n",
      "x = x.reshape(-1,-1)\n",
      "70/6:\n",
      "model = LinearRegression()\n",
      "x = model.fit([x],[y])\n",
      "y_predict = model.predict(x)\n",
      "70/7:\n",
      "model = LinearRegression()\n",
      "x = model.fit([x],y)\n",
      "y_predict = model.predict(x)\n",
      "70/8:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = data.iloc[:,[2]]\n",
      "70/9:\n",
      "model = LinearRegression()\n",
      "x = model.fit(x,y)\n",
      "y_predict = model.predict(x)\n",
      "70/10:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = np.array(data.iloc[:,[2]])\n",
      "x\n",
      "70/11:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = np.array(data.iloc[:,[2]])\n",
      "70/12:\n",
      "model = LinearRegression()\n",
      "x = model.fit(x,y)\n",
      "y_predict = model.predict(x)\n",
      "70/13:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = np.array(data.iloc[:,[2]])\n",
      "x.reshape(-1,1)\n",
      "70/14:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = np.array(data.iloc[:,[2]])\n",
      "x = x.reshape(-1,1)\n",
      "y = y.reshape(-1,1)\n",
      "70/15:\n",
      "model = LinearRegression()\n",
      "x = model.fit(x,y)\n",
      "y_predict = model.predict(x)\n",
      "70/16:\n",
      "model = LinearRegression()\n",
      "x = model.fit(x,y)\n",
      "y_predict = model.predict(x)\n",
      "70/17:\n",
      "model = LinearRegression()\n",
      "x = model.fit(x,y)\n",
      "y_predict = model.predict(x)\n",
      "70/18:\n",
      "model = LinearRegression()\n",
      "model.fit(x,y)\n",
      "y_predict = model.predict(x)\n",
      "70/19:\n",
      "model = LinearRegression()\n",
      "model.fit(x,y)\n",
      "y_predict = model.predict(x)\n",
      "70/20:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = np.array(data.iloc[:,[2]])\n",
      "x = x.reshape(-1,1)\n",
      "y = y.reshape(-1,1)\n",
      "70/21:\n",
      "model = LinearRegression()\n",
      "model.fit(x,y)\n",
      "y_predict = model.predict(x)\n",
      "70/22:\n",
      "model = LinearRegression()\n",
      "model.fit(x,y)\n",
      "y_predict = model.predict(x)\n",
      "70/23:\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "70/24: train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "70/25:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "len(test_x)\n",
      "70/26:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "len(test_x)\n",
      "len(train_x)\n",
      "70/27:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model1 = LinearRegression()\n",
      "model1.fit(trainx_train_y)\n",
      "70/28:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model1 = LinearRegression()\n",
      "model1.fit(train_x,train_y)\n",
      "70/29:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model1 = LinearRegression()\n",
      "model1.fit(train_x,train_y)\n",
      "predict_y = model1.predict(text_x)\n",
      "plt.scatter(test_x,test_y)\n",
      "plt.plot(test_x,predict_y,color=\"m\")\n",
      "70/30:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model1 = LinearRegression()\n",
      "model1.fit(train_x,train_y)\n",
      "predict_y = model1.predict(test_x)\n",
      "plt.scatter(test_x,test_y)\n",
      "plt.plot(test_x,predict_y,color=\"m\")\n",
      "70/31:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "70/32:\n",
      "print(\"Mean squared error\", mean_squared_error(test_y,predict_y))\n",
      "print(\"\\nMean absolute error\", mean_absolute_error(test_y,predict_y))\n",
      "print(\"\\nR square score\", r2_score(test_y,predict_y))\n",
      "70/33:\n",
      "print(\"Mean squared error\", mean_squared_error(test_y,predict_y))\n",
      "print(\"\\nMean absolute error\", mean_absolute_error(test_y,predict_y))\n",
      "print(\"\\nR square score\", r2_score(test_y,predict_y))\n",
      "70/34:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "71/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "71/2:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "71/3:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = np.array(data.iloc[:,[2]])\n",
      "x = x.reshape(-1,1)\n",
      "y = y.reshape(-1,1)\n",
      "71/4:\n",
      "model = LinearRegression()\n",
      "model.fit(x,y)\n",
      "y_predict = model.predict(x)\n",
      "71/5:\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "71/6:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model1 = DecisionTreeRegressor()\n",
      "model1.fit(train_x,train_y)\n",
      "predict_y = model1.predict(test_x)\n",
      "plt.scatter(test_x,test_y)\n",
      "plt.plot(test_x,predict_y,color=\"m\")\n",
      "71/7:\n",
      "print(\"Mean squared error\", mean_squared_error(test_y,predict_y))\n",
      "print(\"\\nMean absolute error\", mean_absolute_error(test_y,predict_y))\n",
      "print(\"\\nR square score\", r2_score(test_y,predict_y))\n",
      "72/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "72/2:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "72/3:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = np.array(data.iloc[:,[2]])\n",
      "x = x.reshape(-1,1)\n",
      "y = y.reshape(-1,1)\n",
      "72/4:\n",
      "model = LinearRegression()\n",
      "model.fit(x,y)\n",
      "y_predict = model.predict(x)\n",
      "72/5:\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "72/6:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model1 = RandomForestRegressor()\n",
      "model1.fit(train_x,train_y)\n",
      "predict_y = model1.predict(test_x)\n",
      "plt.scatter(test_x,test_y)\n",
      "plt.plot(test_x,predict_y,color=\"m\")\n",
      "72/7:\n",
      "print(\"Mean squared error\", mean_squared_error(test_y,predict_y))\n",
      "print(\"\\nMean absolute error\", mean_absolute_error(test_y,predict_y))\n",
      "print(\"\\nR square score\", r2_score(test_y,predict_y))\n",
      "73/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "73/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "73/3:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "73/4:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "73/5:\n",
      "x_axis = data[[\"Medu\",\"Fedu\"]]\n",
      "y_axis = data[\"iq5\"]\n",
      "73/6:\n",
      "x_axis = data[[\"Medu\",\"Fedu\"]]\n",
      "y_axis = data[:,[5]]\n",
      "73/7:\n",
      "x_axis = data[[\"Medu\",\"Fedu\"]]\n",
      "y_axis = data.iloc[:,[5]]\n",
      "73/8:\n",
      "x_axis = data[[\"Medu\",\"Fedu\"]]\n",
      "y_axis = data.iloc[:,[5]]\n",
      "x_axis\n",
      "73/9:\n",
      "x_axis = data[[\"Medu\",\"Fedu\"]]\n",
      "y_axis = data.iloc[:,[5]]\n",
      "73/10:\n",
      "x_axis = data[[\"Medu\",\"Fedu\"]]\n",
      "y_axis = data.iloc[:,[5]]\n",
      "73/11: train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)\n",
      "73/12:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)\n",
      "model2 = LinearRegression()\n",
      "model2.fit(train_x,train_y)\n",
      "predict_y = model2.predict(test_x)\n",
      "predict_y\n",
      "73/13:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)\n",
      "model2 = LinearRegression()\n",
      "model2.fit(train_x,train_y)\n",
      "predict_y = model2.predict(test_x)\n",
      "some_y = predict[:5]\n",
      "some_y\n",
      "73/14:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)\n",
      "model2 = LinearRegression()\n",
      "model2.fit(train_x,train_y)\n",
      "predict_y = model2.predict(test_x)\n",
      "some_y = predict_y[:5]\n",
      "some_y\n",
      "73/15:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)\n",
      "model2 = LinearRegression()\n",
      "model2.fit(train_x,train_y)\n",
      "predict_y = model2.predict(test_x)\n",
      "some_y = predict_y[:5]\n",
      "some_y\n",
      "73/16: test_y[:5]\n",
      "73/17:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)\n",
      "model2 = LinearRegression()\n",
      "model2.fit(train_x,train_y)\n",
      "predict_y = model2.predict(test_x)\n",
      "predict_y[:5]\n",
      "73/18: test_y[:5]\n",
      "73/19:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)\n",
      "model2 = LinearRegression()\n",
      "model2.fit(train_x,train_y)\n",
      "predict_y = model2.predict(test_x)\n",
      "73/20: print(\"MSE\",mean_absolute_error(test_y,predict_y))\n",
      "73/21:\n",
      "print(\"MSE\",mean_absolute_error(test_y,predict_y))\n",
      "print(\"R squared score\",r2_score(test_y,predict_y))\n",
      "74/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "74/2:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "74/3:\n",
      "x_axis = data[[\"Medu\",\"Fedu\"]]\n",
      "y_axis = data.iloc[:,[5]]\n",
      "74/4:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)\n",
      "model2 = DecisionTreeRegressor()\n",
      "model2.fit(train_x,train_y)\n",
      "predict_y = model2.predict(test_x)\n",
      "74/5:\n",
      "print(\"MSE\",mean_absolute_error(test_y,predict_y))\n",
      "print(\"R squared score\",r2_score(test_y,predict_y))\n",
      "75/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "75/2:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "75/3:\n",
      "x_axis = data[[\"Medu\",\"Fedu\"]]\n",
      "y_axis = data.iloc[:,[5]]\n",
      "75/4:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)\n",
      "model2 = RandomForestRegressor()\n",
      "model2.fit(train_x,train_y)\n",
      "predict_y = model2.predict(test_x)\n",
      "75/5:\n",
      "print(\"MSE\",mean_absolute_error(test_y,predict_y))\n",
      "print(\"R squared score\",r2_score(test_y,predict_y))\n",
      "76/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "76/2:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "76/3:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = np.array(data.iloc[:,[2]])\n",
      "x = x.reshape(-1,1)\n",
      "y = y.reshape(-1,1)\n",
      "76/4:\n",
      "model = LinearRegression()\n",
      "model.fit(x,y)\n",
      "y_predict = model.predict(x)\n",
      "76/5:\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/6:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model1 = LinearRegression()\n",
      "model1.fit(train_x,train_y)\n",
      "predict_y = model1.predict(test_x)\n",
      "plt.scatter(test_x,test_y)\n",
      "plt.plot(test_x,predict_y,color=\"m\")\n",
      "76/7:\n",
      "print(\"Mean squared error\", mean_squared_error(test_y,predict_y))\n",
      "print(\"\\nMean absolute error\", mean_absolute_error(test_y,predict_y))\n",
      "print(\"\\nR square score\", r2_score(test_y,predict_y))\n",
      "76/8:\n",
      "plt.figuresize(10,20)\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/9:\n",
      "plt.figsize(10,20)\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/10:\n",
      "plt.figsize(10,20)\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/11:\n",
      "plt.figsize(10,20)\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/12:\n",
      "plt.figsize(10,20)\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/13:\n",
      "plt.figsize(10,20)\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/14:\n",
      "plt.figsize(10,20)\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/15:\n",
      "plt.figsize(10,20)\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/16:\n",
      "plt.figsize(10,20)\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/17:\n",
      "plt.figsize(10,20)\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/18:\n",
      "plt.figsize(10,20)\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/19:\n",
      "plt.figsize(10,20)\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/20:\n",
      "plt.figure(figsize=(10,20))\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/21:\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/22:\n",
      "plt.figure(figsize=(10,10))\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/23:\n",
      "plt.figure(figsize=(5,10))\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/24:\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,y_predict,color=\"m\")\n",
      "76/25:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = np.array(data.iloc[:,[2]])\n",
      "x = x.reshape(-1,1)\n",
      "y = y.reshape(-1,1)\n",
      "\n",
      "plt.sccatter(x,y)\n",
      "76/26:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = np.array(data.iloc[:,[2]])\n",
      "x = x.reshape(-1,1)\n",
      "y = y.reshape(-1,1)\n",
      "\n",
      "plt.scatter(x,y)\n",
      "76/27:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = np.array(data.iloc[:,[2]])\n",
      "x = x.reshape(-1,1)\n",
      "y = y.reshape(-1,1)\n",
      "\n",
      "plt.plot(data)\n",
      "76/28:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = np.array(data.iloc[:,[2]])\n",
      "x = x.reshape(-1,1)\n",
      "y = y.reshape(-1,1)\n",
      "\n",
      "plt.plot(data.iloc[:,[1,2,3,4,5]])\n",
      "76/29:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = np.array(data.iloc[:,[2]])\n",
      "x = x.reshape(-1,1)\n",
      "y = y.reshape(-1,1)\n",
      "\n",
      "plt.scatter(data.iloc[:,[1,2,3,4,5]])\n",
      "76/30:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = np.array(data.iloc[:,[2]])\n",
      "x = x.reshape(-1,1)\n",
      "y = y.reshape(-1,1)\n",
      "\n",
      "plt.scatter(data.iloc[:,[7,8]],data.iloc[:,[1,2,3,4,5]])\n",
      "76/31:\n",
      "x = np.array(data.iloc[:,[1]])\n",
      "\n",
      "y = np.array(data.iloc[:,[2]])\n",
      "plt.scatter(x,y)\n",
      "x = x.reshape(-1,1)\n",
      "y = y.reshape(-1,1)\n",
      "76/32:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2)\n",
      "plt.xlabel(\"IQ1\")\n",
      "plt.ylabel(\"IQ2\")\n",
      "76/33:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "76/34:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "76/35:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3,label = IQ3)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.lagend()\n",
      "76/36:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.lagend()\n",
      "76/37:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.legend()\n",
      "76/38:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.legend()\n",
      "76/39:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis\n",
      "iq4 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ4\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.legend()\n",
      "76/40:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.subplot()\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis\n",
      "iq4 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ4\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.legend()\n",
      "76/41:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "\n",
      "plt.subplot(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis\n",
      "iq4 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ4\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.legend()\n",
      "76/42:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.subplot(1,2)\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis\n",
      "iq4 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ4\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.legend()\n",
      "76/43:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.subplot(1,2,1)\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis\n",
      "iq4 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ4\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.legend()\n",
      "76/44:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.subplot(1,2,2)\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis\n",
      "iq4 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ4\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.legend()\n",
      "76/45:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.subplot(2,1,2)\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis\n",
      "iq4 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ4\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.legend()\n",
      "76/46:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.subplot(3,1,3)\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis\n",
      "iq4 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ4\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.legend()\n",
      "76/47:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "plt.subplot(3,1,3)\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "76/48:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "76/49:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.subplot(1,1,2)\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "76/50:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.subplot(2,1,2)\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "76/51:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.subplot(3,1,2)\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "76/52:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "76/53:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.subplot(,,2)\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "76/54:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "\n",
      "\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.subplot(grid[0,1])\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "76/55:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "\n",
      "plt.scatter(iq1,iq2, label=\"IQ2\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "76/56:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "\n",
      "plt.scatter(iq1,iq2)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "76/57:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.subplot(grid[0,1])\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "76/58:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "76/59:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq3 as y-axis\n",
      "iq4 = iq4.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "76/60:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2)\n",
      "plt.plot(iq1,iq2)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "76/61:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "#plt.scatter(iq1,iq2)\n",
      "plt.plot(iq1,iq2)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "76/62:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "76/63:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/64:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2,s=100)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/65:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2,s=2)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/66:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2,s=10)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/67:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2,s=50)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/68:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2,s=40)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/69:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2,s=30)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/70:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2,s=30,alpha=10)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/71:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2,s=30,alpha=1)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/72:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2,s=30,alpha=.5)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/73:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq2,s=30,alpha=0.5)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/74:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/75:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq3 as y-axis\n",
      "iq4 = iq4.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/76:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/77:\n",
      "iq2 = np.array(data.iloc[:,[1]])      # Taking iq2 as x\n",
      "iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)                                   # IQ2 x IQ3\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/78:\n",
      "iq2 = np.array(data.iloc[:,[1]])      # Taking iq2 as x\n",
      "iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq3)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)                                   # IQ2 x IQ3\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/79:\n",
      "iq2 = np.array(data.iloc[:,[1]])      # Taking iq2 as x\n",
      "iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq3)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)                                   # IQ2 x IQ3\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/80:\n",
      "iq2 = np.array(data.iloc[:,[1]])      # Taking iq2 as x\n",
      "iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "#iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq3)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)                                   # IQ2 x IQ3\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/81:\n",
      "iq2 = np.array(data.iloc[:,[1]])      # Taking iq2 as x\n",
      "iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "#iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq3)\n",
      "plt.xlabel(\"IQ2\", fontsize = 20)                                   # IQ2 x IQ3\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/82:\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x\n",
      "iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "#iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq3)\n",
      "plt.xlabel(\"IQ2\", fontsize = 20)                                   # IQ2 x IQ3\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/83:\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq3 as y-axis\n",
      "iq4 = iq4.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq4)\n",
      "plt.xlabel(\"IQ2\", fontsize = 20)                                   # IQ2 x IQ3\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/84:\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq5)\n",
      "plt.xlabel(\"IQ2\", fontsize = 20)                                   # IQ2 x IQ5\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/85:\n",
      "iq3 = np.array(data.iloc[:,[2]])      # Taking iq3 as x\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis\n",
      "iq4 = iq4.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq3,iq4)\n",
      "plt.xlabel(\"IQ3\", fontsize = 20)                                   # IQ2 x IQ5\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/86:\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis\n",
      "iq4 = iq4.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq3,iq4)\n",
      "plt.xlabel(\"IQ3\", fontsize = 20)                                   # IQ3 x IQ4\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/87:\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq3,iq5)\n",
      "plt.xlabel(\"IQ3\", fontsize = 20)                                   # IQ3 x IQ5\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "76/88:\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq4,iq5)\n",
      "plt.xlabel(\"IQ4\", fontsize = 20)                                   # IQ4 x IQ5\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "77/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "77/2:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "77/3:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)                                              # IQ1 x IQ2\n",
      "\n",
      "plt.scatter(iq1,iq2,s=30,alpha=0.5)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "plt.grid()\n",
      "77/4:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")                                   # IQ1 x IQ3\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.grid()\n",
      "77/5:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq3 as y-axis\n",
      "iq4 = iq4.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)                                    # IQ1 x IQ4\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "plt.grid()\n",
      "77/6:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)                                   # IQ1 x IQ5\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "77/7:\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x\n",
      "iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq3)\n",
      "plt.xlabel(\"IQ2\", fontsize = 20)                                   # IQ2 x IQ3\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.grid()\n",
      "77/8:\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq as y-axis\n",
      "iq4 = iq4.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq4)\n",
      "plt.xlabel(\"IQ2\", fontsize = 20)                                   # IQ2 x IQ4\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "plt.grid()\n",
      "77/9:\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq5)\n",
      "plt.xlabel(\"IQ2\", fontsize = 20)                                   # IQ2 x IQ5\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "77/10:\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis\n",
      "iq4 = iq4.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq3,iq4)\n",
      "plt.xlabel(\"IQ3\", fontsize = 20)                                   # IQ3 x IQ4\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "plt.grid()\n",
      "77/11:\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq3,iq5)\n",
      "plt.xlabel(\"IQ3\", fontsize = 20)                                   # IQ3 x IQ5\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "77/12:\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq4,iq5)\n",
      "plt.xlabel(\"IQ4\", fontsize = 20)                                   # IQ4 x IQ5\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "77/13:\n",
      "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model1 = LinearRegression()\n",
      "model1.fit(train_x,train_y)\n",
      "predict_y = model1.predict(test_x)\n",
      "plt.scatter(test_x,test_y)\n",
      "plt.plot(test_x,predict_y,color=\"m\")\n",
      "77/14:\n",
      "\n",
      "plt.scatter(iq1,iq2,label = \"IQ2\")\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.scatter(iq1,iq4,label = \"IQ4\") \n",
      "plt.scatter(iq1,iq5,label = \"IQ5\")\n",
      "plt.grid()\n",
      "77/15:\n",
      "\n",
      "plt.scatter(iq1,iq2,label = \"IQ2\")\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\")\n",
      "plt.scatter(iq1,iq4,label = \"IQ4\") \n",
      "plt.scatter(iq1,iq5,label = \"IQ5\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "77/16:\n",
      "\n",
      "plt.scatter(iq1,iq2,label = \"IQ2\",alpha = 0.6)\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\",alpha = 0.6)\n",
      "plt.scatter(iq1,iq4,label = \"IQ4\",alpha = 0.6) \n",
      "plt.scatter(iq1,iq5,label = \"IQ5\",alpha = 0.6)\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "77/17:\n",
      "\n",
      "plt.scatter(iq1,iq2,label = \"IQ2\",alpha = 0.6)\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\",alpha = 0.6)\n",
      "plt.scatter(iq1,iq4,label = \"IQ4\",alpha = 0.6) \n",
      "plt.scatter(iq1,iq5,label = \"IQ5\",alpha = 0.6)\n",
      "plt.xlabel(\"IQ1\",fontsize = 20)\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "77/18:\n",
      "plt.scatter(iq2,iq3, label = \"IQ3\",alpha = 0.6)\n",
      "plt.scatter(iq2,iq4,label = \"IQ4\",alpha = 0.6) \n",
      "plt.scatter(iq2,iq5,label = \"IQ5\",alpha = 0.6)\n",
      "plt.xlabel(\"IQ2\",fontsize=20)\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "77/19:\n",
      "plt.scatter(iq3,iq4,label = \"IQ4\",alpha = 0.6) \n",
      "plt.scatter(iq3,iq5,label = \"IQ5\",alpha = 0.6)\n",
      "plt.xlabel(\"IQ3\",fontsize=20)\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "78/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "78/2:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "78/3:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)                                              # IQ1 x IQ2\n",
      "\n",
      "plt.scatter(iq1,iq2,s=30)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "plt.grid()\n",
      "78/4:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3)                                   # IQ1 x IQ3\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.grid()\n",
      "78/5:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq3 as y-axis\n",
      "iq4 = iq4.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq4)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)                                    # IQ1 x IQ4\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "plt.grid()\n",
      "78/6:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq5)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)                                   # IQ1 x IQ5\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "78/7:\n",
      "\n",
      "plt.scatter(iq1,iq2,label = \"IQ2\",alpha = 0.6)\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\",alpha = 0.6)\n",
      "plt.scatter(iq1,iq4,label = \"IQ4\",alpha = 0.6) \n",
      "plt.scatter(iq1,iq5,label = \"IQ5\",alpha = 0.6)\n",
      "plt.xlabel(\"IQ1\",fontsize = 20)\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "78/8:\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x\n",
      "iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq3)\n",
      "plt.xlabel(\"IQ2\", fontsize = 20)                                   # IQ2 x IQ3\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.grid()\n",
      "78/9:\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq as y-axis\n",
      "iq4 = iq4.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq4)\n",
      "plt.xlabel(\"IQ2\", fontsize = 20)                                   # IQ2 x IQ4\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "plt.grid()\n",
      "78/10:\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq5)\n",
      "plt.xlabel(\"IQ2\", fontsize = 20)                                   # IQ2 x IQ5\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "78/11:\n",
      "plt.scatter(iq2,iq3, label = \"IQ3\",alpha = 0.6)\n",
      "plt.scatter(iq2,iq4,label = \"IQ4\",alpha = 0.6) \n",
      "plt.scatter(iq2,iq5,label = \"IQ5\",alpha = 0.6)\n",
      "plt.xlabel(\"IQ2\",fontsize=20)\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "78/12:\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis\n",
      "iq4 = iq4.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq3,iq4)\n",
      "plt.xlabel(\"IQ3\", fontsize = 20)                                   # IQ3 x IQ4\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "plt.grid()\n",
      "78/13:\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq3,iq5)\n",
      "plt.xlabel(\"IQ3\", fontsize = 20)                                   # IQ3 x IQ5\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "78/14:\n",
      "plt.scatter(iq3,iq4,label = \"IQ4\",alpha = 0.6) \n",
      "plt.scatter(iq3,iq5,label = \"IQ5\",alpha = 0.6)\n",
      "plt.xlabel(\"IQ3\",fontsize=20)\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "78/15:\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq4,iq5)\n",
      "plt.xlabel(\"IQ4\", fontsize = 20)                                   # IQ4 x IQ5\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "78/16:\n",
      "train_x,test_x,train_y,test_y = train_test_split(iq1,iq2,test_size=0.2,random_state=42)\n",
      "model1 = LinearRegression()\n",
      "model1.fit(train_x,train_y)\n",
      "predict_y = model1.predict(test_x)\n",
      "plt.scatter(test_x,test_y)\n",
      "plt.plot(test_x,predict_y,color=\"m\")\n",
      "78/17:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(test_y,predict_y))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(test_y,predict_y))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(test_y,predict_y))\n",
      "78/18:\n",
      "plt.scatter(iq2,iq3, label = \"IQ3\",alpha = 0.6,color=\"m\")\n",
      "plt.scatter(iq2,iq4,label = \"IQ4\",alpha = 0.6) \n",
      "plt.scatter(iq2,iq5,label = \"IQ5\",alpha = 0.6)\n",
      "plt.xlabel(\"IQ2\",fontsize=20)\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "78/19:\n",
      "plt.scatter(iq2,iq3, label = \"IQ3\",alpha = 0.6)\n",
      "plt.scatter(iq2,iq4,label = \"IQ4\",alpha = 0.6) \n",
      "plt.scatter(iq2,iq5,label = \"IQ5\",alpha = 0.6)\n",
      "plt.xlabel(\"IQ2\",fontsize=20)\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "82/1:\n",
      "a = [1,2,3,4,4,5]\n",
      "print(set(a))\n",
      "82/2:\n",
      "a = [1,2,3,4,4,5]\n",
      "print(set(a))\n",
      "82/3:\n",
      "a = [1,2,3,4,4,5]\n",
      "print(set(a))\n",
      "82/4:\n",
      "a = [1,2,3,4,4,5]\n",
      "print(set(a))\n",
      "82/5:\n",
      "a = [1,2,3,4,4,5]\n",
      "print(set(a))\n",
      "82/6:\n",
      "a = [1,2,3,4,4,5]\n",
      "print(set(a))\n",
      "82/7:\n",
      "a = [1,2,3,3,4,4,5]\n",
      "print(set(a))\n",
      "82/8:\n",
      "a = [1,2,3,3,4,4,5]\n",
      "print(set(a))\n",
      "82/9:\n",
      "a = [1,1,2,3,3,4,4,5]\n",
      "print(set(a))\n",
      "82/10:\n",
      "a = [1,1,2,3,3,4,4,5]\n",
      "print(set(a))\n",
      "82/11:\n",
      "a = [1,1,2,3,3,4,4,5]\n",
      "print(set(a))\n",
      "82/12:\n",
      "a = [1,1,2,3,3,4,4,5]\n",
      "print(set(a))\n",
      "82/13:\n",
      "a = [1,1,2,3,3,4,4,5]\n",
      "print(set(a))\n",
      "82/14:\n",
      "a = [1,1,2,3,3,4,4,5]\n",
      "print(set(a))\n",
      "82/15:\n",
      "a = [1,1,2,3,3,4,4,5]\n",
      "print(set(a))\n",
      "83/1:\n",
      "a = [1,1,2,3,3,4,4,5]\n",
      "print(set(a))\n",
      "83/2: a = [1,1,2,3,3,4,4,5]\n",
      "83/3:\n",
      "a = [1,1,2,3,3,4,4,5]\n",
      "print(set(a))\n",
      "83/4:\n",
      "a = [1,1,2,3,3,4,4,5]\n",
      "b = []\n",
      "for i in a:\n",
      "    if i not in b:\n",
      "        b.append(i)\n",
      "        \n",
      "b\n",
      "83/5:\n",
      "a = [1,1,2,4,4,3,3,5]\n",
      "set(a)\n",
      "b = []\n",
      "for i in a:\n",
      "    if i not in b:\n",
      "        b.append(i)\n",
      "        \n",
      "b\n",
      "83/6:\n",
      "a = [1,1,2,4,4,3,3,5]\n",
      "print(set(a))\n",
      "b = []\n",
      "for i in a:\n",
      "    if i not in b:\n",
      "        b.append(i)\n",
      "        \n",
      "b\n",
      "84/1:\n",
      "data = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data.head()\n",
      "85/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "85/2:\n",
      "data = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data.head()\n",
      "85/3:\n",
      "data = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data.head(10)\n",
      "85/4:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "85/5:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "data1.head(10)\n",
      "85/6:\n",
      "x = data1[\"YEAR\"]\n",
      "y = data1[\"ANNUAL\"]\n",
      "plt.scatter(x,y)\n",
      "85/7:\n",
      "x = data1[\"YEAR\"],data2[\"YEAR\"]\n",
      "y = data1[\"ANNUAL\"]\n",
      "plt.scatter(x,y)\n",
      "85/8:\n",
      "x = data1[\"YEAR\"],data2[\"YEAR\"]\n",
      "y = data1[\"ANNUAL\"],data2[\"ANNUAL\"]\n",
      "plt.scatter(x,y)\n",
      "85/9: x = data[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",Oct-Dec]]\n",
      "85/10: x = data[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "85/11:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "85/12:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "plt.plot(x,y)\n",
      "85/13:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "plt.hist(bins=20)\n",
      "85/14:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "plt.hist(y,bins=20)\n",
      "85/15:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "plt.hist(x,y,bins=20)\n",
      "85/16:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "plt.hist(x,bins=20)\n",
      "85/17:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "plt.hist(x,bins=5)\n",
      "85/18:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(x,y)\n",
      "85/19:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "x\n",
      "#plt.scatter(x,y)\n",
      "85/20:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(x[0],y)\n",
      "85/21:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y)\n",
      "85/22:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y)\n",
      "plt.scatter(data1[\"Mar-May\"],y)\n",
      "85/23:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.plot(data1[\"Jan-Feb\"],y)\n",
      "plt.scatter(data1[\"Mar-May\"],y)\n",
      "85/24:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y)\n",
      "plt.scatter(data1[\"Mar-May\"],y)\n",
      "85/25:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y)\n",
      "plt.scatter(data1[\"Mar-May\"],y)\n",
      "plt.scatter(data1[\"Jun-Sep\"],y)\n",
      "plt.scatter(data1[\"Oct-Dec\"],y)\n",
      "85/26:\n",
      "x = data1[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "85/27: x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "85/28:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "85/29:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "85/30:\n",
      "y_predict = model.predict(x_test)\n",
      "y_predict\n",
      "85/31: y_test\n",
      "86/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "86/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "data1.head(10)\n",
      "86/3:\n",
      "x = data1[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "86/4:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "86/5:\n",
      "y_predict = model.predict(x_test)\n",
      "y_predict\n",
      "86/6: y_test\n",
      "87/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "87/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "data1.head(10)\n",
      "87/3:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "87/4:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "87/5:\n",
      "y_predict = model.predict(x_test)\n",
      "y_predict\n",
      "87/6: model.predict([[\"2006\",\"0.1\",\"94.3\",\"936.3\",\"22.1\"]])\n",
      "87/7: model.predict([\"2006\",\"0.1\",\"94.3\",\"936.3\",\"22.1\"])\n",
      "87/8:\n",
      "test = data1.iloc[0,[0,2,3,4,5]]\n",
      "test\n",
      "#model.predict([\"2006\",\"0.1\",\"94.3\",\"936.3\",\"22.1\"])\n",
      "87/9:\n",
      "test = data1.iloc[0,[0,2,3,4,5]]\n",
      "test = test.reshape(-1,1)\n",
      "test\n",
      "#model.predict([\"2006\",\"0.1\",\"94.3\",\"936.3\",\"22.1\"])\n",
      "87/10:\n",
      "test = list(data1.iloc[0,[0,2,3,4,5]])\n",
      "test = test.reshape(-1,1)\n",
      "test\n",
      "#model.predict([\"2006\",\"0.1\",\"94.3\",\"936.3\",\"22.1\"])\n",
      "87/11:\n",
      "test = list(data1.iloc[0,[0,2,3,4,5]])\n",
      "#test = test.reshape(-1,1)\n",
      "test\n",
      "#model.predict([\"2006\",\"0.1\",\"94.3\",\"936.3\",\"22.1\"])\n",
      "87/12:\n",
      "test = list(data1.iloc[0,[0,2,3,4,5]])\n",
      "#test = test.reshape(-1,1)\n",
      "test\n",
      "model.predict(test)\n",
      "87/13:\n",
      "test = list(data1.iloc[0,[0,2,3,4,5]])\n",
      "#test = test.reshape(-1,1)\n",
      "test\n",
      "model.predict([test])\n",
      "88/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "88/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "data1.head(10)\n",
      "88/3:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "88/4:\n",
      "x = data2[\"YEAR\"]\n",
      "y = data2[\"Jan-Feb\"]\n",
      "88/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "89/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "89/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "data1.head(10)\n",
      "89/3:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "89/4:\n",
      "x = data2[\"YEAR\"].reshape(-1,1)\n",
      "y = data2[\"Jan-Feb\"].reshape(-1,1)\n",
      "89/5:\n",
      "x = data2[\"YEAR\"]\n",
      "y = data2[\"Jan-Feb\"]\n",
      "x\n",
      "89/6:\n",
      "x = data2[\"YEAR\"]\n",
      "y = data2[\"Jan-Feb\"]\n",
      "x = x.reshape(-1,1)\n",
      "89/7:\n",
      "x = list(data2[\"YEAR\"])\n",
      "y = data2[\"Jan-Feb\"]\n",
      "x = x.reshape(-1,1)\n",
      "89/8:\n",
      "x = np.array(data2[\"YEAR\"])\n",
      "y = data2[\"Jan-Feb\"]\n",
      "x = x.reshape(-1,1)\n",
      "89/9:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"Jan-Feb\"]).reshape(-1,1)\n",
      "89/10:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"Jan-Feb\"]).reshape(-1,1)\n",
      "90/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "90/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "data1.head(10)\n",
      "90/3:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "90/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"Jan-Feb\"]).reshape(-1,1)\n",
      "90/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "90/6:\n",
      "y_predict = model.predict(x_test)\n",
      "y_predict\n",
      "90/7:\n",
      "test = list(data1.iloc[0,[0,2,3,4,5]])\n",
      "#test = test.reshape(-1,1)\n",
      "test\n",
      "model.predict([test])\n",
      "90/8:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(10))\n",
      "90/9: y_test\n",
      "90/10:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = DecisionTreeRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "90/11:\n",
      "y_predict = model.predict(x_test)\n",
      "y_predict\n",
      "90/12: y_test\n",
      "90/13:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = RandomForestRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "90/14:\n",
      "y_predict = model.predict(x_test)\n",
      "y_predict\n",
      "90/15: y_test\n",
      "90/16:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "90/17:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "91/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "91/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(10))\n",
      "91/3:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "91/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"Jan-Feb\"]).reshape(-1,1)\n",
      "scaler = StandardScaler()\n",
      "x = x.fit_transform(x)\n",
      "y = y.fit_transform(y)\n",
      "92/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "92/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(10))\n",
      "92/3:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "92/4:\n",
      "x = data2[\"YEAR\"]\n",
      "y = data2[\"Jan-Feb\"]\n",
      "scaler = StandardScaler()\n",
      "x = x.fit_transform(x)\n",
      "y = y.fit_transform(y)\n",
      "92/5:\n",
      "x = data2.iloc[:,[0]]\n",
      "y = data2[\"Jan-Feb\"]\n",
      "scaler = StandardScaler()\n",
      "x = x.fit_transform(x)\n",
      "y = y.fit_transform(y)\n",
      "92/6:\n",
      "x = data2.iloc[:,[0]]\n",
      "print(x)\n",
      "y = data2[\"Jan-Feb\"]\n",
      "scaler = StandardScaler()\n",
      "x = x.fit_transform(x)\n",
      "y = y.fit_transform(y)\n",
      "92/7:\n",
      "x = data2.iloc[:,[0]].values\n",
      "print(x)\n",
      "y = data2[\"Jan-Feb\"]\n",
      "scaler = StandardScaler()\n",
      "x = x.fit_transform(x)\n",
      "y = y.fit_transform(y)\n",
      "92/8:\n",
      "x = data2.iloc[:,0].values\n",
      "print(x)\n",
      "y = data2[\"Jan-Feb\"]\n",
      "scaler = StandardScaler()\n",
      "x = x.fit_transform(x)\n",
      "y = y.fit_transform(y)\n",
      "92/9:\n",
      "x = data2.iloc[:,0].values\n",
      "print(x)\n",
      "y = data2.iloc[:,2].values\n",
      "print(y)\n",
      "scaler = StandardScaler()\n",
      "x = x.fit_transform(x)\n",
      "y = y.fit_transform(y)\n",
      "92/10:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"Jan-Feb\"]).reshape(-1,1)\n",
      "scaler = StandardScaler()\n",
      "x = scaler.fit_transform(x)\n",
      "y = scaler.fit_transform(y)\n",
      "93/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "93/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(10))\n",
      "93/3:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "93/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"Jan-Feb\"]).reshape(-1,1)\n",
      "scaler = StandardScaler()\n",
      "x = scaler.fit_transform(x)\n",
      "y = scaler.fit_transform(y)\n",
      "93/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = RandomForestRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "93/6:\n",
      "y_predict = model.predict(x_test)\n",
      "y_predict\n",
      "93/7: y_test\n",
      "94/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "94/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(10))\n",
      "94/3:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "94/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"Jan-Feb\"]).reshape(-1,1)\n",
      "scaler = StandardScaler()\n",
      "x = scaler.fit_transform(x)\n",
      "y = scaler.fit_transform(y)\n",
      "94/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "94/6:\n",
      "y_predict = model.predict(x_test)\n",
      "y_predict\n",
      "94/7: y_test\n",
      "94/8: r2_score(y_test,y_predict)\n",
      "95/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "95/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(10))\n",
      "95/3:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "95/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"ANNUAL\"]).reshape(-1,1)\n",
      "scaler = StandardScaler()\n",
      "x = scaler.fit_transform(x)\n",
      "y = scaler.fit_transform(y)\n",
      "95/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "95/6:\n",
      "y_predict = model.predict(x_test)\n",
      "y_predict\n",
      "95/7: y_test\n",
      "95/8: r2_score(y_test,y_predict)\n",
      "96/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "96/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(10))\n",
      "96/3:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "96/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"ANNUAL\"]).reshape(-1,1)\n",
      "#scaler = StandardScaler()\n",
      "#x = scaler.fit_transform(x)\n",
      "#y = scaler.fit_transform(y)\n",
      "96/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "96/6:\n",
      "y_predict = model.predict(x_test)\n",
      "y_predict\n",
      "96/7: y_test\n",
      "96/8: r2_score(y_test,y_predict)\n",
      "96/9: model.predict([[2006]])\n",
      "96/10: model.predict([[2007]])\n",
      "96/11: model.predict([[2006]])\n",
      "98/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "98/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(10))\n",
      "98/3:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "98/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"ANNUAL\"]).reshape(-1,1)\n",
      "#scaler = StandardScaler()\n",
      "#x = scaler.fit_transform(x)\n",
      "#y = scaler.fit_transform(y)\n",
      "98/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "98/6:\n",
      "y_predict = model.predict(x_test)\n",
      "y_predict\n",
      "98/7: model.predict([[2006]])\n",
      "98/8: r2_score(y_test,y_predict)\n",
      "98/9:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "98/10:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(10))\n",
      "98/11:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "98/12:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"ANNUAL\"]).reshape(-1,1)\n",
      "#scaler = StandardScaler()\n",
      "#x = scaler.fit_transform(x)\n",
      "#y = scaler.fit_transform(y)\n",
      "98/13:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/14:\n",
      "y_predict = model.predict(x_test)\n",
      "y_predict\n",
      "98/15: model.predict([[2006]])\n",
      "98/16: r2_score(y_test,y_predict)\n",
      "99/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "99/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(10))\n",
      "99/3:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "100/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "100/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(11))\n",
      "100/3:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "101/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "101/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(11))\n",
      "101/3:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "102/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "102/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(11))\n",
      "102/3:\n",
      "x = data2[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data2[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "103/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "103/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(11))\n",
      "103/3:\n",
      "x = data1[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "103/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"ANNUAL\"]).reshape(-1,1)\n",
      "#scaler = StandardScaler()\n",
      "#x = scaler.fit_transform(x)\n",
      "#y = scaler.fit_transform(y)\n",
      "103/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "103/6:\n",
      "y_predict = model.predict(x_test)\n",
      "y_predict\n",
      "103/7: model.predict([[2006]])\n",
      "103/8: r2_score(y_test,y_predict)\n",
      "103/9: y_test\n",
      "103/10: model.predict([[2007]])\n",
      "104/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "104/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(11))\n",
      "104/3:\n",
      "x = data1[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "104/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"ANNUAL\"]).reshape(-1,1)\n",
      "#scaler = StandardScaler()\n",
      "#x = scaler.fit_transform(x)\n",
      "#y = scaler.fit_transform(y)\n",
      "104/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "104/6:\n",
      "y_predict = model.predict(x_test)\n",
      "y_predict\n",
      "104/7: model.predict([[2007]])\n",
      "104/8: r2_score(y_test,y_predict)\n",
      "105/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "105/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(11))\n",
      "105/3:\n",
      "x = data1[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "105/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"ANNUAL\"]).reshape(-1,1)\n",
      "#scaler = StandardScaler()\n",
      "#x = scaler.fit_transform(x)\n",
      "#y = scaler.fit_transform(y)\n",
      "105/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "105/6:\n",
      "y_predict = model.predict(x_test)\n",
      "y_predict\n",
      "105/7: model.predict([[2007]])\n",
      "105/8: r2_score(y_test,y_predict)\n",
      "106/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "106/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(11))\n",
      "106/3:\n",
      "x = data1[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "106/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"ANNUAL\"]).reshape(-1,1)\n",
      "#scaler = StandardScaler()\n",
      "#x = scaler.fit_transform(x)\n",
      "#y = scaler.fit_transform(y)\n",
      "106/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = RandomForestRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "106/6: y_predict = model.predict(x_test)\n",
      "106/7: model.predict([[2007]])\n",
      "106/8: r2_score(y_test,y_predict)\n",
      "107/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "107/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(11))\n",
      "107/3:\n",
      "x = data1[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "107/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"ANNUAL\"]).reshape(-1,1)\n",
      "#scaler = StandardScaler()\n",
      "#x = scaler.fit_transform(x)\n",
      "#y = scaler.fit_transform(y)\n",
      "107/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = DecisionTreeRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "107/6: y_predict = model.predict(x_test)\n",
      "107/7: model.predict([[2007]])\n",
      "107/8: r2_score(y_test,y_predict)\n",
      "108/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "108/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(11))\n",
      "108/3:\n",
      "x = data1[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "108/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"ANNUAL\"]).reshape(-1,1)\n",
      "#scaler = StandardScaler()\n",
      "#x = scaler.fit_transform(x)\n",
      "#y = scaler.fit_transform(y)\n",
      "108/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "108/6: y_predict = model.predict(x_test)\n",
      "108/7: model.predict([[2007]])\n",
      "108/8: r2_score(y_test,y_predict)\n",
      "109/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "109/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(11))\n",
      "109/3:\n",
      "x = data1[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "109/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"Jan-Feb\"]).reshape(-1,1)\n",
      "#scaler = StandardScaler()\n",
      "#x = scaler.fit_transform(x)\n",
      "#y = scaler.fit_transform(y)\n",
      "109/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "109/6: y_predict = model.predict(x_test)\n",
      "109/7: model.predict([[2007]])\n",
      "109/8: r2_score(y_test,y_predict)\n",
      "110/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "110/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(11))\n",
      "110/3:\n",
      "x = data1[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "110/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"Jun-Sep\"]).reshape(-1,1)\n",
      "#scaler = StandardScaler()\n",
      "#x = scaler.fit_transform(x)\n",
      "#y = scaler.fit_transform(y)\n",
      "110/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "110/6: y_predict = model.predict(x_test)\n",
      "110/7: model.predict([[2007]])\n",
      "110/8: r2_score(y_test,y_predict)\n",
      "110/9: mean_squared_error(y_test,y_predict)\n",
      "110/10: mean_absolute_error(y_test,y_predict)\n",
      "111/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "111/2:\n",
      "data1 = pd.read_csv(\"Bihar-2006-2015.csv\")\n",
      "data2 = pd.read_csv(\"Bihar-1996-2005.csv\")\n",
      "print(data1.head(10))\n",
      "print(data2.head(11))\n",
      "111/3:\n",
      "x = data1[[\"YEAR\",\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y = data1[\"ANNUAL\"]\n",
      "\n",
      "plt.scatter(data1[\"Jan-Feb\"],y,label=\"Jan-Feb\")\n",
      "plt.scatter(data1[\"Mar-May\"],y,label=\"Mar-May\")\n",
      "plt.scatter(data1[\"Jun-Sep\"],y,label=\"Jun-Sep\")\n",
      "plt.scatter(data1[\"Oct-Dec\"],y,label=\"Oct-Dec\")\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "111/4:\n",
      "x = np.array(data2[\"YEAR\"]).reshape(-1,1)\n",
      "y = np.array(data2[\"Oct-Dec\"]).reshape(-1,1)\n",
      "#scaler = StandardScaler()\n",
      "#x = scaler.fit_transform(x)\n",
      "#y = scaler.fit_transform(y)\n",
      "111/5:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "111/6: y_predict = model.predict(x_test)\n",
      "111/7: model.predict([[2007]])\n",
      "111/8: mean_absolute_error(y_test,y_predict)\n",
      "114/1:\n",
      "arr = [32,1,0,34,0,23]\n",
      "arr1 = [0]*len(arr)\n",
      "j = 0\n",
      "\n",
      "for i in arr:\n",
      "    if i!=arr1[j]:\n",
      "        arr1[j] = i\n",
      "        j++\n",
      "        \n",
      "arr1\n",
      "114/2:\n",
      "arr = [32,1,0,34,0,23]\n",
      "arr1 = [0]*len(arr)\n",
      "j = 0\n",
      "\n",
      "for i in arr:\n",
      "    if i!=arr1[j]:\n",
      "        arr1[j] = i\n",
      "        j=j+1\n",
      "        \n",
      "arr1\n",
      "114/3:\n",
      "game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "print(game_board)\n",
      "114/4:\n",
      "game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "\n",
      "for i in range(0,3):\n",
      "    for j in range(0,3):\n",
      "        print(game_board[i][j])\n",
      "    print()\n",
      "114/5:\n",
      "game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "\n",
      "for i in range(0,3):\n",
      "    for j in range(0,3):\n",
      "        print(game_board[i][j],end = \" \")\n",
      "    print()\n",
      "114/6:\n",
      "import random\n",
      "\n",
      "\n",
      "def random_number_generator():\n",
      "    random.randint(1,100)\n",
      "114/7:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/8:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/9:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/10:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/11:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/12:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/13:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/14:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/15:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/16:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/17:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/18:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/19:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/20:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/21:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/22:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/23:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/24:\n",
      "import random\n",
      "\n",
      "\n",
      "random.randint(1,100)\n",
      "114/25:\n",
      "import random\n",
      "\n",
      "\n",
      "def random_number_generator():\n",
      "    return random.randint(1,100)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def print_board(game_board):\n",
      "    for i in range(0,3):\n",
      "        for j in range(0,3):\n",
      "            print(game_board[i][j],end = \" \")\n",
      "        print()\n",
      "\n",
      "\n",
      "def game():\n",
      "    game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "    i = 0\n",
      "    while(i!=3):\n",
      "        j = 0\n",
      "        while(j!=3):\n",
      "            value = random_number_generator()\n",
      "            if(value not in game_board):\n",
      "                game_board[i][j] = value\n",
      "                j+=1\n",
      "                \n",
      "    print_board()\n",
      "115/1:\n",
      "import random\n",
      "\n",
      "\n",
      "def random_number_generator():\n",
      "    return random.randint(1,100)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def print_board(game_board):\n",
      "    for i in range(0,3):\n",
      "        for j in range(0,3):\n",
      "            print(game_board[i][j],end = \" \")\n",
      "        print()\n",
      "\n",
      "\n",
      "def game():\n",
      "    game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "    i = 0\n",
      "    while(i!=3):\n",
      "        j = 0\n",
      "        while(j!=3):\n",
      "            value = random_number_generator()\n",
      "            if(value not in game_board):\n",
      "                game_board[i][j] = value\n",
      "                j+=1\n",
      "                \n",
      "    print_board()\n",
      "115/2:\n",
      "import random\n",
      "\n",
      "\n",
      "def random_number_generator():\n",
      "    return random.randint(1,100)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def print_board(game_board):\n",
      "    for i in range(0,3):\n",
      "        for j in range(0,3):\n",
      "            print(game_board[i][j],end = \" \")\n",
      "        print()\n",
      "\n",
      "\n",
      "def game():\n",
      "    game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "    i = 0\n",
      "    while(i!=3):\n",
      "        j = 0\n",
      "        while(j!=3):\n",
      "            value = random_number_generator()\n",
      "            if(value not in game_board):\n",
      "                game_board[i][j] = value\n",
      "                j+=1\n",
      "                \n",
      "    print_board()\n",
      "\n",
      "game()\n",
      "115/3:\n",
      "import random\n",
      "\n",
      "\n",
      "def random_number_generator():\n",
      "    return random.randint(1,100)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def print_board(game_board):\n",
      "    for i in range(0,3):\n",
      "        for j in range(0,3):\n",
      "            print(game_board[i][j],end = \" \")\n",
      "        print()\n",
      "\n",
      "\n",
      "def game():\n",
      "    game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "    i = 0\n",
      "    while(i!=3):\n",
      "        j = 0\n",
      "        while(j!=3):\n",
      "            value = random_number_generator()\n",
      "            if(value not in game_board):\n",
      "                game_board[i][j] = value\n",
      "                j+=1\n",
      "        i+=1\n",
      "                \n",
      "    print_board()\n",
      "\n",
      "game()\n",
      "115/4:\n",
      "import random\n",
      "\n",
      "\n",
      "def random_number_generator():\n",
      "    return random.randint(1,100)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def print_board(game_board):\n",
      "    for i in range(0,3):\n",
      "        for j in range(0,3):\n",
      "            print(game_board[i][j],end = \" \")\n",
      "        print()\n",
      "\n",
      "\n",
      "def game():\n",
      "    game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "    i = 0\n",
      "    while(i!=3):\n",
      "        j = 0\n",
      "        while(j!=3):\n",
      "            value = random_number_generator()\n",
      "            if(value not in game_board):\n",
      "                game_board[i][j] = value\n",
      "                j+=1\n",
      "        i+=1\n",
      "                \n",
      "    print_board()\n",
      "\n",
      "game()\n",
      "115/5:\n",
      "import random\n",
      "\n",
      "\n",
      "def random_number_generator():\n",
      "    return random.randint(1,100)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def print_board(game_board):\n",
      "    for i in range(0,3):\n",
      "        for j in range(0,3):\n",
      "            print(game_board[i][j],end = \" \")\n",
      "        print()\n",
      "\n",
      "\n",
      "def game():\n",
      "    game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "    i = 0\n",
      "    while(i!=3):\n",
      "        j = 0\n",
      "        while(j!=3):\n",
      "            value = random_number_generator()\n",
      "            if(value not in game_board):\n",
      "                game_board[i][j] = value\n",
      "                j+=1\n",
      "        i+=1\n",
      "                \n",
      "    print_board(game_board)\n",
      "\n",
      "game()\n",
      "115/6:\n",
      "import random\n",
      "\n",
      "\n",
      "def random_number_generator():\n",
      "    return random.randint(1,100)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def print_board(game_board):\n",
      "    for i in range(0,3):\n",
      "        for j in range(0,3):\n",
      "            print(game_board[i][j],end = \" \")\n",
      "        print()\n",
      "\n",
      "\n",
      "def game():\n",
      "    game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "    i = 0\n",
      "    while(i!=3):\n",
      "        j = 0\n",
      "        while(j!=3):\n",
      "            value = random_number_generator()\n",
      "            if(value not in game_board):\n",
      "                game_board[i][j] = value\n",
      "                j+=1\n",
      "                \n",
      "        i+=1\n",
      "                \n",
      "    print_board(game_board)\n",
      "\n",
      "game()\n",
      "115/7:\n",
      "import random\n",
      "\n",
      "\n",
      "def random_number_generator():\n",
      "    return random.randint(1,100)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def print_board(game_board):\n",
      "    for i in range(0,3):\n",
      "        for j in range(0,3):\n",
      "            print(game_board[i][j],end = \" \")\n",
      "        print()\n",
      "\n",
      "\n",
      "def game():\n",
      "    game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "    i = 0\n",
      "    while(i!=3):\n",
      "        j = 0\n",
      "        while(j!=3):\n",
      "            value = random_number_generator()\n",
      "            if(value not in game_board):\n",
      "                game_board[i][j] = value\n",
      "                j+=1\n",
      "                \n",
      "        i+=1\n",
      "                \n",
      "    print_board(game_board)\n",
      "\n",
      "game()\n",
      "115/8:\n",
      "import random\n",
      "\n",
      "score = 0\n",
      "\n",
      "def random_number_generator():\n",
      "    return random.randint(1,100)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def print_board(game_board):\n",
      "    for i in range(0,3):\n",
      "        for j in range(0,3):\n",
      "            print(game_board[i][j],end = \" \")\n",
      "        print()\n",
      "\n",
      "        \n",
      "def win_lose(game_board):\n",
      "    user_input = int(input(\"Gusse the max value on the board: \"))\n",
      "    if user_input == max(game_board):\n",
      "        return 2\n",
      "    \n",
      "    return -1        \n",
      "\n",
      "def game():\n",
      "    game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "    i = 0\n",
      "    while(i!=3):\n",
      "        j = 0\n",
      "        while(j!=3):\n",
      "            value = random_number_generator()\n",
      "            if(value not in game_board):\n",
      "                game_board[i][j] = value\n",
      "                j+=1          \n",
      "        i+=1\n",
      "    print_board(game_board)\n",
      "    temp_score = win_lose(game_board)\n",
      "    if temp_score == -1:\n",
      "        print(\"Game over\\n\")\n",
      "        print(\"Your score = \",score)\n",
      "    else:\n",
      "        score = score + temp_score\n",
      "        game() \n",
      "    \n",
      "game()\n",
      "115/9:\n",
      "import random\n",
      "\n",
      "\n",
      "def random_number_generator():\n",
      "    return random.randint(1,100)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def print_board(game_board):\n",
      "    for i in range(0,3):\n",
      "        for j in range(0,3):\n",
      "            print(game_board[i][j],end = \" \")\n",
      "        print()\n",
      "\n",
      "        \n",
      "def win_lose(game_board):\n",
      "    user_input = int(input(\"Gusse the max value on the board: \"))\n",
      "    if user_input == max(game_board):\n",
      "        return 2\n",
      "    \n",
      "    return -1        \n",
      "\n",
      "def game(score):\n",
      "    game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "    i = 0\n",
      "    while(i!=3):\n",
      "        j = 0\n",
      "        while(j!=3):\n",
      "            value = random_number_generator()\n",
      "            if(value not in game_board):\n",
      "                game_board[i][j] = value\n",
      "                j+=1          \n",
      "        i+=1\n",
      "    print_board(game_board)\n",
      "    temp_score = win_lose(game_board)\n",
      "    if temp_score == -1:\n",
      "        print(\"Game over\\n\")\n",
      "        print(\"Your score = \",score)\n",
      "    else:\n",
      "        score = score + temp_score\n",
      "        game(score) \n",
      "\n",
      "if __name__ == \"__main__\"\n",
      "    score = 0\n",
      "    game(score)\n",
      "115/10:\n",
      "import random\n",
      "\n",
      "\n",
      "def random_number_generator():\n",
      "    return random.randint(1,100)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def print_board(game_board):\n",
      "    for i in range(0,3):\n",
      "        for j in range(0,3):\n",
      "            print(game_board[i][j],end = \" \")\n",
      "        print()\n",
      "\n",
      "        \n",
      "def win_lose(game_board):\n",
      "    user_input = int(input(\"Gusse the max value on the board: \"))\n",
      "    if user_input == max(game_board):\n",
      "        return 2\n",
      "    \n",
      "    return -1        \n",
      "\n",
      "def game(score):\n",
      "    game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "    i = 0\n",
      "    while(i!=3):\n",
      "        j = 0\n",
      "        while(j!=3):\n",
      "            value = random_number_generator()\n",
      "            if(value not in game_board):\n",
      "                game_board[i][j] = value\n",
      "                j+=1          \n",
      "        i+=1\n",
      "    print_board(game_board)\n",
      "    temp_score = win_lose(game_board)\n",
      "    if temp_score == -1:\n",
      "        print(\"Game over\\n\")\n",
      "        print(\"Your score = \",score)\n",
      "    else:\n",
      "        score = score + temp_score\n",
      "        game(score) \n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    score = 0\n",
      "    game(score)\n",
      "115/11:\n",
      "import random\n",
      "\n",
      "\n",
      "def random_number_generator():\n",
      "    return random.randint(1,100)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def print_board(game_board):\n",
      "    for i in range(0,3):\n",
      "        for j in range(0,3):\n",
      "            print(game_board[i][j],end = \" \")\n",
      "        print()\n",
      "\n",
      "        \n",
      "def win_lose(game_board):\n",
      "    user_input = int(input(\"Gusse the max value on the board: \"))\n",
      "    if user_input == max(game_board):\n",
      "        return 2\n",
      "    \n",
      "    return -1        \n",
      "\n",
      "def game(score):\n",
      "    game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "    i = 0\n",
      "    while(i!=3):\n",
      "        j = 0\n",
      "        while(j!=3):\n",
      "            value = random_number_generator()\n",
      "            if(value not in game_board):\n",
      "                game_board[i][j] = value\n",
      "                j+=1          \n",
      "        i+=1\n",
      "    print_board(game_board)\n",
      "    temp_score = win_lose(game_board)\n",
      "    if temp_score == -1:\n",
      "        print(\"Game over\\n\")\n",
      "        print(\"Your score = \",score)\n",
      "    else:\n",
      "        score = score + temp_score\n",
      "        game(score) \n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    score = 0\n",
      "    game(score)\n",
      "115/12:\n",
      "import random\n",
      "\n",
      "\n",
      "def random_number_generator():\n",
      "    return random.randint(1,100)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def print_board(game_board):\n",
      "    for i in range(0,3):\n",
      "        for j in range(0,3):\n",
      "            print(game_board[i][j],end = \" \")\n",
      "        print()\n",
      "\n",
      "        \n",
      "def win_lose(game_board):\n",
      "    user_input = int(input(\"Gusse the max value on the board: \"))\n",
      "    if user_input == max(game_board):\n",
      "        return 2\n",
      "    \n",
      "    return -1        \n",
      "\n",
      "def game(score):\n",
      "    game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "    i = 0\n",
      "    while(i!=3):\n",
      "        j = 0\n",
      "        while(j!=3):\n",
      "            value = random_number_generator()\n",
      "            if(value not in game_board):\n",
      "                game_board[i][j] = value\n",
      "                j+=1          \n",
      "        i+=1\n",
      "    print_board(game_board)\n",
      "    temp_score = win_lose(game_board)\n",
      "    print(temp_score)\n",
      "    if (temp_score == -1):\n",
      "        print(\"Game over\\n\")\n",
      "        print(\"Your score = \",score)\n",
      "    else:\n",
      "        score = score + temp_score\n",
      "        game(score) \n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    score = 0\n",
      "    game(score)\n",
      "115/13:\n",
      "a = [[1,2,3],[4,5,6],[7,8,9]]\n",
      "max(a)\n",
      "116/1:\n",
      "import random\n",
      "\n",
      "\n",
      "def random_number_generator():\n",
      "    return random.randint(1,100)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def print_board(game_board):\n",
      "    for i in range(0,3):\n",
      "        for j in range(0,3):\n",
      "            print(game_board[i][j],end = \" \")\n",
      "        print()\n",
      "\n",
      "        \n",
      "def win_lose(game_board):\n",
      "    user_input = int(input(\"Gusse the max value on the board: \"))\n",
      "    if ((user_input == max(game_board[0])) or (user_input == max(game_board[1])) or (user_input == max(game_board[2]))) :\n",
      "        return 2\n",
      "    \n",
      "    return -1        \n",
      "\n",
      "def game(score):\n",
      "    game_board = [[0,0,0],[0,0,0],[0,0,0]]\n",
      "    i = 0\n",
      "    while(i!=3):\n",
      "        j = 0\n",
      "        while(j!=3):\n",
      "            value = random_number_generator()\n",
      "            if(value not in game_board):\n",
      "                game_board[i][j] = value\n",
      "                j+=1          \n",
      "        i+=1\n",
      "    print_board(game_board)\n",
      "    temp_score = win_lose(game_board)\n",
      "    print(temp_score)\n",
      "    if (temp_score == -1):\n",
      "        print(\"Game over\\n\")\n",
      "        print(\"Your score = \",score)\n",
      "    else:\n",
      "        score = score + temp_score\n",
      "        game(score) \n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    score = 0\n",
      "    game(score)\n",
      "116/2:\n",
      "a = [[1,2,3],[4,5,6],[7,8,9]]\n",
      "max(a)\n",
      "117/1:\n",
      "import pandas as pd\n",
      "import matplotlib as plt\n",
      "import numpy as np\n",
      "117/2: bihar1 = pd.read_csv(\"Bihar1.csv\")\n",
      "117/3:\n",
      "bihar1 = pd.read_csv(\"Bihar1.csv\")\n",
      "bihar2 = pd.read_csv(\"Bihar2.csv\")\n",
      "jharkhand1 = pd.read_csv(\"Jharkhand1.csv\")\n",
      "jharkhand2 = pd.read_csv(\"Jharkhand2.csv\")\n",
      "117/4:\n",
      "bihar1 = pd.read_csv(\"Bihar1.csv\")\n",
      "bihar2 = pd.read_csv(\"Bihar2.csv\")\n",
      "jharkhand1 = pd.read_csv(\"Jharkhand1.csv\")\n",
      "jharkhand2 = pd.read_csv(\"Jharkhand2.csv\")\n",
      "117/5: plt.scatter(bihar)\n",
      "117/6:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "117/7: plt.scatter(bihar)\n",
      "117/8: plt.scatter(bihar)\n",
      "117/9:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "117/10: plt.scatter(bihar1)\n",
      "117/11: plt.plot(bihar1)\n",
      "117/12: bihar1.head(2)\n",
      "117/13:\n",
      "bihar1_x = bihar1.iloc[:,[0]].values()\n",
      "bihar1_y = bihar1.iloc[:,[1:]].values()\n",
      "plt.scatter(bihar)\n",
      "117/14:\n",
      "bihar1_x = bihar1.iloc[:,[0]].values()\n",
      "bihar1_y = bihar1.iloc[:,[1:]].values()\n",
      "plt.scatter(bihar1_x,bihar_y)\n",
      "117/15:\n",
      "bihar1_x = bihar1.iloc[:,[0]].values()\n",
      "bihar1_y = bihar1.iloc[:,[1:]].values()\n",
      "plt.scatter(bihar1_x,bihar_y)\n",
      "117/16:\n",
      "bihar1_x = bihar1.iloc[:,[0]].values()\n",
      "bihar1_y = bihar1.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12,13]].values()\n",
      "plt.scatter(bihar1_x,bihar_y)\n",
      "117/17:\n",
      "bihar1_x = bihar1.iloc[:,[0]].values()\n",
      "bihar1_y = bihar1.iloc[:,[1]].values()\n",
      "plt.scatter(bihar1_x,bihar_y)\n",
      "117/18:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[1]]\n",
      "plt.scatter(bihar1_x,bihar_y)\n",
      "117/19:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[1]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "117/20:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[1]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[1]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "117/21:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[1]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[1]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "117/22:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[1]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[1]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "117/23:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[1]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[1]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.show()\n",
      "117/24:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[1]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[1]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.show()\n",
      "117/25:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[1]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[1]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.show()\n",
      "plt.grid()\n",
      "117/26:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[1]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[1]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.show()\n",
      "plt.grid()\n",
      "117/27:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[1]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[1]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "118/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "118/2:\n",
      "bihar1 = pd.read_csv(\"Bihar1.csv\")\n",
      "bihar2 = pd.read_csv(\"Bihar2.csv\")\n",
      "jharkhand1 = pd.read_csv(\"Jharkhand1.csv\")\n",
      "jharkhand2 = pd.read_csv(\"Jharkhand2.csv\")\n",
      "118/3: bihar1.head(2)\n",
      "118/4:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[1]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[1]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "118/5:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[2]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[2]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "119/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "119/2:\n",
      "bihar1 = pd.read_csv(\"Bihar1.csv\")\n",
      "bihar2 = pd.read_csv(\"Bihar2.csv\")\n",
      "jharkhand1 = pd.read_csv(\"Jharkhand1.csv\")\n",
      "jharkhand2 = pd.read_csv(\"Jharkhand2.csv\")\n",
      "119/3: bihar1.head(2)\n",
      "119/4:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[14]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[14]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "119/5:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[15]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[15]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "120/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "120/2:\n",
      "bihar1 = pd.read_csv(\"Bihar1.csv\")\n",
      "bihar2 = pd.read_csv(\"Bihar2.csv\")\n",
      "jharkhand1 = pd.read_csv(\"Jharkhand1.csv\")\n",
      "jharkhand2 = pd.read_csv(\"Jharkhand2.csv\")\n",
      "120/3: bihar1.head(2)\n",
      "120/4:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[14]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[14]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "120/5:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[15]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[15]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "120/6:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[16]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[16]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "120/7:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[17]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[17]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "121/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "121/2:\n",
      "bihar1 = pd.read_csv(\"Bihar1.csv\")\n",
      "bihar2 = pd.read_csv(\"Bihar2.csv\")\n",
      "jharkhand1 = pd.read_csv(\"Jharkhand1.csv\")\n",
      "jharkhand2 = pd.read_csv(\"Jharkhand2.csv\")\n",
      "121/3: bihar1.head(2)\n",
      "121/4:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[14]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[14]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "121/5:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[15]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[15]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "121/6:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[16]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[16]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "121/7:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[17]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[17]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "121/8:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[13]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[13]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "123/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "123/2:\n",
      "bihar1 = pd.read_csv(\"Bihar1.csv\")\n",
      "\n",
      "jharkhand1 = pd.read_csv(\"Jharkhand1.csv\")\n",
      "123/3: bihar1.head(2)\n",
      "123/4:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[14]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[14]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "123/5:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[15]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[15]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "123/6:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[16]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[16]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "123/7:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[17]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[17]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "123/8:\n",
      "bihar1_x = bihar1.iloc[:,[0]]\n",
      "bihar1_y = bihar1.iloc[:,[13]]\n",
      "plt.scatter(bihar1_x,bihar1_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand1_x = jharkhand1.iloc[:,[0]]\n",
      "jharkhand1_y = jharkhand1.iloc[:,[13]]\n",
      "plt.scatter(jharkhand1_x,jharkhand1_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1901-1958)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1901-1958)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "122/1: bihar2.head(2)\n",
      "122/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "122/3:\n",
      "\n",
      "bihar2 = pd.read_csv(\"Bihar2.csv\")\n",
      "\n",
      "jharkhand2 = pd.read_csv(\"Jharkhand2.csv\")\n",
      "122/4: bihar2.head(2)\n",
      "122/5:\n",
      "bihar2_x = bihar2.iloc[:,[0]]\n",
      "bihar2_y = bihar2.iloc[:,[14]]\n",
      "plt.scatter(bihar2_x,bihar2_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand2_x = jharkhand2.iloc[:,[0]]\n",
      "jharkhand2_y = jharkhand2.iloc[:,[14]]\n",
      "plt.scatter(jharkhand2_x,jharkhand2_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar1_x,bihar1_y,label = \"Bihar(1959-2015)\")\n",
      "plt.plot(jharkhand1_x,jharkhand1_y,label = \"Jharkhand(1959-2015)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "122/6:\n",
      "bihar2_x = bihar2.iloc[:,[0]]\n",
      "bihar2_y = bihar2.iloc[:,[14]]\n",
      "plt.scatter(bihar2_x,bihar2_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand2_x = jharkhand2.iloc[:,[0]]\n",
      "jharkhand2_y = jharkhand2.iloc[:,[14]]\n",
      "plt.scatter(jharkhand2_x,jharkhand2_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar2_x,bihar2_y,label = \"Bihar(1959-2015)\")\n",
      "plt.plot(jharkhand2_x,jharkhand2_y,label = \"Jharkhand(1959-2015)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "124/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "124/2:\n",
      "\n",
      "bihar2 = pd.read_csv(\"Bihar2.csv\")\n",
      "\n",
      "jharkhand2 = pd.read_csv(\"Jharkhand2.csv\")\n",
      "124/3: bihar2.head(2)\n",
      "124/4:\n",
      "bihar2_x = bihar2.iloc[:,[0]]\n",
      "bihar2_y = bihar2.iloc[:,[14]]\n",
      "plt.scatter(bihar2_x,bihar2_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand2_x = jharkhand2.iloc[:,[0]]\n",
      "jharkhand2_y = jharkhand2.iloc[:,[14]]\n",
      "plt.scatter(jharkhand2_x,jharkhand2_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar2_x,bihar2_y,label = \"Bihar(1959-2015)\")\n",
      "plt.plot(jharkhand2_x,jharkhand2_y,label = \"Jharkhand(1959-2015)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "124/5:\n",
      "bihar2_x = bihar2.iloc[:,[0]]\n",
      "bihar2_y = bihar2.iloc[:,[15]]\n",
      "plt.scatter(bihar2_x,bihar2_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand2_x = jharkhand2.iloc[:,[0]]\n",
      "jharkhand2_y = jharkhand2.iloc[:,[15]]\n",
      "plt.scatter(jharkhand2_x,jharkhand2_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar2_x,bihar2_y,label = \"Bihar(1959-2015)\")\n",
      "plt.plot(jharkhand2_x,jharkhand2_y,label = \"Jharkhand(1959-2015)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "124/6:\n",
      "bihar2_x = bihar2.iloc[:,[0]]\n",
      "bihar2_y = bihar2.iloc[:,[16]]\n",
      "plt.scatter(bihar2_x,bihar2_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand2_x = jharkhand2.iloc[:,[0]]\n",
      "jharkhand2_y = jharkhand2.iloc[:,[16]]\n",
      "plt.scatter(jharkhand2_x,jharkhand2_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar2_x,bihar2_y,label = \"Bihar(1959-2015)\")\n",
      "plt.plot(jharkhand2_x,jharkhand2_y,label = \"Jharkhand(1959-2015)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "124/7:\n",
      "bihar2_x = bihar2.iloc[:,[0]]\n",
      "bihar2_y = bihar2.iloc[:,[17]]\n",
      "plt.scatter(bihar2_x,bihar2_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand2_x = jharkhand2.iloc[:,[0]]\n",
      "jharkhand2_y = jharkhand2.iloc[:,[17]]\n",
      "plt.scatter(jharkhand2_x,jharkhand2_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar2_x,bihar2_y,label = \"Bihar(1959-2015)\")\n",
      "plt.plot(jharkhand2_x,jharkhand2_y,label = \"Jharkhand(1959-2015)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "124/8:\n",
      "bihar2_x = bihar2.iloc[:,[0]]\n",
      "bihar2_y = bihar2.iloc[:,[13]]\n",
      "plt.scatter(bihar2_x,bihar2_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand2_x = jharkhand2.iloc[:,[0]]\n",
      "jharkhand2_y = jharkhand2.iloc[:,[13]]\n",
      "plt.scatter(jharkhand2_x,jharkhand2_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar2_x,bihar2_y,label = \"Bihar(1959-2015)\")\n",
      "plt.plot(jharkhand2_x,jharkhand2_y,label = \"Jharkhand(1959-2015)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "124/9: bihar2.tail(2)\n",
      "125/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "125/2:\n",
      "bihar3 = pd.read_csv(\"Bihar3.csv\")\n",
      "\n",
      "\n",
      "jharkhand3 = pd.read_csv(\"Jharkhand3.csv\")\n",
      "125/3:\n",
      "bihar3 = pd.read_csv(\"Bihar3(2016-2018).csv\")\n",
      "\n",
      "\n",
      "jharkhand3 = pd.read_csv(\"Jharkhand3(2016-2018).csv\")\n",
      "125/4: bihar3.head()\n",
      "125/5:\n",
      "bihar3_x = bihar3.iloc[:,[0]]\n",
      "bihar3_y = bihar3.iloc[:,[14]]\n",
      "plt.scatter(bihar3_x,bihar3_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand3_x = jharkhand3.iloc[:,[0]]\n",
      "jharkhand3_y = jharkhand3.iloc[:,[14]]\n",
      "plt.scatter(jharkhand3_x,jharkhand3_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar3_x,bihar3_y,label = \"Bihar(2016-2018)\")\n",
      "plt.plot(jharkhand3_x,jharkhand3_y,label = \"Jharkhand(2016-2018)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "125/6:\n",
      "print(bihar3.head())\n",
      "jharkhand3\n",
      "125/7:\n",
      "bihar3_x = bihar3.iloc[:,[0]]\n",
      "bihar3_y = bihar3.iloc[:,[15]]\n",
      "plt.scatter(bihar3_x,bihar3_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand3_x = jharkhand3.iloc[:,[0]]\n",
      "jharkhand3_y = jharkhand3.iloc[:,[15]]\n",
      "plt.scatter(jharkhand3_x,jharkhand3_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar3_x,bihar3_y,label = \"Bihar(2016-2018)\")\n",
      "plt.plot(jharkhand3_x,jharkhand3_y,label = \"Jharkhand(2016-2018)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "126/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "126/2:\n",
      "bihar3 = pd.read_csv(\"Bihar3(2016-2018).csv\")\n",
      "\n",
      "\n",
      "jharkhand3 = pd.read_csv(\"Jharkhand3(2016-2018).csv\")\n",
      "126/3:\n",
      "print(bihar3.head())\n",
      "jharkhand3\n",
      "126/4:\n",
      "bihar3_x = bihar3.iloc[:,[0]]\n",
      "bihar3_y = bihar3.iloc[:,[14]]\n",
      "plt.scatter(bihar3_x,bihar3_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand3_x = jharkhand3.iloc[:,[0]]\n",
      "jharkhand3_y = jharkhand3.iloc[:,[14]]\n",
      "plt.scatter(jharkhand3_x,jharkhand3_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar3_x,bihar3_y,label = \"Bihar(2016-2018)\")\n",
      "plt.plot(jharkhand3_x,jharkhand3_y,label = \"Jharkhand(2016-2018)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "126/5:\n",
      "bihar3_x = bihar3.iloc[:,[0]]\n",
      "bihar3_y = bihar3.iloc[:,[15]]\n",
      "plt.scatter(bihar3_x,bihar3_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand3_x = jharkhand3.iloc[:,[0]]\n",
      "jharkhand3_y = jharkhand3.iloc[:,[15]]\n",
      "plt.scatter(jharkhand3_x,jharkhand3_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar3_x,bihar3_y,label = \"Bihar(2016-2018)\")\n",
      "plt.plot(jharkhand3_x,jharkhand3_y,label = \"Jharkhand(2016-2018)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "126/6:\n",
      "bihar3_x = bihar3.iloc[:,[0]]\n",
      "bihar3_y = bihar3.iloc[:,[16]]\n",
      "plt.scatter(bihar3_x,bihar3_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand3_x = jharkhand3.iloc[:,[0]]\n",
      "jharkhand3_y = jharkhand3.iloc[:,[16]]\n",
      "plt.scatter(jharkhand3_x,jharkhand3_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar3_x,bihar3_y,label = \"Bihar(2016-2018)\")\n",
      "plt.plot(jharkhand3_x,jharkhand3_y,label = \"Jharkhand(2016-2018)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "126/7:\n",
      "bihar3_x = bihar3.iloc[:,[0]]\n",
      "bihar3_y = bihar3.iloc[:,[17]]\n",
      "plt.scatter(bihar3_x,bihar3_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand3_x = jharkhand3.iloc[:,[0]]\n",
      "jharkhand3_y = jharkhand3.iloc[:,[17]]\n",
      "plt.scatter(jharkhand3_x,jharkhand3_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar3_x,bihar3_y,label = \"Bihar(2016-2018)\")\n",
      "plt.plot(jharkhand3_x,jharkhand3_y,label = \"Jharkhand(2016-2018)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "126/8:\n",
      "bihar3_x = bihar3.iloc[:,[0]]\n",
      "bihar3_y = bihar3.iloc[:,[13]]\n",
      "plt.scatter(bihar3_x,bihar3_y)\n",
      "plt.title(\"BIHAR\")\n",
      "plt.show()\n",
      "\n",
      "jharkhand3_x = jharkhand3.iloc[:,[0]]\n",
      "jharkhand3_y = jharkhand3.iloc[:,[13]]\n",
      "plt.scatter(jharkhand3_x,jharkhand3_y)\n",
      "plt.title(\"JHARKHAND\")\n",
      "plt.show()\n",
      "\n",
      "plt.plot(bihar3_x,bihar3_y,label = \"Bihar(2016-2018)\")\n",
      "plt.plot(jharkhand3_x,jharkhand3_y,label = \"Jharkhand(2016-2018)\" )\n",
      "plt.legend()\n",
      "plt.grid()\n",
      "plt.show()\n",
      "128/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "128/2:\n",
      "data = pd.read_csv(\"practice.csv\")\n",
      "data.head()\n",
      "128/3:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis\n",
      "\n",
      "iq1 = iq1.reshape(-1,1)\n",
      "iq2 = iq2.reshape(-1,1)                                              # IQ1 x IQ2\n",
      "\n",
      "plt.scatter(iq1,iq2,s=30)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ2\", fontsize = 20)\n",
      "plt.grid()\n",
      "128/4:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq3)                                   # IQ1 x IQ3\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.grid()\n",
      "128/5:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq3 as y-axis\n",
      "iq4 = iq4.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq4)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)                                    # IQ1 x IQ4\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "plt.grid()\n",
      "128/6:\n",
      "iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq1,iq5)\n",
      "plt.xlabel(\"IQ1\", fontsize = 20)                                   # IQ1 x IQ5\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "128/7:\n",
      "\n",
      "plt.scatter(iq1,iq2,label = \"IQ2\",alpha = 0.6)\n",
      "plt.scatter(iq1,iq3, label = \"IQ3\",alpha = 0.6)\n",
      "plt.scatter(iq1,iq4,label = \"IQ4\",alpha = 0.6) \n",
      "plt.scatter(iq1,iq5,label = \"IQ5\",alpha = 0.6)\n",
      "plt.xlabel(\"IQ1\",fontsize = 20)\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "128/8:\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x\n",
      "iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "iq3 = iq3.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq3)\n",
      "plt.xlabel(\"IQ2\", fontsize = 20)                                   # IQ2 x IQ3\n",
      "plt.ylabel(\"IQ3\", fontsize = 20)\n",
      "plt.grid()\n",
      "128/9:\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq as y-axis\n",
      "iq4 = iq4.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq4)\n",
      "plt.xlabel(\"IQ2\", fontsize = 20)                                   # IQ2 x IQ4\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "plt.grid()\n",
      "128/10:\n",
      "iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq2,iq5)\n",
      "plt.xlabel(\"IQ2\", fontsize = 20)                                   # IQ2 x IQ5\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "128/11:\n",
      "plt.scatter(iq2,iq3, label = \"IQ3\",alpha = 0.6)\n",
      "plt.scatter(iq2,iq4,label = \"IQ4\",alpha = 0.6) \n",
      "plt.scatter(iq2,iq5,label = \"IQ5\",alpha = 0.6)\n",
      "plt.xlabel(\"IQ2\",fontsize=20)\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "128/12:\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis\n",
      "iq4 = iq4.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq3,iq4)\n",
      "plt.xlabel(\"IQ3\", fontsize = 20)                                   # IQ3 x IQ4\n",
      "plt.ylabel(\"IQ4\", fontsize = 20)\n",
      "plt.grid()\n",
      "128/13:\n",
      "iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq3,iq5)\n",
      "plt.xlabel(\"IQ3\", fontsize = 20)                                   # IQ3 x IQ5\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "128/14:\n",
      "plt.scatter(iq3,iq4,label = \"IQ4\",alpha = 0.6) \n",
      "plt.scatter(iq3,iq5,label = \"IQ5\",alpha = 0.6)\n",
      "plt.xlabel(\"IQ3\",fontsize=20)\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "128/15:\n",
      "iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as x\n",
      "iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis\n",
      "iq5 = iq5.reshape(-1,1)\n",
      "\n",
      "plt.scatter(iq4,iq5)\n",
      "plt.xlabel(\"IQ4\", fontsize = 20)                                   # IQ4 x IQ5\n",
      "plt.ylabel(\"IQ5\", fontsize = 20)\n",
      "plt.grid()\n",
      "128/16:\n",
      "train_x,test_x,train_y,test_y = train_test_split(iq1,iq2,test_size=0.2,random_state=42)\n",
      "model1 = LinearRegression()\n",
      "model1.fit(train_x,train_y)\n",
      "predict_y = model1.predict(test_x)\n",
      "plt.scatter(test_x,test_y)\n",
      "plt.plot(test_x,predict_y,color=\"m\")\n",
      "128/17:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(test_y,predict_y))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(test_y,predict_y))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(test_y,predict_y))\n",
      "129/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "129/2:\n",
      "bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "jharkhand  = pd.read_csv(\"Jharkhand_entier.csv\")\n",
      "129/3: bihar.head(2)\n",
      "129/4:\n",
      "x_axis = bihar.iloc[:,[0]]\n",
      "bihar_y = bihar.iloc[:,[13]]\n",
      "jharkhand_y = jharkhand.iloc[:,[13]]\n",
      "129/5:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plt.scatter(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.scatter(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "129/6:\n",
      "plt.scatter(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.scatter(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.legend()\n",
      "129/7:\n",
      "plt.scatter(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.scatter(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.legend()\n",
      "129/8:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.legend()\n",
      "129/9: bihar.tail(10)\n",
      "130/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "130/2:\n",
      "bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "jharkhand  = pd.read_csv(\"Jharkhand_entier.csv\")\n",
      "130/3: bihar.tail(10)\n",
      "130/4:\n",
      "x_axis = bihar.iloc[:,[0]]\n",
      "bihar_y = bihar.iloc[:,[13]]\n",
      "jharkhand_y = jharkhand.iloc[:,[13]]\n",
      "130/5:\n",
      "plt.scatter(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.scatter(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.legend()\n",
      "130/6:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.legend()\n",
      "130/7:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"Year(1901-2018)\")\n",
      "plt.ylabel(\"Rainfall\")\n",
      "plt.legend()\n",
      "130/8:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"Year(1901-2018)\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "130/9:\n",
      "x_axis = bihar.iloc[[115:],[0]]\n",
      "bihar_y = bihar.iloc[:,[13]]\n",
      "jharkhand_y = jharkhand.iloc[[115]:],[13]]\n",
      "130/10:\n",
      "plt.scatter(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.scatter(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.legend()\n",
      "130/11:\n",
      "plt.scatter(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.scatter(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.legend()\n",
      "130/12:\n",
      "x_axis = bihar.iloc[[115:],[0]]\n",
      "bihar_y = bihar.iloc[:,[13]]\n",
      "jharkhand_y = jharkhand.iloc[[115:],[13]]\n",
      "130/13:\n",
      "x_axis = bihar.iloc[[115:],[0]]\n",
      "bihar_y = bihar.iloc[:,[13]]\n",
      "jharkhand_y = jharkhand.iloc[115:,[13]]\n",
      "130/14:\n",
      "x_axis = bihar.iloc[:,[0]]\n",
      "bihar_y = bihar.iloc[:,[13]]\n",
      "jharkhand_y = jharkhand.iloc[:,[13]]\n",
      "131/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "131/2:\n",
      "bihar = pd.read_csv(\"Bihar3.csv\")\n",
      "jharkhand  = pd.read_csv(\"Jharkhand3.csv\")\n",
      "131/3:\n",
      "bihar = pd.read_csv(\"Bihar3(2016-2018).csv\")\n",
      "jharkhand  = pd.read_csv(\"Jharkhand3(2016-2018).csv\")\n",
      "132/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "132/2:\n",
      "bihar = pd.read_csv(\"Bihar3(2016-2018).csv\")\n",
      "jharkhand  = pd.read_csv(\"Jharkhand3(2016-2018).csv\")\n",
      "132/3: bihar.tail(10)\n",
      "132/4:\n",
      "x_axis = bihar.iloc[:,[0]]\n",
      "bihar_y = bihar.iloc[:,[13]]\n",
      "jharkhand_y = jharkhand.iloc[:,[13]]\n",
      "132/5:\n",
      "plt.scatter(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.scatter(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.legend()\n",
      "132/6:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"Year(1901-2018)\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "132/7: jharkhand.tail(10)\n",
      "132/8:\n",
      "x_axis = bihar.iloc[:,[0]]\n",
      "bihar_y = bihar.iloc[:,[6]]\n",
      "jharkhand_y = jharkhand.iloc[:,[6]]\n",
      "132/9:\n",
      "plt.scatter(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.scatter(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.legend()\n",
      "132/10:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"Year(1901-2018)\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "133/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "133/2:\n",
      "bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "jharkhand  = pd.read_csv(\"Jharkhand_entier.csv\")\n",
      "133/3: jharkhand.tail(10)\n",
      "133/4:\n",
      "x_axis = bihar.iloc[:,[0]]\n",
      "bihar_y = bihar.iloc[:,[6]]\n",
      "jharkhand_y = jharkhand.iloc[:,[6]]\n",
      "133/5:\n",
      "plt.scatter(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.scatter(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.legend()\n",
      "133/6:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"Year(1901-2018)\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "133/7:\n",
      "x_axis = bihar.iloc[:,[0]]\n",
      "bihar_y = bihar.iloc[:,[7]]\n",
      "jharkhand_y = jharkhand.iloc[:,[7]]\n",
      "133/8:\n",
      "plt.scatter(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.scatter(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.legend()\n",
      "133/9:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"Year(1901-2018)\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "133/10:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"July\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "133/11:\n",
      "#plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"July\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "134/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "134/2:\n",
      "bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "jharkhand  = pd.read_csv(\"Jharkhand_entier.csv\")\n",
      "134/3: jharkhand.tail(10)\n",
      "134/4:\n",
      "x_axis = bihar.iloc[:,[0]]\n",
      "bihar_y = bihar.iloc[:,[6]]\n",
      "jharkhand_y = jharkhand.iloc[:,[6]]\n",
      "134/5:\n",
      "#plt.scatter(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.scatter(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "#plt.legend()\n",
      "134/6:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"June\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "134/7:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"June\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "avg(bihar[\"Jun\"])\n",
      "134/8:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"June\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "bihar[\"Jun\"].mean()\n",
      "134/9:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"June\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "bihar[\"JUN\"].mean()\n",
      "134/10:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"June\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "bihar[\"JUN\"].avg()\n",
      "134/11:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"June\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "bihar[\"JUN\"].average()\n",
      "134/12:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"June\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "134/13:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"June\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "bihar[\"JUN\"].mean()\n",
      "134/14:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"June\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "bihar[\"JUL\"].mean()\n",
      "134/15:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"June\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "#bihar[\"JUL\"].mean()\n",
      "134/16:\n",
      "#plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"June\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "#bihar[\"JUL\"].mean()\n",
      "134/17:\n",
      "x_axis = bihar.iloc[:,[0]]\n",
      "bihar_y = bihar.iloc[:,[7]]\n",
      "jharkhand_y = jharkhand.iloc[:,[7]]\n",
      "134/18:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"June\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "bihar[\"JUL\"].mean()\n",
      "134/19:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"July\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "bihar[\"JUL\"].mean()\n",
      "134/20:\n",
      "#plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"July\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "bihar[\"JUL\"].mean()\n",
      "134/21:\n",
      "#plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"July\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "jharkhand[\"JUL\"].mean()\n",
      "134/22:\n",
      "x_axis = bihar.iloc[:,[0]]\n",
      "bihar_y = bihar.iloc[:,[8]]\n",
      "jharkhand_y = jharkhand.iloc[:,[8]]\n",
      "134/23:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"July\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "bihar[\"AUG\"].mean()\n",
      "jharkhand[\"JUL\"].mean()\n",
      "134/24:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"July\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "bihar[\"AUG\"].mean()\n",
      "#jharkhand[\"AUG\"].mean()\n",
      "134/25:\n",
      "#plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"July\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "#bihar[\"AUG\"].mean()\n",
      "jharkhand[\"AUG\"].mean()\n",
      "134/26:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"September\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "bihar[\"SEP\"].mean()\n",
      "#jharkhand[\"AUG\"].mean()\n",
      "134/27:\n",
      "#plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"September\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "#bihar[\"SEP\"].mean()\n",
      "jharkhand[\"SEP\"].mean()\n",
      "134/28:\n",
      "x_axis = bihar.iloc[:,[0]]\n",
      "bihar_y = bihar.iloc[:,[8]]\n",
      "jharkhand_y = jharkhand.iloc[:,[8]]\n",
      "134/29:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"August\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "bihar[\"SEP\"].mean()\n",
      "#jharkhand[\"SEP\"].mean()\n",
      "134/30:\n",
      "plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "#plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"August\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "bihar[\"AUG\"].mean()\n",
      "#jharkhand[\"SEP\"].mean()\n",
      "134/31:\n",
      "#plt.plot(x_axis,bihar_y,label=\"Bihar\")\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"August\",fontsize = 20)\n",
      "plt.ylabel(\"Rainfall\",fontsize = 20)\n",
      "plt.legend()\n",
      "#bihar[\"AUG\"].mean()\n",
      "jharkhand[\"SEP\"].mean()\n",
      "134/32:\n",
      "x_axis = bihar.iloc[:,[0]]\n",
      "bihar_y = bihar.iloc[:,[6]]\n",
      "jharkhand_y = jharkhand.iloc[:,[6]]\n",
      "134/33:\n",
      "print(bihar[\"JUN\"].mean())\n",
      "jharkhand[\"JUN\"].mean()\n",
      "134/34:\n",
      "print(bihar[\"JUN\"].mean())\n",
      "jharkhand[\"AUG\"].mean()\n",
      "134/35: plt.plt(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "134/36: plt.pl0t(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "134/37: plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "134/38:\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"Rainfall\")\n",
      "plt.ylabel(\"August\")\n",
      "plt.legend()\n",
      "134/39:\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"Rainfall\",fontsize = 20)\n",
      "plt.ylabel(\"August\",fontsize = 20)\n",
      "plt.legend()\n",
      "135/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "135/2:\n",
      "bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "jharkhand  = pd.read_csv(\"Jharkhand_entier.csv\")\n",
      "135/3: jharkhand.tail(10)\n",
      "135/4:\n",
      "x_axis = bihar.iloc[:,[0]]\n",
      "bihar_y = bihar.iloc[:,[6]]\n",
      "jharkhand_y = jharkhand.iloc[:,[6]]\n",
      "135/5:\n",
      "plt.plot(x_axis,jharkhand_y,label=\"Jharkhand\")\n",
      "plt.xlabel(\"Rainfall\",fontsize = 20)\n",
      "plt.ylabel(\"August\",fontsize = 20)\n",
      "plt.legend()\n",
      "135/6:\n",
      "print(bihar[\"JUN\"].mean())\n",
      "jharkhand[\"AUG\"].mean()\n",
      "135/7: jharkhand.tail(2)\n",
      "135/8:\n",
      "plt.plot(bihar[\"YEAR\"],bihar[\"JUN\"],label = \"Bihar\")\n",
      "plt.xlabel(\"JUNE\",fontsize=20)\n",
      "plt.ylabel(\"Rainfall\",fontsize=20)\n",
      "plt.legend()\n",
      "135/9:\n",
      "plt.plot(bihar[\"YEAR\"],bihar[\"JUL\"],label = \"Bihar\")\n",
      "plt.xlabel(\"JULY\",fontsize=20)\n",
      "plt.ylabel(\"Rainfall\",fontsize=20)\n",
      "plt.legend()\n",
      "135/10:\n",
      "plt.plot(bihar[\"YEAR\"],bihar[\"AUG\"],label = \"Bihar\")\n",
      "plt.xlabel(\"August\",fontsize=20)\n",
      "plt.ylabel(\"Rainfall\",fontsize=20)\n",
      "plt.legend()\n",
      "136/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "136/2:\n",
      "bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "jharkhand  = pd.read_csv(\"Jharkhand_entier.csv\")\n",
      "136/3: jharkhand.tail(2)\n",
      "136/4:\n",
      "plt.plot(bihar[\"YEAR\"],bihar[\"JUN\"],label = \"Bihar\")\n",
      "plt.xlabel(\"JUNE\",fontsize=20)\n",
      "plt.ylabel(\"Rainfall\",fontsize=20)\n",
      "plt.legend()\n",
      "136/5:\n",
      "plt.plot(bihar[\"YEAR\"],bihar[\"JUL\"],label = \"Bihar\")\n",
      "plt.xlabel(\"JULY\",fontsize=20)\n",
      "plt.ylabel(\"Rainfall\",fontsize=20)\n",
      "plt.legend()\n",
      "136/6:\n",
      "plt.plot(bihar[\"YEAR\"],bihar[\"AUG\"],label = \"Bihar\")\n",
      "plt.xlabel(\"AUGUST\",fontsize=20)\n",
      "plt.ylabel(\"Rainfall\",fontsize=20)\n",
      "plt.legend()\n",
      "136/7:\n",
      "plt.plot(bihar[\"YEAR\"],bihar[\"SEP\"],label = \"Bihar\")\n",
      "plt.xlabel(\"SEPTEMBER\",fontsize=20)\n",
      "plt.ylabel(\"Rainfall\",fontsize=20)\n",
      "plt.legend()\n",
      "137/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "137/2:\n",
      "#bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "bihar  = pd.read_csv(\"Jharkhand_entier.csv\")\n",
      "137/3: jharkhand.tail(2)\n",
      "138/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "138/2:\n",
      "#bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "bihar  = pd.read_csv(\"Jharkhand_entier.csv\")\n",
      "138/3: bihar.tail(2)\n",
      "138/4:\n",
      "plt.plot(bihar[\"YEAR\"],bihar[\"JUN\"],label = \"Jharkhand\")\n",
      "plt.xlabel(\"JUNE\",fontsize=20)\n",
      "plt.ylabel(\"Rainfall\",fontsize=20)\n",
      "plt.legend()\n",
      "138/5:\n",
      "plt.plot(bihar[\"YEAR\"],bihar[\"JUL\"],label = \"Jharkhand\")\n",
      "plt.xlabel(\"JULY\",fontsize=20)\n",
      "plt.ylabel(\"Rainfall\",fontsize=20)\n",
      "plt.legend()\n",
      "138/6:\n",
      "plt.plot(bihar[\"YEAR\"],bihar[\"AUG\"],label = \"Jharkhand\")\n",
      "plt.xlabel(\"AUGUST\",fontsize=20)\n",
      "plt.ylabel(\"Rainfall\",fontsize=20)\n",
      "plt.legend()\n",
      "138/7:\n",
      "plt.plot(bihar[\"YEAR\"],bihar[\"SEP\"],label = \"Jharkhand\")\n",
      "plt.xlabel(\"SEPTEMBER\",fontsize=20)\n",
      "plt.ylabel(\"Rainfall\",fontsize=20)\n",
      "plt.legend()\n",
      "139/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "139/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "139/3: bihar.head(2)\n",
      "139/4: x_data = bihar[[\"JAN\"],[\"FEB\"]]\n",
      "139/5: x_data = bihar[[\"JAN\",\"FEB\"]]\n",
      "139/6:\n",
      "x_data = bihar[[\"JAN\",\"FEB\"]]\n",
      "x_data\n",
      "139/7:\n",
      "x_data = bihar[[\"JAN\",\"FEB\",\"MAR\",\"APR\",\"MAY\",\"JUN\"]]\n",
      "y_data = bihar[\"YEAR\"]\n",
      "139/8: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "139/9:\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "139/10: y_predict = model.predict(x_test)\n",
      "139/11: y_predict[0]\n",
      "139/12: y_predict[0]\n",
      "139/13: x_test[0]\n",
      "139/14: x_test[[0]]\n",
      "139/15: x_test\n",
      "139/16: y_predict\n",
      "139/17: bihar.head(56)\n",
      "139/18: bihar.head(60)\n",
      "140/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "140/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "140/3: bihar.head(60)\n",
      "140/4:\n",
      "x_data = bihar[[\"JAN\",\"FEB\",\"MAR\",\"APR\",\"MAY\",\"JUN\"]]\n",
      "y_data = bihar[\"JUL\"]\n",
      "140/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "140/6:\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "140/7: y_predict = model.predict(x_test)\n",
      "140/8: y_predict\n",
      "140/9: x_test\n",
      "141/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "141/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "141/3: bihar.head(2)\n",
      "141/4:\n",
      "x_data = bihar[[\"JAN\",\"FEB\",\"MAR\",\"APR\",\"MAY\",\"JUN\"]]\n",
      "y_data = bihar[\"JUL\"]\n",
      "141/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "141/6:\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "141/7: y_predict = model.predict(x_test)\n",
      "141/8: y_predict\n",
      "142/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "142/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "142/3: bihar.head(2)\n",
      "142/4:\n",
      "x_data = bihar[[\"JAN\",\"FEB\",\"MAR\",\"APR\",\"MAY\",\"JUN\"]]\n",
      "y_data = bihar[\"JUL\"]\n",
      "142/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "142/6:\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "142/7: y_predict = model.predict(x_test)\n",
      "142/8: y_predict\n",
      "142/9:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "142/10: y_test\n",
      "142/11: list(y_test)\n",
      "143/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "143/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "143/3: bihar.head(2)\n",
      "143/4:\n",
      "x_data = bihar[[\"JAN\",\"FEB\",\"MAR\",\"APR\",\"MAY\",\"JUN\"]]\n",
      "y_data = bihar[\"JUL\"]\n",
      "143/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "143/6:\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "143/7: y_predict = model.predict(x_test)\n",
      "143/8: y_predict\n",
      "143/9:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "144/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "144/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "144/3: bihar.head(2)\n",
      "144/4:\n",
      "x_data = bihar[[\"JAN\",\"FEB\",\"MAR\",\"APR\",\"MAY\",\"JUN\"]]\n",
      "y_data = bihar[\"JUL\"]\n",
      "144/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "144/6:\n",
      "model = DecisionTreeRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "144/7: y_predict = model.predict(x_test)\n",
      "144/8: y_predict\n",
      "144/9:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "145/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "145/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "145/3: bihar.head(2)\n",
      "145/4:\n",
      "x_data = bihar[[\"JAN\",\"FEB\",\"MAR\",\"APR\",\"MAY\",\"JUN\"]]\n",
      "y_data = bihar[\"JUL\"]\n",
      "145/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "145/6:\n",
      "model = RandomForestRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "145/7: y_predict = model.predict(x_test)\n",
      "145/8: y_predict\n",
      "145/9:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "145/10: y_test\n",
      "146/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "146/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "146/3: bihar.head(2)\n",
      "146/4:\n",
      "x_data = bihar[[\"JAN\",\"FEB\",\"MAR\",\"APR\",\"MAY\",\"JUN\"]]\n",
      "y_data = bihar[\"JUL\"]\n",
      "146/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "146/6:\n",
      "model = DecisionTreeRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "146/7: y_predict = model.predict(x_test)\n",
      "146/8: y_predict\n",
      "146/9: y_test\n",
      "146/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "147/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "147/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "147/3: bihar.head(2)\n",
      "147/4:\n",
      "x_data = bihar[\"JUN\"]\n",
      "y_data = bihar[\"JUL\"]\n",
      "147/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "147/6:\n",
      "model = DecisionTreeRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "147/7:\n",
      "x_data = bihar[\"JUN\"].reshape(-1,1)\n",
      "y_data = bihar[\"JUL\"].reshape(-1,1)\n",
      "147/8:\n",
      "x_data = bihar[\"JUN\"]\n",
      "x_data = x_data.reshape(-1,1)\n",
      "y_data = bihar[\"JUL\"].reshape(-1,1)\n",
      "147/9:\n",
      "x_data = bihar[[\"JUN\"]]\n",
      "\n",
      "y_data = bihar[[\"JUL\"]]\n",
      "148/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "148/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "148/3: bihar.head(2)\n",
      "148/4:\n",
      "x_data = bihar[[\"JUN\"]]\n",
      "\n",
      "y_data = bihar[[\"JUL\"]]\n",
      "148/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "148/6:\n",
      "model = DecisionTreeRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "148/7: y_predict = model.predict(x_test)\n",
      "148/8: y_predict\n",
      "148/9: y_test\n",
      "148/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "149/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "149/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "149/3: bihar.head(2)\n",
      "149/4:\n",
      "x_data = bihar[[\"JUN\"]]\n",
      "\n",
      "y_data = bihar[[\"JUL\"]]\n",
      "149/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "149/6:\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "149/7: y_predict = model.predict(x_test)\n",
      "149/8: y_predict\n",
      "149/9: y_test\n",
      "149/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "149/11:\n",
      "x_data = bihar[[\"JAN\",\"FEB\",\"MAR\",\"APR\",\"JUN\"]]\n",
      "y_data = bihar[[\"JUL\"]]\n",
      "150/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "150/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "150/3: bihar.head(2)\n",
      "150/4:\n",
      "x_data = bihar[[\"JAN\",\"FEB\",\"MAR\",\"APR\",\"JUN\"]]\n",
      "y_data = bihar[[\"JUL\"]]\n",
      "150/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "150/6:\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "150/7: y_predict = model.predict(x_test)\n",
      "150/8: y_predict\n",
      "150/9: y_test\n",
      "150/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "151/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "151/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "151/3: bihar.head(2)\n",
      "151/4:\n",
      "x_data = bihar[[\"JAN\",\"FEB\",\"MAR\",\"APR\",\"JUN\"]]\n",
      "y_data = bihar[[\"JUL\"]]\n",
      "151/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "151/6:\n",
      "model = DecisionTreeRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "151/7: y_predict = model.predict(x_test)\n",
      "151/8: y_predict\n",
      "151/9: y_test\n",
      "151/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "152/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "152/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "152/3: bihar.head(2)\n",
      "152/4:\n",
      "x_data = bihar[[\"JAN\",\"FEB\",\"MAR\",\"APR\",\"JUN\"]]\n",
      "y_data = bihar[[\"JUL\"]]\n",
      "152/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "152/6:\n",
      "model = DecisionTreeRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "152/7: y_predict = model.predict(x_test)\n",
      "152/8: y_predict\n",
      "152/9: y_test\n",
      "152/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "153/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "153/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "153/3: bihar.head(2)\n",
      "153/4:\n",
      "x_data = bihar[[\"JAN\",\"FEB\",\"MAR\",\"APR\",\"JUN\"]]\n",
      "y_data = bihar[[\"JUL\"]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "153/6:\n",
      "model = SVR()\n",
      "model.fit(x_train,y_train)\n",
      "153/7: y_predict = model.predict(x_test)\n",
      "153/8: y_predict\n",
      "153/9: y_test\n",
      "153/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "154/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "154/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "154/3: bihar.head(2)\n",
      "154/4:\n",
      "x_data = bihar[[\"Jan-Feb\",\"Mar-May\"]]\n",
      "y_data = bihar[[\"Jun-Sep\"]]\n",
      "154/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "154/6:\n",
      "model = SVR()\n",
      "model.fit(x_train,y_train)\n",
      "154/7: y_predict = model.predict(x_test)\n",
      "154/8: y_predict\n",
      "154/9: y_test\n",
      "154/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "155/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "155/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "155/3: bihar.head(2)\n",
      "155/4:\n",
      "x_data = bihar[[\"Jan-Feb\",\"Mar-May\"]]\n",
      "y_data = bihar[[\"Jun-Sep\"]]\n",
      "155/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "155/6:\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "155/7: y_predict = model.predict(x_test)\n",
      "155/8: y_predict\n",
      "155/9: y_test\n",
      "155/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "156/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "156/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "156/3: bihar.head(2)\n",
      "156/4:\n",
      "x_data = bihar[[\"Jan-Feb\",\"Mar-May\"]]\n",
      "y_data = bihar[[\"Jun-Sep\"]]\n",
      "156/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "156/6:\n",
      "model = DecisionTreeRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "156/7: y_predict = model.predict(x_test)\n",
      "156/8: y_predict\n",
      "156/9: y_test\n",
      "156/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "157/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "157/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "157/3: bihar.head(2)\n",
      "157/4:\n",
      "x_data = bihar[[\"Jan-Feb\",\"Mar-May\"]]\n",
      "y_data = bihar[[\"Jun-Sep\"]]\n",
      "157/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "157/6:\n",
      "model = RandomForestRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "157/7: y_predict = model.predict(x_test)\n",
      "157/8: y_predict\n",
      "157/9: y_test\n",
      "157/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "158/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "158/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "158/3: bihar.head(2)\n",
      "158/4:\n",
      "x_data = bihar[[\"Jan-Feb\",\"Mar-May\"]]\n",
      "y_data = bihar[[\"Jun-Sep\"]]\n",
      "158/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "158/6:\n",
      "model = SVR()\n",
      "model.fit(x_train,y_train)\n",
      "158/7: y_predict = model.predict(x_test)\n",
      "158/8: y_predict\n",
      "158/9: y_test\n",
      "158/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "159/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "159/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "159/3: bihar.head(2)\n",
      "159/4:\n",
      "x_data = bihar[[\"Mar-May\"]]\n",
      "y_data = bihar[[\"Jun-Sep\"]]\n",
      "159/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "159/6:\n",
      "model = SVR()\n",
      "model.fit(x_train,y_train)\n",
      "159/7: y_predict = model.predict(x_test)\n",
      "159/8: y_predict\n",
      "159/9: y_test\n",
      "159/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "160/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "160/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "160/3: bihar.head(2)\n",
      "160/4:\n",
      "x_data = bihar[[\"Mar-May\"]]\n",
      "y_data = bihar[[\"Jun-Sep\"]]\n",
      "160/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "160/6:\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "160/7: y_predict = model.predict(x_test)\n",
      "160/8: y_predict\n",
      "160/9: y_test\n",
      "160/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "161/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "161/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "161/3: bihar.head(2)\n",
      "161/4:\n",
      "x_data = bihar[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y_data = bihar[[\"ANNUAL\"]]\n",
      "161/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "161/6:\n",
      "model = LinearRegression()\n",
      "model.fit(x_train,y_train)\n",
      "161/7: y_predict = model.predict(x_test)\n",
      "161/8: y_predict\n",
      "161/9: y_test\n",
      "161/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "162/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,root\n",
      "163/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "163/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "163/3: bihar.head(2)\n",
      "163/4:\n",
      "x_data = bihar[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y_data = bihar[[\"ANNUAL\"]]\n",
      "163/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "163/6:\n",
      "model = DecisionTreeRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "163/7: y_predict = model.predict(x_test)\n",
      "163/8: y_predict\n",
      "163/9: y_test\n",
      "163/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "print(\"\\nRoot mean square error\",)\n",
      "164/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "164/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "164/3: bihar.head(2)\n",
      "164/4:\n",
      "x_data = bihar[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y_data = bihar[[\"ANNUAL\"]]\n",
      "164/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "164/6:\n",
      "model = RandomForestRegressor()\n",
      "model.fit(x_train,y_train)\n",
      "164/7: y_predict = model.predict(x_test)\n",
      "164/8: y_predict\n",
      "164/9: y_test\n",
      "164/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "print(\"\\nRoot mean square error\",)\n",
      "165/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
      "165/2: bihar = pd.read_csv(\"Bihar_entier.csv\")\n",
      "165/3: bihar.head(2)\n",
      "165/4:\n",
      "x_data = bihar[[\"Jan-Feb\",\"Mar-May\",\"Jun-Sep\",\"Oct-Dec\"]]\n",
      "y_data = bihar[[\"ANNUAL\"]]\n",
      "165/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)\n",
      "165/6:\n",
      "model = SVR()\n",
      "model.fit(x_train,y_train)\n",
      "165/7: y_predict = model.predict(x_test)\n",
      "165/8: y_predict\n",
      "165/9: y_test\n",
      "165/10:\n",
      "print(\"Mean squared error(mse)\", mean_squared_error(y_test,y_predict))\n",
      "print(\"\\nMean absolute error(mae)\", mean_absolute_error(y_test,y_predict))\n",
      "print(\"\\nR square score(r2_score)\", r2_score(y_test,y_predict))\n",
      "print(\"\\nRoot mean square error\",)\n",
      "165/11:\n",
      "plt.scatter(x_test,y_test)\n",
      "plt.plot(x_test,y_predict,color=\"m\")\n",
      "166/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "166/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "166/3: suv_data = pd.read_csv()\n",
      "166/4: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "166/5: SUV_data.head(5)\n",
      "166/6:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "166/7:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "167/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "167/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "167/3: SUV_data.head(5)\n",
      "168/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "168/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "168/3: SUV_data.head(5)\n",
      "168/4: SUV_data.info()\n",
      "168/5:\n",
      "SUV_data.isnull()                                  # to check if any null values is present or not it will return a bollen\n",
      "                                                   #    if null then True else False\n",
      "168/6:\n",
      "#print(SUV_data.isnull())                                  # to check if any null values is present or not it will return a bollen\n",
      "                                                   #    if null then True else False\n",
      "\n",
      "SUV_data.isnull().sum()\n",
      "168/7: sns.countplot.plt(x=\"Purchased\",SUV_data)\n",
      "168/8: sns.countplot.plt(x=\"Purchased\",data = SUV_data)\n",
      "168/9: sns.countplot(x=\"Purchased\",data = SUV_data)\n",
      "168/10: sns.countplot(x=\"purchased\", hue = \"Gender\", data = SUV_data)\n",
      "168/11: sns.countplot(x=\"Purchased\", hue = \"Gender\", data = SUV_data)\n",
      "168/12: sns.countplot(x=\"Purchased\", hue = \"Age\", data = SUV_data)\n",
      "168/13: SUV_data.\"Age\".plt.hist()\n",
      "168/14: SUV_data[\"Age\"].plt.hist()\n",
      "168/15: SUV_data.\"Age\".plot.hist()\n",
      "168/16: SUV_data[\"Age\"].plot.hist()\n",
      "168/17: gender = pd.get_dummies(SUV_data[\"Gender\"])\n",
      "168/18:\n",
      "gender = pd.get_dummies(SUV_data[\"Gender\"])\n",
      "gender\n",
      "168/19:\n",
      "gender = pd.get_dummies(SUV_data[\"Gender\"],drop_first=True)\n",
      "gender\n",
      "168/20:\n",
      "SUV_data = pd.concat([\"gender\"],axis=1)\n",
      "SUV_data\n",
      "168/21:\n",
      "SUV_data = pd.concat([gender],axis=1)\n",
      "SUV_data\n",
      "168/22:\n",
      "SUV_data = pd.concat([gender],axis=1)\n",
      "SUV_data.head(2)\n",
      "168/23:\n",
      "SUV_data = pd.concat([SUV_data,gender],axis=1)\n",
      "SUV_data.head(2)\n",
      "168/24: SUV_data.head(5)\n",
      "169/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "169/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "169/3: SUV_data.head(5)\n",
      "169/4: SUV_data.info()\n",
      "169/5:\n",
      "#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen\n",
      "                                    #    if null then True else False\n",
      "\n",
      "SUV_data.isnull().sum()            # return the sum of each column\n",
      "169/6: sns.countplot(x=\"Purchased\",data = SUV_data)\n",
      "169/7: sns.countplot(x=\"Purchased\", hue = \"Gender\", data = SUV_data)    # hue = particular column name\n",
      "169/8: SUV_data[\"Age\"].plot.hist()\n",
      "169/9:\n",
      "gender = pd.get_dummies(SUV_data[\"Gender\"],drop_first=True)\n",
      "gender\n",
      "169/10:\n",
      "SUV_data = pd.concat([SUV_data,gender],axis=1)\n",
      "SUV_data.head(2)\n",
      "169/11:\n",
      "SUV_data.drop([\"User ID\",\"Gender\"],axis=1,inplace=True)\n",
      "SUV_data\n",
      "169/12:\n",
      "x = SUV_data.drop([\"Purchased\"],axis=1)\n",
      "y = SUV_data[\"Purchased\"]\n",
      "169/13: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "169/14:\n",
      "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model = LogisticRegression()\n",
      "model.fit(x_tarin,y_test)\n",
      "169/15:\n",
      "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train,y_test)\n",
      "169/16:\n",
      "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train,y_train)\n",
      "169/17: predicted = model.predict(y_train)\n",
      "169/18: predicted = model.predict([y_train])\n",
      "169/19: predicted = model.predict(x_test)\n",
      "169/20:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "169/21: y_test\n",
      "169/22: list(y_test)\n",
      "169/23:\n",
      "z = list(y_test)\n",
      "z\n",
      "170/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "170/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "170/3: SUV_data.head(5)\n",
      "170/4: SUV_data.info()\n",
      "170/5:\n",
      "#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen\n",
      "                                    #    if null then True else False\n",
      "\n",
      "SUV_data.isnull().sum()            # return the sum of each column\n",
      "170/6: sns.countplot(x=\"Purchased\",data = SUV_data)\n",
      "170/7: sns.countplot(x=\"Purchased\", hue = \"Gender\", data = SUV_data)    # hue = particular column name\n",
      "170/8: SUV_data[\"Age\"].plot.hist()\n",
      "170/9:\n",
      "gender = pd.get_dummies(SUV_data[\"Gender\"],drop_first=True)\n",
      "gender\n",
      "170/10:\n",
      "SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code\n",
      "SUV_data.head(2)\n",
      "170/11:\n",
      "SUV_data.drop([\"User ID\",\"Gender\"],axis=1,inplace=True)\n",
      "SUV_data\n",
      "170/12:\n",
      "x = SUV_data.([[\"Age\",\"EstimatedSalary\",\"Male\"]])\n",
      "y = SUV_data[\"Purchased\"]\n",
      "171/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "171/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "171/3: SUV_data.head(5)\n",
      "171/4: SUV_data.info()\n",
      "171/5:\n",
      "#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen\n",
      "                                    #    if null then True else False\n",
      "\n",
      "SUV_data.isnull().sum()            # return the sum of each column\n",
      "171/6: sns.countplot(x=\"Purchased\",data = SUV_data)\n",
      "171/7: sns.countplot(x=\"Purchased\", hue = \"Gender\", data = SUV_data)    # hue = particular column name\n",
      "171/8: SUV_data[\"Age\"].plot.hist()\n",
      "171/9:\n",
      "gender = pd.get_dummies(SUV_data[\"Gender\"],drop_first=True)\n",
      "gender\n",
      "171/10:\n",
      "SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code\n",
      "SUV_data.head(2)\n",
      "171/11:\n",
      "SUV_data.drop([\"User ID\",\"Gender\"],axis=1,inplace=True)\n",
      "SUV_data\n",
      "171/12:\n",
      "x = SUV_data.[\"Age\",\"EstimatedSalary\",\"Male\"]\n",
      "y = SUV_data[\"Purchased\"]\n",
      "172/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "172/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "172/3: SUV_data.head(5)\n",
      "172/4: SUV_data.info()\n",
      "172/5:\n",
      "#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen\n",
      "                                    #    if null then True else False\n",
      "\n",
      "SUV_data.isnull().sum()            # return the sum of each column\n",
      "172/6: sns.countplot(x=\"Purchased\",data = SUV_data)\n",
      "172/7: sns.countplot(x=\"Purchased\", hue = \"Gender\", data = SUV_data)    # hue = particular column name\n",
      "172/8: SUV_data[\"Age\"].plot.hist()\n",
      "172/9:\n",
      "gender = pd.get_dummies(SUV_data[\"Gender\"],drop_first=True)\n",
      "gender\n",
      "172/10:\n",
      "SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code\n",
      "SUV_data.head(2)\n",
      "172/11:\n",
      "SUV_data.drop([\"User ID\",\"Gender\"],axis=1,inplace=True)\n",
      "SUV_data\n",
      "172/12:\n",
      "x = SUV_data[\"Age\",\"EstimatedSalary\",\"Male\"]\n",
      "y = SUV_data[\"Purchased\"]\n",
      "172/13:\n",
      "x = SUV_data[[\"Age\",\"EstimatedSalary\",\"Male\"]]\n",
      "y = SUV_data[\"Purchased\"]\n",
      "173/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "173/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "173/3: SUV_data.head(5)\n",
      "173/4: SUV_data.info()\n",
      "173/5:\n",
      "#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen\n",
      "                                    #    if null then True else False\n",
      "\n",
      "SUV_data.isnull().sum()            # return the sum of each column\n",
      "173/6: sns.countplot(x=\"Purchased\",data = SUV_data)\n",
      "173/7: sns.countplot(x=\"Purchased\", hue = \"Gender\", data = SUV_data)    # hue = particular column name\n",
      "173/8: SUV_data[\"Age\"].plot.hist()\n",
      "173/9:\n",
      "gender = pd.get_dummies(SUV_data[\"Gender\"],drop_first=True)\n",
      "gender\n",
      "173/10:\n",
      "SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code\n",
      "SUV_data.head(2)\n",
      "173/11:\n",
      "SUV_data.drop([\"User ID\",\"Gender\"],axis=1,inplace=True)\n",
      "SUV_data\n",
      "173/12:\n",
      "x = SUV_data[[\"Age\",\"EstimatedSalary\",\"Male\"]]\n",
      "y = SUV_data[\"Purchased\"]\n",
      "173/13:\n",
      "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train,y_train)\n",
      "173/14:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "174/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "174/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "174/3: SUV_data.head(5)\n",
      "174/4: SUV_data.info()\n",
      "174/5:\n",
      "#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen\n",
      "                                    #    if null then True else False\n",
      "\n",
      "SUV_data.isnull().sum()            # return the sum of each column\n",
      "174/6: sns.countplot(x=\"Purchased\",data = SUV_data)\n",
      "174/7: sns.countplot(x=\"Purchased\", hue = \"Gender\", data = SUV_data)    # hue = particular column name\n",
      "174/8: SUV_data[\"Age\"].plot.hist()\n",
      "174/9:\n",
      "gender = pd.get_dummies(SUV_data[\"Gender\"],drop_first=True)\n",
      "gender\n",
      "174/10:\n",
      "SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code\n",
      "SUV_data.head(2)\n",
      "174/11:\n",
      "SUV_data.drop([\"User ID\",\"Gender\"],axis=1,inplace=True)\n",
      "SUV_data\n",
      "174/12:\n",
      "x = SUV_data[[\"Age\",\"EstimatedSalary\"]]\n",
      "y = SUV_data[\"Purchased\"]\n",
      "174/13:\n",
      "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train,y_train)\n",
      "174/14:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "174/15:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "174/16: accuracy_score(y_test,predicted)\n",
      "175/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "175/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "175/3: SUV_data.head(5)\n",
      "175/4: SUV_data.info()\n",
      "175/5:\n",
      "#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen\n",
      "                                    #    if null then True else False\n",
      "\n",
      "SUV_data.isnull().sum()            # return the sum of each column\n",
      "175/6: sns.countplot(x=\"Purchased\",data = SUV_data)\n",
      "175/7: sns.countplot(x=\"Purchased\", hue = \"Gender\", data = SUV_data)    # hue = particular column name\n",
      "175/8: SUV_data[\"Age\"].plot.hist()\n",
      "175/9:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender = pd.get_dummies(SUV_data[\"Gender\"],drop_first=True)\n",
      "gender\n",
      "175/10:\n",
      "SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code\n",
      "SUV_data.head(2)\n",
      "175/11:\n",
      "SUV_data.drop([\"User ID\",\"Gender\"],axis=1,inplace=True)\n",
      "SUV_data\n",
      "175/12:\n",
      "x = SUV_data[[\"Age\",\"EstimatedSalary\"]]\n",
      "y = SUV_data[\"Purchased\"]\n",
      "175/13:\n",
      "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train,y_train)\n",
      "175/14:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "175/15: accuracy_score(y_test,predicted)\n",
      "176/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "176/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "176/3: SUV_data.head(5)\n",
      "176/4: SUV_data.info()\n",
      "176/5:\n",
      "#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen\n",
      "                                    #    if null then True else False\n",
      "\n",
      "SUV_data.isnull().sum()            # return the sum of each column\n",
      "176/6: sns.countplot(x=\"Purchased\",data = SUV_data)\n",
      "176/7: sns.countplot(x=\"Purchased\", hue = \"Gender\", data = SUV_data)    # hue = particular column name\n",
      "176/8: SUV_data[\"Age\"].plot.hist()\n",
      "176/9:\n",
      "gender = pd.get_dummies(SUV_data[\"Gender\"],drop_first=True)\n",
      "gender\n",
      "176/10:\n",
      "SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code\n",
      "SUV_data.head(2)\n",
      "176/11:\n",
      "SUV_data.drop([\"User ID\",\"Gender\"],axis=1,inplace=True)\n",
      "SUV_data\n",
      "176/12:\n",
      "x = SUV_data[[\"Age\",\"EstimatedSalary\",\"Male\"]]\n",
      "y = SUV_data[\"Purchased\"]\n",
      "176/13:\n",
      "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train,y_train)\n",
      "176/14:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "176/15: accuracy_score(y_test,predicted)\n",
      "176/16:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "176/17:\n",
      "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "print(x_train)\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train,y_train)\n",
      "176/18:\n",
      "sc = StandardScaler()\n",
      "x_train = sc.fit_transform(x_train)\n",
      "x_test = sc.fit_transform(x_test)\n",
      "pritn(x_train)\n",
      "176/19:\n",
      "sc = StandardScaler()\n",
      "x_train = sc.fit_transform(x_train)\n",
      "x_test = sc.fit_transform(x_test)\n",
      "print(x_train)\n",
      "176/20:\n",
      "sc = StandardScaler()\n",
      "x_train = sc.fit_transform(x_train)\n",
      "x_test = sc.fit_transform(x_test)\n",
      "x_train.head(10)\n",
      "176/21:\n",
      "sc = StandardScaler()\n",
      "x_train = sc.fit_transform(x_train)\n",
      "x_test = sc.fit_transform(x_test)\n",
      "x_train\n",
      "177/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "177/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "177/3: SUV_data.head(5)\n",
      "177/4: SUV_data.info()\n",
      "177/5:\n",
      "#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen\n",
      "                                    #    if null then True else False\n",
      "\n",
      "SUV_data.isnull().sum()            # return the sum of each column\n",
      "177/6: sns.countplot(x=\"Purchased\",data = SUV_data)\n",
      "177/7: sns.countplot(x=\"Purchased\", hue = \"Gender\", data = SUV_data)    # hue = particular column name\n",
      "177/8: SUV_data[\"Age\"].plot.hist()\n",
      "177/9:\n",
      "gender = pd.get_dummies(SUV_data[\"Gender\"],drop_first=True)\n",
      "gender\n",
      "177/10:\n",
      "SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code\n",
      "SUV_data.head(2)\n",
      "177/11:\n",
      "SUV_data.drop([\"User ID\",\"Gender\"],axis=1,inplace=True)\n",
      "SUV_data\n",
      "177/12:\n",
      "x = SUV_data[[\"Age\",\"EstimatedSalary\",\"Male\"]]\n",
      "y = SUV_data[\"Purchased\"]\n",
      "177/13: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "177/14:\n",
      "sc = StandardScaler()\n",
      "x_train = sc.fit_transform(x_train)\n",
      "x_test = sc.fit_transform(x_test)\n",
      "177/15:\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train,y_train)\n",
      "177/16:\n",
      "#    Age  EstimatedSalary  Male\n",
      "#3     27            57000     0\n",
      "#18    46            28000     1\n",
      "#202   39           134000     0\n",
      "#250   44            39000     0\n",
      "#274   57            26000     0\n",
      "..   ...              ...   ...\n",
      "#71    24            27000     0\n",
      "#106   26            35000     0\n",
      "#270   43           133000     0\n",
      "#348   39            77000     1\n",
      "#102   32            86000     0\n",
      "178/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "178/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "178/3: SUV_data.head(5)\n",
      "178/4: SUV_data.info()\n",
      "178/5:\n",
      "#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen\n",
      "                                    #    if null then True else False\n",
      "\n",
      "SUV_data.isnull().sum()            # return the sum of each column\n",
      "178/6: sns.countplot(x=\"Purchased\",data = SUV_data)\n",
      "178/7: sns.countplot(x=\"Purchased\", hue = \"Gender\", data = SUV_data)    # hue = particular column name\n",
      "178/8: SUV_data[\"Age\"].plot.hist()\n",
      "178/9:\n",
      "gender = pd.get_dummies(SUV_data[\"Gender\"],drop_first=True)\n",
      "gender\n",
      "178/10:\n",
      "SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code\n",
      "SUV_data.head(2)\n",
      "178/11:\n",
      "SUV_data.drop([\"User ID\",\"Gender\"],axis=1,inplace=True)\n",
      "SUV_data\n",
      "178/12:\n",
      "x = SUV_data[[\"Age\",\"EstimatedSalary\",\"Male\"]]\n",
      "y = SUV_data[\"Purchased\"]\n",
      "178/13: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "178/14:\n",
      "sc = StandardScaler()\n",
      "x_train = sc.fit_transform(x_train)\n",
      "x_test = sc.fit_transform(x_test)\n",
      "178/15:\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train,y_train)\n",
      "178/16:\n",
      "#    Age  EstimatedSalary  Male\n",
      "#3     27            57000     0\n",
      "#18    46            28000     1\n",
      "#202   39           134000     0\n",
      "#250   44            39000     0\n",
      "#274   57            26000     0\n",
      "#..   ...              ...   ...\n",
      "#71    24            27000     0\n",
      "#106   26            35000     0\n",
      "#270   43           133000     0\n",
      "#348   39            77000     1\n",
      "#102   32            86000     0\n",
      "178/17:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "178/18: accuracy_score(y_test,predicted)\n",
      "179/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "179/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "179/3: SUV_data.head(5)\n",
      "179/4: SUV_data.info()\n",
      "179/5:\n",
      "#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen\n",
      "                                    #    if null then True else False\n",
      "\n",
      "SUV_data.isnull().sum()            # return the sum of each column\n",
      "179/6: sns.countplot(x=\"Purchased\",data = SUV_data)\n",
      "179/7: sns.countplot(x=\"Purchased\", hue = \"Gender\", data = SUV_data)    # hue = particular column name\n",
      "179/8: SUV_data[\"Age\"].plot.hist()\n",
      "179/9:\n",
      "gender = pd.get_dummies(SUV_data[\"Gender\"],drop_first=True)\n",
      "gender\n",
      "179/10:\n",
      "SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code\n",
      "SUV_data.head(2)\n",
      "179/11:\n",
      "SUV_data.drop([\"User ID\",\"Gender\"],axis=1,inplace=True)\n",
      "SUV_data\n",
      "179/12:\n",
      "x = SUV_data[[\"Age\",\"EstimatedSalary\",\"Male\"]]\n",
      "y = SUV_data[\"Purchased\"]\n",
      "179/13: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "179/14:\n",
      "sc = StandardScaler()\n",
      "x_train = sc.fit_transform(x_train)\n",
      "x_test = sc.fit_transform(x_test)\n",
      "179/15:\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train,y_train)\n",
      "179/16:\n",
      "#    Age  EstimatedSalary  Male\n",
      "#3     27            57000     0\n",
      "#18    46            28000     1\n",
      "#202   39           134000     0\n",
      "#250   44            39000     0\n",
      "#274   57            26000     0\n",
      "#..   ...              ...   ...\n",
      "#71    24            27000     0\n",
      "#106   26            35000     0\n",
      "#270   43           133000     0\n",
      "#348   39            77000     1\n",
      "#102   32            86000     0\n",
      "179/17:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "179/18: accuracy_score(y_test,predicted)\n",
      "179/19:\n",
      "x_data = [27,57000,0]\n",
      "x_data = sc.fit_transform(x_data)\n",
      "179/20:\n",
      "x_data = [[27,57000,0]]\n",
      "x_data = sc.fit_transform(x_data)\n",
      "179/21:\n",
      "x_data = [[27,57000,0]]\n",
      "x_data = sc.fit_transform(x_data)\n",
      "model.predict(x_data)\n",
      "179/22:\n",
      "x_data = [[46,41000,0]]\n",
      "x_data = sc.fit_transform(x_data)\n",
      "model.predict(x_data)\n",
      "179/23:\n",
      "x_data = [[46,41000,0]]\n",
      "x_data = sc.fit_transform(x_data)\n",
      "model.predict(x_data)\n",
      "179/24:\n",
      "x_data = [[51,23000,1]]\n",
      "x_data = sc.fit_transform(x_data)\n",
      "model.predict(x_data)\n",
      "179/25: y_test\n",
      "179/26: list(y_test)\n",
      "180/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "180/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "180/3: SUV_data.head(5)\n",
      "180/4: SUV_data.info()\n",
      "180/5:\n",
      "#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen\n",
      "                                    #    if null then True else False\n",
      "\n",
      "SUV_data.isnull().sum()            # return the sum of each column\n",
      "180/6: sns.countplot(x=\"Purchased\",data = SUV_data)\n",
      "180/7: sns.countplot(x=\"Purchased\", hue = \"Gender\", data = SUV_data)    # hue = particular column name\n",
      "180/8: SUV_data[\"Age\"].plot.hist()\n",
      "180/9:\n",
      "gender = pd.get_dummies(SUV_data[\"Gender\"],drop_first=True)\n",
      "gender\n",
      "180/10:\n",
      "SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code\n",
      "SUV_data.head(2)\n",
      "180/11:\n",
      "SUV_data.drop([\"User ID\",\"Gender\"],axis=1,inplace=True)\n",
      "SUV_data\n",
      "180/12:\n",
      "x = SUV_data[[\"Age\",\"EstimatedSalary\",\"Male\"]]\n",
      "y = SUV_data[\"Purchased\"]\n",
      "180/13: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "180/14:\n",
      "sc = StandardScaler()                    # we use Standered scaller to scall down the values to eqal limits because some\n",
      "x_train = sc.fit_transform(x_train)      # columns are having very high values as compare to the other column which are not\n",
      "x_test = sc.fit_transform(x_test)        # even close to those values(high) which will affecet out prediction.\n",
      "180/15:\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train,y_train)\n",
      "180/16:\n",
      "predicted = model.predict(x_test)\n",
      "#predicted\n",
      "180/17: accuracy_score(y_test,predicted)\n",
      "181/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, r2_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "181/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "181/3: SUV_data.head(5)\n",
      "181/4: SUV_data.info()\n",
      "181/5:\n",
      "#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen\n",
      "                                    #    if null then True else False\n",
      "\n",
      "SUV_data.isnull().sum()            # return the sum of each column\n",
      "181/6: sns.countplot(x=\"Purchased\",data = SUV_data)\n",
      "181/7: sns.countplot(x=\"Purchased\", hue = \"Gender\", data = SUV_data)    # hue = particular column name\n",
      "181/8: SUV_data[\"Age\"].plot.hist()\n",
      "181/9:\n",
      "gender = pd.get_dummies(SUV_data[\"Gender\"],drop_first=True)\n",
      "gender\n",
      "181/10:\n",
      "SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code\n",
      "SUV_data.head(2)\n",
      "181/11:\n",
      "SUV_data.drop([\"User ID\",\"Gender\"],axis=1,inplace=True)\n",
      "SUV_data\n",
      "181/12:\n",
      "x = SUV_data[[\"Age\",\"EstimatedSalary\",\"Male\"]]\n",
      "y = SUV_data[\"Purchased\"]\n",
      "181/13: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "181/14:\n",
      "sc = StandardScaler()                    # we use Standered scaller to scall down the values to eqal limits because some\n",
      "x_train = sc.fit_transform(x_train)      # columns are having very high values as compare to the other column which are not\n",
      "x_test = sc.fit_transform(x_test)        # even close to those values(high) which will affecet out prediction.\n",
      "181/15:\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train,y_train)\n",
      "181/16:\n",
      "predicted = model.predict(x_test)\n",
      "#predicted\n",
      "181/17: accuracy_score(y_test,predicted)\n",
      "181/18: r2_score(y_test,predicted)\n",
      "182/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "182/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "182/3: Breast.info()\n",
      "182/4:\n",
      "Breast.info()\n",
      "Breast.head(4)\n",
      "182/5:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "183/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "183/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "183/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "183/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "183/5: Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "183/6:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast.head(4)\n",
      "183/7:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast.head(4)\n",
      "183/8:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast.head(4)\n",
      "183/9:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "183/10:\n",
      "Breast = Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat: \"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(3)\n",
      "183/11:\n",
      "Breast = Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(3)\n",
      "183/12:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(3)\n",
      "184/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "184/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "184/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "184/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "184/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "184/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(3)\n",
      "184/7:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "184/8:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "184/9:\n",
      "x_data = Breast.drop(Breast[\"Irradiat\"])\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "\n",
      "#x_train, x_test, y_train, y_test = train_test_split()\n",
      "184/10:\n",
      "x_data = Breast.drop(Breast[\"Irradiat\"])\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_data\n",
      "#x_train, x_test, y_train, y_test = train_test_split()\n",
      "184/11:\n",
      "x_data = Breast.drop([\"Irradiat\"])\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_data\n",
      "#x_train, x_test, y_train, y_test = train_test_split()\n",
      "184/12:\n",
      "x_data = Breast.drop(\"Irradiat\")\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_data\n",
      "#x_train, x_test, y_train, y_test = train_test_split()\n",
      "185/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "186/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "186/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "186/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "186/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "186/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "186/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(3)\n",
      "186/7:\n",
      "x_data = Breast.drop(\"Irradiat\")\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_data\n",
      "#x_train, x_test, y_train, y_test = train_test_split()\n",
      "186/8:\n",
      "x_data = Breast.drop([\"Irradiat\"])\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_data\n",
      "#x_train, x_test, y_train, y_test = train_test_split()\n",
      "187/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "187/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "187/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "187/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "187/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "187/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(3)\n",
      "187/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_data\n",
      "#x_train, x_test, y_train, y_test = train_test_split()\n",
      "187/8:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "187/9: predicted = model.predicr(x_test)\n",
      "187/10:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "187/11: predicted = model.predicr(x_test)\n",
      "187/12: predicted = model.predict(x_test)\n",
      "187/13: predicted = model.predict(x_test)\n",
      "187/14:\n",
      "predicted = model.predict(x_test)\n",
      "predict\n",
      "187/15:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "187/16:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "187/17: accuracy_score(y_test,predicted)\n",
      "187/18: y_test\n",
      "187/19: list(y_test)\n",
      "187/20: y_test\n",
      "188/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "188/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "188/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "188/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "188/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "188/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "188/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "188/8:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "188/9: y_test\n",
      "188/10: accuracy_score(y_test,predicted)\n",
      "189/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "189/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "189/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "189/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "#deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "189/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "#Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "189/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "189/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "189/8:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "189/9: y_test\n",
      "189/10: accuracy_score(y_test,predicted)\n",
      "190/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "190/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "190/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "190/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "190/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "190/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "190/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "190/8:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "190/9: y_test\n",
      "190/10: accuracy_score(y_test,predicted)\n",
      "191/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "191/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "191/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "191/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "191/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "191/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "191/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "191/8:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "191/9: y_test\n",
      "191/10: accuracy_score(y_test,predicted)\n",
      "192/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "192/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "192/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "192/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "192/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "192/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "192/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "192/8:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "192/9: y_test\n",
      "192/10: accuracy_score(y_test,predicted)\n",
      "193/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "193/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "193/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "193/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "193/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "193/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "193/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "193/8:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "193/9: y_test\n",
      "193/10: accuracy_score(y_test,predicted)\n",
      "194/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "194/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "194/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "194/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "194/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "194/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "194/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "194/8:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "194/9: y_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/10: accuracy_score(y_test,predicted)\n",
      "195/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "195/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "195/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "195/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "195/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "195/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "195/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "195/8:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "195/9: y_test\n",
      "195/10: accuracy_score(y_test,predicted)\n",
      "196/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "196/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "196/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "196/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "196/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "196/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "196/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "196/8:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "196/9: y_test\n",
      "196/10: accuracy_score(y_test,predicted)\n",
      "197/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "197/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "197/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "197/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "197/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "197/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "197/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "197/8:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "197/9: y_test\n",
      "197/10: accuracy_score(y_test,predicted)\n",
      "198/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "198/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "198/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "198/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "198/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "198/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "198/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "198/8:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "198/9: y_test\n",
      "198/10: accuracy_score(y_test,predicted)\n",
      "198/11:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "199/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "199/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "199/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "199/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "199/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "199/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "199/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = RandomForestRegressor()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "199/8:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "199/9: y_test\n",
      "199/10: accuracy_score(y_test,predicted)\n",
      "200/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "200/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "200/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "200/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "200/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "200/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "200/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "200/8:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "200/9: y_test\n",
      "200/10: accuracy_score(y_test,predicted)\n",
      "201/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "201/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "201/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "201/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "201/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "201/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "201/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "201/8:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "201/9: #y_test\n",
      "201/10: accuracy_score(y_test,predicted)\n",
      "202/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "202/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "202/3:\n",
      "#Breast.info()\n",
      "Breast.head(4)\n",
      "202/4:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "202/5:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "202/6:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "202/7:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "202/8:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "202/9: #y_test\n",
      "202/10: accuracy_score(y_test,predicted)\n",
      "202/11:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "202/12: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "202/13:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "203/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "203/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "203/3:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "203/4:\n",
      "# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can \n",
      "# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels\n",
      "# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event\n",
      "# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event\n",
      "# and 1 as recurrence-event or some other values.\n",
      "203/5:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "203/6:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "203/7:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "203/8:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "203/9:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "203/10: #y_test\n",
      "203/11: accuracy_score(y_test,predicted)\n",
      "204/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "204/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "204/3:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "204/4:\n",
      "# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can \n",
      "# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels\n",
      "# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event\n",
      "# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event\n",
      "# and 1 as recurrence-event or some other values.\n",
      "204/5:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "#deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "204/6:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "#Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "204/7:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "204/8:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "204/9:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "204/10: #y_test\n",
      "204/11: accuracy_score(y_test,predicted)\n",
      "205/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "205/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "205/3:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "205/4:\n",
      "# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can \n",
      "# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels\n",
      "# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event\n",
      "# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event\n",
      "# and 1 as recurrence-event or some other values.\n",
      "205/5:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "#deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "205/6:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "#Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "205/7:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "205/8:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "205/9:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "205/10: #y_test\n",
      "205/11: accuracy_score(y_test,predicted)\n",
      "206/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "206/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "206/3:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "206/4:\n",
      "# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can \n",
      "# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels\n",
      "# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event\n",
      "# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event\n",
      "# and 1 as recurrence-event or some other values.\n",
      "206/5:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "#deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "206/6:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "#Breast[\"Deg-malig\"] = deg_malig_le.fit_transform(Breast[\"deg-malig:\"])\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "206/7:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "206/8:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "206/9:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "206/10: #y_test\n",
      "206/11: accuracy_score(y_test,predicted)\n",
      "207/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "207/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "207/3:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "207/4:\n",
      "# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can \n",
      "# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels\n",
      "# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event\n",
      "# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event\n",
      "# and 1 as recurrence-event or some other values.\n",
      "207/5:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "#deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "207/6:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = Breast[\"deg-malig:\"]\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "207/7:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "207/8:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "207/9:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "207/10: #y_test\n",
      "207/11: accuracy_score(y_test,predicted)\n",
      "208/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "208/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "208/3:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "208/4:\n",
      "# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can \n",
      "# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels\n",
      "# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event\n",
      "# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event\n",
      "# and 1 as recurrence-event or some other values.\n",
      "208/5:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "#deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "208/6:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = Breast[\"deg-malig:\"]\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "208/7:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "208/8:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "208/9:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "208/10: #y_test\n",
      "208/11: accuracy_score(y_test,predicted)\n",
      "208/12:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = Breast[\"deg-malig:\"]\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast\n",
      "208/13:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = Breast[\"deg-malig:\"]\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.head(4)\n",
      "208/14:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = Breast[\"deg-malig:\"]\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast\n",
      "209/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "209/2: Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "209/3:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "209/4:\n",
      "# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can \n",
      "# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels\n",
      "# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event\n",
      "# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event\n",
      "# and 1 as recurrence-event or some other values.\n",
      "209/5:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "#deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "209/6:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = Breast[\"deg-malig:\"]\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast\n",
      "209/7:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "209/8:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "209/9:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "209/10: #y_test\n",
      "209/11: accuracy_score(y_test,predicted)\n",
      "210/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "210/2:\n",
      "Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "Test_data = pd.read_csv(\"test_data.csv\")\n",
      "210/3:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "210/4:\n",
      "# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can \n",
      "# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels\n",
      "# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event\n",
      "# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event\n",
      "# and 1 as recurrence-event or some other values.\n",
      "210/5:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "#deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "210/6:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = Breast[\"deg-malig:\"]\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.to_csv(\"New_data.csv\",index=False)\n",
      "210/7:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "210/8:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "210/9:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "210/10: #y_test\n",
      "210/11: accuracy_score(y_test,predicted)\n",
      "210/12: Test_data.head(4)\n",
      "211/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "211/2:\n",
      "Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "Test_data = pd.read_csv(\"test_data.csv\")\n",
      "211/3:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "211/4:\n",
      "# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can \n",
      "# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels\n",
      "# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event\n",
      "# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event\n",
      "# and 1 as recurrence-event or some other values.\n",
      "211/5:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "#deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "211/6:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = Breast[\"deg-malig:\"]\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "Breast.to_csv(\"New_data.csv\",index=False)\n",
      "212/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "212/2:\n",
      "Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "Test_data = pd.read_csv(\"test_data.csv\")\n",
      "212/3:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "212/4:\n",
      "# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can \n",
      "# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels\n",
      "# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event\n",
      "# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event\n",
      "# and 1 as recurrence-event or some other values.\n",
      "212/5:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "#deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "212/6:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = Breast[\"deg-malig:\"]\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "#Breast.to_csv(\"New_data.csv\",index=False)\n",
      "212/7:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "212/8:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "212/9:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "212/10: Test_data.head(4)\n",
      "212/11: accuracy_score(y_test,predicted)\n",
      "212/12: Test_data_x = Test_data.drop([\"irradit\"],axis=1)\n",
      "212/13: Test_data_x = Test_data.drop([\"irradit:\"],axis=1)\n",
      "213/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "213/2:\n",
      "Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "Test_data = pd.read_csv(\"test_data.csv\")\n",
      "213/3:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "213/4:\n",
      "# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can \n",
      "# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels\n",
      "# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event\n",
      "# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event\n",
      "# and 1 as recurrence-event or some other values.\n",
      "213/5:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "#deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "213/6:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = Breast[\"deg-malig:\"]\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "#Breast.to_csv(\"New_data.csv\",index=False)\n",
      "213/7:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "213/8:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "213/9:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "213/10: Test_data.head(4)\n",
      "213/11: Test_data_x = Test_data.drop([\"irradit:\"],axis=1)\n",
      "213/12: Test_data_x = Test_data.drop([\"irradiat:\"],axis=1)\n",
      "213/13:\n",
      "Test_data_x = Test_data.drop([\"irradiat:\"],axis=1)\n",
      "Test_data[\"Predicted\"] = model.predict(Test_data_x)\n",
      "214/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "214/2:\n",
      "Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "Test_data = pd.read_csv(\"test_data.csv\")\n",
      "214/3:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "214/4:\n",
      "# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can \n",
      "# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels\n",
      "# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event\n",
      "# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event\n",
      "# and 1 as recurrence-event or some other values.\n",
      "214/5:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "#deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "214/6:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = Breast[\"deg-malig:\"]\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "#Breast.to_csv(\"New_data.csv\",index=False)\n",
      "214/7:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "214/8:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "214/9:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "214/10: Test_data.head(4)\n",
      "214/11:\n",
      "Test_data_x = Test_data.drop([\"irradiat:\"],axis=1)\n",
      "Test_data[\"Predicted\"] = model.predict(Test_data_x)\n",
      "214/12: Test_data.head(25)\n",
      "214/13: accuracy_score(y_test,predicted)\n",
      "215/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import accuracy_score\n",
      "215/2:\n",
      "Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "Test_data = pd.read_csv(\"test_data.csv\")\n",
      "215/3:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "215/4:\n",
      "# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can \n",
      "# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels\n",
      "# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event\n",
      "# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event\n",
      "# and 1 as recurrence-event or some other values.\n",
      "215/5:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "#deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "215/6:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = Breast[\"deg-malig:\"]\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "#Breast.to_csv(\"New_data.csv\",index=False)\n",
      "215/7:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "215/8:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "215/9:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "215/10: Test_data.head(4)\n",
      "215/11:\n",
      "Test_data_x = Test_data.drop([\"irradiat:\"],axis=1)\n",
      "Test_data[\"Predicted\"] = model.predict(Test_data_x)\n",
      "215/12: Test_data.head(31)\n",
      "215/13: accuracy_score(y_test,predicted)\n",
      "215/14: accuracy_score(Test_data[\"irradiat:\"],Test_data[\"Predicted\"])\n",
      "215/15:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "215/16:\n",
      "rmodel = RandomForestClassifier(n_estimators=10)\n",
      "rmodel.fit(x_train, y_train)\n",
      "215/17:\n",
      "rmodel = RandomForestClassifier(n_estimators=10)\n",
      "rmodel.fit(x_train, y_train)\n",
      "215/18: rmodelprediction = rmodel.predict(x_test)\n",
      "215/19: rmodelprediction = rmodel.predict(x_test)\n",
      "215/20: accuracy_score(y_test, rmodelprediction)\n",
      "216/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "216/2:\n",
      "Breast = pd.read_csv(\"Breast_cancer.csv\")\n",
      "Test_data = pd.read_csv(\"test_data.csv\")\n",
      "216/3:\n",
      "#Breast.info()\n",
      "Breast.tail(4)\n",
      "216/4:\n",
      "# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can \n",
      "# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels\n",
      "# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event\n",
      "# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event\n",
      "# and 1 as recurrence-event or some other values.\n",
      "216/5:\n",
      "Class_le = LabelEncoder()\n",
      "age_le = LabelEncoder()\n",
      "Meno_le = LabelEncoder()\n",
      "Tumor_le = LabelEncoder()\n",
      "inv_nodes_le = LabelEncoder()\n",
      "node_caps_le = LabelEncoder()\n",
      "#deg_malig_le = LabelEncoder()\n",
      "breast_le = LabelEncoder()\n",
      "breast_quad_le = LabelEncoder()\n",
      "irradiat_le = LabelEncoder()\n",
      "216/6:\n",
      "Breast[\"class\"] = Class_le.fit_transform(Breast[\"Class:\"])\n",
      "Breast[\"Age\"] = age_le.fit_transform(Breast[\"age:\"])\n",
      "Breast[\"Menopause\"] = Meno_le.fit_transform(Breast[\"menopause:\"])\n",
      "Breast[\"Tumor-size\"] = Tumor_le.fit_transform(Breast[\"tumor-size:\"])\n",
      "Breast[\"Inv-nodes\"] = inv_nodes_le.fit_transform(Breast[\"inv-nodes:\"])\n",
      "Breast[\"Node-caps\"] = node_caps_le.fit_transform(Breast[\"node-caps:\"])\n",
      "Breast[\"Deg-malig\"] = Breast[\"deg-malig:\"]\n",
      "Breast[\"Breast\"] = breast_le.fit_transform(Breast[\"breast:\"])\n",
      "Breast[\"Breast-quad\"] = breast_quad_le.fit_transform(Breast[\"breast-quad:\"])\n",
      "Breast[\"Irradiat\"] = irradiat_le.fit_transform(Breast[\"irradiat:\"])\n",
      "#Breast.to_csv(\"New_data.csv\",index=False)\n",
      "216/7:\n",
      "Breast.drop([\"Class:\",\"age:\",\"menopause:\",\"tumor-size:\",\"inv-nodes:\",\"node-caps:\",\"deg-malig:\",\"breast:\",\"breast-quad:\",\"irradiat:\"],\n",
      "                     axis=1,inplace=True)\n",
      "Breast.head(10)\n",
      "216/8:\n",
      "x_data = Breast.drop([\"Irradiat\"],axis=1)\n",
      "y_data = Breast[\"Irradiat\"]\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "\n",
      "x_train\n",
      "216/9:\n",
      "predicted = model.predict(x_test)\n",
      "predicted\n",
      "216/10: Test_data.head(4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/11:\n",
      "Test_data_x = Test_data.drop([\"irradiat:\"],axis=1)\n",
      "Test_data[\"Predicted\"] = model.predict(Test_data_x)\n",
      "216/12: Test_data.head(31)\n",
      "216/13: accuracy_score(Test_data[\"irradiat:\"],Test_data[\"Predicted\"])\n",
      "216/14: accuracy_score(y_test,predicted)\n",
      "216/15:\n",
      "rmodel = RandomForestClassifier(n_estimators=20)\n",
      "rmodel.fit(x_train, y_train)\n",
      "216/16: rmodelprediction = rmodel.predict(x_test)\n",
      "216/17: accuracy_score(y_test, rmodelprediction)\n",
      "217/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "217/2: car_data = pd.read_csv(\"CAR.csv\")\n",
      "217/3: car_data = pd.read_csv(\"CAR.csv\")\n",
      "217/4: car_head()\n",
      "217/5: car.head()\n",
      "217/6: car.head()\n",
      "217/7: car_data.head()\n",
      "217/8: car_data.info()\n",
      "217/9: sns.countplot(x=\"Class Values\",hue=\"buying\",data = car_data)\n",
      "219/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, r2_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "219/2: SUV_data = pd.read_csv(\"suv_data.csv\")\n",
      "219/3: SUV_data.head(5)\n",
      "219/4: SUV_data.info()\n",
      "219/5:\n",
      "#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen\n",
      "                                    #    if null then True else False\n",
      "\n",
      "SUV_data.isnull().sum()            # return the sum of each column\n",
      "219/6: sns.countplot(x=\"Purchased\",data = SUV_data)\n",
      "219/7: sns.countplot(x=\"Purchased\", hue = \"Gender\", data = SUV_data)    # hue = particular column name\n",
      "219/8: SUV_data[\"Age\"].plot.hist()\n",
      "219/9:\n",
      "gender = pd.get_dummies(SUV_data[\"Gender\"],drop_first=True)\n",
      "gender\n",
      "219/10:\n",
      "SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code\n",
      "SUV_data.head(2)\n",
      "219/11:\n",
      "SUV_data.drop([\"User ID\",\"Gender\"],axis=1,inplace=True)\n",
      "SUV_data\n",
      "219/12:\n",
      "x = SUV_data[[\"Age\",\"EstimatedSalary\",\"Male\"]]\n",
      "y = SUV_data[\"Purchased\"]\n",
      "219/13: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
      "219/14:\n",
      "sc = StandardScaler()                    # we use Standered scaller to scall down the values to eqal limits because some\n",
      "x_train = sc.fit_transform(x_train)      # columns are having very high values as compare to the other column which are not\n",
      "x_test = sc.fit_transform(x_test)        # even close to those values(high) which will affecet out prediction.\n",
      "219/15:\n",
      "model = LogisticRegression()\n",
      "model.fit(x_train,y_train)\n",
      "219/16:\n",
      "predicted = model.predict(x_test)\n",
      "#predicted\n",
      "219/17: accuracy_score(y_test,predicted)\n",
      "220/1: a = list(\"43Ah*ck0rr0nk\")\n",
      "220/2:\n",
      "a = list(\"43Ah*ck0rr0nk\")\n",
      "a\n",
      "220/3:\n",
      "a = list(\"43Ah*ck0rr0nk\")\n",
      "a.count('0')\n",
      "220/4:\n",
      "a = list(\"43Ah*ck0rr0nk\")\n",
      "a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a),-1):\n",
      "    print(a[i])\n",
      "220/5:\n",
      "a = list(\"43Ah*ck0rr0nk\")\n",
      "a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1,-1):\n",
      "    print(a[i])\n",
      "220/6:\n",
      "a = list(\"43Ah*ck0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "220/7:\n",
      "a = list(\"43Ah*ck0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "a\n",
      "220/8: ord(a)\n",
      "220/9: ord('a')\n",
      "220/10: ord('A')\n",
      "220/11: ord('Z')\n",
      "220/12:\n",
      "a = list(\"43Ah*ck0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "a\n",
      "\n",
      "for i in range(len(a)):\n",
      "    if (ord(a[i])>64 and ord(a[i])<91):\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        del(a[i+2])\n",
      "        i = i+2\n",
      "a\n",
      "220/13:\n",
      "a = list(\"43Ah*ck0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "a\n",
      "\n",
      "for i in range(len(a)):\n",
      "    if (ord(a[i])>64 and ord(a[i])<91):\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        i = i+2\n",
      "a\n",
      "220/14:\n",
      "a = list(\"43Ah*ck0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (ord(a[i])>64 and ord(a[i])<91):\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        i = i+2\n",
      "    i+=1\n",
      "a\n",
      "220/15:\n",
      "a = list(\"43Ah*ck0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (ord(a[i])>64 and ord(a[i])<91):\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        del(a[i+2])\n",
      "        i = i+2\n",
      "    i+=1\n",
      "a\n",
      "220/16:\n",
      "a = list(\"43Ah*ck0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (ord(a[i])>64 and ord(a[i])<91):\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        del(a[i+2])\n",
      "        i = i+2\n",
      "    i+=1\n",
      "*a\n",
      "220/17:\n",
      "a = list(\"43Ah*ck0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (ord(a[i])>64 and ord(a[i])<91):\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        del(a[i+2])\n",
      "        i = i+2\n",
      "    i+=1\n",
      "print(*a)\n",
      "220/18:\n",
      "a = list(\"43Ah*ck0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (ord(a[i])>64 and ord(a[i])<91):\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        del(a[i+2])\n",
      "        i = i+2\n",
      "    i+=1\n",
      "print(''.join(a))\n",
      "220/19:\n",
      "a = list(\"43Ah*kC*0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (ord(a[i])>64 and ord(a[i])<91):\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        del(a[i+2])\n",
      "        i = i+2\n",
      "    i+=1\n",
      "print(''.join(a))\n",
      "220/20:\n",
      "a = list(\"43Ah*kC*0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (ord(a[i])>64 and ord(a[i])<91):\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        del(a[i+2])\n",
      "        i = i+1\n",
      "    i+=1\n",
      "print(''.join(a))\n",
      "220/21:\n",
      "a = list(\"43Ah*kC*0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (ord(a[i])>64 and ord(a[i])<91):\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        del(a[i+2])\n",
      "        i = i+2\n",
      "    i+=1\n",
      "print(''.join(a))\n",
      "220/22:\n",
      "a = list(\"43Ah*Kc*0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (ord(a[i])>64 and ord(a[i])<91):\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        del(a[i+2])\n",
      "        i = i+2\n",
      "    i+=1\n",
      "print(''.join(a))\n",
      "220/23:\n",
      "a = list(\"43Ah*Kc*0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (ord(a[i])>64 and ord(a[i])<91):\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        del(a[i+2])\n",
      "        i = i+2-1\n",
      "    i+=1\n",
      "print(''.join(a))\n",
      "220/24:\n",
      "a = list(\"43Ah*Kc*0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (ord(a[i])>64 and ord(a[i])<91):\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        del(a[i+2])\n",
      "        i = i+1\n",
      "    i+=1\n",
      "print(''.join(a))\n",
      "220/25:\n",
      "a = list(\"43Ah*ck0rr0nk\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (ord(a[i])>64 and ord(a[i])<91):\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        del(a[i+2])\n",
      "        i = i+1\n",
      "    i+=1\n",
      "print(''.join(a))\n",
      "220/26:\n",
      "a = list(\"43Ah*ck0rr0Kn*\")\n",
      "zero = a.count('0')\n",
      "\n",
      "for i in range(-1,-len(a)-1+zero,-1):\n",
      "    if a[i] == '0':\n",
      "        a[i] = a[0]\n",
      "        del(a[0])\n",
      "        \n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (ord(a[i])>64 and ord(a[i])<91):\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        del(a[i+2])\n",
      "        i = i+1\n",
      "    i+=1\n",
      "print(''.join(a))\n",
      "220/27:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (a[i]>0 and a[i]<=10):\n",
      "        a.insert(0,a[i])\n",
      "        a[i] = 0\n",
      "    i+=1\n",
      "    \n",
      "a\n",
      "220/28:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (int(a[i])>0 and int(a[i])<=10):\n",
      "        a.insert(0,a[i])\n",
      "        a[i] = 0\n",
      "    i+=1\n",
      "    \n",
      "a\n",
      "220/29:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if a[i] is not isaplha():\n",
      "        a.insert(0,a[i])\n",
      "        a[i] = 0\n",
      "    i+=1\n",
      "    \n",
      "a\n",
      "220/30:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if a[i].isalpha()==False:\n",
      "        a.insert(0,a[i])\n",
      "        a[i] = 0\n",
      "    i+=1\n",
      "    \n",
      "a\n",
      "220/31:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if (a[i].isalpha())==False:\n",
      "        a.insert(0,a[i])\n",
      "        a[i] = 0\n",
      "    i+=1\n",
      "    \n",
      "a\n",
      "220/32:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if a[i].isnumeric()==True:\n",
      "        a.insert(0,a[i])\n",
      "        a[i] = 0\n",
      "    i+=1\n",
      "    \n",
      "a\n",
      "220/33:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if a[i].isnumeric()==True:\n",
      "        a.insert(0,a[i])\n",
      "        a[i] = '0'\n",
      "    i+=1\n",
      "    \n",
      "a\n",
      "220/34:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "print(a)\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if a[i].isnumeric()==True:\n",
      "        a.insert(0,a[i])\n",
      "        a[i] = '0'\n",
      "    i+=1\n",
      "    \n",
      "a\n",
      "220/35:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "print(a)\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if a[i].isnumeric()==True:\n",
      "        print(a[i])\n",
      "    i+=1\n",
      "    \n",
      "a\n",
      "220/36:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "print(a)\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if a[i].isnumeric()==Frue:\n",
      "        print(a[i])\n",
      "    i+=1\n",
      "    \n",
      "a\n",
      "220/37:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "print(a)\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if a[i].isnumeric()==True:\n",
      "        print(a[i])\n",
      "    i+=1\n",
      "    \n",
      "a\n",
      "220/38:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "print(a)\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if a[i].isnumeric()==True:\n",
      "        print(a[i])\n",
      "        a.insert(0,a[i])\n",
      "    i+=1\n",
      "    \n",
      "a\n",
      "220/39:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "a\n",
      "220/40:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i] = 0\n",
      "        i+=1\n",
      "    i+=1\n",
      "    \n",
      "a\n",
      "220/41:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = 0\n",
      "        i+=1\n",
      "    i+=1\n",
      "    \n",
      "a\n",
      "220/42:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = 0\n",
      "        i+=1\n",
      "    i+=1\n",
      "    \n",
      "print(''.join(a))\n",
      "220/43:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = 0\n",
      "        i+=1\n",
      "    i+=1\n",
      "    \n",
      "print(''.join(a))\n",
      "220/44:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "    \n",
      "print(''.join(a))\n",
      "220/45: ord('a')\n",
      "220/46: ord('z')\n",
      "220/47:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "    \n",
      "while(i<len(a)):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(\"*\")\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/48:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "    \n",
      "while(i<len(a)):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(\"*\")\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/49:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "    \n",
      "while(i<len(a)):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(\"*\")\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/50:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "    \n",
      "while(i<len(a)):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(\"*\")\n",
      "    i+=1\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/51:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "i=0    \n",
      "while(i<len(a)):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(\"*\")\n",
      "    i+=1\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/52:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "i=0    \n",
      "while(i<len(a)):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(a[i])\n",
      "    i+=1\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/53:\n",
      "a = list(\"hAck3rr4nk\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "i=0    \n",
      "while(i<len(a)):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(a[i])\n",
      "    i+=1\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/54:\n",
      "a = list(\"hAck3rr4nk\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "i=0    \n",
      "while(i<len(a)-1):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(a[i])\n",
      "    i+=1\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/55:\n",
      "a = list(\"hAck3rr4nk\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "i=0    \n",
      "while(i<len(a)-1):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(a[i])\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        \n",
      "    i+=1\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/56:\n",
      "a = list(\"hAck3rr4nk\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "i=0    \n",
      "while(i<len(a)-1):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(a[i])\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i] = temp\n",
      "        \n",
      "    i+=1\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/57:\n",
      "a = list(\"hAck3rr4nk\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "i=0    \n",
      "while(i<len(a)-1):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(a[i])\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i] = temp\n",
      "        insert(i+2,\"*\")\n",
      "    i+=1\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/58:\n",
      "a = list(\"hAck3rr4nk\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "i=0    \n",
      "while(i<len(a)-1):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(a[i])\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i] = temp\n",
      "        a.insert(i+2,\"*\")\n",
      "    i+=1\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/59:\n",
      "a = list(\"hAck3rr4nk\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "i=0    \n",
      "while(i<len(a)-1):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(a[i])\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        a.insert(i+2,\"*\")\n",
      "    i+=1\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/60:\n",
      "a = list(\"hAck3rr4nK\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "i=0    \n",
      "while(i<len(a)-1):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(a[i])\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        a.insert(i+2,\"*\")\n",
      "    i+=1\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/61:\n",
      "a = list(\"hAck3Rr4nk\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "i=0    \n",
      "while(i<len(a)-1):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(a[i])\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        a.insert(i+2,\"*\")\n",
      "    i+=1\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/62:\n",
      "a = list(\"hAck3Rr4nk\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "i=0    \n",
      "while(i<len(a)-1):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(a[i])\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        a.insert(i+2,\"*\")\n",
      "    i+=1\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "220/63:\n",
      "a = list(\"hAck3rR4nk\")\n",
      "\n",
      "a\n",
      "i=0\n",
      "while(i<len(a)):\n",
      "    if(a[i].isnumeric()==True):\n",
      "        a.insert(0,a[i])\n",
      "        a[i+1] = '0'\n",
      "        i+=1\n",
      "    i+=1\n",
      "i=0    \n",
      "while(i<len(a)-1):\n",
      "    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :\n",
      "        print(a[i])\n",
      "        temp = a[i]\n",
      "        a[i] = a[i+1]\n",
      "        a[i+1] = temp\n",
      "        a.insert(i+2,\"*\")\n",
      "    i+=1\n",
      "        \n",
      "    \n",
      "print(''.join(a))\n",
      "221/1:\n",
      "arr = [0]*5\n",
      "arr\n",
      "221/2:\n",
      "arr[:] = [1]*arr[:]\n",
      "arr\n",
      "221/3:\n",
      "arr[:] = [1]*5\n",
      "arr\n",
      "221/4:\n",
      "def cout(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            while(i<n):\n",
      "                arr[i]=1\n",
      "                i+=2\n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 5\n",
      "    print(count(n))\n",
      "221/5:\n",
      "def count(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            while(i<n):\n",
      "                arr[i]=1\n",
      "                i+=2\n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 5\n",
      "    print(count(n))\n",
      "221/6:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            while(i<n):\n",
      "                arr[i]=1\n",
      "                i+=2\n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 5\n",
      "    print(count1(n))\n",
      "221/7:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            while(i<n):\n",
      "                arr[i]=1\n",
      "                i+=2\n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 5\n",
      "    print(count1(n))\n",
      "221/8:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            while(i<n):\n",
      "                arr[i]=1\n",
      "                i+=2\n",
      "        arr\n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 5\n",
      "    print(count1(n))\n",
      "221/9:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            while(i<n):\n",
      "                arr[i]=1\n",
      "                i+=2\n",
      "        print(arr)\n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 5\n",
      "    print(count1(n))\n",
      "221/10:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            while(i<n):\n",
      "                arr[i]=0\n",
      "                i+=2\n",
      "        print(arr)\n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 5\n",
      "    print(count1(n))\n",
      "221/11:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            while(i<n):\n",
      "                if arr[i] == 1:\n",
      "                    arr[i]=0\n",
      "                else:\n",
      "                    arr[i]=1\n",
      "                \n",
      "                i+=2\n",
      "        print(arr)\n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 5\n",
      "    print(count1(n))\n",
      "221/12:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            while(i<n):\n",
      "                if arr[i] == 1:\n",
      "                    arr[i] = 0\n",
      "                else:\n",
      "                    arr[i] = 1\n",
      "                \n",
      "                i+=2\n",
      "        print(arr)\n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 5\n",
      "    print(count1(n))\n",
      "221/13:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            while(i<n):\n",
      "                if arr[i] == 1:\n",
      "                    arr[i] = 0\n",
      "                else:\n",
      "                    arr[i] = 1\n",
      "                \n",
      "                i+=2\n",
      "        print(arr)\n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 4\n",
      "    print(count1(n))\n",
      "221/14:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            while(i<n):\n",
      "                if arr[i] == 1:\n",
      "                    arr[i] = 0\n",
      "                else:\n",
      "                    arr[i] = 1\n",
      "                \n",
      "                i+=2\n",
      "     \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 372\n",
      "    print(count1(n))\n",
      "221/15:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            while(i<n):\n",
      "                if arr[i] == 1:\n",
      "                    arr[i] = 0\n",
      "                else:\n",
      "                    arr[i] = 1\n",
      "                \n",
      "                i+=2\n",
      "     \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 100\n",
      "    print(count1(n))\n",
      "221/16:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            while(i<n):\n",
      "                if arr[i] == 1:\n",
      "                    arr[i] = 0\n",
      "                else:\n",
      "                    arr[i] = 1\n",
      "                \n",
      "                i**=2\n",
      "     \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 100\n",
      "    print(count1(n))\n",
      "221/17:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            while(i<n):\n",
      "                if arr[i] == 1:\n",
      "                    arr[i] = 0\n",
      "                else:\n",
      "                    arr[i] = 1\n",
      "                \n",
      "                i = i**2\n",
      "     \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 100\n",
      "    print(count1(n))\n",
      "221/18:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        k = 1\n",
      "        j = k*i\n",
      "        while(j<n):\n",
      "            if arr[j] == 0:\n",
      "                arr[j] = 1\n",
      "            else:\n",
      "                arr[j] = 0\n",
      "            k+=1\n",
      "            j = k*i\n",
      "            \n",
      "     \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 100\n",
      "    print(count1(n))\n",
      "221/19:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        k = 1\n",
      "        j = k*i\n",
      "        while(j<n):\n",
      "            if arr[j] == 0:\n",
      "                arr[j] = 1\n",
      "            else:\n",
      "                arr[j] = 0\n",
      "            k+=1\n",
      "            j = k*i\n",
      "            \n",
      "     \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 5\n",
      "    print(count1(n))\n",
      "221/20:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        k = 1\n",
      "        j = k*i\n",
      "        while(j<n):\n",
      "            if arr[j] == 0:\n",
      "                arr[j] = 1\n",
      "            else:\n",
      "                arr[j] = 0\n",
      "            k+=1\n",
      "            j = k*i\n",
      "            print(j)\n",
      "            \n",
      "     \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 5\n",
      "    print(count1(n))\n",
      "221/21:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                print(j)\n",
      "            \n",
      "     \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 5\n",
      "    print(count1(n))\n",
      "221/22:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "            \n",
      "     \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 100\n",
      "    print(count1(n))\n",
      "221/23:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "            \n",
      "     \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 372\n",
      "    print(count1(n))\n",
      "221/24:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "            \n",
      "     \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 100\n",
      "    print(count1(n))\n",
      "221/25:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "            \n",
      "     \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 10\n",
      "    print(count1(n))\n",
      "221/26:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<=n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "            \n",
      "     \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 10\n",
      "    print(count1(n))\n",
      "221/27:\n",
      "def count1(n):\n",
      "    arr = [0]*n+1\n",
      "    arr[0] = 1\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<=n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "            \n",
      "     \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 10\n",
      "    print(count1(n))\n",
      "221/28:\n",
      "def count1(n):\n",
      "    arr = [0]*(n+1)\n",
      "    arr[0] = 1\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<=n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "            \n",
      "     \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 10\n",
      "    print(count1(n))\n",
      "221/29:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    print(arr)\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "        print(arr)\n",
      "                 \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 10\n",
      "    print(count1(n))\n",
      "221/30:\n",
      "def count1(n):\n",
      "    arr = [0]*(n+1)\n",
      "    print(arr)\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "        print(arr)\n",
      "                 \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 10\n",
      "    print(count1(n))\n",
      "221/31:\n",
      "def count1(n):\n",
      "    arr = [0]*(n+1)\n",
      "    print(arr)\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*(n+1)\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "        print(arr)\n",
      "                 \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 10\n",
      "    print(count1(n))\n",
      "221/32:\n",
      "def count1(n):\n",
      "    arr = [0]*(n+1)\n",
      "    arr[0]=1\n",
      "    print(arr)\n",
      "    for i in range(1:n):\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "        print(arr)\n",
      "                 \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 10\n",
      "    print(count1(n))\n",
      "221/33:\n",
      "def count1(n):\n",
      "    arr = [0]*(n+1)\n",
      "    arr[0]=1\n",
      "    print(arr)\n",
      "    for i in range(1,n):\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "        print(arr)\n",
      "                 \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 10\n",
      "    print(count1(n))\n",
      "221/34:\n",
      "def count1(n):\n",
      "    arr = [0]*(n+1)\n",
      "    arr[0]=1\n",
      "    print(arr)\n",
      "    for i in range(1,n):\n",
      "        \n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "        print(arr)\n",
      "                 \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 10\n",
      "    print(count1(n))\n",
      "221/36:\n",
      "def count1(n):\n",
      "    arr = [0]*(n+1)\n",
      "    arr[0]=1\n",
      "    print(arr)\n",
      "    for i in range(1,n):\n",
      "        \n",
      "        k = 1\n",
      "        j = k*i\n",
      "        while(j<n):\n",
      "            if arr[j] == 0:\n",
      "                arr[j] = 1\n",
      "            else:\n",
      "                arr[j] = 0\n",
      "            k+=1\n",
      "            j = k*i\n",
      "            #print(j)\n",
      "        print(arr)\n",
      "                 \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 10\n",
      "    print(count1(n))\n",
      "221/37:\n",
      "def count1(n):\n",
      "    arr = [0]*(n+1)\n",
      "    arr[0]=1\n",
      "    print(arr)\n",
      "    for i in range(1,n):\n",
      "        \n",
      "        k = 1\n",
      "        j = k*i\n",
      "        while(j<(n+1)):\n",
      "            if arr[j] == 0:\n",
      "                arr[j] = 1\n",
      "            else:\n",
      "                arr[j] = 0\n",
      "            k+=1\n",
      "            j = k*i\n",
      "            #print(j)\n",
      "        print(arr)\n",
      "                 \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 10\n",
      "    print(count1(n))\n",
      "221/38:\n",
      "def count1(n):\n",
      "    arr = [0]*(n+1)\n",
      "    arr[0]=1\n",
      "    print(arr)\n",
      "    for i in range(1,n):\n",
      "        \n",
      "        k = 1\n",
      "        j = k*i\n",
      "        while(j<(n+1)):\n",
      "            if arr[j] == 0:\n",
      "                arr[j] = 1\n",
      "            else:\n",
      "                arr[j] = 0\n",
      "            k+=1\n",
      "            j = k*i\n",
      "            #print(j)\n",
      "        print(arr)\n",
      "                 \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 100\n",
      "    print(count1(n))\n",
      "221/39:\n",
      "def count1(n):\n",
      "    arr = [0]*(n+1)\n",
      "    arr[0]=1\n",
      "    print(arr)\n",
      "    for i in range(1,n): \n",
      "        k = 1\n",
      "        j = k*i\n",
      "        while(j<(n+1)):\n",
      "            if arr[j] == 0:\n",
      "                arr[j] = 1\n",
      "            else:\n",
      "                arr[j] = 0\n",
      "            k+=1\n",
      "            j = k*i\n",
      "            #print(j)\n",
      "        print(arr)\n",
      "                 \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 10\n",
      "    print(count1(n))\n",
      "221/40:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    print(arr)\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "        print(arr)\n",
      "                 \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 10\n",
      "    print(count1(n))\n",
      "221/41:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    print(arr)\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "        print(arr)\n",
      "                 \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 100\n",
      "    print(count1(n))\n",
      "221/42:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    print(arr)\n",
      "    for i in range(n):\n",
      "        if i==0:\n",
      "            arr[:] = [1]*n\n",
      "        else:\n",
      "            k = 1\n",
      "            j = k*i\n",
      "            while(j<n):\n",
      "                if arr[j] == 0:\n",
      "                    arr[j] = 1\n",
      "                else:\n",
      "                    arr[j] = 0\n",
      "                k+=1\n",
      "                j = k*i\n",
      "                #print(j)\n",
      "        #print(arr)\n",
      "                 \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 200\n",
      "    print(count1(n))\n",
      "221/43:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    \n",
      "                 \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 5\n",
      "    print(count1(n))\n",
      "221/44:\n",
      "def count1(n):\n",
      "    arr = [0]*n\n",
      "    print(arr)\n",
      "                 \n",
      "    return arr.count(0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    n = 5\n",
      "    print(count1(n))\n",
      "222/1: from collections import defaultdict\n",
      "222/2:\n",
      "from collections import defaultdict\n",
      "\n",
      "class Graph:\n",
      "    def __init__(self):\n",
      "        self.graph = defaultdict(list)\n",
      "        \n",
      "    def add_points(self,u,v):\n",
      "        self.graph[u].append(v)\n",
      "        \n",
      "    def view(self):\n",
      "        print(graph)\n",
      "        \n",
      "g = Graph()\n",
      "g.add_points(0,1)\n",
      "g.add_points(0,2)\n",
      "g.add_points(1,2)\n",
      "g.view()\n",
      "222/3:\n",
      "from collections import defaultdict\n",
      "\n",
      "class Graph:\n",
      "    def __init__(self):\n",
      "        self.graph = defaultdict(list)\n",
      "        \n",
      "    def add_points(self,u,v):\n",
      "        self.graph[u].append(v)\n",
      "        \n",
      "    def view(self):\n",
      "        print(self.graph)\n",
      "        \n",
      "g = Graph()\n",
      "g.add_points(0,1)\n",
      "g.add_points(0,2)\n",
      "g.add_points(1,2)\n",
      "g.view()\n",
      "222/4:\n",
      "from collections import defaultdict\n",
      "\n",
      "class Graph:\n",
      "    def __init__(self):\n",
      "        self.graph = defaultdict(list)\n",
      "        \n",
      "    def add_points(self,u,v):\n",
      "        self.graph[u].append(v)\n",
      "        \n",
      "    def view(self):\n",
      "        print(self.graph,len(self.graph))\n",
      "        \n",
      "g = Graph()\n",
      "g.add_points(0,1)\n",
      "g.add_points(0,2)\n",
      "g.add_points(1,2)\n",
      "g.view()\n",
      "222/5:\n",
      "from collections import defaultdict\n",
      "\n",
      "class Graph:\n",
      "    def __init__(self):\n",
      "        self.graph = defaultdict(list)\n",
      "        \n",
      "    def add_points(self,u,v):\n",
      "        self.graph[u].append(v)\n",
      "        \n",
      "    def view(self):\n",
      "        print(self.graph,len(self.graph))\n",
      "        print(self.graph[2])\n",
      "        \n",
      "g = Graph()\n",
      "g.add_points(0,1)\n",
      "g.add_points(0,2)\n",
      "g.add_points(1,2)\n",
      "g.view()\n",
      "222/6:\n",
      "from collections import defaultdict\n",
      "\n",
      "class Graph:\n",
      "    def __init__(self):\n",
      "        self.graph = defaultdict(list)\n",
      "        \n",
      "    def add_points(self,u,v):\n",
      "        self.graph[u].append(v)\n",
      "        \n",
      "    def view(self):\n",
      "        print(self.graph,len(self.graph))\n",
      "        print(self.graph[1])\n",
      "        \n",
      "g = Graph()\n",
      "g.add_points(0,1)\n",
      "g.add_points(0,2)\n",
      "g.add_points(1,2)\n",
      "g.view()\n",
      "222/7:\n",
      "from collections import defaultdict\n",
      "\n",
      "class Graph:\n",
      "    def __init__(self):\n",
      "        self.graph = defaultdict(list)\n",
      "        \n",
      "    def add_points(self,u,v):\n",
      "        self.graph[u].append(v)\n",
      "        \n",
      "    def view(self):\n",
      "        print(self.graph,len(self.graph))\n",
      "        print(self.graph[0])\n",
      "        \n",
      "g = Graph()\n",
      "g.add_points(0,1)\n",
      "g.add_points(0,2)\n",
      "g.add_points(1,2)\n",
      "g.view()\n",
      "223/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "223/2:\n",
      "data = pd.read_csv(\"heart1.csv\")\n",
      "data\n",
      "224/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "228/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "228/2: data = pd.read_csv(\"Breast_cancer_dataset.csv\")\n",
      "228/3: data.head()\n",
      "228/4: data.describe()\n",
      "228/5: data.lebels()\n",
      "228/6: data.labels()\n",
      "228/7: data.label()\n",
      "228/8: data.columns()\n",
      "228/9: data.columns\n",
      "228/10: data.info()\n",
      "228/11: data.isnull().sum()\n",
      "228/12: data.drop([\"id\",\"Unname: 32\"], axis=1, inplace = True)\n",
      "228/13: data.drop([[\"id\"],[\"Unname: 32\"]], axis=1, inplace = True)\n",
      "228/14: data.drop([[\"id\"],[\"Unnamed: 32\"]], axis=1, inplace = True)\n",
      "228/15: data.drop([\"id\",\"Unnamed: 32\"], axis=1, inplace = True)\n",
      "228/16: data.head()\n",
      "228/17: data.unique()\n",
      "228/18: data[:].unique()\n",
      "228/19: data[\"diagnosis\"].unique()\n",
      "228/20:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "228/21:\n",
      "print(data[\"diagnosis\"].unique())\n",
      "label_encoder = preprocessing.LabelEncoder()\n",
      "data[\"diagnosis\"] = label_encoder.fit_transform(data[\"diagnosis\"])\n",
      "print(data[\"diagnosis\"].unique())\n",
      "228/22: data.head()\n",
      "228/23: data.corr()\n",
      "228/24:\n",
      "print(data.corr())\n",
      "sns.heatmap(data)\n",
      "228/25: sns.heatmap(data)\n",
      "228/26: sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
      "228/27:\n",
      "f,ax = plt.subplots(figsize=(18, 18))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
      "230/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "230/2: data = pd.read_csv(\"Breast_cancer_dataset.csv\")\n",
      "230/3: data.head()\n",
      "230/4: data.describe()\n",
      "230/5: data.info()\n",
      "230/6: data.isnull().sum()\n",
      "230/7: data.drop([\"id\",\"Unnamed: 32\"], axis=1, inplace = True)\n",
      "230/8: data.head()\n",
      "230/9:\n",
      "print(data[\"diagnosis\"].unique())\n",
      "label_encoder = preprocessing.LabelEncoder()\n",
      "data[\"diagnosis\"] = label_encoder.fit_transform(data[\"diagnosis\"])\n",
      "print(data[\"diagnosis\"].unique())\n",
      "230/10:\n",
      "data.head()      # M = 1\n",
      "                 # B = 0\n",
      "230/11: print(data.corr())\n",
      "230/12:\n",
      "f,ax = plt.subplots(figsize=(18, 18))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
      "230/13:\n",
      "f,ax = plt.subplots(figsize=(18, 18))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=5, fmt= '.1f',ax=ax)\n",
      "230/14:\n",
      "f,ax = plt.subplots(figsize=(18, 18))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
      "230/15: sns.pairplot(data,hue = \"diagnosis\")\n",
      "230/16: sns.pairplot(data, hue = \"diagnosis\", var = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "230/17: sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "231/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "231/2: data = pd.read_csv(\"Breast_cancer_dataset.csv\")\n",
      "231/3: data.head()\n",
      "231/4: data.describe()\n",
      "231/5: data.info()\n",
      "231/6: data.isnull().sum()\n",
      "231/7: data.drop([\"id\",\"Unnamed: 32\"], axis=1, inplace = True)\n",
      "231/8: data.head()\n",
      "231/9: sns.countplot(data[\"diagnosis\"])\n",
      "231/10:\n",
      "print(data[\"diagnosis\"].unique())\n",
      "label_encoder = preprocessing.LabelEncoder()\n",
      "data[\"diagnosis\"] = label_encoder.fit_transform(data[\"diagnosis\"])\n",
      "print(data[\"diagnosis\"].unique())\n",
      "231/11: sns.countplot(data[\"diagnosis\"])\n",
      "231/12:\n",
      "data.head()      # M = 1\n",
      "                 # B = 0\n",
      "231/13: print(data.corr())\n",
      "231/14:\n",
      "f,ax = plt.subplots(figsize=(18, 18))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
      "231/15: #sns.pairplot(data,hue = \"diagnosis\")\n",
      "231/16: sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "232/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "232/2: data = pd.read_csv(\"Breast_cancer_dataset.csv\")\n",
      "232/3: data.head()\n",
      "232/4: data.describe()\n",
      "232/5: data.info()\n",
      "232/6: data.isnull().sum()\n",
      "232/7: data.drop([\"id\",\"Unnamed: 32\"], axis=1, inplace = True)\n",
      "232/8: data.head()\n",
      "232/9: sns.countplot(data[\"diagnosis\"])\n",
      "232/10: sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "232/11:\n",
      "print(data[\"diagnosis\"].unique())\n",
      "label_encoder = preprocessing.LabelEncoder()\n",
      "data[\"diagnosis\"] = label_encoder.fit_transform(data[\"diagnosis\"])\n",
      "print(data[\"diagnosis\"].unique())\n",
      "232/12: sns.countplot(data[\"diagnosis\"])\n",
      "232/13:\n",
      "data.head()      # M = 1\n",
      "                 # B = 0\n",
      "232/14: print(data.corr())\n",
      "232/15:\n",
      "f,ax = plt.subplots(figsize=(18, 18))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
      "232/16: #sns.pairplot(data,hue = \"diagnosis\")\n",
      "232/17: sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "232/18: sns.countplot(data[\"radius_mean\"])\n",
      "232/19:\n",
      "plt.figure(figsize=(20,0))\n",
      "sns.countplot(data[\"radius_mean\"])\n",
      "232/20:\n",
      "plt.figure(figsize=(20,8))\n",
      "sns.countplot(data[\"radius_mean\"])\n",
      "232/21:\n",
      "plt.figure(figsize=(20,10))\n",
      "sns.countplot(data[\"radius_mean\"])\n",
      "232/22:\n",
      "plt.figure(figsize=(20,8))\n",
      "sns.countplot(data[\"radius_mean\"])\n",
      "233/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "233/2: data = pd.read_csv(\"Breast_cancer_dataset.csv\")\n",
      "233/3: data.head()\n",
      "233/4: data.describe()\n",
      "233/5: data.info()\n",
      "233/6: data.isnull().sum()\n",
      "233/7: data.drop([\"id\",\"Unnamed: 32\"], axis=1, inplace = True)\n",
      "233/8: data.head()\n",
      "233/9: #sns.countplot(data[\"diagnosis\"])\n",
      "233/10: #sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "233/11:\n",
      "print(data[\"diagnosis\"].unique())\n",
      "label_encoder = preprocessing.LabelEncoder()\n",
      "data[\"diagnosis\"] = label_encoder.fit_transform(data[\"diagnosis\"])\n",
      "print(data[\"diagnosis\"].unique())\n",
      "233/12: sns.countplot(data[\"diagnosis\"])\n",
      "233/13:\n",
      "data.head()      # M = 1\n",
      "                 # B = 0\n",
      "233/14: print(data.corr())\n",
      "233/15:\n",
      "f,ax = plt.subplots(figsize=(18, 18))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
      "233/16: #sns.pairplot(data,hue = \"diagnosis\")\n",
      "233/17:\n",
      "sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1 = M, 0 = B\n",
      "233/18:\n",
      "plt.figure(figsize=(20,8))\n",
      "sns.countplot(data[\"radius_mean\"])\n",
      "233/19: sns.heatmap(data)\n",
      "233/20:\n",
      "plt.figure(figsize=(20,5))\n",
      "sns.heatmap(data)\n",
      "233/21:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "233/22: data.index\n",
      "233/23: data.columns\n",
      "233/24: data_x_labels = data.drop(data[\"diagnosis\"], axis=1)\n",
      "233/25: data_x_labels = data.drop((\"diagnosis\"), axis=1)\n",
      "233/26:\n",
      "data_x_labels = data.drop((\"diagnosis\"), axis=1)\n",
      "data_x_labels.head()\n",
      "233/27:\n",
      "data_x_labels = data.drop((\"diagnosis\"), axis=1)\n",
      "#data_x_labels.head()\n",
      "\n",
      "data_y_label = data[\"diagnosis\"]\n",
      "233/28:\n",
      "data_x_labels = data.drop((\"diagnosis\"), axis=1)\n",
      "#data_x_labels.head()\n",
      "\n",
      "data_y_label = data[\"diagnosis\"]\n",
      "data_y_label.head()\n",
      "233/29:\n",
      "data_x_labels = data.drop((\"diagnosis\"), axis=1)\n",
      "#data_x_labels.head()\n",
      "\n",
      "data_y_label = data[\"diagnosis\"]\n",
      "233/30: x_train,x_test,y_train,y_test = train_test_split(data_x_labels, data_y_label, test_size = 0.2, random_state = 42)\n",
      "233/31: x_train.head()\n",
      "233/32: x_train,x_test,y_train,y_test = train_test_split(data_x_labels, data_y_label, test_size = 0.2, random_state = 42)\n",
      "233/33: x_train.head()\n",
      "233/34:\n",
      "print(x_train.head())\n",
      "print(y_train.head())\n",
      "233/35:\n",
      "# print(x_train.head())\n",
      "# print(y_train.head())\n",
      "233/36:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.preprocessing immport StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "233/37:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "233/38:\n",
      "stnd_scl = StandardScaler()\n",
      "x_train = stnd_scl.fit_transform(x_train)\n",
      "x_test = stnd_scl.fit_transform(x_test)\n",
      "233/39:\n",
      "print(x_train.head())\n",
      "# print(y_train.head())\n",
      "233/40:\n",
      "stnd_scl = StandardScaler()\n",
      "x_train = stnd_scl.fit_transform(x_train)\n",
      "x_test = stnd_scl.fit_transform(x_test)\n",
      "x_train\n",
      "234/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "234/2: data = pd.read_csv(\"Breast_cancer_dataset.csv\")\n",
      "234/3: data.head()\n",
      "234/4: data.columns\n",
      "234/5: data.describe()\n",
      "234/6: data.info()\n",
      "234/7: data.isnull().sum() # looking for null values\n",
      "234/8: data.drop([\"id\",\"Unnamed: 32\"], axis=1, inplace = True) # droping the unwanted columns\n",
      "234/9: data.head()\n",
      "234/10: #sns.countplot(data[\"diagnosis\"])\n",
      "234/11: #sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "234/12:\n",
      "print(data[\"diagnosis\"].unique())\n",
      "label_encoder = preprocessing.LabelEncoder()\n",
      "data[\"diagnosis\"] = label_encoder.fit_transform(data[\"diagnosis\"])\n",
      "print(data[\"diagnosis\"].unique())\n",
      "234/13: sns.countplot(data[\"diagnosis\"])\n",
      "234/14:\n",
      "data.head()      # M = 1\n",
      "                 # B = 0\n",
      "234/15:\n",
      "plt.figure(figsize=(15,8))              #HEAT MAP\n",
      "sns.heatmap(data)\n",
      "234/16: #sns.pairplot(data,hue = \"diagnosis\")\n",
      "234/17:\n",
      "plt.figure(figsize=(20,8))           \n",
      "sns.countplot(data[\"radius_mean\"])\n",
      "234/18: data.corr()\n",
      "234/19:\n",
      "f,ax = plt.subplots(figsize=(18, 18))    #CORRELATION HEAT MAP\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
      "234/20:\n",
      "sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "# 1 = M, 0 = B\n",
      "234/21:\n",
      "data_x_labels = data.drop((\"diagnosis\"), axis=1)\n",
      "#data_x_labels.head()\n",
      "\n",
      "data_y_label = data[\"diagnosis\"]\n",
      "234/22: x_train,x_test,y_train,y_test = train_test_split(data_x_labels, data_y_label, test_size = 0.2, random_state = 42)\n",
      "234/23:\n",
      "print(x_train.head())\n",
      "# print(y_train.head())\n",
      "234/24:\n",
      "stnd_scl = StandardScaler()\n",
      "x_train = stnd_scl.fit_transform(x_train)\n",
      "x_test = stnd_scl.fit_transform(x_test)\n",
      "x_train\n",
      "234/25: model_1 = LogisticRegression()\n",
      "235/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "235/2: data = pd.read_csv(\"Breast_cancer_dataset.csv\")\n",
      "235/3: data.head()\n",
      "235/4: data.columns\n",
      "235/5: data.describe()\n",
      "235/6: data.info()\n",
      "235/7: data.isnull().sum() # looking for null values\n",
      "235/8: data.drop([\"id\",\"Unnamed: 32\"], axis=1, inplace = True) # droping the unwanted columns\n",
      "235/9: data.head()\n",
      "235/10: #sns.countplot(data[\"diagnosis\"])\n",
      "235/11: #sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "235/12:\n",
      "print(data[\"diagnosis\"].unique())\n",
      "label_encoder = preprocessing.LabelEncoder()\n",
      "data[\"diagnosis\"] = label_encoder.fit_transform(data[\"diagnosis\"])\n",
      "print(data[\"diagnosis\"].unique())\n",
      "235/13: sns.countplot(data[\"diagnosis\"])\n",
      "235/14:\n",
      "data.head()      # M = 1\n",
      "                 # B = 0\n",
      "235/15:\n",
      "plt.figure(figsize=(15,8))              #HEAT MAP\n",
      "sns.heatmap(data)\n",
      "235/16: #sns.pairplot(data,hue = \"diagnosis\")\n",
      "235/17:\n",
      "plt.figure(figsize=(20,8))           \n",
      "sns.countplot(data[\"radius_mean\"])\n",
      "235/18: data.corr()\n",
      "235/19:\n",
      "f,ax = plt.subplots(figsize=(18, 18))    #CORRELATION HEAT MAP\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
      "235/20:\n",
      "sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "# 1 = M, 0 = B\n",
      "235/21:\n",
      "data_x_labels = data.drop((\"diagnosis\"), axis=1)\n",
      "#data_x_labels.head()\n",
      "\n",
      "data_y_label = data[\"diagnosis\"]\n",
      "235/22:\n",
      "stnd_scl = StandardScaler()\n",
      "data_x_labels = stnd_scl.fit_transform(data_x_labels)\n",
      "#x_test = stnd_scl.fit_transform(x_test)\n",
      "#x_train\n",
      "#-1.44075296, -0.43531947, -1.36208497, ...,  0.9320124 ,\n",
      " #        2.09724217,  1.88645014]\n",
      "235/23: x_train,x_test,y_train,y_test = train_test_split(data_x_labels, data_y_label, test_size = 0.2, random_state = 42)\n",
      "235/24:\n",
      "print(x_train)\n",
      "# print(y_train.head())\n",
      "235/25: model_1 = LogisticRegression()\n",
      "236/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "236/2: data = pd.read_csv(\"Breast_cancer_dataset.csv\")\n",
      "236/3: data.head()\n",
      "236/4: data.columns\n",
      "236/5: data.describe()\n",
      "236/6: data.info()\n",
      "236/7: data.isnull().sum() # looking for null values\n",
      "236/8: data.drop([\"id\",\"Unnamed: 32\"], axis=1, inplace = True) # droping the unwanted columns\n",
      "236/9: data.head()\n",
      "236/10: #sns.countplot(data[\"diagnosis\"])\n",
      "236/11: #sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "236/12:\n",
      "print(data[\"diagnosis\"].unique())\n",
      "label_encoder = preprocessing.LabelEncoder()\n",
      "data[\"diagnosis\"] = label_encoder.fit_transform(data[\"diagnosis\"])\n",
      "print(data[\"diagnosis\"].unique())\n",
      "236/13: sns.countplot(data[\"diagnosis\"])\n",
      "236/14:\n",
      "data.head()      # M = 1\n",
      "                 # B = 0\n",
      "236/15:\n",
      "plt.figure(figsize=(15,8))              #HEAT MAP\n",
      "sns.heatmap(data)\n",
      "236/16: #sns.pairplot(data,hue = \"diagnosis\")\n",
      "236/17:\n",
      "plt.figure(figsize=(20,8))           \n",
      "sns.countplot(data[\"radius_mean\"])\n",
      "236/18: data.corr()\n",
      "236/19:\n",
      "f,ax = plt.subplots(figsize=(18, 18))    #CORRELATION HEAT MAP\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
      "236/20:\n",
      "sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "# 1 = M, 0 = B\n",
      "236/21:\n",
      "data_x_labels = data.drop((\"diagnosis\"), axis=1)\n",
      "#data_x_labels.head()\n",
      "\n",
      "data_y_label = data[\"diagnosis\"]\n",
      "236/22:\n",
      "stnd_scl = StandardScaler()\n",
      "data_x_labels = stnd_scl.fit_transform(data_x_labels)\n",
      "#x_test = stnd_scl.fit_transform(x_test)\n",
      "#x_train\n",
      "#-1.44075296, -0.43531947, -1.36208497, ...,  0.9320124 ,\n",
      " #        2.09724217,  1.88645014]\n",
      "    #[-1.44798723 -0.45602336 -1.36665103 ...  0.91959172  2.14719008\n",
      "   #1.85943247]\n",
      "236/23: x_train,x_test,y_train,y_test = train_test_split(data_x_labels, data_y_label, test_size = 0.2, random_state = 42)\n",
      "236/24:\n",
      "print(x_train)\n",
      "# print(y_train.head())\n",
      "236/25: model_1 = LogisticRegression()\n",
      "236/26:\n",
      "#print(x_train)\n",
      "# print(y_train.head())\n",
      "236/27:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.matrics import accuuracy_score\n",
      "236/28:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.metrics import accuuracy_score\n",
      "236/29:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.metrics import accuracy_score\n",
      "236/30:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "236/31:\n",
      "model_1 = LogisticRegression()\n",
      "model_1.fit(x_train,y_train)\n",
      "model_1_y_prediction = model_1.predict(x_test)\n",
      "print(\"Accurecy of Logistic Regrassion: \",accuracy_score(y_test, model_1_y_prediction))\n",
      "236/32:\n",
      "model_1 = LogisticRegression()\n",
      "model_1.fit(x_train,y_train)\n",
      "model_1_y_prediction = model_1.predict(x_test)\n",
      "print(\"Accuracy of Logistic Regrassion: \",accuracy_score(y_test, model_1_y_prediction))\n",
      "236/33:\n",
      "model_2 = DecisionTreeClassifier()\n",
      "model_2.fit(x_train,y_train)\n",
      "model_2_y_prediction = model_2.predict(x_test)\n",
      "print(\"Accuracy of Logistic Regrassion: \",accuracy_score(y_test, model_2_y_prediction))\n",
      "236/34:\n",
      "model_2 = DecisionTreeClassifier()\n",
      "model_2.fit(x_train,y_train)\n",
      "model_2_y_prediction = model_2.predict(x_test)\n",
      "print(\"Accuracy of Logistic Regrassion: \",accuracy_score(y_test, model_2_y_prediction))\n",
      "236/35:\n",
      "model_2 = DecisionTreeClassifier()\n",
      "model_2.fit(x_train,y_train)\n",
      "model_2_y_prediction = model_2.predict(x_test)\n",
      "print(\"Accuracy of Logistic Regrassion: \",accuracy_score(y_test, model_2_y_prediction))\n",
      "236/36:\n",
      "model_2 = DecisionTreeClassifier()\n",
      "model_2.fit(x_train,y_train)\n",
      "model_2_y_prediction = model_2.predict(x_test)\n",
      "print(\"Accuracy of Logistic Regrassion: \",accuracy_score(y_test, model_2_y_prediction))\n",
      "236/37:\n",
      "model_2 = DecisionTreeClassifier(random_state = 45)\n",
      "model_2.fit(x_train,y_train)\n",
      "model_2_y_prediction = model_2.predict(x_test)\n",
      "print(\"Accuracy of Logistic Regrassion: \",accuracy_score(y_test, model_2_y_prediction))\n",
      "236/38:\n",
      "model_2 = DecisionTreeClassifier(random_state = 45)\n",
      "model_2.fit(x_train,y_train)\n",
      "model_2_y_prediction = model_2.predict(x_test)\n",
      "print(\"Accuracy of Logistic Regrassion: \",accuracy_score(y_test, model_2_y_prediction))\n",
      "236/39:\n",
      "model_2 = DecisionTreeClassifier(random_state = 45)\n",
      "model_2.fit(x_train,y_train)\n",
      "model_2_y_prediction = model_2.predict(x_test)\n",
      "print(\"Accuracy of Logistic Regrassion: \",accuracy_score(y_test, model_2_y_prediction))\n",
      "236/40:\n",
      "model_1 = LogisticRegression()\n",
      "model_1.fit(x_train,y_train)\n",
      "model_1_y_prediction = model_1.predict(x_test)\n",
      "print(\"Accuracy of Logistic Regrassion: \",accuracy_score(y_test, model_1_y_prediction))\n",
      "236/41:\n",
      "model_1 = LogisticRegression()\n",
      "model_1.fit(x_train,y_train)\n",
      "model_1_y_prediction = model_1.predict(x_test)\n",
      "print(\"Accuracy of Logistic Regrassion: \",accuracy_score(y_test, model_1_y_prediction))\n",
      "236/42:\n",
      "model_1 = LogisticRegression()\n",
      "model_1.fit(x_train,y_train)\n",
      "model_1_y_prediction = model_1.predict(x_test)\n",
      "print(\"Accuracy of Logistic Regrassion: \",accuracy_score(y_test, model_1_y_prediction))\n",
      "236/43:\n",
      "model_3 = RandomForestClassifier(n_estimators=100, random_state=45)\n",
      "model_3.fit(x_train, y_train)\n",
      "model_3_y_prediction = model_3.predict(x_test)\n",
      "print(\"Accuracy of Random Forest Classifier: \",accuracy_score(y_test, model_1_y_prediction))\n",
      "236/44:\n",
      "confu_matrix = confusion_matrix(y_test, model_3_y_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "236/45:\n",
      "confu_matrix = confusion_matrix(y_test, model_3_y_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=False)\n",
      "plt.show()\n",
      "236/46:\n",
      "confu_matrix = confusion_matrix(y_test, model_3_y_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "236/47:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "236/48:\n",
      "cross_val = cross_val_score(estimator=model_3, x=x_train, y=y_train)\n",
      "printf(\"Cross validation accuracy of Random Forest Classifier: \",cross_val)\n",
      "printf(\"Cross validation mean accuracy of Random Forest Classifier: \", cross_val.mean())\n",
      "236/49:\n",
      "cross_val = cross_val_score(estimator=model_3, X=x_train, y=y_train)\n",
      "printf(\"Cross validation accuracy of Random Forest Classifier: \",cross_val)\n",
      "printf(\"Cross validation mean accuracy of Random Forest Classifier: \", cross_val.mean())\n",
      "236/50:\n",
      "cross_val = cross_val_score(estimator=model_3, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Random Forest Classifier: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Random Forest Classifier: \", cross_val.mean())\n",
      "239/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "239/2: data = pd.read_csv(\"Breast_cancer_dataset.csv\")\n",
      "239/3: data.head()\n",
      "239/4: data.columns\n",
      "239/5: data.describe()\n",
      "239/6: data.info()\n",
      "239/7: data.isnull().sum() # looking for null values\n",
      "239/8: data.drop([\"id\",\"Unnamed: 32\"], axis=1, inplace = True) # droping the unwanted columns\n",
      "239/9: data.head()\n",
      "239/10: #sns.countplot(data[\"diagnosis\"])\n",
      "239/11: #sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "239/12:\n",
      "print(data[\"diagnosis\"].unique())\n",
      "label_encoder = preprocessing.LabelEncoder()\n",
      "data[\"diagnosis\"] = label_encoder.fit_transform(data[\"diagnosis\"])\n",
      "print(data[\"diagnosis\"].unique())\n",
      "239/13: sns.countplot(data[\"diagnosis\"])\n",
      "239/14:\n",
      "data.head()      # M = 1 (It means the the tumor is cancerous or having breast cancer.)\n",
      "                 # B = 0 ( It means the tumor is normal and not a cancerous type.)\n",
      "239/15:\n",
      "plt.figure(figsize=(15,8))              #HEAT MAP\n",
      "sns.heatmap(data)\n",
      "239/16: #sns.pairplot(data,hue = \"diagnosis\")\n",
      "239/17:\n",
      "plt.figure(figsize=(20,8))           \n",
      "sns.countplot(data[\"radius_mean\"])\n",
      "239/18: data.corr()\n",
      "239/19:\n",
      "f,ax = plt.subplots(figsize=(18, 18))    #CORRELATION HEAT MAP\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
      "239/20:\n",
      "sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "# 1 = M, 0 = B\n",
      "240/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "240/2: data = pd.read_csv(\"Breast_cancer_dataset.csv\")\n",
      "240/3: data.head()\n",
      "240/4: data.columns\n",
      "240/5: data.describe()\n",
      "240/6: data.info()\n",
      "240/7: data.isnull().sum() # looking for null values\n",
      "240/8: data.drop([\"id\",\"Unnamed: 32\"], axis=1, inplace = True) # droping the unwanted columns\n",
      "240/9: data.head()\n",
      "240/10: #sns.countplot(data[\"diagnosis\"])\n",
      "240/11: #sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "240/12:\n",
      "print(data[\"diagnosis\"].unique())\n",
      "label_encoder = preprocessing.LabelEncoder()\n",
      "data[\"diagnosis\"] = label_encoder.fit_transform(data[\"diagnosis\"])\n",
      "print(data[\"diagnosis\"].unique())\n",
      "240/13: sns.countplot(data[\"diagnosis\"])\n",
      "240/14:\n",
      "data.head()      # M = 1 (It means the the tumor is cancerous or having breast cancer.)\n",
      "                 # B = 0 ( It means the tumor is normal and not a cancerous type.)\n",
      "240/15:\n",
      "plt.figure(figsize=(15,8))              #HEAT MAP\n",
      "sns.heatmap(data)\n",
      "240/16: #sns.pairplot(data,hue = \"diagnosis\")\n",
      "240/17:\n",
      "plt.figure(figsize=(20,8))           \n",
      "sns.countplot(data[\"radius_mean\"])\n",
      "240/18: data.corr()\n",
      "240/19:\n",
      "f,ax = plt.subplots(figsize=(18, 18))    #CORRELATION HEAT MAP\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
      "240/20:\n",
      "sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "# 1 = M, 0 = B\n",
      "240/21:\n",
      "data_x_labels = data.drop((\"diagnosis\"), axis=1)\n",
      "#data_x_labels.head()\n",
      "\n",
      "data_y_label = data[\"diagnosis\"]\n",
      "240/22:\n",
      "stnd_scl = StandardScaler()\n",
      "data_x_labels = stnd_scl.fit_transform(data_x_labels)\n",
      "240/23: x_train,x_test,y_train,y_test = train_test_split(data_x_labels, data_y_label, test_size = 0.2, random_state = 42)\n",
      "240/24:\n",
      "#print(x_train)\n",
      "# print(y_train.head())\n",
      "240/25:\n",
      "model_1 = LogisticRegression()\n",
      "model_1.fit(x_train,y_train)\n",
      "model_1_y_prediction = model_1.predict(x_test)\n",
      "print(\"Accuracy of Logistic Regrassion: \",accuracy_score(y_test, model_1_y_prediction))\n",
      "240/26:\n",
      "model_2 = DecisionTreeClassifier(random_state = 45)\n",
      "model_2.fit(x_train,y_train)\n",
      "model_2_y_prediction = model_2.predict(x_test)\n",
      "print(\"Accuracy of Decission Tree Classifier: \",accuracy_score(y_test, model_2_y_prediction))\n",
      "240/27:\n",
      "model_3 = RandomForestClassifier(n_estimators=100, random_state=45)\n",
      "model_3.fit(x_train, y_train)\n",
      "model_3_y_prediction = model_3.predict(x_test)\n",
      "print(\"Accuracy of Random Forest Classifier: \",accuracy_score(y_test, model_1_y_prediction))\n",
      "240/28:\n",
      "confu_matrix = confusion_matrix(y_test, model_3_y_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "240/29:\n",
      "cross_val = cross_val_score(estimator=model_3, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Random Forest Classifier: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Random Forest Classifier: \", cross_val.mean())\n",
      "242/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "242/2:\n",
      "data = pd.read_csv(\"diabetes.csv\")\n",
      "data.head()\n",
      "242/3: data.isnull().sum()\n",
      "242/4: data.info()\n",
      "242/5: data.describe()\n",
      "242/6: data.corr()\n",
      "242/7:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "242/8:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True)\n",
      "242/9:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "242/10: sns.heatmap(data)\n",
      "242/11:\n",
      "plt.figure(figsize=(20,8))\n",
      "sns.heatmap(data)\n",
      "242/12:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "242/13: sns.boxenplot(data)\n",
      "242/14: plt.boxenplot(data)\n",
      "242/15: plt.boxplot(data)\n",
      "242/16: plt.boxplot(data[\"outcome\"])\n",
      "242/17: plt.boxplot(data(\"outcome\"))\n",
      "242/18: plt.boxplot(data(\"Outcome\"))\n",
      "242/19: plt.boxplot(data[\"Outcome\"])\n",
      "242/20: data_2 = data.drop([\"Outcome\"],axise=1)\n",
      "242/21: data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "242/22:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "data_2\n",
      "242/23:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/24:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "ax = sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "ax.tick_params(labelrotation=90)\n",
      "242/25:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "242/26: data[\"Outcome\"].countplot()\n",
      "242/27: sns.countplot(data[\"Outcome\"])\n",
      "242/28:\n",
      "print(data[\"Outcome\"].count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "242/29:\n",
      "print(data[\"Outcome\"].count(0))\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "242/30:\n",
      "print(data[\"Outcome\"].count(\"0\"))\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "242/31:\n",
      "print(data[\"Outcome\"].count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "242/32:\n",
      "print(data[\"Outcome\"].count().unique())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "242/33:\n",
      "print(data[\"Outcome\"].count().unique\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "242/34:\n",
      "print(data[\"Outcome\"].count().unique)\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "242/35:\n",
      "print(data[\"Outcome\"].unique.count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "242/36:\n",
      "print(data[\"Outcome\"].unique().count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "242/37:\n",
      "print(data[\"Outcome\"].unique().count)\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "242/38:\n",
      "print(data[\"Outcome\"].unique())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "242/39:\n",
      "print(count(data[\"Outcome\"].unique())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "242/40:\n",
      "print(count(data[\"Outcome\"].unique()))\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "242/41:\n",
      "print(data.groupby([\"Oucome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "242/42:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "242/43: sns.pairplot(data)\n",
      "242/44: sns.pairplot(data, hue=\"Outcome\")\n",
      "242/45: data.columns\n",
      "242/46: data = pd.read_csv(\"diabetes.csv\")\n",
      "242/47: data.head()\n",
      "242/48:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "242/49:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "242/50:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(data.drop(\"Outcome\"))\n",
      "242/51:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(data.drop[(\"Outcome\")])\n",
      "242/52:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(data.drop[\"Outcome\"])\n",
      "242/53:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(data.drop((\"Outcome\"),axis=1))\n",
      "242/54:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(data.drop((\"Outcome\"),axis=1))\n",
      "std_scl_x\n",
      "242/55:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(data.drop((\"Outcome\"),axis=1))\n",
      "#std_scl_x\n",
      "242/56:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "x_data\n",
      "242/57:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "x_data.head()\n",
      "242/58:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "y_data = data[\"Outcome\"]\n",
      "y_data\n",
      "242/59:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "y_data = data[\"Outcome\"]\n",
      "242/60:\n",
      "# Splitting the non scaled data.\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "242/61:\n",
      "# Splitting the non scaled data.\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "x_train\n",
      "242/62:\n",
      "# Splitting the non scaled data.\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "x_train\n",
      "242/63:\n",
      "# Featured scaled data.\n",
      "std_scl_x_train,std_scl_x_test,y_train,y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)\n",
      "std_scl_x_train\n",
      "242/64:\n",
      "# Featured scaled data.\n",
      "std_scl_x_train,std_scl_x_test,y_train,y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)\n",
      "std_scl_x_train\n",
      "y_train\n",
      "242/65:\n",
      "# Splitting the non scaled data.\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "x_train\n",
      "y_train\n",
      "242/66:\n",
      "# Splitting the non scaled data.\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "# Featured scaled data.\n",
      "std_scl_x_train,std_scl_x_test,y_train,y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)\n",
      "242/67:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "242/68:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_reg_predict1 = log_reg.predict(y_test)\n",
      "print(\"Accuracy of Non scaled data in logistic regration: \",accuracy_score(log_reg_predict1,y_test))\n",
      "242/69:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_reg_predict1 = log_reg.predict(y_test)\n",
      "print(\"Accuracy of Non scaled data in logistic regration: \",accuracy_score(y_test,log_reg_predict1))\n",
      "242/70:\n",
      "# Splitting the non scaled data.\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "# Featured scaled data.\n",
      "std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)\n",
      "242/71:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_reg_predict1 = log_reg.predict(y_test)\n",
      "print(\"Accuracy of Non scaled data in logistic regration: \",accuracy_score(y_test,log_reg_predict1))\n",
      "244/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "244/2: data = pd.read_csv(\"diabetes.csv\")\n",
      "244/3: data.head()\n",
      "244/4:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "244/5: data.isnull().sum()\n",
      "244/6: data.info()\n",
      "244/7: data.describe()\n",
      "244/8:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "244/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "244/10: sns.pairplot(data, hue=\"Outcome\")\n",
      "244/11: data.corr()\n",
      "244/12:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "244/13:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "244/14:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(data.drop((\"Outcome\"),axis=1))\n",
      "#std_scl_x\n",
      "244/15:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "y_data = data[\"Outcome\"]\n",
      "244/16:\n",
      "# Splitting the non scaled data.\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "\n",
      "# Featured scaled data.\n",
      "std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)\n",
      "244/17:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_reg_predict1 = log_reg.predict(y_test)\n",
      "print(\"Accuracy of Non scaled data in logistic regration: \",accuracy_score(y_test,log_reg_predict1))\n",
      "244/18:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "y_data = data[\"Outcome\"]\n",
      "y_data\n",
      "244/19:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "y_data = data[\"Outcome\"]reshape(-1,1)\n",
      "y_data\n",
      "244/20:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "y_data = data[\"Outcome\"].reshape(-1,1)\n",
      "y_data\n",
      "244/21:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "y_data = data[\"Outcome\"]\n",
      "y_data.reshape(-1,1)\n",
      "244/22:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "y_data = data[[\"Outcome\"]\n",
      "244/23:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "y_data = data[[\"Outcome\"]]\n",
      "244/24:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "y_data = data[[\"Outcome\"]].reshape(-1,1)\n",
      "244/25:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "y_data = data[[\"Outcome\"]]\n",
      "y_data.reshape(-1,1)\n",
      "244/26:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "y_data = data[[\"Outcome\"]]\n",
      "y_data\n",
      "244/27:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "y_data = np.array(data[\"Outcome\"])\n",
      "y_data\n",
      "244/28:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "y_data = np.array(data[\"Outcome\"]).reshape(-1,1)\n",
      "244/29:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_reg_predict1 = log_reg.predict(y_test)\n",
      "print(\"Accuracy of Non scaled data in logistic regration: \",accuracy_score(y_test,log_reg_predict1))\n",
      "244/30:\n",
      "# Splitting the non scaled data.\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "x_train = np.array(x_train).reshape(-1,1)\n",
      "\n",
      "# Featured scaled data.\n",
      "std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)\n",
      "244/31:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_reg_predict1 = log_reg.predict(y_test)\n",
      "print(\"Accuracy of Non scaled data in logistic regration: \",accuracy_score(y_test,log_reg_predict1))\n",
      "245/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "245/2: data = pd.read_csv(\"diabetes.csv\")\n",
      "245/3: data.head()\n",
      "245/4:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "245/5: data.isnull().sum()\n",
      "245/6: data.info()\n",
      "245/7: data.describe()\n",
      "245/8:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "245/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "245/10: sns.pairplot(data, hue=\"Outcome\")\n",
      "245/11: data.corr()\n",
      "245/12:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "245/13:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "245/14:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(data.drop((\"Outcome\"),axis=1))\n",
      "#std_scl_x\n",
      "245/15:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "y_data = data[\"Outcome\"]\n",
      "#y_data = np.array(data[\"Outcome\"]).reshape(-1,1)\n",
      "245/16:\n",
      "# Splitting the non scaled data.\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "#x_train = np.array(x_train).reshape(-1,1)\n",
      "\n",
      "# Featured scaled data.\n",
      "#std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)\n",
      "245/17:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_reg_predict1 = log_reg.predict(y_test)\n",
      "print(\"Accuracy of Non scaled data in logistic regration: \",accuracy_score(y_test,log_reg_predict1))\n",
      "245/18:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "#y_data = data[\"Outcome\"]\n",
      "y_data = np.array(data[\"Outcome\"]).reshape(-1,1)\n",
      "245/19:\n",
      "# Splitting the non scaled data.\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "#x_train = np.array(x_train).reshape(-1,1)\n",
      "\n",
      "# Featured scaled data.\n",
      "#std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)\n",
      "245/20:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_reg_predict1 = log_reg.predict(y_test)\n",
      "print(\"Accuracy of Non scaled data in logistic regration: \",accuracy_score(y_test,log_reg_predict1))\n",
      "245/21:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "x_data = np.array(x_data)\n",
      "print(x_data)\n",
      "#y_data = data[\"Outcome\"]\n",
      "y_data = np.array(data[\"Outcome\"]).reshape(-1,1)\n",
      "245/22:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "print(x_data)\n",
      "x_data = np.array(x_data)\n",
      "print(x_data)\n",
      "#y_data = data[\"Outcome\"]\n",
      "y_data = np.array(data[\"Outcome\"]).reshape(-1,1)\n",
      "245/23:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "print(x_data)\n",
      "x_data = np.array(x_data).reshape(-1,1)\n",
      "print(x_data)\n",
      "#y_data = data[\"Outcome\"]\n",
      "y_data = np.array(data[\"Outcome\"]).reshape(-1,1)\n",
      "245/24:\n",
      "# Splitting the non scaled data.\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "#x_train = np.array(x_train).reshape(-1,1)\n",
      "\n",
      "# Featured scaled data.\n",
      "#std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)\n",
      "245/25:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(data.drop((\"Outcome\"),axis=1))\n",
      "std_scl_x\n",
      "245/26:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "\n",
      "x_data = np.array(x_data).reshape(-1,1)\n",
      "\n",
      "#y_data = data[\"Outcome\"]\n",
      "y_data = np.array(data[\"Outcome\"]).reshape(-1,1)\n",
      "245/27:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "\n",
      "x_data = np.array(x_data)\n",
      "\n",
      "#y_data = data[\"Outcome\"]\n",
      "y_data = np.array(data[\"Outcome\"]).reshape(-1,1)\n",
      "245/28:\n",
      "# Splitting the non scaled data.\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "#x_train = np.array(x_train).reshape(-1,1)\n",
      "\n",
      "# Featured scaled data.\n",
      "#std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)\n",
      "245/29:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_reg_predict1 = log_reg.predict(y_test)\n",
      "print(\"Accuracy of Non scaled data in logistic regration: \",accuracy_score(y_test,log_reg_predict1))\n",
      "245/30:\n",
      "# Splitting the non scaled data.\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "#x_train = np.array(x_train).reshape(-1,1)\n",
      "y_train\n",
      "# Featured scaled data.\n",
      "#std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)\n",
      "245/31:\n",
      "# Splitting the non scaled data.\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "#x_train = np.array(x_train).reshape(-1,1)\n",
      "y_test\n",
      "# Featured scaled data.\n",
      "#std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)\n",
      "243/1:\n",
      "stnd_scl = StandardScaler()\n",
      "data_x_labels = stnd_scl.fit_transform(data_x_labels)\n",
      "246/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "246/2: data = pd.read_csv(\"Breast_cancer_dataset.csv\")\n",
      "245/32:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(data.drop((\"Outcome\"),axis=1))\n",
      "#std_scl_x\n",
      "245/33:\n",
      "x_data = data.drop((\"Outcome\"), axis=1)\n",
      "\n",
      "#x_data = np.array(x_data)\n",
      "\n",
      "y_data = data[\"Outcome\"]\n",
      "#y_data = np.array(data[\"Outcome\"]).reshape(-1,1)\n",
      "245/34:\n",
      "# Splitting the non scaled data.\n",
      "#x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
      "#x_train = np.array(x_train).reshape(-1,1)\n",
      "\n",
      "# Featured scaled data.\n",
      "std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)\n",
      "245/35:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(std_scl_x_train,std_scl_y_trainy_)\n",
      "log_reg_predict1 = log_reg.predict(std_scl_y_test)\n",
      "print(\"Accuracy of Non scaled data in logistic regration: \",accuracy_score(std_scl_y_test,log_reg_predict1))\n",
      "245/36:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(std_scl_x_train,std_scl_y_train)\n",
      "log_reg_predict1 = log_reg.predict(std_scl_y_test)\n",
      "print(\"Accuracy of Non scaled data in logistic regration: \",accuracy_score(std_scl_y_test,log_reg_predict1))\n",
      "247/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "247/2: data = pd.read_csv(\"diabetes.csv\")\n",
      "247/3: data.head()\n",
      "247/4:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "247/5: data.isnull().sum()\n",
      "247/6: data.info()\n",
      "247/7: data.describe()\n",
      "247/8:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "247/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "247/10: sns.pairplot(data, hue=\"Outcome\")\n",
      "247/11: data.corr()\n",
      "247/12:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "247/13:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "247/14:\n",
      "#std_scl = StandardScaler()\n",
      "#std_scl_x = std_scl.fit_transform(data.drop((\"Outcome\"),axis=1))\n",
      "#std_scl_x\n",
      "247/15: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "247/16:\n",
      "x_data = data.drop((\"Outcome\"),axis=1)\n",
      "y_data = data[\"outcome\"]\n",
      "247/17:\n",
      "x_data = data.drop((\"Outcome\"),axis=1)\n",
      "y_data = data[\"Outcome\"]\n",
      "247/18: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "247/19:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_reg_predict = log_reg.predict(y_test)\n",
      "printf(accuracy_score(y_test,log_reg_predict))\n",
      "247/20:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "247/21:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(y_test)\n",
      "247/22:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict([y_test])\n",
      "247/23:\n",
      "x_data = np.array(data.drop((\"Outcome\"),axis=1))\n",
      "y_data = np.array(data[\"Outcome\"])\n",
      "247/24: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "247/25: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "247/26:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(y_test)\n",
      "247/27:\n",
      "x_data = np.array(data.drop((\"Outcome\"),axis=1)).reshape(-1,1)\n",
      "y_data = np.array(data[\"Outcome\"]).reshape(-1,1)\n",
      "247/28: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "247/29:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(y_test)\n",
      "247/30:\n",
      "x_data = np.array(data.drop((\"Outcome\"),axis=1)).reshape(-1,1)\n",
      "y_data = (data[\"Outcome\"])\n",
      "247/31: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "247/32:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(y_test)\n",
      "247/33:\n",
      "x_data = data.drop((\"Outcome\"),axis=1)\n",
      "y_data = data[\"Outcome\"]\n",
      "248/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "248/2: data = pd.read_csv(\"diabetes.csv\")\n",
      "248/3: data.head()\n",
      "248/4:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "248/5: data.isnull().sum()\n",
      "248/6: data.info()\n",
      "248/7: data.describe()\n",
      "248/8:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "248/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "248/10: sns.pairplot(data, hue=\"Outcome\")\n",
      "248/11: data.corr()\n",
      "248/12:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "248/13:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "248/14:\n",
      "#std_scl = StandardScaler()\n",
      "#std_scl_x = std_scl.fit_transform(data.drop((\"Outcome\"),axis=1))\n",
      "#std_scl_x\n",
      "248/15:\n",
      "x_data = data.drop((\"Outcome\"),axis=1)\n",
      "y_data = data[\"Outcome\"]\n",
      "248/16: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "248/17:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(y_test)\n",
      "248/18:\n",
      "x_data = data.drop((\"Outcome\"),axis=1)\n",
      "y_data = data[\"Outcome\"]\n",
      "x_data\n",
      "248/19:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "248/20:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_test\n",
      "248/21:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "y_test\n",
      "248/22:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_test\n",
      "248/23:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "y_test = np.array(y_test)\n",
      "y_test\n",
      "248/24:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(y_test)\n",
      "248/25:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "y_test = np.array(y_test).reshape(-1,1)\n",
      "y_test\n",
      "248/26:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(y_test)\n",
      "248/27:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "y_test = np.array(y_test).reshape(-1,1)\n",
      "x_test\n",
      "248/28:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "y_test = np.array(y_test).reshape(-1,1)\n",
      "x_test = np.array(x_test).reshape(-1,1)\n",
      "248/29:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "y_test = np.array(y_test).reshape(-1,1)\n",
      "x_test = np.array(x_test).reshape(-1,1)\n",
      "x_test\n",
      "248/30:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(y_test)\n",
      "248/31:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "y_test = np.array(y_test).reshape(-1,1)\n",
      "x_train = np.array(x_train).reshape(-1,1)\n",
      "x_train\n",
      "248/32:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(y_test)\n",
      "248/33:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "y_test = np.array(y_test).reshape(-1,1)\n",
      "x_train = np.array(x_train)\n",
      "x_train\n",
      "248/34:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(y_test)\n",
      "248/35:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "y_test = np.array(y_test).reshape(-1,1)\n",
      "\n",
      "x_train\n",
      "248/36:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(y_test)\n",
      "249/1:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data[\"Outcome\"]\n",
      "250/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "250/2: data = pd.read_csv(\"diabetes.csv\")\n",
      "250/3: data.head()\n",
      "250/4:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "250/5: data.isnull().sum()\n",
      "250/6: data.info()\n",
      "250/7: data.describe()\n",
      "250/8:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "250/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "250/10: sns.pairplot(data, hue=\"Outcome\")\n",
      "250/11: data.corr()\n",
      "250/12:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "250/13:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "250/14:\n",
      "#std_scl = StandardScaler()\n",
      "#std_scl_x = std_scl.fit_transform(data.drop((\"Outcome\"),axis=1))\n",
      "#std_scl_x\n",
      "250/15:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data[\"Outcome\"]\n",
      "250/16:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "y_test = np.array(y_test).reshape(-1,1)\n",
      "x_train = np.array(x_train)\n",
      "x_train\n",
      "250/17:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(y_test)\n",
      "250/18:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data[\"Outcome\"]\n",
      "x_data\n",
      "250/19:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data[\"Outcome\"]\n",
      "y_data\n",
      "250/20:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "y_data\n",
      "250/21: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "250/22:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(y_test)\n",
      "250/23:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train.reshape(-1,1))\n",
      "predict = log_reg.predict(y_test)\n",
      "250/24:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train.reshape(-1,1),y_train.reshape(-1,1))\n",
      "predict = log_reg.predict(y_test)\n",
      "250/25: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "250/26:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "x_data\n",
      "250/27:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "x_data.head()\n",
      "250/28: x_train\n",
      "250/29: y_train\n",
      "250/30: x_test\n",
      "250/31: log = LogisticRegression()\n",
      "250/32:\n",
      "log = LogisticRegression()\n",
      "log.fit(x_train,y_train)\n",
      "250/33:\n",
      "log = LogisticRegression()\n",
      "log.fit(x_train,y_train)\n",
      "predict = log.predict(x_test)\n",
      "250/34:\n",
      "log = LogisticRegression()\n",
      "log.fit(x_train,y_train)\n",
      "predict = log.predict(x_test)\n",
      "print(accuracy_score(y_test,predict))\n",
      "250/35:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "x_data\n",
      "250/36:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(x_test)\n",
      "print(accuracy_score(y_test,predict))\n",
      "250/37:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(x_test)\n",
      "print(accuracy_score(y_test,predict))\n",
      "250/38:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(data.drop((\"Outcome\"),axis=1))\n",
      "#std_scl_x\n",
      "250/39: std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "250/40:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(x_test)\n",
      "print(accuracy_score(y_test,predict))\n",
      "250/41:\n",
      "log_reg1 = LogisticRegression()\n",
      "log_reg1.fit(std_x_train,y_train)\n",
      "predict = log_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,predict))\n",
      "250/42:\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Logistic regrasion:\",accuracy_score(y_test,predict))\n",
      "250/43:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "250/44:\n",
      "imput = SimpleImputer(missing_values=0, strategy=\"mean\", axis = 1)\n",
      "imput.fit(data[\"Insulin\"], inplace=True)\n",
      "250/45:\n",
      "imput = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "imput.fit(data[\"Insulin\"], inplace=True)\n",
      "250/46:\n",
      "imput = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "imput.fit(data[\"Insulin\"])\n",
      "250/47:\n",
      "imput = SimpleImputer(missing_values=0, strategy=\"mean\", axis=0)\n",
      "imput.fit(data[\"Insulin\"])\n",
      "250/48:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import Imputer\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "250/49:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler, Imputer\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "250/50:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "250/51:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "x_data\n",
      "251/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "251/2: data = pd.read_csv(\"diabetes.csv\")\n",
      "251/3: data.head()\n",
      "251/4:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "251/5: data.isnull().sum()\n",
      "251/6: data.info()\n",
      "251/7: data.describe()\n",
      "251/8:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "251/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "251/10: sns.pairplot(data, hue=\"Outcome\")\n",
      "251/11: data.corr()\n",
      "251/12:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "251/13:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "251/14:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(data.drop((\"Outcome\"),axis=1))\n",
      "#std_scl_x\n",
      "251/15:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "251/16:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "x_data\n",
      "251/17: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "251/18: std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "251/19:\n",
      "# Non standarise data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Logistic regrassion:\",accuracy_score(y_test,predict))\n",
      "251/20:\n",
      "# standarise data\n",
      "log_reg1 = LogisticRegression()\n",
      "log_reg1.fit(std_x_train,y_train)\n",
      "predict = log_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,predict))\n",
      "251/21:\n",
      "d_tree = DecisionTreeClassifier()\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,predict))\n",
      "251/22:\n",
      "d_tree = DecisionTreeClassifier()\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "251/23:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "251/24:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(std_x_train,y_train)\n",
      "d_predict = d_tree.predict(y_test)\n",
      "print(\"Accuracy of Non standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "251/25:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(std_x_train,y_train)\n",
      "d_predict = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "251/26:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(std_x_train,y_train)\n",
      "d_predict = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "251/27:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "std_scl_x = impute.fit_transform(std_scl_x)\n",
      "x_data\n",
      "251/28: std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "251/29: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "251/30: std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "251/31:\n",
      "# Non standarise data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Logistic regrassion:\",accuracy_score(y_test,predict))\n",
      "251/32:\n",
      "# standarise data\n",
      "log_reg1 = LogisticRegression()\n",
      "log_reg1.fit(std_x_train,y_train)\n",
      "predict = log_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,predict))\n",
      "251/33:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "251/34:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(std_x_train,y_train)\n",
      "d_predict = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "252/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "252/2: data = pd.read_csv(\"diabetes.csv\")\n",
      "252/3: data.head()\n",
      "252/4:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "252/5: data.isnull().sum()\n",
      "252/6: data.info()\n",
      "252/7: data.describe()\n",
      "252/8:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "252/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "252/10: sns.pairplot(data, hue=\"Outcome\")\n",
      "252/11: data.corr()\n",
      "252/12:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "252/13:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "252/14:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(data.drop((\"Outcome\"),axis=1))\n",
      "#std_scl_x\n",
      "252/15:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "252/16:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "std_scl_x = impute.fit_transform(std_scl_x)\n",
      "x_data\n",
      "252/17: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "252/18: std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "252/19:\n",
      "# Non standarise data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Logistic regrassion:\",accuracy_score(y_test,predict))\n",
      "252/20:\n",
      "# standarise data\n",
      "log_reg1 = LogisticRegression()\n",
      "log_reg1.fit(std_x_train,y_train)\n",
      "predict = log_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,predict))\n",
      "252/21:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "252/22:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(std_x_train,y_train)\n",
      "d_predict = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "252/23:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(std_x_train,y_train)\n",
      "d_predict = d_tree.predict(std_x_test)\n",
      "print(\"Accuracy of Non standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "252/24:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(std_x_train,y_train)\n",
      "d_predict = d_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "252/25:\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Random Forest Classifier:\",accuracy_score(y_test,r_predict))\n",
      "252/26:\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(std_scl_x,y_train)\n",
      "r_predict = r_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standarise data in Random Forest Classifier:\",accuracy_score(y_test,r_predict))\n",
      "252/27:\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(std_x_train,y_train)\n",
      "r_predict = r_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standarise data in Random Forest Classifier:\",accuracy_score(y_test,r_predict))\n",
      "253/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "253/2: data = pd.read_csv(\"diabetes.csv\")\n",
      "253/3: data.head()\n",
      "253/4:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "253/5: data.isnull().sum()\n",
      "253/6: data.info()\n",
      "253/7: data.describe()\n",
      "253/8:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "253/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "253/10: sns.pairplot(data, hue=\"Outcome\")\n",
      "253/11: data.corr()\n",
      "253/12:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "253/13:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "253/14:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(data.drop((\"Outcome\"),axis=1))\n",
      "#std_scl_x\n",
      "253/15:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "253/16:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "std_scl_x = impute.fit_transform(std_scl_x)\n",
      "x_data\n",
      "253/17: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "253/18: std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "253/19:\n",
      "# Non standarise data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Logistic regrassion:\",accuracy_score(y_test,predict))\n",
      "253/20:\n",
      "# standarise data\n",
      "log_reg1 = LogisticRegression()\n",
      "log_reg1.fit(std_x_train,y_train)\n",
      "predict = log_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,predict))\n",
      "253/21:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "253/22:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(std_x_train,y_train)\n",
      "d_predict = d_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "253/23:\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Random Forest Classifier:\",accuracy_score(y_test,r_predict))\n",
      "253/24:\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(std_x_train,y_train)\n",
      "r_predict = r_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standarise data in Random Forest Classifier:\",accuracy_score(y_test,r_predict))\n",
      "254/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "254/2: data = pd.read_csv(\"diabetes.csv\")\n",
      "254/3: data.head()\n",
      "254/4:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "254/5: data.isnull().sum()\n",
      "254/6: data.info()\n",
      "254/7: data.describe()\n",
      "254/8:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "254/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "254/10: #sns.pairplot(data, hue=\"Outcome\")\n",
      "254/11: data.corr()\n",
      "254/12:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "254/13:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "254/14:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(data.drop((\"Outcome\"),axis=1))\n",
      "#std_scl_x\n",
      "254/15:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "254/16:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "std_scl_x = impute.fit_transform(std_scl_x)\n",
      "x_data\n",
      "254/17:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "254/18:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "254/19:\n",
      "# Non standarise data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Logistic regrassion:\",accuracy_score(y_test,predict))\n",
      "254/20:\n",
      "# standarise data\n",
      "log_reg1 = LogisticRegression()\n",
      "log_reg1.fit(std_x_train,y_train)\n",
      "predict = log_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,predict))\n",
      "254/21:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "254/22:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(std_x_train,y_train)\n",
      "d_predict = d_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "254/23:\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Random Forest Classifier:\",accuracy_score(y_test,r_predict))\n",
      "254/24:\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(std_x_train,y_train)\n",
      "r_predict = r_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standarise data in Random Forest Classifier:\",accuracy_score(y_test,r_predict))\n",
      "255/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "255/2: data = pd.read_csv(\"diabetes.csv\")\n",
      "255/3: data.head()\n",
      "255/4:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "255/5: data.isnull().sum()\n",
      "255/6: data.info()\n",
      "255/7: data.describe()\n",
      "255/8:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "255/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "255/10: #sns.pairplot(data, hue=\"Outcome\")\n",
      "255/11: data.corr()\n",
      "255/12:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "255/13:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "255/14:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "255/15:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "x_data\n",
      "255/16:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "255/17:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "255/18:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "255/19:\n",
      "# Non standarise data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "predict = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Logistic regrassion:\",accuracy_score(y_test,predict))\n",
      "255/20:\n",
      "# standarise data\n",
      "log_reg1 = LogisticRegression()\n",
      "log_reg1.fit(std_x_train,y_train)\n",
      "predict = log_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,predict))\n",
      "255/21:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "255/22:\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(std_x_train,y_train)\n",
      "d_predict = d_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standarise data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict))\n",
      "255/23:\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standarise data in Random Forest Classifier:\",accuracy_score(y_test,r_predict))\n",
      "255/24:\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(std_x_train,y_train)\n",
      "r_predict = r_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standarise data in Random Forest Classifier:\",accuracy_score(y_test,r_predict))\n",
      "257/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "257/2: data = pd.read_csv(\"diabetes.csv\")\n",
      "257/3: data.head()\n",
      "257/4:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "257/5: data.isnull().sum()\n",
      "257/6: data.info()\n",
      "257/7: data.describe()\n",
      "257/8:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "257/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "257/10: #sns.pairplot(data, hue=\"Outcome\")\n",
      "257/11: data.corr()\n",
      "257/12:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "257/13:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "257/14:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "257/15:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "x_data\n",
      "257/16:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "257/17:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "257/18:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "257/19:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "257/20:\n",
      "# standardize data\n",
      "log_reg1 = LogisticRegression()\n",
      "log_reg1.fit(std_x_train,y_train)\n",
      "log_predict2 = log_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,log_predict2))\n",
      "257/21:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "257/22:\n",
      "#  standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(std_x_train,y_train)\n",
      "d_predict2 = d_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict2))\n",
      "257/23:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "257/24:\n",
      "# standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(std_x_train,y_train)\n",
      "r_predict2 = r_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict2))\n",
      "257/25:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "257/26:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "258/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "258/2: data = pd.read_csv(\"diabetes.csv\")\n",
      "258/3: data.head()\n",
      "258/4:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "258/5: data.isnull().sum()\n",
      "258/6: data.info()\n",
      "258/7: data.describe()\n",
      "258/8:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "258/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "258/10: #sns.pairplot(data, hue=\"Outcome\")\n",
      "258/11: data.corr()\n",
      "258/12:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "258/13:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "258/14:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "258/15:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "x_data\n",
      "258/16:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "258/17:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "258/18:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "258/19:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "258/20:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = log_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "259/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "259/2: data = pd.read_csv(\"diabetes.csv\")\n",
      "259/3: data.head()\n",
      "259/4:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "259/5: data.isnull().sum()\n",
      "259/6: data.info()\n",
      "259/7: data.describe()\n",
      "259/8:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "259/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "259/10: #sns.pairplot(data, hue=\"Outcome\")\n",
      "259/11: data.corr()\n",
      "259/12:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "259/13:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "259/14:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "259/15:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "x_data\n",
      "259/16:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "259/17:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "259/18:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "259/19:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "259/20:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "259/21:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "259/22:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "259/23:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "259/24:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "259/25:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "259/26:\n",
      "cross_val = cross_val_score(estimator=model_3, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "259/27:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "260/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "260/2: data = pd.read_csv(\"diabetes.csv\")\n",
      "260/3: data.head()\n",
      "260/4:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "260/5: data.isnull().sum()\n",
      "260/6: data.info()\n",
      "260/7: data.describe()\n",
      "260/8:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "260/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "260/10: sns.pairplot(data, hue=\"Outcome\")\n",
      "260/11: data.corr()\n",
      "260/12:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "260/13:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "260/14:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "260/15:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "x_data\n",
      "260/16:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "260/17:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "260/18:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "260/19:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "260/20:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "260/21:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "260/22:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "260/23:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "260/24:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "260/25:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "260/26:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "261/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import mtplotlib.pyplot as plt\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "261/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "261/3:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "261/4: data.info()\n",
      "263/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "263/2:\n",
      "data = pd.read_csv(\"heart1.csv\")\n",
      "data\n",
      "264/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "264/2:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "264/3: data.info()\n",
      "264/4: data.isnull().sum()\n",
      "265/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "265/2:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "265/3: data.info()\n",
      "265/4: data.isnull().sum()\n",
      "265/5: data.columns\n",
      "265/6: data.define\n",
      "265/7: data.define()\n",
      "265/8: data.describe\n",
      "265/9: data.info()\n",
      "265/10: data.describe()\n",
      "265/11: sns.pairplot(data[\"target\"])\n",
      "265/12: sns.countplot(data[\"target\"])\n",
      "265/13: data.corr()\n",
      "265/14:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "265/15:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "265/16: sns.pairplot(data, hue=\"target\")\n",
      "265/17:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "265/18:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "265/19:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "267/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "267/2:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "267/3: data.info()\n",
      "267/4: data.describe()\n",
      "267/5: data.columns\n",
      "267/6: data.info()\n",
      "267/7: data.isnull().sum()\n",
      "267/8: data.corr()\n",
      "267/9:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "267/10:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "267/11: sns.pairplot(data, hue=\"target\")\n",
      "267/12:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "267/13:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "267/14: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\",\"smoothness_mean\"])\n",
      "267/15: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "267/16:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "267/17:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_y = data[\"target\"]\n",
      "267/18:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_y = data[\"target\"]\n",
      "print(all_x)\n",
      "print(all_y)\n",
      "267/19:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[\"cp\",\"restecg\",\"thalach\",\"slope\"]\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "\n",
      "print(all_x)\n",
      "print(imp)\n",
      "print(all_y)\n",
      "267/20:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[\"cp\",\"restecg\",\"thalach\",\"slope\"]\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(all_y)\n",
      "267/21:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\"],[\"restecg\"],[\"thalach\"],[\"slope\"]]\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(all_y)\n",
      "267/22:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data(\"cp\",\"restecg\",\"thalach\",\"slope\")\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(all_y)\n",
      "267/23:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[\"cp\",\"restecg\",\"thalach\",\"slope\"]\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(all_y)\n",
      "267/24:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[(\"cp\",\"restecg\",\"thalach\",\"slope\")]\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(all_y)\n",
      "267/25:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(all_y)\n",
      "267/26:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "267/27: all_x_train,all_test_x,all_y_train,all_y_test = train_test_split(all_x,y)\n",
      "267/28: all_x_train,all_test_x,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "267/29:\n",
      "all_x_train,all_test_x,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "\n",
      "all_x_train.head()\n",
      "267/30:\n",
      "all_x_train,all_test_x,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "\n",
      "all_x_train\n",
      "267/31:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "\n",
      "all_x_train\n",
      "267/32: imp_x_train,imp_x_test,imp\n",
      "267/33:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "267/34:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "all_y_train\n",
      "267/35:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "imp_y_train\n",
      "267/36:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "all_y_train\n",
      "267/37:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "imp_y_train\n",
      "267/38:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "267/39:\n",
      "# Training and testing on all selected features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train)\n",
      "lg_model_prediction = lg_model.predict(all_y_test)\n",
      "print(\"Accuracy: \", accuracy_score(all_y_test,lg_model_prediction))\n",
      "267/40:\n",
      "# Training and testing on all selected features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_y_test)\n",
      "print(\"Accuracy: \", accuracy_score(all_y_test,lg_model_prediction))\n",
      "267/41:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(all_y)\n",
      "267/42:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "267/43:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "all_y_train\n",
      "267/44:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "imp_y_train\n",
      "267/45:\n",
      "# Training and testing on all selected features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_y_test)\n",
      "print(\"Accuracy: \", accuracy_score(all_y_test,lg_model_prediction))\n",
      "267/46:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x).reshape(-1,1)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x).reshape(-1,1)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(all_y)\n",
      "267/47:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "267/48:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "all_y_train\n",
      "267/49:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "imp_y_train\n",
      "267/50:\n",
      "# Training and testing on all selected features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_y_test)\n",
      "print(\"Accuracy: \", accuracy_score(all_y_test,lg_model_prediction))\n",
      "267/51:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x).reshape(-1,1)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x).reshape(-1,1)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(all_y)\n",
      "267/52:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "267/53:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "all_y_train\n",
      "267/54:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(all_y)\n",
      "267/55:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "267/56:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "all_y_train\n",
      "267/57:\n",
      "# Training and testing on all selected features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_y_test)\n",
      "print(\"Accuracy: \", accuracy_score(all_y_test,lg_model_prediction))\n",
      "267/58:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(all_y)\n",
      "267/59:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "267/60:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "all_y_train\n",
      "267/61:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "imp_y_train\n",
      "267/62:\n",
      "# Training and testing on all selected features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_y_test)\n",
      "print(\"Accuracy: \", accuracy_score(all_y_test,lg_model_prediction))\n",
      "267/63:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x).reshape(-1,1)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(all_y)\n",
      "267/64:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "267/65:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "all_y_train\n",
      "267/66:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(all_y)\n",
      "267/67:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "267/68:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "all_y_train\n",
      "267/69:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "imp_y_train\n",
      "267/70:\n",
      "# Training and testing on all selected features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_y_test)\n",
      "print(\"Accuracy: \", accuracy_score(all_y_test,lg_model_prediction))\n",
      "270/1:\n",
      "arr = [[1,1,1,1,1],\n",
      "       [0,0,0,0,0]]\n",
      "270/2: arr\n",
      "270/3: arr.shape()\n",
      "270/4: arr.shape\n",
      "270/5:\n",
      "arr = np.array([[1,1,1,1,1],\n",
      "       [0,0,0,0,0]])\n",
      "270/6:\n",
      "import numpy ans np\n",
      "arr = np.array([[1,1,1,1,1],\n",
      "       [0,0,0,0,0]])\n",
      "270/7:\n",
      "import numpy as np\n",
      "arr = np.array([[1,1,1,1,1],\n",
      "       [0,0,0,0,0]])\n",
      "270/8: arr\n",
      "270/9:\n",
      "arr\n",
      "arr.shape\n",
      "270/10:\n",
      "arr\n",
      "arr.reshape(1-,1)\n",
      "270/11:\n",
      "arr\n",
      "arr.reshape(-1,1)\n",
      "270/12:\n",
      "arr\n",
      "arr.reshape(1,-1)\n",
      "267/71:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "#all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "#imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "#imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "#y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "#print(imp_x)\n",
      "#print(all_y)\n",
      "267/72:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "#imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "#imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "#y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "#print(imp_x)\n",
      "#print(all_y)\n",
      "267/73:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "#imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "#imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "#y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "#print(imp_x)\n",
      "print(all_y)\n",
      "267/74:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "267/75:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "267/76:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "all_y_train\n",
      "267/77:\n",
      "# Training and testing on all selected features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_y_test)\n",
      "print(\"Accuracy: \", accuracy_score(all_y_test,lg_model_prediction))\n",
      "267/78:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "#imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "#imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y)\n",
      "\n",
      "print(all_x)\n",
      "#print(imp_x)\n",
      "print(all_y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267/79:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "267/80:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "all_y_train\n",
      "267/81:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "imp_y_train\n",
      "267/82:\n",
      "# Training and testing on all selected features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_y_test)\n",
      "print(\"Accuracy: \", accuracy_score(all_y_test,lg_model_prediction))\n",
      "267/83:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "#imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "#imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "#print(imp_x)\n",
      "print(all_y)\n",
      "267/84:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "267/85:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "all_y_train\n",
      "267/86:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "imp_y_train\n",
      "267/87:\n",
      "# Training and testing on all selected features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_y_test)\n",
      "print(\"Accuracy: \", accuracy_score(all_y_test,lg_model_prediction))\n",
      "267/88:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "#imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "#imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "#print(imp_x)\n",
      "print(y)\n",
      "267/89:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "267/90:\n",
      "# Training and testing on all selected features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy: \", accuracy_score(all_y_test,lg_model_prediction))\n",
      "267/91:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warning\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "267/92:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "267/93: warnings.filewarnings(\"ignore\")\n",
      "267/94:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "267/95: data.info()\n",
      "267/96: data.describe()\n",
      "267/97: data.columns\n",
      "267/98: data.info()\n",
      "267/99: data.isnull().sum()\n",
      "267/100: data.corr()\n",
      "267/101:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "267/102:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "267/103: sns.pairplot(data, hue=\"target\")\n",
      "267/104:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "267/105:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "267/106: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "267/107:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "#imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "#imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "#print(imp_x)\n",
      "print(y)\n",
      "267/108:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "267/109:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "267/110:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "imp_y_train\n",
      "267/111:\n",
      "# Training and testing on all selected features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy: \", accuracy_score(all_y_test,lg_model_prediction))\n",
      "271/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "271/2: warnings.filterwarnings(\"ignore\")\n",
      "271/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "271/4: data.info()\n",
      "271/5: data.describe()\n",
      "271/6: data.columns\n",
      "271/7: data.info()\n",
      "271/8: data.isnull().sum()\n",
      "271/9: data.corr()\n",
      "271/10:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "271/11:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "271/12: sns.pairplot(data, hue=\"target\")\n",
      "271/13:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "271/14:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "271/15: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "271/16:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "#imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "#imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "#print(imp_x)\n",
      "print(y)\n",
      "271/17:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "271/18:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(y)\n",
      "271/19:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "271/20:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "271/21:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "imp_y_train\n",
      "271/22:\n",
      "# Training and testing on all selected features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy: \", accuracy_score(all_y_test,lg_model_prediction))\n",
      "271/23:\n",
      "# Training and testing on all selected features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction))\n",
      "271/24:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model1 = LogisticRegression()\n",
      "lg_model1.fit(imp_x_train,imp_y_train)\n",
      "lg_model1_prediction = lg_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on Selected features: \", accuracy_score(imp_y_test,lg_model1_prediction))\n",
      "271/25: dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "271/26:\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction))\n",
      "271/27:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test,dt_model1_prediction))\n",
      "271/28:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier()\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(all_y_test, rf_model_prediction))\n",
      "271/29:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(all_y_test, rf_model_prediction))\n",
      "271/30:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test,dt_model1_prediction))\n",
      "271/31:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction))\n",
      "271/32:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction))\n",
      "271/33:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction))\n",
      "271/34:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=10, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction))\n",
      "271/35:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction))\n",
      "271/36:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction))\n",
      "272/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "272/2: warnings.filterwarnings(\"ignore\")\n",
      "272/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "272/4: data.info()\n",
      "272/5: data.describe()\n",
      "272/6: data.columns\n",
      "272/7: data.info()\n",
      "272/8: data.isnull().sum()\n",
      "272/9: data.corr()\n",
      "272/10:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "272/11:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "272/12: sns.pairplot(data, hue=\"target\")\n",
      "272/13:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "272/14:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "272/15: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "272/16:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x).reshape(-1,1)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(y)\n",
      "272/17:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "272/18:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "272/19:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "imp_y_train\n",
      "272/20:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x).reshape(-1,1)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(\"###################\")\n",
      "print(imp_x)\n",
      "print(y)\n",
      "272/21:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(\"###################\")\n",
      "print(imp_x)\n",
      "print(y)\n",
      "273/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "273/2: warnings.filterwarnings(\"ignore\")\n",
      "273/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "273/4: data.info()\n",
      "273/5: data.describe()\n",
      "273/6: data.columns\n",
      "273/7: data.info()\n",
      "273/8: data.isnull().sum()\n",
      "273/9: data.corr()\n",
      "273/10:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "273/11:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "273/12: sns.pairplot(data, hue=\"target\")\n",
      "273/13:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "273/14:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "273/15: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "273/16:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(y)\n",
      "273/17:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "273/18:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "273/19:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "imp_y_train\n",
      "273/20:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction))\n",
      "273/21:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "lg_model1 = LogisticRegression()\n",
      "lg_model1.fit(imp_x_train,imp_y_train)\n",
      "lg_model1_prediction = lg_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on Selected features: \", accuracy_score(imp_y_test,lg_model1_prediction))\n",
      "273/22:\n",
      "# Training and testing on all features.\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction))\n",
      "273/23:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test,dt_model1_prediction))\n",
      "273/24:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction))\n",
      "273/25:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction))\n",
      "273/26:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "273/27:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction))\n",
      "273/28:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction)*100)\n",
      "273/29:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "nb_model1 = GaussianNB()\n",
      "nb_model1.fit(imp_x_train,imp_y_train)\n",
      "nb_model1_prediction = nb_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test, nb_model1_prediction)*100)\n",
      "273/30:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "273/31:\n",
      "# Training and testing on all features.\n",
      "\n",
      "svm_model = SVC()\n",
      "svm_model.fit(all_x_train,all_y_train)\n",
      "svm_model_prediction = svm_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, svm_model_prediction)*100)\n",
      "273/32:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "svm_model1 = SVC()\n",
      "svm_model1.fit(imp_x_train,imp_y_train)\n",
      "svm_model1_prediction = svm_model1.predict(immp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test, svm_model1_prediction)*100)\n",
      "273/33:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "svm_model1 = SVC()\n",
      "svm_model1.fit(imp_x_train,imp_y_train)\n",
      "svm_model1_prediction = svm_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test, svm_model1_prediction)*100)\n",
      "274/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "274/2: warnings.filterwarnings(\"ignore\")\n",
      "274/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "274/4: data.info()\n",
      "274/5: data.describe()\n",
      "274/6: data.columns\n",
      "274/7: data.info()\n",
      "274/8: data.isnull().sum()\n",
      "274/9: data.corr()\n",
      "274/10:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "274/11:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "274/12: sns.pairplot(data, hue=\"target\")\n",
      "274/13:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "274/14:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "274/15: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "274/16:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(y)\n",
      "274/17:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "274/18:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "274/19:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "imp_y_train\n",
      "274/20:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction)*100)\n",
      "274/21:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "lg_model1 = LogisticRegression()\n",
      "lg_model1.fit(imp_x_train,imp_y_train)\n",
      "lg_model1_prediction = lg_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on Selected features: \", accuracy_score(imp_y_test,lg_model1_prediction)*100)\n",
      "274/22:\n",
      "# Training and testing on all features.\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction)*100)\n",
      "274/23:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test,dt_model1_prediction)*100)\n",
      "274/24:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction)*100)\n",
      "274/25:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction)*100)\n",
      "274/26:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction)*100)\n",
      "274/27:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "nb_model1 = GaussianNB()\n",
      "nb_model1.fit(imp_x_train,imp_y_train)\n",
      "nb_model1_prediction = nb_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, nb_model1_prediction)*100)\n",
      "274/28:\n",
      "# Training and testing on all features.\n",
      "\n",
      "svm_model = SVC()\n",
      "svm_model.fit(all_x_train,all_y_train)\n",
      "svm_model_prediction = svm_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, svm_model_prediction)*100)\n",
      "274/29:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "svm_model1 = SVC()\n",
      "svm_model1.fit(imp_x_train,imp_y_train)\n",
      "svm_model1_prediction = svm_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test, svm_model1_prediction)*100)\n",
      "279/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "279/2: warnings.filterwarnings(\"ignore\")\n",
      "279/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "279/4: data.info()\n",
      "279/5: data.describe()\n",
      "279/6: data.columns\n",
      "279/7: data.info()\n",
      "279/8: data.isnull().sum()\n",
      "279/9: data.corr()\n",
      "279/10:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "279/11:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "279/12: sns.pairplot(data, hue=\"target\")\n",
      "279/13:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "279/14:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "279/15: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "279/16:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(y)\n",
      "279/17:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "279/18:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "279/19:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "imp_y_train\n",
      "279/20:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction)*100)\n",
      "279/21:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "lg_model1 = LogisticRegression()\n",
      "lg_model1.fit(imp_x_train,imp_y_train)\n",
      "lg_model1_prediction = lg_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on Selected features: \", accuracy_score(imp_y_test,lg_model1_prediction)*100)\n",
      "279/22:\n",
      "confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "279/23:\n",
      "# Training and testing on all features.\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction)*100)\n",
      "279/24:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test,dt_model1_prediction)*100)\n",
      "279/25:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction)*100)\n",
      "279/26:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction)*100)\n",
      "279/27:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction)*100)\n",
      "279/28:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "nb_model1 = GaussianNB()\n",
      "nb_model1.fit(imp_x_train,imp_y_train)\n",
      "nb_model1_prediction = nb_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, nb_model1_prediction)*100)\n",
      "279/29:\n",
      "# Training and testing on all features.\n",
      "\n",
      "svm_model = SVC()\n",
      "svm_model.fit(all_x_train,all_y_train)\n",
      "svm_model_prediction = svm_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, svm_model_prediction)*100)\n",
      "279/30:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "svm_model1 = SVC()\n",
      "svm_model1.fit(imp_x_train,imp_y_train)\n",
      "svm_model1_prediction = svm_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test, svm_model1_prediction)*100)\n",
      "279/31:\n",
      "confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "279/32:\n",
      "confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "279/33:\n",
      "confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "279/34:\n",
      "confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "279/35:\n",
      "confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)\n",
      "print(confu_matrix)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "279/36:\n",
      "confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)\n",
      "print(confu_matrix)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')\n",
      "plt.show()\n",
      "279/37:\n",
      "confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')\n",
      "plt.show()\n",
      "279/38:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "#imp_y_train\n",
      "279/39:\n",
      "confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')\n",
      "plt.show()\n",
      "279/40: data.head()\n",
      "280/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "280/2: warnings.filterwarnings(\"ignore\")\n",
      "280/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "280/4: data.head()\n",
      "280/5: data.info()\n",
      "280/6: data.describe()\n",
      "280/7: data.columns\n",
      "280/8: data.info()\n",
      "280/9: data.isnull().sum()\n",
      "280/10: data.corr()\n",
      "280/11:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "280/12:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "280/13: sns.pairplot(data, hue=\"target\")\n",
      "280/14:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "280/15:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "280/16: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "280/17:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(y)\n",
      "280/18:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "280/19:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "280/20:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "#imp_y_train\n",
      "280/21:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction)*100)\n",
      "280/22:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "lg_model1 = LogisticRegression()\n",
      "lg_model1.fit(imp_x_train,imp_y_train)\n",
      "lg_model1_prediction = lg_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on Selected features: \", accuracy_score(imp_y_test,lg_model1_prediction)*100)\n",
      "280/23:\n",
      "confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "280/24:\n",
      "# Training and testing on all features.\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction)*100)\n",
      "280/25:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test,dt_model1_prediction)*100)\n",
      "280/26:\n",
      "confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')\n",
      "plt.show()\n",
      "280/27:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction)*100)\n",
      "280/28:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction)*100)\n",
      "280/29:\n",
      "confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')\n",
      "plt.show()\n",
      "280/30:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction)*100)\n",
      "280/31:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "nb_model1 = GaussianNB()\n",
      "nb_model1.fit(imp_x_train,imp_y_train)\n",
      "nb_model1_prediction = nb_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, nb_model1_prediction)*100)\n",
      "280/32:\n",
      "confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "280/33:\n",
      "# Training and testing on all features.\n",
      "\n",
      "svm_model = SVC()\n",
      "svm_model.fit(all_x_train,all_y_train)\n",
      "svm_model_prediction = svm_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, svm_model_prediction)*100)\n",
      "280/34:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "svm_model1 = SVC()\n",
      "svm_model1.fit(imp_x_train,imp_y_train)\n",
      "svm_model1_prediction = svm_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test, svm_model1_prediction)*100)\n",
      "280/35:\n",
      "confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "280/36:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "data = std_scl.fit(data)\n",
      "280/37:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "data = std_scl.fit(data)\n",
      "data\n",
      "281/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/2: warnings.filterwarnings(\"ignore\")\n",
      "281/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "281/4: data.head()\n",
      "281/5: data.info()\n",
      "281/6: data.describe()\n",
      "281/7: data.columns\n",
      "281/8: data.info()\n",
      "281/9: data.isnull().sum()\n",
      "281/10: data.corr()\n",
      "281/11:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "281/12:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "281/13: #sns.pairplot(data, hue=\"target\")\n",
      "281/14:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "281/15:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "281/16:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "data = std_scl.fit(data)\n",
      "data\n",
      "281/17: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "282/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "282/2: warnings.filterwarnings(\"ignore\")\n",
      "282/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "282/4: data.head()\n",
      "282/5: data.info()\n",
      "282/6: data.describe()\n",
      "282/7: data.columns\n",
      "282/8: data.info()\n",
      "282/9: data.isnull().sum()\n",
      "282/10: data.corr()\n",
      "282/11:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "282/12:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "282/13: #sns.pairplot(data, hue=\"target\")\n",
      "282/14:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "282/15:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "282/16: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "282/17:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(y)\n",
      "282/18:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "282/19:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(all_x)\n",
      "282/20:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "282/21:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "#imp_y_train\n",
      "282/22:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction)*100)\n",
      "282/23:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "lg_model1 = LogisticRegression()\n",
      "lg_model1.fit(imp_x_train,imp_y_train)\n",
      "lg_model1_prediction = lg_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on Selected features: \", accuracy_score(imp_y_test,lg_model1_prediction)*100)\n",
      "282/24:\n",
      "confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "282/25:\n",
      "# Training and testing on all features.\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction)*100)\n",
      "282/26:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test,dt_model1_prediction)*100)\n",
      "282/27:\n",
      "confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')\n",
      "plt.show()\n",
      "282/28:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction)*100)\n",
      "282/29:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction)*100)\n",
      "282/30:\n",
      "confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')\n",
      "plt.show()\n",
      "282/31:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction)*100)\n",
      "282/32:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "nb_model1 = GaussianNB()\n",
      "nb_model1.fit(imp_x_train,imp_y_train)\n",
      "nb_model1_prediction = nb_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, nb_model1_prediction)*100)\n",
      "282/33:\n",
      "confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "282/34:\n",
      "# Training and testing on all features.\n",
      "\n",
      "svm_model = SVC()\n",
      "svm_model.fit(all_x_train,all_y_train)\n",
      "svm_model_prediction = svm_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, svm_model_prediction)*100)\n",
      "282/35:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "svm_model1 = SVC()\n",
      "svm_model1.fit(imp_x_train,imp_y_train)\n",
      "svm_model1_prediction = svm_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test, svm_model1_prediction)*100)\n",
      "282/36:\n",
      "confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "282/37:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "282/38:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction)*100)\n",
      "print(\"R-square on all features: \", r2_score(all_y_test,lg_model_prediction)*100)\n",
      "282/39:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction)*100)\n",
      "print(\"R-square on all features: \", r2_score(all_y_test,lg_model_prediction))\n",
      "282/40:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "lg_model1 = LogisticRegression()\n",
      "lg_model1.fit(imp_x_train,imp_y_train)\n",
      "lg_model1_prediction = lg_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on Selected features: \", accuracy_score(imp_y_test,lg_model1_prediction)*100)\n",
      "print(\"Accuracy on Selected features: \", r2_score(imp_y_test,lg_model1_prediction))\n",
      "282/41:\n",
      "# Training and testing on all features.\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction)*100)\n",
      "print(\"Accuracy on all features: \", r2_score(all_y_test,dt_model_prediction)*100)\n",
      "282/42:\n",
      "# Training and testing on all features.\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction)*100)\n",
      "print(\"Accuracy on all features: \", r2_score(all_y_test,dt_model_prediction))\n",
      "282/43:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test,dt_model1_prediction)*100)\n",
      "print(\"Accuracy on selected features: \", r2_score(imp_y_test,dt_model1_prediction))\n",
      "285/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "285/2: warnings.filterwarnings(\"ignore\")\n",
      "285/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "285/4: data.head()\n",
      "285/5: data.info()\n",
      "285/6: data.describe()\n",
      "285/7: data.columns\n",
      "285/8: data.info()\n",
      "285/9: data.isnull().sum()\n",
      "285/10: data.corr()\n",
      "285/11:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "285/12:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "285/13: #sns.pairplot(data, hue=\"target\")\n",
      "285/14:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "285/15:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "285/16: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "285/17:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(y)\n",
      "285/18:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "285/19:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(all_x)\n",
      "285/20:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "285/21:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "#imp_y_train\n",
      "285/22:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction)*100)\n",
      "print(\"R-square on all features: \", r2_score(all_y_test,lg_model_prediction))\n",
      "285/23:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "lg_model1 = LogisticRegression()\n",
      "lg_model1.fit(imp_x_train,imp_y_train)\n",
      "lg_model1_prediction = lg_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on Selected features: \", accuracy_score(imp_y_test,lg_model1_prediction)*100)\n",
      "print(\"Accuracy on Selected features: \", r2_score(imp_y_test,lg_model1_prediction))\n",
      "285/24:\n",
      "confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "285/25:\n",
      "# Training and testing on all features.\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction)*100)\n",
      "print(\"Accuracy on all features: \", r2_score(all_y_test,dt_model_prediction))\n",
      "285/26:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test,dt_model1_prediction)*100)\n",
      "print(\"Accuracy on selected features: \", r2_score(imp_y_test,dt_model1_prediction))\n",
      "285/27:\n",
      "confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')\n",
      "plt.show()\n",
      "285/28:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction)*100)\n",
      "285/29:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction)*100)\n",
      "285/30:\n",
      "confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')\n",
      "plt.show()\n",
      "285/31:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction)*100)\n",
      "285/32:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "nb_model1 = GaussianNB()\n",
      "nb_model1.fit(imp_x_train,imp_y_train)\n",
      "nb_model1_prediction = nb_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, nb_model1_prediction)*100)\n",
      "285/33:\n",
      "confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "285/34:\n",
      "# Training and testing on all features.\n",
      "\n",
      "svm_model = SVC()\n",
      "svm_model.fit(all_x_train,all_y_train)\n",
      "svm_model_prediction = svm_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, svm_model_prediction)*100)\n",
      "285/35:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "svm_model1 = SVC()\n",
      "svm_model1.fit(imp_x_train,imp_y_train)\n",
      "svm_model1_prediction = svm_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test, svm_model1_prediction)*100)\n",
      "285/36:\n",
      "confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "285/37:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction)*100)\n",
      "285/38:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction)*100)\n",
      "285/39:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction)*100)\n",
      "286/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "286/2: warnings.filterwarnings(\"ignore\")\n",
      "286/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "286/4: data.head()\n",
      "286/5: data.info()\n",
      "286/6: data.describe()\n",
      "286/7: data.columns\n",
      "286/8: data.info()\n",
      "286/9: data.isnull().sum()\n",
      "286/10: data.corr()\n",
      "286/11:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "286/12:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "286/13: #sns.pairplot(data, hue=\"target\")\n",
      "286/14:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "286/15:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "286/16: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "286/17:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(y)\n",
      "286/18:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "286/19:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(all_x)\n",
      "286/20:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "286/21:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "#imp_y_train\n",
      "286/22:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction)*100)\n",
      "print(\"R-square on all features: \", r2_score(all_y_test,lg_model_prediction))\n",
      "286/23:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "lg_model1 = LogisticRegression()\n",
      "lg_model1.fit(imp_x_train,imp_y_train)\n",
      "lg_model1_prediction = lg_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on Selected features: \", accuracy_score(imp_y_test,lg_model1_prediction)*100)\n",
      "print(\"Accuracy on Selected features: \", r2_score(imp_y_test,lg_model1_prediction))\n",
      "286/24:\n",
      "confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "286/25:\n",
      "# Training and testing on all features.\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction)*100)\n",
      "print(\"Accuracy on all features: \", r2_score(all_y_test,dt_model_prediction))\n",
      "286/26:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test,dt_model1_prediction)*100)\n",
      "print(\"Accuracy on selected features: \", r2_score(imp_y_test,dt_model1_prediction))\n",
      "286/27:\n",
      "confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')\n",
      "plt.show()\n",
      "286/28:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction)*100)\n",
      "286/29:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction)*100)\n",
      "286/30:\n",
      "confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')\n",
      "plt.show()\n",
      "286/31:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction)*100)\n",
      "286/32:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "nb_model1 = GaussianNB()\n",
      "nb_model1.fit(imp_x_train,imp_y_train)\n",
      "nb_model1_prediction = nb_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, nb_model1_prediction)*100)\n",
      "286/33:\n",
      "confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "286/34:\n",
      "# Training and testing on all features.\n",
      "\n",
      "svm_model = SVC()\n",
      "svm_model.fit(all_x_train,all_y_train)\n",
      "svm_model_prediction = svm_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, svm_model_prediction)*100)\n",
      "286/35:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "svm_model1 = SVC()\n",
      "svm_model1.fit(imp_x_train,imp_y_train)\n",
      "svm_model1_prediction = svm_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test, svm_model1_prediction)*100)\n",
      "286/36:\n",
      "confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "287/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "287/2: warnings.filterwarnings(\"ignore\")\n",
      "287/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "287/4: data.head()\n",
      "287/5: data.info()\n",
      "287/6: data.describe()\n",
      "287/7: data.columns\n",
      "287/8: data.info()\n",
      "287/9: data.isnull().sum()\n",
      "287/10: data.corr()\n",
      "287/11:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "287/12:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "287/13: #sns.pairplot(data, hue=\"target\")\n",
      "287/14:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "287/15:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "287/16: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "287/17:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(y)\n",
      "287/18:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "287/19:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(all_x)\n",
      "287/20:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "287/21:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "#imp_y_train\n",
      "287/22:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction)*100)\n",
      "print(\"R-square on all features: \", r2_score(all_y_test,lg_model_prediction))\n",
      "287/23:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "lg_model1 = LogisticRegression()\n",
      "lg_model1.fit(imp_x_train,imp_y_train)\n",
      "lg_model1_prediction = lg_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on Selected features: \", accuracy_score(imp_y_test,lg_model1_prediction)*100)\n",
      "print(\"Accuracy on Selected features: \", r2_score(imp_y_test,lg_model1_prediction))\n",
      "287/24:\n",
      "confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "287/25:\n",
      "# Training and testing on all features.\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction)*100)\n",
      "print(\"Accuracy on all features: \", r2_score(all_y_test,dt_model_prediction))\n",
      "287/26:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test,dt_model1_prediction)*100)\n",
      "print(\"Accuracy on selected features: \", r2_score(imp_y_test,dt_model1_prediction))\n",
      "287/27:\n",
      "confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')\n",
      "plt.show()\n",
      "287/28:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction)*100)\n",
      "287/29:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction)*100)\n",
      "287/30:\n",
      "confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')\n",
      "plt.show()\n",
      "287/31:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction)*100)\n",
      "287/32:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "nb_model1 = GaussianNB()\n",
      "nb_model1.fit(imp_x_train,imp_y_train)\n",
      "nb_model1_prediction = nb_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, nb_model1_prediction)*100)\n",
      "287/33:\n",
      "confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "287/34:\n",
      "# Training and testing on all features.\n",
      "\n",
      "svm_model = SVC()\n",
      "svm_model.fit(all_x_train,all_y_train)\n",
      "svm_model_prediction = svm_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, svm_model_prediction)*100)\n",
      "287/35:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "svm_model1 = SVC()\n",
      "svm_model1.fit(imp_x_train,imp_y_train)\n",
      "svm_model1_prediction = svm_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test, svm_model1_prediction)*100)\n",
      "287/36:\n",
      "confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "287/37:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "import pickel\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "287/38:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "import pickle\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "287/39: warnings.filterwarnings(\"ignore\")\n",
      "287/40:\n",
      "# save model\n",
      "pickle.dump(rf_model,open('Heart_disease_prediction.pickle','wb'))\n",
      "\n",
      "# Load Model\n",
      "heart_disease_prediction_model = pickle.load(open('Heart_disease_prediction.pickle','rb'))\n",
      "\n",
      "y_predict = heart_disease_prediction_model.predict(all_x_test)\n",
      "\n",
      "print(\"confusion matrix: \\n\", confusion_matrix(all_y_test,y_predict))\n",
      "287/41:\n",
      "# save model\n",
      "pickle.dump(rf_model,open('Heart_disease_prediction.pickle','wb'))\n",
      "\n",
      "# Load Model\n",
      "heart_disease_prediction_model = pickle.load(open('Heart_disease_prediction.pickle','rb'))\n",
      "\n",
      "y_predict = heart_disease_prediction_model.predict(all_x_test)\n",
      "\n",
      "print(\"confusion matrix: \\n\", confusion_matrix(all_y_test,y_predict))\n",
      "accuracy_score(all_y_test,y_predict)\n",
      "288/1: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/2: pip install flask\n",
      "288/3: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/4: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/5: clear\n",
      "288/6: runcell(0, 'F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py')\n",
      "288/7: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/8: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/9: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/10: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/11: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/12: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "287/42:\n",
      "# save model\n",
      "pickle.dump(rf_model,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "heart_disease_prediction_model = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = heart_disease_prediction_model.predict(all_x_test)\n",
      "\n",
      "print(\"confusion matrix: \\n\", confusion_matrix(all_y_test,y_predict))\n",
      "accuracy_score(all_y_test,y_predict)\n",
      "288/13: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/14: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/15: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/16: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/17: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/18: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/19: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/20: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/21: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "289/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "import pickle\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "289/2: warnings.filterwarnings(\"ignore\")\n",
      "289/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "289/4: data.head()\n",
      "289/5: data.info()\n",
      "289/6: data.describe()\n",
      "289/7: data.columns\n",
      "289/8: data.info()\n",
      "289/9: data.isnull().sum()\n",
      "289/10: data.corr()\n",
      "289/11:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "289/12:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "289/13: #sns.pairplot(data, hue=\"target\")\n",
      "289/14:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "289/15:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "289/16: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "289/17:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(y)\n",
      "289/18:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "289/19:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(all_x)\n",
      "289/20:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "289/21:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "#imp_y_train\n",
      "289/22:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction)*100)\n",
      "print(\"R-square on all features: \", r2_score(all_y_test,lg_model_prediction))\n",
      "289/23:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "lg_model1 = LogisticRegression()\n",
      "lg_model1.fit(imp_x_train,imp_y_train)\n",
      "lg_model1_prediction = lg_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on Selected features: \", accuracy_score(imp_y_test,lg_model1_prediction)*100)\n",
      "print(\"Accuracy on Selected features: \", r2_score(imp_y_test,lg_model1_prediction))\n",
      "289/24:\n",
      "confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "289/25:\n",
      "# Training and testing on all features.\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction)*100)\n",
      "print(\"Accuracy on all features: \", r2_score(all_y_test,dt_model_prediction))\n",
      "289/26:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test,dt_model1_prediction)*100)\n",
      "print(\"Accuracy on selected features: \", r2_score(imp_y_test,dt_model1_prediction))\n",
      "289/27:\n",
      "confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')\n",
      "plt.show()\n",
      "289/28:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction)*100)\n",
      "289/29:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction)*100)\n",
      "289/30:\n",
      "confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')\n",
      "plt.show()\n",
      "289/31:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction)*100)\n",
      "289/32:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "nb_model1 = GaussianNB()\n",
      "nb_model1.fit(imp_x_train,imp_y_train)\n",
      "nb_model1_prediction = nb_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, nb_model1_prediction)*100)\n",
      "289/33:\n",
      "confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "289/34:\n",
      "# Training and testing on all features.\n",
      "\n",
      "svm_model = SVC()\n",
      "svm_model.fit(all_x_train,all_y_train)\n",
      "svm_model_prediction = svm_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, svm_model_prediction)*100)\n",
      "289/35:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "svm_model1 = SVC()\n",
      "svm_model1.fit(imp_x_train,imp_y_train)\n",
      "svm_model1_prediction = svm_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test, svm_model1_prediction)*100)\n",
      "289/36:\n",
      "confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "289/37:\n",
      "# save model\n",
      "pickle.dump(rf_model,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "heart_disease_prediction_model = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = heart_disease_prediction_model.predict(all_x_test)\n",
      "\n",
      "print(\"confusion matrix: \\n\", confusion_matrix(all_y_test,y_predict))\n",
      "accuracy_score(all_y_test,y_predict)\n",
      "288/22: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "290/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "import pickle\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "290/2: warnings.filterwarnings(\"ignore\")\n",
      "290/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "290/4: data.head()\n",
      "290/5: data.info()\n",
      "290/6: data.describe()\n",
      "290/7: data.columns\n",
      "290/8: data.info()\n",
      "290/9: data.isnull().sum()\n",
      "290/10: data.corr()\n",
      "290/11:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "290/12:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "290/13: #sns.pairplot(data, hue=\"target\")\n",
      "290/14:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "290/15:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "290/16: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "290/17:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(y)\n",
      "290/18:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "290/19:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(all_x)\n",
      "290/20:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "290/21:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "#imp_y_train\n",
      "290/22:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction)*100)\n",
      "print(\"R-square on all features: \", r2_score(all_y_test,lg_model_prediction))\n",
      "290/23:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "lg_model1 = LogisticRegression()\n",
      "lg_model1.fit(imp_x_train,imp_y_train)\n",
      "lg_model1_prediction = lg_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on Selected features: \", accuracy_score(imp_y_test,lg_model1_prediction)*100)\n",
      "print(\"Accuracy on Selected features: \", r2_score(imp_y_test,lg_model1_prediction))\n",
      "290/24:\n",
      "confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/25:\n",
      "# Training and testing on all features.\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction)*100)\n",
      "print(\"Accuracy on all features: \", r2_score(all_y_test,dt_model_prediction))\n",
      "290/26:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test,dt_model1_prediction)*100)\n",
      "print(\"Accuracy on selected features: \", r2_score(imp_y_test,dt_model1_prediction))\n",
      "290/27:\n",
      "confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')\n",
      "plt.show()\n",
      "290/28:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction)*100)\n",
      "290/29:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction)*100)\n",
      "290/30:\n",
      "confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')\n",
      "plt.show()\n",
      "290/31:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction)*100)\n",
      "290/32:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "nb_model1 = GaussianNB()\n",
      "nb_model1.fit(imp_x_train,imp_y_train)\n",
      "nb_model1_prediction = nb_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, nb_model1_prediction)*100)\n",
      "290/33:\n",
      "confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "290/34:\n",
      "# Training and testing on all features.\n",
      "\n",
      "svm_model = SVC()\n",
      "svm_model.fit(all_x_train,all_y_train)\n",
      "svm_model_prediction = svm_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, svm_model_prediction)*100)\n",
      "290/35:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "svm_model1 = SVC()\n",
      "svm_model1.fit(imp_x_train,imp_y_train)\n",
      "svm_model1_prediction = svm_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test, svm_model1_prediction)*100)\n",
      "290/36:\n",
      "confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "290/37:\n",
      "# save model\n",
      "pickle.dump(rf_model,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "heart_disease_prediction_model = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = heart_disease_prediction_model.predict(all_x_test)\n",
      "\n",
      "print(\"confusion matrix: \\n\", confusion_matrix(all_y_test,y_predict))\n",
      "accuracy_score(all_y_test,y_predict)\n",
      "291/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "import pickle\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "291/2: warnings.filterwarnings(\"ignore\")\n",
      "291/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "291/4: data.head()\n",
      "291/5: data.info()\n",
      "291/6: data.describe()\n",
      "291/7: data.columns\n",
      "291/8: data.info()\n",
      "291/9: data.isnull().sum()\n",
      "291/10: data.corr()\n",
      "291/11:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "291/12:\n",
      "sns.kdeplot(data[data['target']=='Unwell']['chol'],shade=True,color=\"orange\", label=\"Unwell\", alpha=.7)\n",
      "sns.kdeplot(data[data['target']=='Healthy']['chol'],shade=True,color=\"dodgerblue\", label=\"Healthy\", alpha=.7)\n",
      "plt.title('Cholesterol in mg/d for both case')\n",
      "plt.show()\n",
      "291/13:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "291/14: #sns.pairplot(data, hue=\"target\")\n",
      "291/15:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "291/16:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "291/17: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "291/18:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(y)\n",
      "291/19:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "291/20:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(all_x)\n",
      "291/21:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "291/22:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "#imp_y_train\n",
      "291/23:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction)*100)\n",
      "print(\"R-square on all features: \", r2_score(all_y_test,lg_model_prediction))\n",
      "291/24:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "lg_model1 = LogisticRegression()\n",
      "lg_model1.fit(imp_x_train,imp_y_train)\n",
      "lg_model1_prediction = lg_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on Selected features: \", accuracy_score(imp_y_test,lg_model1_prediction)*100)\n",
      "print(\"Accuracy on Selected features: \", r2_score(imp_y_test,lg_model1_prediction))\n",
      "291/25:\n",
      "confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "291/26:\n",
      "# Training and testing on all features.\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction)*100)\n",
      "print(\"Accuracy on all features: \", r2_score(all_y_test,dt_model_prediction))\n",
      "291/27:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test,dt_model1_prediction)*100)\n",
      "print(\"Accuracy on selected features: \", r2_score(imp_y_test,dt_model1_prediction))\n",
      "291/28:\n",
      "confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')\n",
      "plt.show()\n",
      "291/29:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction)*100)\n",
      "291/30:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction)*100)\n",
      "291/31:\n",
      "confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')\n",
      "plt.show()\n",
      "291/32:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction)*100)\n",
      "291/33:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "nb_model1 = GaussianNB()\n",
      "nb_model1.fit(imp_x_train,imp_y_train)\n",
      "nb_model1_prediction = nb_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, nb_model1_prediction)*100)\n",
      "291/34:\n",
      "confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "291/35:\n",
      "# Training and testing on all features.\n",
      "\n",
      "svm_model = SVC()\n",
      "svm_model.fit(all_x_train,all_y_train)\n",
      "svm_model_prediction = svm_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, svm_model_prediction)*100)\n",
      "291/36:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "svm_model1 = SVC()\n",
      "svm_model1.fit(imp_x_train,imp_y_train)\n",
      "svm_model1_prediction = svm_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test, svm_model1_prediction)*100)\n",
      "291/37:\n",
      "confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "291/38:\n",
      "# save model\n",
      "pickle.dump(rf_model,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "heart_disease_prediction_model = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = heart_disease_prediction_model.predict(all_x_test)\n",
      "\n",
      "print(\"confusion matrix: \\n\", confusion_matrix(all_y_test,y_predict))\n",
      "accuracy_score(all_y_test,y_predict)\n",
      "292/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "import pickle\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "292/2: warnings.filterwarnings(\"ignore\")\n",
      "292/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "292/4: data.head()\n",
      "292/5: data.info()\n",
      "292/6: data.describe()\n",
      "292/7: data.columns\n",
      "292/8: data.info()\n",
      "292/9: data.isnull().sum()\n",
      "292/10: data.corr()\n",
      "292/11:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "292/12:\n",
      "sns.kdeplot(data[data['target']==1]['chol'],shade=True,color=\"orange\", label=\"Unwell\", alpha=.7)\n",
      "sns.kdeplot(data[data['target']==0]['chol'],shade=True,color=\"dodgerblue\", label=\"Healthy\", alpha=.7)\n",
      "plt.title('Cholesterol in mg/d for both case')\n",
      "plt.show()\n",
      "292/13:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "292/14: #sns.pairplot(data, hue=\"target\")\n",
      "292/15:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "292/16:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "292/17: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "292/18:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(y)\n",
      "292/19:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "292/20:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(all_x)\n",
      "292/21:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "292/22:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "#imp_y_train\n",
      "292/23:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction)*100)\n",
      "print(\"R-square on all features: \", r2_score(all_y_test,lg_model_prediction))\n",
      "292/24:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "lg_model1 = LogisticRegression()\n",
      "lg_model1.fit(imp_x_train,imp_y_train)\n",
      "lg_model1_prediction = lg_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on Selected features: \", accuracy_score(imp_y_test,lg_model1_prediction)*100)\n",
      "print(\"Accuracy on Selected features: \", r2_score(imp_y_test,lg_model1_prediction))\n",
      "292/25:\n",
      "confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "292/26:\n",
      "# Training and testing on all features.\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction)*100)\n",
      "print(\"Accuracy on all features: \", r2_score(all_y_test,dt_model_prediction))\n",
      "292/27:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test,dt_model1_prediction)*100)\n",
      "print(\"Accuracy on selected features: \", r2_score(imp_y_test,dt_model1_prediction))\n",
      "292/28:\n",
      "confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')\n",
      "plt.show()\n",
      "292/29:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction)*100)\n",
      "292/30:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction)*100)\n",
      "292/31:\n",
      "confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')\n",
      "plt.show()\n",
      "292/32:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction)*100)\n",
      "292/33:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "nb_model1 = GaussianNB()\n",
      "nb_model1.fit(imp_x_train,imp_y_train)\n",
      "nb_model1_prediction = nb_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, nb_model1_prediction)*100)\n",
      "292/34:\n",
      "confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "292/35:\n",
      "# Training and testing on all features.\n",
      "\n",
      "svm_model = SVC()\n",
      "svm_model.fit(all_x_train,all_y_train)\n",
      "svm_model_prediction = svm_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, svm_model_prediction)*100)\n",
      "292/36:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "svm_model1 = SVC()\n",
      "svm_model1.fit(imp_x_train,imp_y_train)\n",
      "svm_model1_prediction = svm_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test, svm_model1_prediction)*100)\n",
      "292/37:\n",
      "confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "292/38:\n",
      "# save model\n",
      "pickle.dump(rf_model,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "heart_disease_prediction_model = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = heart_disease_prediction_model.predict(all_x_test)\n",
      "\n",
      "print(\"confusion matrix: \\n\", confusion_matrix(all_y_test,y_predict))\n",
      "accuracy_score(all_y_test,y_predict)\n",
      "288/23: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/24: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/25: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "288/26: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')\n",
      "294/1: import pandas as pd\n",
      "294/2: df=pd.read_csv('car data.csv')\n",
      "294/3: df.shape\n",
      "294/4:\n",
      "print(df['Seller_Type'].unique())\n",
      "print(df['Fuel_Type'].unique())\n",
      "print(df['Transmission'].unique())\n",
      "print(df['Owner'].unique())\n",
      "294/5:\n",
      "##check missing values\n",
      "df.isnull().sum()\n",
      "294/6: df.describe()\n",
      "294/7: final_dataset=df[['Year','Selling_Price','Present_Price','Kms_Driven','Fuel_Type','Seller_Type','Transmission','Owner']]\n",
      "294/8: final_dataset.head()\n",
      "294/9: final_dataset['Current Year']=2020\n",
      "294/10: final_dataset.head()\n",
      "294/11: final_dataset['no_year']=final_dataset['Current Year']- final_dataset['Year']\n",
      "294/12: final_dataset.head()\n",
      "294/13: final_dataset.drop(['Year'],axis=1,inplace=True)\n",
      "294/14: final_dataset.head()\n",
      "294/15: final_dataset=pd.get_dummies(final_dataset,drop_first=True)\n",
      "294/16: final_dataset.head()\n",
      "294/17: final_dataset.head()\n",
      "294/18: final_dataset=final_dataset.drop(['Current Year'],axis=1)\n",
      "294/19: final_dataset.head()\n",
      "294/20: final_dataset.corr()\n",
      "294/21: import seaborn as sns\n",
      "294/22: sns.pairplot(final_dataset)\n",
      "294/23:\n",
      "\n",
      "import seaborn as sns\n",
      "#get correlations of each features in dataset\n",
      "corrmat = df.corr()\n",
      "top_corr_features = corrmat.index\n",
      "plt.figure(figsize=(20,20))\n",
      "#plot heat map\n",
      "g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
      "294/24:\n",
      "### Feature Importance\n",
      "\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "import matplotlib.pyplot as plt\n",
      "model = ExtraTreesRegressor()\n",
      "model.fit(X,y)\n",
      "294/25:\n",
      "\n",
      "import seaborn as sns\n",
      "#get correlations of each features in dataset\n",
      "corrmat = df.corr()\n",
      "top_corr_features = corrmat.index\n",
      "plt.figure(figsize=(20,20))\n",
      "#plot heat map\n",
      "g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
      "294/26:\n",
      "X=final_dataset.iloc[:,1:]\n",
      "y=final_dataset.iloc[:,0]\n",
      "294/27: X['Owner'].unique()\n",
      "294/28: X.head()\n",
      "294/29: y.head()\n",
      "294/30:\n",
      "### Feature Importance\n",
      "\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "import matplotlib.pyplot as plt\n",
      "model = ExtraTreesRegressor()\n",
      "model.fit(X,y)\n",
      "294/31: print(model.feature_importances_)\n",
      "294/32:\n",
      "#plot graph of feature importances for better visualization\n",
      "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
      "feat_importances.nlargest(5).plot(kind='barh')\n",
      "plt.show()\n",
      "294/33:\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
      "294/34: from sklearn.ensemble import RandomForestRegressor\n",
      "294/35: regressor=RandomForestRegressor()\n",
      "294/36:\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "print(n_estimators)\n",
      "294/37:\n",
      "from sklearn.model_selection import train_test_split\n",
      "import numpy as np\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
      "294/38: from sklearn.ensemble import RandomForestRegressor\n",
      "294/39: regressor=RandomForestRegressor()\n",
      "294/40:\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "print(n_estimators)\n",
      "294/41: from sklearn.model_selection import RandomizedSearchCV\n",
      "294/42:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "294/43:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "294/44:\n",
      "# Use the random grid to search for best hyperparameters\n",
      "# First create the base model to tune\n",
      "rf = RandomForestRegressor()\n",
      "294/45:\n",
      "# Random search of parameters, using 3 fold cross validation, \n",
      "# search across 100 different combinations\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "294/46: rf_random.fit(X_train,y_train)\n",
      "294/47: rf_random.best_params_\n",
      "294/48: rf_random.best_score_\n",
      "294/49: predictions=rf_random.predict(X_test)\n",
      "294/50: sns.distplot(y_test-predictions)\n",
      "294/51: plt.scatter(y_test,predictions)\n",
      "294/52: from sklearn import metrics\n",
      "294/53:\n",
      "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
      "print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
      "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
      "297/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "297/2: warnings.filterwarnings(\"ignore\")\n",
      "297/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "297/4: data.head()\n",
      "297/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "297/6: data.isnull().sum()\n",
      "297/7: data.info()\n",
      "297/8: data.describe()\n",
      "297/9:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "297/10:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "297/11: sns.pairplot(data, hue=\"Outcome\")\n",
      "297/12: data.corr()\n",
      "297/13:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "297/14:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "297/15:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "297/16:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "x_data\n",
      "297/17:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "297/18:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "297/19:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "297/20:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "297/21:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "297/22:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "297/23:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "297/24:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "297/25:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "297/26:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "297/27:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "297/28:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "298/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "298/2: warnings.filterwarnings(\"ignore\")\n",
      "298/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "298/4: data.head()\n",
      "298/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "298/6: data.isnull().sum()\n",
      "298/7: data.info()\n",
      "298/8: data.describe()\n",
      "298/9:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "298/10:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "298/11: sns.pairplot(data, hue=\"Outcome\")\n",
      "298/12: data.corr()\n",
      "298/13:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "298/14:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "298/15:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "298/16:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "x_data\n",
      "298/17:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "298/18:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "298/19:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "298/20:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "298/21:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "298/22:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "298/23:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "298/24:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "298/25:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "298/26:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "298/27:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "298/28:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "298/29: rf_random.best_params_\n",
      "299/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "299/2: warnings.filterwarnings(\"ignore\")\n",
      "299/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "299/4: data.head()\n",
      "299/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "299/6: data.isnull().sum()\n",
      "299/7: data.info()\n",
      "299/8: data.describe()\n",
      "299/9:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "299/10:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "299/11: sns.pairplot(data, hue=\"Outcome\")\n",
      "300/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "300/2: warnings.filterwarnings(\"ignore\")\n",
      "300/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "300/4: data.head()\n",
      "300/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "300/6: data.isnull().sum()\n",
      "300/7: data.info()\n",
      "300/8: data.describe()\n",
      "300/9:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "300/10:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/11: #sns.pairplot(data, hue=\"Outcome\")\n",
      "300/12: data.corr()\n",
      "300/13:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "300/14:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "300/15:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "300/16:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "x_data\n",
      "300/17:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "300/18:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "300/19:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "300/20:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "300/21:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "300/22:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "300/23:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "300/24:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "300/25:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "300/26:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "300/27: rf_random.fit(X_train,y_train)\n",
      "300/28: rf_random.fit(x_train,y_train)\n",
      "300/29: rf_random.best_params_\n",
      "300/30: predictions=rf_random.predict(X_test)\n",
      "300/31: predictions=rf_random.predict(x_test)\n",
      "300/32: accuracy_score(y_test,predictions)\n",
      "300/33:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "300/34:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "301/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "301/2: warnings.filterwarnings(\"ignore\")\n",
      "301/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "301/4: data.head()\n",
      "301/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "301/6: data.isnull().sum()\n",
      "301/7: data.info()\n",
      "301/8: data.describe()\n",
      "301/9:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "301/10:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "301/11: #sns.pairplot(data, hue=\"Outcome\")\n",
      "301/12: data.corr()\n",
      "301/13:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "301/14:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "301/15:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "301/16:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "x_data\n",
      "301/17:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "301/18:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "301/19:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "301/20:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "301/21:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "301/22:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "301/23:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "301/24:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "301/25:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "301/26:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "301/27: rf_random.fit(std_x_train,y_train)\n",
      "301/28: rf_random.best_params_\n",
      "301/29: predictions=rf_random.predict(std_x_test)\n",
      "301/30: accuracy_score(y_test,predictions)\n",
      "301/31:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "301/32:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "301/33:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "301/34:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "304/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "304/2: warnings.filterwarnings(\"ignore\")\n",
      "304/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "304/4: data.head()\n",
      "304/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "304/6: data.isnull().sum()\n",
      "304/7: data.info()\n",
      "304/8: data.describe()\n",
      "304/9:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "304/10:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "304/11: #sns.pairplot(data, hue=\"Outcome\")\n",
      "304/12: data.corr()\n",
      "304/13:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "304/14:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "304/15:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "304/16:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "x_data\n",
      "304/17:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "304/18:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "304/19:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "304/20:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "304/21:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "304/22:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "304/23:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "304/24:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "304/25:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "304/26:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "304/27: rf_random.fit(std_x_train,y_train)\n",
      "304/28: rf_random.best_params_\n",
      "304/29: predictions=rf_random.predict(std_x_test)\n",
      "304/30: accuracy_score(y_test,predictions)\n",
      "304/31:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "304/32:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "304/33:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "304/34:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "305/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "305/2: warnings.filterwarnings(\"ignore\")\n",
      "305/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "305/4: data.head()\n",
      "305/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "305/6:\n",
      "data.isnull().sum()\n",
      "a = data[\"Insulin\"].count(0)\n",
      "a\n",
      "305/7:\n",
      "data.isnull().sum()\n",
      "a = data.count(data['Insulin']==0)\n",
      "a\n",
      "305/8:\n",
      "data.isnull().sum()\n",
      "a = data[\"Insulin\"].count(data['Insulin']==0)\n",
      "a\n",
      "305/9:\n",
      "data.isnull().sum()\n",
      "data.[\"Insulin\"].count(axis=0)\n",
      "a\n",
      "305/10:\n",
      "data.isnull().sum()\n",
      "data[\"Insulin\"].count(axis=0)\n",
      "a\n",
      "305/11:\n",
      "data.isnull().sum()\n",
      "data[\"Insulin\"].count(axis=0)\n",
      "305/12:\n",
      "data.isnull().sum()\n",
      "data[\"Insulin\"].count(axis=1)\n",
      "305/13:\n",
      "data.isnull().sum()\n",
      "data.count(axis=1)\n",
      "305/14:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==o])\n",
      "305/15:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "305/16: data['Insulin'].replace(0, np.nan, inplace=True)\n",
      "305/17: data.insull.sum()\n",
      "305/18: data.isnull.sum()\n",
      "305/19: data.isnull().sum()\n",
      "305/20: data.info()\n",
      "305/21: data.head()\n",
      "305/22: data[\"Insulin\"].fillna(values=data[\"Insulin\"].mean(), inplace=True)\n",
      "305/23: data[\"Insulin\"].fillna(value=data[\"Insulin\"].mean(), inplace=True)\n",
      "305/24: data.info()\n",
      "305/25: data.head()\n",
      "305/26: data[\"Insulin\"].fillna(value=data[\"Insulin\"].median(), inplace=True)\n",
      "305/27: data.info()\n",
      "305/28: data.head()\n",
      "306/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "306/2: warnings.filterwarnings(\"ignore\")\n",
      "306/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "306/4: data.head()\n",
      "306/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "306/6:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "306/7: data['Insulin'].replace(0, np.nan, inplace=True)\n",
      "306/8: data.isnull().sum()\n",
      "306/9: data.head()\n",
      "306/10: data[\"Insulin\"].fillna(value=data[\"Insulin\"].median(), inplace=True)\n",
      "306/11: data.info()\n",
      "306/12: data.head()\n",
      "306/13: data.describe()\n",
      "306/14:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "306/15:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "306/16: #sns.pairplot(data, hue=\"Outcome\")\n",
      "306/17: data.corr()\n",
      "306/18:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "306/19:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "306/20:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "306/21:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "x_data\n",
      "306/22:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "306/23:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "306/24:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "306/25:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "306/26:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "306/27:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "306/28:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "306/29:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "306/30:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "306/31:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "306/32: rf_random.fit(std_x_train,y_train)\n",
      "306/33: rf_random.best_params_\n",
      "306/34: predictions=rf_random.predict(std_x_test)\n",
      "306/35: accuracy_score(y_test,predictions)\n",
      "306/36:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "306/37:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "306/38:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "306/39:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "306/40:\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "max_depth\n",
      "306/41:\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 10)]\n",
      "max_depth\n",
      "306/42:\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 11)]\n",
      "max_depth\n",
      "308/1: import pandas as pd\n",
      "308/2: import pandas as pd\n",
      "308/3: import pandas as pd\n",
      "308/4: import jsldhashkjdfasgjk\n",
      "308/5:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "308/6:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "308/7:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "308/8:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titatic = pd.read_csv(\"full.csv\")\n",
      "308/9: titanic\n",
      "308/10:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")\n",
      "308/11: titanic\n",
      "308/12: titanic.head()\n",
      "308/13: titanic.head(10)\n",
      "308/14: titanic.head()\n",
      "308/15: titanic.head(10)\n",
      "308/16: titanic.head()\n",
      "308/17: titanic.tail()\n",
      "308/18: titanic.shape\n",
      "308/19: titanic.column\n",
      "308/20: titanic.columns\n",
      "308/21: data.head()\n",
      "308/22: titanic.head()\n",
      "308/23: titanic.info()\n",
      "308/24: titanic.describe\n",
      "308/25: titanic.describe()\n",
      "308/26: titanic.isnull().sum()\n",
      "308/27: titanic.isnull().any()\n",
      "308/28: titanic.isnull()\n",
      "308/29: titanic.isnull()\n",
      "308/30: titanic.drop(titanic.isnull(),axis=0,inplace=True)\n",
      "308/31: titanic.drop(titanic[\"Survived\"].isnull(),axis=0,inplace=True)\n",
      "308/32: titanic.drop(titanic[\"Survived\"].isnull()==True,axis=0,inplace=True)\n",
      "308/33: titanic.drop(labels=titanic[\"Survived\"].isnull()==True,axis=0,inplace=True)\n",
      "308/34: titanic.drop(labels=(titanic[\"Survived\"].isnull()==True),axis=0,inplace=True)\n",
      "308/35: titanic.dropna((titanic[\"Survived\"].isnull()==True),axis=0,inplace=True)\n",
      "308/36: titanic.dropna(titanic[\"Survived\"].isnull()==True,axis=0,inplace=True)\n",
      "308/37: titanic.dropna(titanic[\"Survived\"].isnull(),axis=0,inplace=True)\n",
      "308/38: titanic.dropna(\"Survived\",axis=0,inplace=True)\n",
      "308/39: titanic.dropna(\"Survived\",inplace=True)\n",
      "308/40: titanic.dropna([\"Survived\"],inplace=True)\n",
      "308/41: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "308/42: titanic.isnull().sum()\n",
      "308/43: titanic.info()\n",
      "309/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "309/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")\n",
      "309/3: titanic.head()\n",
      "309/4: titanic.tail()\n",
      "309/5: titanic.shape      # to know the shape of the data\n",
      "309/6: titanic.columns\n",
      "309/7: titanic.info()\n",
      "309/8: titanic.describe()\n",
      "309/9: titanic.isnull().sum()\n",
      "309/10: titanic.isnull()\n",
      "310/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "310/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")\n",
      "310/3: titanic.head()\n",
      "310/4: titanic.tail()\n",
      "310/5: titanic.shape      # to know the shape of the data\n",
      "310/6: titanic.columns\n",
      "310/7: titanic.info()\n",
      "310/8: titanic.describe()\n",
      "310/9: titanic.isnull().sum()\n",
      "310/10: titanic.isnull()\n",
      "310/11: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "310/12: data.isnull()\n",
      "310/13: titanic.isnull().sum()\n",
      "310/14: import seaborn as sns\n",
      "311/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "311/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")\n",
      "311/3: titanic.head()\n",
      "311/4: titanic.tail()\n",
      "311/5: titanic.shape      # to know the shape of the data\n",
      "311/6: titanic.columns\n",
      "311/7: titanic.info()\n",
      "311/8: titanic.describe()\n",
      "311/9: titanic.isnull().sum()\n",
      "311/10: titanic.isnull()\n",
      "313/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "313/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")\n",
      "313/3: titanic.head()\n",
      "313/4: titanic.tail()\n",
      "313/5: titanic.shape      # to know the shape of the data\n",
      "313/6: titanic.columns\n",
      "313/7: titanic.info()\n",
      "313/8: titanic.describe()\n",
      "313/9: titanic.isnull().sum()\n",
      "313/10: titanic.isnull()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/11: titanic.drop(labels=(titanic[\"Survived\"].isnull()==True),axis=0,inplace=True)\n",
      "313/12: titanic.dropna(subset=[\"Survived\"],inplace=true)\n",
      "313/13: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "313/14: titanic.shape\n",
      "313/15: demo = titanic[\"Age\"]\n",
      "313/16: demo\n",
      "313/17: demo.mean()\n",
      "313/18: demo.isnull().mean()\n",
      "313/19: demo.median()\n",
      "313/20: demo.fillna(demo.median(),inplace=True)\n",
      "313/21: titanic[\"Age\"].std()\n",
      "313/22: demo.std()\n",
      "313/23: titanic.isnull().sum()\n",
      "313/24: titanic.isnull().sum()\n",
      "314/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "314/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")\n",
      "314/3: titanic.head()\n",
      "314/4: titanic.tail()\n",
      "314/5: titanic.shape      # to know the shape of the data\n",
      "314/6: titanic.columns\n",
      "314/7: titanic.info()\n",
      "314/8: titanic.describe()\n",
      "314/9: titanic.isnull().sum()\n",
      "314/10: titanic.isnull()\n",
      "314/11: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "314/12: titanic.isnull().sum()\n",
      "314/13: demo = titanic[\"Age\"]\n",
      "314/14: demo.median()\n",
      "314/15: demo.fillna(demo.median(),inplace=True)\n",
      "314/16: titanic[\"Age\"].std()\n",
      "314/17: demo.std()\n",
      "314/18: titanic.isnull().sum()\n",
      "314/19:\n",
      "demo = titanic[\"Age\"]\n",
      "demo.isnull().sum()\n",
      "315/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "315/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")\n",
      "315/3: titanic.head()\n",
      "315/4: titanic.tail()\n",
      "315/5: titanic.shape      # to know the shape of the data\n",
      "315/6: titanic.columns\n",
      "315/7: titanic.info()\n",
      "315/8: titanic.describe()\n",
      "315/9: titanic.isnull().sum()\n",
      "315/10: titanic.isnull()\n",
      "315/11: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "315/12: titanic.isnull().sum()\n",
      "315/13:\n",
      "demo = titanic[\"Age\"]\n",
      "demo.isnull().sum()\n",
      "315/14: demo.median()\n",
      "315/15: demo.fillna(demo.median(),inplace=True)\n",
      "315/16: titanic[\"Age\"].std()\n",
      "315/17: demo.std()\n",
      "315/18: titanic.isnull().sum()\n",
      "316/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "316/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")\n",
      "316/3: titanic.head()\n",
      "316/4: titanic.tail()\n",
      "316/5: titanic.shape      # to know the shape of the data\n",
      "316/6: titanic.columns\n",
      "316/7: titanic.info()\n",
      "316/8: titanic.describe()\n",
      "316/9: titanic.isnull().sum()\n",
      "316/10: titanic.isnull()\n",
      "316/11: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "316/12: titanic.isnull().sum()\n",
      "316/13:\n",
      "titanic[\"new_age\"] = titanic[\"Age\"]\n",
      "titanic[\"new_age\"].isnull().sum()\n",
      "316/14: titanic[\"new_age\"].median()\n",
      "316/15: titanic[\"new_age\"].fillna(titanic[\"new_age\"].median(),inplace=True)\n",
      "316/16: titanic[\"Age\"].std()\n",
      "316/17: titanic[\"new_age\"].std()\n",
      "316/18: titanic.isnull().sum()\n",
      "316/19: titanic.isnull()\n",
      "317/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "317/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")\n",
      "317/3: titanic.head()\n",
      "317/4: titanic.tail()\n",
      "317/5: titanic.shape      # to know the shape of the data\n",
      "317/6: titanic.columns\n",
      "317/7: titanic.info()\n",
      "317/8: titanic.describe()\n",
      "317/9: titanic.isnull().sum()\n",
      "317/10: titanic.isnull()\n",
      "319/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "319/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")\n",
      "319/3: titanic.head()\n",
      "319/4: titanic.tail()\n",
      "319/5: titanic.shape      # to know the shape of the data\n",
      "319/6: titanic.columns\n",
      "319/7: titanic.info()\n",
      "319/8: titanic.describe()\n",
      "319/9: titanic.isnull().sum()\n",
      "319/10: titanic.isnull()\n",
      "319/11: titanic.dropna(subse)\n",
      "319/12: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "319/13: titanic.shape\n",
      "319/14: titanic[\"Age\"].isnull().sum()\n",
      "319/15: titanic[\"Age\"].isnull().index\n",
      "319/16: titanic[titanic[\"Age\"].isnull()].index\n",
      "319/17: df['Age'].dropna().sample(df['Age'].isnull().sum(),random_state=0)\n",
      "319/18: titanic['Age'].dropna().sample(df['Age'].isnull().sum(),random_state=0)\n",
      "319/19: titanic['Age'].dropna().sample(titanic['Age'].isnull().sum(),random_state=0)\n",
      "319/20: titanic['Age'].dropna().sample(titanic['Age'].isnull().sum(),random_state=42)\n",
      "319/21: titanic[\"Age\"].isnull().any()\n",
      "319/22: titanic[\"Age\"].isnull().sum()\n",
      "319/23:\n",
      "def impute_nan(df,variable,median):\n",
      "    df[variable+\"_median\"]=df[variable].fillna(median)\n",
      "    df[variable+\"_random\"]=df[variable]\n",
      "    ##It will have the random sample to fill the na\n",
      "    random_sample=df[variable].dropna().sample(df[variable].isnull().sum(),random_state=0)\n",
      "    ##pandas need to have same index in order to merge the dataset\n",
      "    random_sample.index=df[df[variable].isnull()].index\n",
      "    df.loc[df[variable].isnull(),variable+'_random']=random_sample\n",
      "319/24:\n",
      "impute_nan(titanic,\"Age\",median)\n",
      "titanic.head()\n",
      "319/25:\n",
      "impute_nan(titanic,\"Age\",'median')\n",
      "titanic.head()\n",
      "319/26: titanic['Age'].std\n",
      "319/27: titanic['Age'].std()\n",
      "319/28: titanic['Age_random'].std()\n",
      "320/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "320/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")\n",
      "320/3: titanic.head()\n",
      "320/4: titanic.tail()\n",
      "320/5: titanic.shape      # to know the shape of the data\n",
      "320/6: titanic.columns\n",
      "320/7: titanic.info()\n",
      "320/8: titanic.describe()\n",
      "320/9: titanic.isnull().sum()\n",
      "320/10: titanic.isnull()\n",
      "320/11: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "320/12: titanic.shape\n",
      "320/13: titanic[\"Age\"].isnull().sum()\n",
      "320/14: titanic[titanic[\"Age\"].isnull()].index\n",
      "320/15: titanic['Age'].dropna().sample(titanic['Age'].isnull().sum(),random_state=42)\n",
      "320/16: titanic[\"Age\"].isnull().sum()\n",
      "320/17:\n",
      "def impute_nan(df,variable,median):\n",
      "    df[variable+\"_median\"]=df[variable].fillna(median)\n",
      "    df[variable+\"_random\"]=df[variable]\n",
      "    ##It will have the random sample to fill the na\n",
      "    random_sample=df[variable].dropna().sample(df[variable].isnull().sum(),random_state=0)\n",
      "    ##pandas need to have same index in order to merge the dataset\n",
      "    random_sample.index=df[df[variable].isnull()].index\n",
      "    print(random_sample)\n",
      "    df.loc[df[variable].isnull(),variable+'_random']=random_sample\n",
      "320/18:\n",
      "impute_nan(titanic,\"Age\",'median')\n",
      "titanic.head()\n",
      "320/19: titanic['Age'].std()\n",
      "320/20: titanic['Age_random'].std()\n",
      "321/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "321/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")\n",
      "321/3: titanic.head()\n",
      "321/4: titanic.tail()\n",
      "321/5: titanic.shape      # to know the shape of the data\n",
      "321/6: titanic.columns\n",
      "321/7: titanic.info()\n",
      "321/8: titanic.describe()\n",
      "321/9: titanic.isnull().sum()\n",
      "321/10: titanic.isnull()\n",
      "321/11: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "321/12: titanic.shape\n",
      "321/13: titanic[\"Age\"].isnull().sum()\n",
      "321/14: titanic[titanic[\"Age\"].isnull()].index\n",
      "321/15: titanic['Age'].dropna().sample(titanic['Age'].isnull().sum(),random_state=42)\n",
      "321/16: titanic[\"Age\"].isnull().sum()\n",
      "321/17:\n",
      "def impute_nan(df,variable,median):\n",
      "    df[variable+\"_median\"]=df[variable].fillna(median)\n",
      "    df[variable+\"_random\"]=df[variable]\n",
      "    ##It will have the random sample to fill the na\n",
      "    random_sample=df[variable].dropna().sample(df[variable].isnull().sum(),random_state=0)\n",
      "    ##pandas need to have same index in order to merge the dataset\n",
      "    random_sample.index=df[df[variable].isnull()].index\n",
      "    print(random_sample.index)\n",
      "    df.loc[df[variable].isnull(),variable+'_random']=random_sample\n",
      "321/18:\n",
      "impute_nan(titanic,\"Age\",'median')\n",
      "titanic.head()\n",
      "321/19: titanic['Age'].std()\n",
      "321/20: titanic['Age_random'].std()\n",
      "323/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "323/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")\n",
      "323/3: titanic.head()\n",
      "323/4: titanic.tail()\n",
      "323/5: titanic.shape      # to know the shape of the data\n",
      "323/6: titanic.columns\n",
      "323/7: titanic.info()\n",
      "323/8: titanic.describe()\n",
      "323/9: titanic.isnull().sum()\n",
      "323/10: titanic.isnull()\n",
      "324/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "324/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")\n",
      "324/3: titanic.head()\n",
      "324/4: titanic.tail()\n",
      "324/5: titanic.shape      # to know the shape of the data\n",
      "324/6: titanic.columns\n",
      "324/7: titanic.info()\n",
      "324/8: titanic.describe()\n",
      "324/9: titanic.isnull().sum()\n",
      "324/10: titanic.isnull()\n",
      "325/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "326/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "326/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")\n",
      "326/3: titanic.head()\n",
      "326/4: titanic.tail()\n",
      "326/5: titanic.shape      # to know the shape of the data\n",
      "326/6: titanic.columns\n",
      "326/7: titanic.info()\n",
      "326/8: titanic.describe()\n",
      "326/9: titanic.isnull().sum()\n",
      "326/10: titanic.isnull()\n",
      "326/11:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "326/12:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "326/13: titanic\n",
      "326/14: titanic.head()\n",
      "326/15:\n",
      "print(type(titanic))\n",
      "titanic.head()\n",
      "326/16:\n",
      "print(type(titanic))\n",
      "#titanic.head()\n",
      "326/17:\n",
      "#print(type(titanic))\n",
      "titanic.head()\n",
      "326/18: titanic.isnull().mean()*100\n",
      "326/19: titanic.isnull().mean()      # to find the missing value percentage in each column.\n",
      "326/20: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "326/21: 1309/418\n",
      "326/22: 418/1309\n",
      "326/23: (418/1309)*100\n",
      "326/24: titanic.drop(\"Class\",axis=1)\n",
      "326/25: titanic.head()\n",
      "326/26: titanic.drop(\"Class\",axis=1,inplace=True)\n",
      "326/27: titanic.head()\n",
      "327/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "327/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "327/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "327/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "327/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "327/6: titanic.columns     # to know all the labes, features, columns\n",
      "327/7: titanic.info()\n",
      "327/8: titanic.describe()\n",
      "327/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "327/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "327/11: titanic.drop(\"Class\",axis=1,inplace=True)\n",
      "327/12: titanic.head()\n",
      "327/13: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "327/14: titanic.head()\n",
      "327/15: titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True)\n",
      "327/16: titanic.head()\n",
      "327/17: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "327/18: titanic.head()\n",
      "327/19: titanic.isnull().sum()\n",
      "327/20: titanic.isnull().mean\n",
      "327/21: titanic.isnull().mean()*100\n",
      "327/22: titanic.info()\n",
      "327/23: titanic.dropna(subset=[\"Survived\"],inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327/24: titanic.isnull().mean()*100\n",
      "327/25: titanic.info()\n",
      "327/26: titanic.shape\n",
      "327/27: titanic.isnull().sum()\n",
      "329/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "329/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "329/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "329/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "329/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "329/6: titanic.columns     # to know all the labes, features, columns\n",
      "329/7: titanic.info()\n",
      "329/8: titanic.describe()\n",
      "329/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "329/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "329/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "329/12: titanic.head()\n",
      "329/13: titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True)\n",
      "329/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "329/15: titanic.isnull().mean()*100\n",
      "329/16: titanic.info()\n",
      "329/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "329/18: titanic.isnull().sum()\n",
      "329/19: titanic.info()\n",
      "329/20: titanic.shape\n",
      "329/21: from sklearn.preprocessing import OneHotEncoder\n",
      "329/22: sex = pd.get_dummies(df[\"Sex\"])\n",
      "329/23: sex = pd.get_dummies(titanic[\"Sex\"])\n",
      "329/24:\n",
      "sex = pd.get_dummies(titanic[\"Sex\"])\n",
      "sex\n",
      "329/25:\n",
      "sex = pd.get_dummies(titanic[\"Sex\"])\n",
      "sex.head()\n",
      "329/26:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "sex1 = (titanic[\"Sex\"])\n",
      "329/27: sex1.head()\n",
      "329/28:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "sex1 = encoder(titanic[\"Sex\"])\n",
      "329/29:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "sex1 = encoder.fit_transform(titanic[\"Sex\"])\n",
      "329/30:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "titanic_sex = np.array(titanic[\"Sex\"])\n",
      "sex1 = encoder.fit_transform(titanic[\"Sex\"])\n",
      "329/31:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "titanic_sex = np.array(titanic[\"Sex\"])\n",
      "titanic_sex\n",
      "#sex1 = encoder.fit_transform(titanic[\"Sex\"])\n",
      "329/32:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "titanic_sex = np.array(titanic[\"Sex\"]).shape(-1,1)\n",
      "titanic_sex\n",
      "#sex1 = encoder.fit_transform(titanic[\"Sex\"])\n",
      "329/33:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "titanic_sex = np.array(titanic[\"Sex\"])\n",
      "titanic_sex.shape(-1,1)\n",
      "#sex1 = encoder.fit_transform(titanic[\"Sex\"])\n",
      "329/34:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "titanic_sex = np.array(titanic[\"Sex\"]).reshape(-1,1)\n",
      "titanic_sex\n",
      "#sex1 = encoder.fit_transform(titanic[\"Sex\"])\n",
      "329/35:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "titanic_sex = np.array(titanic[\"Sex\"]).reshape(-1,1)\n",
      "titanic_sex\n",
      "sex1 = encoder.fit_transform(titanic[\"Sex\"])\n",
      "329/36:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "titanic_sex = np.array(titanic[\"Sex\"]).reshape(-1,1)\n",
      "titanic_sex\n",
      "sex1 = encoder.fit_transform(tianic_sex)\n",
      "329/37:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "titanic_sex = np.array(titanic[\"Sex\"]).reshape(-1,1)\n",
      "titanic_sex\n",
      "sex1 = encoder.fit_transform(titanic_sex)\n",
      "329/38:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "titanic_sex = np.array(titanic[\"Sex\"])\n",
      "titanic_sex\n",
      "sex1 = encoder.fit_transform(titanic_sex)\n",
      "329/39:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "titanic_sex = np.array(titanic[\"Sex\"]).reshape(-1,1)\n",
      "titanic_sex\n",
      "sex1 = encoder.fit_transform(titanic_sex)\n",
      "329/40:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "titanic_sex = np.array(titanic[\"Sex\"]).reshape(-1,1)\n",
      "print(titanic_sex.head())\n",
      "sex1 = encoder.fit_transform(titanic_sex)\n",
      "329/41:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "titanic_sex = np.array(titanic[\"Sex\"]).reshape(-1,1)\n",
      "print(titanic_sex)\n",
      "sex1 = encoder.fit_transform(titanic_sex)\n",
      "329/42: sex1\n",
      "329/43: sex1.head()\n",
      "329/44: sex1\n",
      "329/45: pd.concat([titanic,sex,sex1],axis=1)\n",
      "329/46: sex1 = pd.DataFrame(data=sex1)\n",
      "329/47: pd.concat([titanic,sex,sex1],axis=1)\n",
      "329/48: sex1 = pd.DataFrame(data=sex1,columns=['female','male'])\n",
      "329/49: pd.concat([titanic,sex,sex1],axis=1)\n",
      "329/50: sex1 = pd.DataFrame(data=sex1,columns=['female1','male1'])\n",
      "329/51: pd.concat([titanic,sex,sex1],axis=1)\n",
      "330/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "330/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "330/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "330/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "330/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "330/6: titanic.columns     # to know all the labes, features, columns\n",
      "330/7: titanic.info()\n",
      "330/8: titanic.describe()\n",
      "330/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "330/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "330/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "330/12: titanic.head()\n",
      "330/13: titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True)\n",
      "330/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "330/15: titanic.isnull().mean()*100\n",
      "330/16: titanic.info()\n",
      "330/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "330/18: titanic.isnull().sum()\n",
      "330/19: titanic.info()\n",
      "330/20: titanic.shape\n",
      "330/21: from sklearn.preprocessing import OneHotEncoder\n",
      "330/22:\n",
      "sex = pd.get_dummies(titanic[\"Sex\"])\n",
      "sex.head()\n",
      "330/23:\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "titanic_sex = np.array(titanic[\"Sex\"]).reshape(-1,1)\n",
      "print(titanic_sex)\n",
      "sex1 = encoder.fit_transform(titanic_sex)\n",
      "330/24: sex1\n",
      "330/25: sex1 = pd.DataFrame(data=sex1,columns=['female1','male1'])\n",
      "330/26: pd.concat([titanic,sex,sex1],axis=1)\n",
      "331/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "331/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "331/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "331/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "331/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "331/6: titanic.columns     # to know all the labes, features, columns\n",
      "331/7: titanic.info()\n",
      "331/8: titanic.describe()\n",
      "331/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "331/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "331/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "331/12: titanic.head()\n",
      "331/13: titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True)\n",
      "331/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "331/15: titanic.isnull().mean()*100\n",
      "331/16: titanic.info()\n",
      "331/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "331/18: titanic.isnull().sum()\n",
      "331/19: titanic.info()\n",
      "331/20: titanic.shape\n",
      "332/1:\n",
      "## MEan/ Midean Imputation\n",
      "\n",
      "\n",
      "titanic.head()\n",
      "333/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "333/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "333/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "333/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "333/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "333/6: titanic.columns     # to know all the labes, features, columns\n",
      "333/7: titanic.info()\n",
      "333/8: titanic.describe()\n",
      "333/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "333/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "333/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "333/12: titanic.head()\n",
      "333/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "333/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "333/15: titanic.isnull().mean()*100\n",
      "333/16: titanic.info()\n",
      "333/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "333/18: titanic.isnull().sum()\n",
      "333/19: titanic.info()\n",
      "333/20: titanic.shape\n",
      "333/21:\n",
      "## MEan/ Midean Imputation\n",
      "\n",
      "\n",
      "titanic.head()\n",
      "333/22:\n",
      "## MEan/ Midean Imputation\n",
      "\n",
      "\n",
      "titanic[\"Age\"].hist(bins =50)\n",
      "333/23:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age']\n",
      "333/24:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age']\n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median())\n",
      "333/25:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age']\n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median())\n",
      "\n",
      "titani.head()\n",
      "333/26:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age']\n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median())\n",
      "\n",
      "titanic.head()\n",
      "333/27:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age']\n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True)\n",
      "\n",
      "titanic.head()\n",
      "333/28: titanic[\"Age_median\"].median()\n",
      "333/29: titanic.isnull().sum()\n",
      "333/30: titanic[\"Age\"].std()\n",
      "333/31: titanic[\"Age_median\"].std()\n",
      "333/32:\n",
      "\n",
      "import matplotlip.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend-handle_labels()\n",
      "ax.legend(lines,labels,loc = \"best\")\n",
      "333/33:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend-handle_labels()\n",
      "ax.legend(lines,labels,loc = \"best\")\n",
      "333/34:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handle_labels()\n",
      "ax.legend(lines,labels,loc = \"best\")\n",
      "333/35:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_lagend_handle_labels()\n",
      "ax.legend(lines,labels,loc = \"best\")\n",
      "333/36:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(lines,labels,loc = \"best\")\n",
      "333/37:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "334/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "334/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "334/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "334/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "334/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "334/6: titanic.columns     # to know all the labes, features, columns\n",
      "334/7: titanic.info()\n",
      "334/8: titanic.describe()\n",
      "334/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "334/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "334/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "334/12: titanic.head()\n",
      "334/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "334/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "334/15: titanic.isnull().mean()*100\n",
      "334/16: titanic.info()\n",
      "334/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "334/18: titanic.isnull().sum()\n",
      "334/19: titanic.info()\n",
      "334/20: titanic.shape\n",
      "334/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age']\n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True)\n",
      "\n",
      "titanic.head()\n",
      "334/22: titanic[\"Age\"].std()\n",
      "334/23: titanic[\"Age_median\"].std()\n",
      "334/24:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "334/25: ## Random sample Imputation\n",
      "334/26: titanic[\"Age\"].median()\n",
      "334/27: titanic[\"Age_median\"].median()\n",
      "334/28: titanic[\"Age\"].std()\n",
      "334/29: titanic[\"Age_median\"].std()\n",
      "334/30:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age\"].dropna().sample()\n",
      "334/31:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age\"].dropna().sample()\n",
      "334/32:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age\"].dropna().sample()\n",
      "334/33:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age\"].dropna().sample()\n",
      "334/34:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age\"].dropna().sample()\n",
      "334/35:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum())\n",
      "\n",
      "random_sample\n",
      "334/36:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum())\n",
      "\n",
      "random_samples\n",
      "334/37: titanic[\"Age_random\"].dropna().sample()\n",
      "334/38:\n",
      "titanic[\"Age_random\"].dropna().sample()\n",
      "titanic[\"Age_random\"].isnull().sum()\n",
      "334/39:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(177)\n",
      "\n",
      "random_samples\n",
      "334/40:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(177)\n",
      "\n",
      "random_samples\n",
      "334/41:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum())\n",
      "\n",
      "\n",
      "random_samples\n",
      "334/42:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum())\n",
      "\n",
      "\n",
      "random_samples\n",
      "334/43:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "\n",
      "\n",
      "random_samples\n",
      "334/44:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "\n",
      "\n",
      "random_samples\n",
      "334/45:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "\n",
      "\n",
      "random_samples\n",
      "334/46:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "\n",
      "\n",
      "random_samples\n",
      "334/47:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_sample.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),Age_random]=random_sample\n",
      "\n",
      "random_samples\n",
      "334/48:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_sample.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),Age_random]=random_samples\n",
      "\n",
      "random_samples\n",
      "334/49:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),Age_random]=random_samples\n",
      "\n",
      "random_samples\n",
      "334/50:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "random_samples\n",
      "334/51:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "titanic.head()\n",
      "334/52:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "titanic.samples\n",
      "335/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "335/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "335/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "335/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "335/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "335/6: titanic.columns     # to know all the labes, features, columns\n",
      "335/7: titanic.info()\n",
      "335/8: titanic.describe()\n",
      "335/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "335/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "335/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "335/12: titanic.head()\n",
      "335/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "335/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "335/15: titanic.isnull().mean()*100\n",
      "335/16: titanic.info()\n",
      "335/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "335/18: titanic.isnull().sum()\n",
      "335/19: titanic.info()\n",
      "335/20: titanic.shape\n",
      "335/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age']\n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "335/22: titanic[\"Age\"].std()\n",
      "335/23: titanic[\"Age_median\"].std()\n",
      "335/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "335/25:\n",
      "titanic[\"Age_random\"].dropna().sample()\n",
      "titanic[\"Age_random\"].isnull().sum()\n",
      "335/26:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "335/27:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "titanic.samples\n",
      "335/28:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "random_samples\n",
      "335/29:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "titanic.info()\n",
      "335/30:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "random_samples.index\n",
      "336/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "336/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "336/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "336/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "336/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "336/6: titanic.columns     # to know all the labes, features, columns\n",
      "336/7: titanic.info()\n",
      "336/8: titanic.describe()\n",
      "336/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "336/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "336/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "336/12: titanic.head()\n",
      "336/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "336/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "336/15: titanic.isnull().mean()*100\n",
      "336/16: titanic.info()\n",
      "336/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/18: titanic.isnull().sum()\n",
      "336/19: titanic.info()\n",
      "336/20: titanic.shape\n",
      "336/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age']\n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "336/22: titanic[\"Age\"].std()\n",
      "336/23: titanic[\"Age_median\"].std()\n",
      "336/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "336/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "336/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "random_samples.index\n",
      "336/27: from sklearn.preprocessing import OneHotEncoder\n",
      "336/28: sex = np.array(titanic[\"Sex\"])\n",
      "336/29: sex\n",
      "336/30: sex = np.array(titanic[\"Sex\"]).reshape(-1,1)\n",
      "336/31: sex\n",
      "336/32:\n",
      "enc = OneHotEncoder()\n",
      "sex = enc.fit_transform(sex)\n",
      "sex\n",
      "336/33:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "337/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "337/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "337/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "337/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "337/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "337/6: titanic.columns     # to know all the labes, features, columns\n",
      "337/7: titanic.info()\n",
      "337/8: titanic.describe()\n",
      "337/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "337/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "337/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "337/12: titanic.head()\n",
      "337/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "337/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "337/15: titanic.isnull().mean()*100\n",
      "337/16: titanic.info()\n",
      "337/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "337/18: titanic.isnull().sum()\n",
      "337/19: titanic.info()\n",
      "337/20: titanic.shape\n",
      "337/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age']\n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "337/22: titanic[\"Age\"].std()\n",
      "337/23: titanic[\"Age_median\"].std()\n",
      "337/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "337/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "337/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "random_samples.index\n",
      "337/27: from sklearn.preprocessing import OneHotEncoder\n",
      "337/28: sex = np.array(titanic[\"Sex\"]).reshape(-1,1)\n",
      "337/29:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "337/30:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "sex\n",
      "337/31:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "titanic.concat([titanic,sex],axis=1)\n",
      "337/32:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "df = pd.concat([titanic,sex],axis=1)\n",
      "337/33:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "titanic = pd.concat([titanic,sex],axis=1)\n",
      "337/34:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "sex = pd.DataFrame(sex)\n",
      "titanic = pd.concat([titanic,sex],axis=1)\n",
      "337/35:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "sex = pd.DataFrame(sex)\n",
      "titanic = pd.concat([titanic,sex],axis=1)\n",
      "titanic\n",
      "337/36:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "sex = pd.DataFrame(sex)\n",
      "print(sex)\n",
      "titanic = pd.concat([titanic,sex],axis=1)\n",
      "titanic\n",
      "338/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "338/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "338/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "338/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "338/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "338/6: titanic.columns     # to know all the labes, features, columns\n",
      "338/7: titanic.info()\n",
      "338/8: titanic.describe()\n",
      "338/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "338/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "338/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "338/12: titanic.head()\n",
      "338/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "338/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "338/15: titanic.isnull().mean()*100\n",
      "338/16: titanic.info()\n",
      "338/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "338/18: titanic.isnull().sum()\n",
      "338/19: titanic.info()\n",
      "338/20: titanic.shape\n",
      "338/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age']\n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "338/22: titanic[\"Age\"].std()\n",
      "338/23: titanic[\"Age_median\"].std()\n",
      "338/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "338/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "338/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "random_samples.index\n",
      "338/27: from sklearn.preprocessing import OneHotEncoder\n",
      "338/28: sex = np.array(titanic[\"Sex\"]).reshape(-1,1)\n",
      "338/29:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "338/30:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "sex\n",
      "338/31: sex = np.array(titanic[\"Sex\"]).reshape(1,-1)\n",
      "338/32:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "sex\n",
      "338/33:\n",
      "sex = np.array(titanic[\"Sex\"]).reshape(1,-1)\n",
      "sex\n",
      "338/34:\n",
      "sex = np.array(titanic[\"Sex\"]).reshape(-1,1)\n",
      "sex\n",
      "338/35:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "sex\n",
      "338/36:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "sex = pd.DataFrame(data=sex, columns=['male','female'])\n",
      "sex\n",
      "338/37:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "sex = pd.DataFrame(data=sex, columns=['male','female'])\n",
      "titanic = pd.concat([titanic,sex],axis=1)\n",
      "338/38:\n",
      "sex = nparray(titanic[\"Sex\"]).reshape(-1,1)\n",
      "sex\n",
      "338/39:\n",
      "sex = np.array(titanic[\"Sex\"])\n",
      "sex\n",
      "338/40:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "sex = pd.DataFrame(data=sex, columns=['male','female'])\n",
      "titanic = pd.concat([titanic,sex],axis=1)\n",
      "338/41:\n",
      "sex = np.array(titanic[\"Sex\"])\n",
      "sex.reshape(-1,1)\n",
      "sex\n",
      "338/42:\n",
      "sex = np.array(titanic[\"Sex\"])\n",
      "sex.reshape(1,-1)\n",
      "sex\n",
      "338/43:\n",
      "sex = np.array(titanic[\"Sex\"])\n",
      "sex.reshape(1,-1)\n",
      "sex\n",
      "338/44:\n",
      "sex = np.array(titanic[\"Sex\"])\n",
      "sex = sex.reshape(1,-1)\n",
      "sex\n",
      "338/45:\n",
      "sex = np.array(titanic[\"Sex\"])\n",
      "sex = sex.reshape(-1,1)\n",
      "sex\n",
      "338/46:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "sex = pd.DataFrame(data=sex, columns=['male','female'])\n",
      "titanic = pd.concat([titanic,sex],axis=1)\n",
      "338/47: titanic\n",
      "338/48:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "sex = pd.DataFrame(data=sex, columns=['female','male'])\n",
      "titanic = pd.concat([titanic,sex],axis=1)\n",
      "339/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "339/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "339/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "339/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "339/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "339/6: titanic.columns     # to know all the labes, features, columns\n",
      "339/7: titanic.info()\n",
      "339/8: titanic.describe()\n",
      "339/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "339/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "339/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "339/12: titanic.head()\n",
      "339/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "339/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "339/15: titanic.isnull().mean()*100\n",
      "339/16: titanic.info()\n",
      "339/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "339/18: titanic.isnull().sum()\n",
      "339/19: titanic.info()\n",
      "339/20: titanic.shape\n",
      "339/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age']\n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "339/22: titanic[\"Age\"].std()\n",
      "339/23: titanic[\"Age_median\"].std()\n",
      "339/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "339/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "339/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "random_samples.index\n",
      "339/27: from sklearn.preprocessing import OneHotEncoder\n",
      "339/28:\n",
      "sex = np.array(titanic[\"Sex\"])\n",
      "sex = sex.reshape(-1,1)\n",
      "sex\n",
      "339/29:\n",
      "enc = OneHotEncoder(sparse=False)\n",
      "sex = enc.fit_transform(sex)\n",
      "sex = pd.DataFrame(data=sex, columns=['female','male'])\n",
      "titanic = pd.concat([titanic,sex],axis=1)\n",
      "339/30: titanic\n",
      "340/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "340/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "340/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "340/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "340/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "340/6: titanic.columns     # to know all the labes, features, columns\n",
      "340/7: titanic.info()\n",
      "340/8: titanic.describe()\n",
      "340/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "340/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "340/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "340/12: titanic.head()\n",
      "340/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "340/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "340/15: titanic.isnull().mean()*100\n",
      "340/16: titanic.info()\n",
      "340/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "340/18: titanic.isnull().sum()\n",
      "340/19: titanic.info()\n",
      "340/20: titanic.shape\n",
      "340/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age']\n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "340/22: titanic[\"Age\"].std()\n",
      "340/23: titanic[\"Age_median\"].std()\n",
      "340/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "340/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "340/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "random_samples.index\n",
      "341/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "341/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "341/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "341/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "341/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "341/6: titanic.columns     # to know all the labes, features, columns\n",
      "341/7: titanic.info()\n",
      "341/8: titanic.describe()\n",
      "341/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "341/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "341/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "341/12: titanic.head()\n",
      "341/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "341/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "341/15: titanic.isnull().mean()*100\n",
      "341/16: titanic.info()\n",
      "341/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "341/18: titanic.isnull().sum()\n",
      "341/19: titanic.info()\n",
      "341/20: titanic.shape\n",
      "341/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age']\n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "341/22: titanic[\"Age\"].std()\n",
      "341/23: titanic[\"Age_median\"].std()\n",
      "341/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "341/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "341/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "random_samples.index\n",
      "341/27:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "341/28: 891-177\n",
      "341/29:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "titanic.isnull().sum()\n",
      "341/30:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "#titanic.isnull().sum()\n",
      "341/31: titanic[\"Age\"].std()\n",
      "341/32: titanic[\"Age_random\"].std()\n",
      "341/33: titanic[\"Age_median\"].std()\n",
      "341/34:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "341/35:\n",
      "import seaborn as sns\n",
      "\n",
      "\n",
      "sns.heatmap(titanic.isnull(), yticklabels = False)\n",
      "341/36:\n",
      "import seaborn as sns\n",
      "\n",
      "sns.heatmap(titanic.isnull(), yticklabels = False, cmap=\"viridis\")\n",
      "341/37:\n",
      "import seaborn as sns\n",
      "\n",
      "sns.heatmap(titanic.isnull(), yticklabels = False, cmap=\"viridis\", cbar=False)\n",
      "343/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "343/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "343/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "343/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "343/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "343/6: titanic.columns     # to know all the labes, features, columns\n",
      "343/7: titanic.info()\n",
      "343/8: titanic.describe()\n",
      "343/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "343/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "343/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "343/12: titanic.head()\n",
      "343/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "343/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "343/15: titanic.isnull().mean()*100\n",
      "343/16: titanic.info()\n",
      "343/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "343/18: titanic.isnull().sum()\n",
      "343/19: titanic.info()\n",
      "343/20: titanic.shape\n",
      "343/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "343/22: titanic[\"Age\"].std()\n",
      "343/23: titanic[\"Age_median\"].std()\n",
      "343/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "343/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "343/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "#titanic.isnull().sum()\n",
      "343/27: 891-177\n",
      "343/28: titanic[\"Age\"].std()\n",
      "343/29: titanic[\"Age_random\"].std()\n",
      "343/30: titanic[\"Age_median\"].std()\n",
      "343/31:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "343/32: titanic.isnull().sum()\n",
      "343/33: #titanic.isnull().sum()\n",
      "343/34:\n",
      "import seaborn as sns\n",
      "\n",
      "sns.heatmap(titanic.isnull(),yticklabel=False,cbar=False,cmap=\"viridic\")\n",
      "343/35:\n",
      "import seaborn as sns\n",
      "\n",
      "sns.heatmap(titanic.isnull(),yticklabel=False,cbar=False,cmap=\"viridis\")\n",
      "343/36:\n",
      "import seaborn as sns\n",
      "\n",
      "sns.heatmap(titanic.isnull(),yticklabels=False,cbar=False,cmap=\"viridis\")\n",
      "343/37:\n",
      "#\n",
      "titanic.isnull().sum()\n",
      "343/38: 2/891\n",
      "343/39:\n",
      "import seaborn as sns\n",
      "\n",
      "sns.heatmap(titanic.isnull(),yticklabels=True,cbar=False,cmap=\"viridis\")\n",
      "343/40:\n",
      "import seaborn as sns\n",
      "\n",
      "sns.heatmap(titanic.isnull(),yticklabels=True,cmap=\"viridis\")\n",
      "343/41:\n",
      "import seaborn as sns\n",
      "\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cmap=\"viridis\")\n",
      "343/42:\n",
      "import seaborn as sns\n",
      "\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto')\n",
      "343/43:\n",
      "import seaborn as sns\n",
      "\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"viridis\")\n",
      "343/44:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"winter\")\n",
      "343/45:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"viridi\")\n",
      "343/46:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "343/47: titanic[\"Embarked\"].unique()\n",
      "343/48: titanic[\"Cabin\"].unique()\n",
      "343/49:\n",
      "#\n",
      "titanic.isnull().mean()\n",
      "343/50: #titanic.isnull()\n",
      "343/51: titanic[\"Cabin\"].unique().count()    # this show the\n",
      "343/52: titanic[\"Cabin\"].unique().sum()    # this show the\n",
      "343/53: titanic[\"Cabin\"].value_count()    # this show the\n",
      "343/54: titanic[\"Cabin\"].values_count()    # this show the\n",
      "343/55: titanic[\"Cabin\"].value_counts()    # this show the\n",
      "343/56: titanic[\"Cabin\"].unique()    # this show the\n",
      "343/57:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titani.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "343/58:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "343/59:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "343/60:\n",
      "# Number of person survived and number of persion dies\n",
      "\n",
      "sns.coumtplot(x='Survived',data=titanic)\n",
      "343/61:\n",
      "# Number of person survived and number of persion dies\n",
      "\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "343/62:\n",
      "# Number of person survived and number of persion dies\n",
      "print(titanic['Survived'].unique())\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "343/63:\n",
      "# Number of person survived and number of persion dies\n",
      "print(titanic['Survived'].value_counts())\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "343/64: sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "343/65:\n",
      "print(titanic.value_conts())\n",
      "\n",
      "sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "343/66:\n",
      "print(titanic[\"Sex\"].value_counts())\n",
      "\n",
      "sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "343/67: titanic.isnull()\n",
      "343/68: titanic.isnull().sum()\n",
      "343/69:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "343/70: titanic[\"Embarked\"].mode()\n",
      "343/71: titanic[\"Embarked\"].mode()[0]\n",
      "343/72:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "# imputing Catagorical values\n",
      "#  -> Frequency Imputation/Mode imputation.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0], inplace=True)\n",
      "343/73: titanic[\"Embarked\"].mode()[0]\n",
      "343/74: titanic[\"Embarked\"]unique()\n",
      "343/75: titanic[\"Embarked\"].unique()\n",
      "343/76: sns.countplot(x='Survived', hue='Embarked', data=titanic)\n",
      "344/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "344/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "344/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "344/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "344/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "344/6: titanic.columns     # to know all the labes, features, columns\n",
      "344/7: titanic.info()\n",
      "344/8: titanic.describe()\n",
      "344/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "344/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "344/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "344/12:\n",
      "titanic.head()\n",
      "titanic[\"Borded\"].unique()\n",
      "344/13:\n",
      "titanic.head()\n",
      "titanic[\"Boarded\"].unique()\n",
      "345/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "345/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "345/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "345/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "345/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "345/6: titanic.columns     # to know all the labes, features, columns\n",
      "345/7: titanic.info()\n",
      "345/8: titanic.describe()\n",
      "345/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "345/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "345/12:\n",
      "titanic.head()\n",
      "titanic[\"Boarded\"].unique()\n",
      "345/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "345/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "345/15: titanic.isnull().mean()*100\n",
      "345/16: titanic.info()\n",
      "345/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "345/18: titanic.isnull().sum()\n",
      "345/19: titanic.info()\n",
      "345/20: titanic.shape\n",
      "345/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "345/22: titanic[\"Age\"].std()\n",
      "345/23: titanic[\"Age_median\"].std()\n",
      "345/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "345/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "345/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "#titanic.isnull().sum()\n",
      "345/27: 891-177\n",
      "345/28: titanic[\"Age\"].std()\n",
      "345/29: titanic[\"Age_random\"].std()\n",
      "345/30: titanic[\"Age_median\"].std()\n",
      "345/31:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "345/32: titanic.isnull().sum()\n",
      "345/33:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "345/34: 2/891\n",
      "345/35: titanic[\"Cabin\"].unique()    # this show the unique values in a particular column.\n",
      "345/36:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "345/37:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "# imputing Catagorical values\n",
      "#  -> Frequency Imputation/Mode imputation.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0], inplace=True)\n",
      "345/38: titanic[\"Embarked\"].unique()\n",
      "345/39:\n",
      "# Number of person survived and number of person dinot survived.\n",
      "print(titanic['Survived'].value_counts())\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "345/40: sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "345/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)\n",
      "345/42: titanic[\"Pclass\"].unique()\n",
      "345/43: sns.contplot(x='Survived',hue=\"Pclass\", data = titanic)\n",
      "345/44: sns.countplot(x='Survived',hue=\"Pclass\", data = titanic)\n",
      "345/45:\n",
      "plt.figure(figsize=(12,10))\n",
      "sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "345/46:\n",
      "plt.figure(figsize=(10,10))\n",
      "sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "345/47:\n",
      "plt.figure(figsize=(5,10))\n",
      "sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "345/48:\n",
      "plt.figure(figsize=(12,8))\n",
      "sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "345/49:\n",
      "##plt.figure(figsize=(12,8))\n",
      "##sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "345/50: titanic.head()\n",
      "346/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "346/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                      # overhere titanic is basically a pandas dataframe\n",
      "346/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "346/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "346/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "346/6: titanic.columns     # to know all the labes, features, columns\n",
      "346/7: titanic.info()\n",
      "346/8: titanic.describe()\n",
      "346/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "346/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "346/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "346/12:\n",
      "titanic.head()\n",
      "titanic[\"Boarded\"].unique()\n",
      "346/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "346/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "346/15: titanic.isnull().mean()*100\n",
      "346/16: titanic.info()\n",
      "346/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "346/18: titanic.isnull().sum()\n",
      "346/19: titanic.info()\n",
      "346/20: titanic.shape\n",
      "346/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "346/22: titanic[\"Age\"].std()\n",
      "346/23: titanic[\"Age_median\"].std()\n",
      "346/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "346/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "346/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "#titanic.isnull().sum()\n",
      "346/27: 891-177\n",
      "346/28: titanic[\"Age\"].std()\n",
      "346/29: titanic[\"Age_random\"].std()\n",
      "346/30: titanic[\"Age_median\"].std()\n",
      "346/31:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "346/32: titanic.isnull().sum()\n",
      "346/33:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "346/34: 2/891\n",
      "346/35: titanic[\"Cabin\"].unique()    # this show the unique values in a particular column.\n",
      "346/36:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "346/37:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "# imputing Catagorical values\n",
      "#  -> Frequency Imputation/Mode imputation.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0], inplace=True)\n",
      "346/38: titanic[\"Embarked\"].unique()\n",
      "346/39:\n",
      "# Number of person survived and number of person dinot survived.\n",
      "print(titanic['Survived'].value_counts())\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "346/40: sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "346/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)\n",
      "346/42: titanic[\"Pclass\"].unique()\n",
      "346/43: sns.countplot(x='Survived',hue=\"Pclass\", data = titanic)\n",
      "346/44:\n",
      "##plt.figure(figsize=(12,8))\n",
      "##sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "346/45: titanic.head()\n",
      "346/46: titanic[\"Ag\"].std()\n",
      "346/47: titanic[\"Age\"].std()\n",
      "346/48:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "type(titanic)                                  # overhere titanic is basically a pandas dataframe\n",
      "346/49:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                 # overhere titanic is basically a pandas dataframe\n",
      "346/50:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "346/51:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "346/52:\n",
      "#print(type(titanic))\n",
      "titanic.head(10)        # head() returns first first five rows of the whole data.\n",
      "346/53:\n",
      "#print(type(titanic))\n",
      "titanic        # head() returns first first five rows of the whole data.\n",
      "346/54:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "346/55: titanic     # tail() returns last five rows of the whole data.\n",
      "346/56: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "346/57: type(titanic)\n",
      "346/58: titanic.info()\n",
      "346/59: titanic.info()\n",
      "346/60: titanic.describe()\n",
      "347/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "347/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                 # overhere titanic is basically a pandas dataframe\n",
      "347/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "347/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "347/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "347/6: titanic.columns     # to know all the labes, features, columns\n",
      "347/7: titanic.info()\n",
      "347/8: titanic.describe()\n",
      "347/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "347/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "347/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "347/12:\n",
      "titanic.head()\n",
      "titanic[\"Boarded\"].unique()\n",
      "347/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "347/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "347/15: titanic.isnull().mean()*100\n",
      "347/16: titanic.info()\n",
      "347/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "347/18: titanic.isnull().sum()\n",
      "347/19: titanic.info()\n",
      "347/20: titanic.shape\n",
      "347/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "347/22: titanic[\"Age\"].std()\n",
      "347/23: titanic[\"Age_median\"].std()\n",
      "347/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "347/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "347/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "#titanic.isnull().sum()\n",
      "347/27: 891-177\n",
      "347/28: titanic[\"Age\"].std()\n",
      "347/29: titanic[\"Age_random\"].std()\n",
      "347/30: titanic[\"Age_median\"].std()\n",
      "347/31:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "347/32: titanic.isnull().sum()\n",
      "347/33:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "347/34: 2/891\n",
      "347/35: titanic[\"Cabin\"].unique()    # this show the unique values in a particular column.\n",
      "347/36:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "347/37:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "# imputing Catagorical values\n",
      "#  -> Frequency Imputation/Mode imputation.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0], inplace=True)\n",
      "347/38: titanic[\"Embarked\"].unique()\n",
      "347/39:\n",
      "# Number of person survived and number of person dinot survived.\n",
      "print(titanic['Survived'].value_counts())\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "347/40: sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "347/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)\n",
      "347/42: titanic[\"Pclass\"].unique()\n",
      "347/43: sns.countplot(x='Survived',hue=\"Pclass\", data = titanic)\n",
      "347/44:\n",
      "##plt.figure(figsize=(12,8))\n",
      "##sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "347/45: titanic.head()\n",
      "347/46: titanic.corr()\n",
      "347/47: sns.heatmap(x=titanic.corr())\n",
      "347/48: sns.heatmap(x=titanic.corr(), data=titanic)\n",
      "347/49: sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)\n",
      "347/50: sns.pairplot(titanic)\n",
      "347/51: sns.heatmap(titanic.corr(), annot=False, linewidth=0.5)\n",
      "347/52: sns.heatmap(titanic.corr(), annot=False, linewidth=1)\n",
      "347/53: #sns.pairplot(titanic)\n",
      "347/54: sns.heatmap(titanic.corr(), annot=False, linewidth=5)\n",
      "347/55: sns.heatmap(titanic.corr(), annot=True, linewidth=5)\n",
      "347/56: sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)\n",
      "347/57:\n",
      "#\n",
      "sns.pairplot(titanic)\n",
      "347/58: #sns.pairplot(titanic)\n",
      "347/59: sns.pairplot(titanic)\n",
      "347/60: sns.pairplot(titanic, hue=\"Survived\")\n",
      "347/61: sns.pairplot(titanic, hue=\"Survived\", vars=[\"Pclass\"])\n",
      "347/62: sns.pairplot(titanic, hue=\"Survived\", vars=[\"Pclass\",\"Age\"])\n",
      "347/63: #sns.pairplot(titanic, hue=\"Survived\", vars=[\"Pclass\",\"Age\"])\n",
      "347/64: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)\n",
      "347/65: titanic.head()\n",
      "347/66: # Handeling Categorical features\n",
      "347/67:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"])\n",
      "347/68:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"])\n",
      "Embarked1\n",
      "347/69:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"])\n",
      "Embarked1\n",
      "\n",
      "\n",
      "titanic = titanic.concat([titanic,Embarked1],axis=1)\n",
      "347/70:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"])\n",
      "Embarked1\n",
      "\n",
      "\n",
      "titanic = titanic.Concat([titanic,Embarked1],axis=1)\n",
      "347/71:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"])\n",
      "Embarked1\n",
      "\n",
      "\n",
      "titanic = pd.concat([titanic,Embarked1],axis=1)\n",
      "347/72: titanic.head\n",
      "347/73: titanic.head()\n",
      "347/74: Embarked1\n",
      "347/75:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"],drop_first=True)\n",
      "Embarked1\n",
      "\n",
      "\n",
      "titanic = pd.concat([titanic,Embarked1],axis=1)\n",
      "347/76: Embarked1\n",
      "347/77: titanic.head()\n",
      "348/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "348/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                 # overhere titanic is basically a pandas dataframe\n",
      "348/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "348/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "348/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "348/6: titanic.columns     # to know all the labes, features, columns\n",
      "348/7: titanic.info()\n",
      "348/8: titanic.describe()\n",
      "348/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "348/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "348/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "348/12:\n",
      "titanic.head()\n",
      "titanic[\"Boarded\"].unique()\n",
      "348/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "348/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "348/15: titanic.isnull().mean()*100\n",
      "348/16: titanic.info()\n",
      "348/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "348/18: titanic.isnull().sum()\n",
      "348/19: titanic.info()\n",
      "348/20: titanic.shape\n",
      "348/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "348/22: titanic[\"Age\"].std()\n",
      "348/23: titanic[\"Age_median\"].std()\n",
      "348/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "348/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "348/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "#titanic.isnull().sum()\n",
      "348/27: 891-177\n",
      "348/28: titanic[\"Age\"].std()\n",
      "348/29: titanic[\"Age_random\"].std()\n",
      "348/30: titanic[\"Age_median\"].std()\n",
      "348/31:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "348/32: titanic.isnull().sum()\n",
      "348/33:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "348/34: 2/891\n",
      "348/35: titanic[\"Cabin\"].unique()    # this show the unique values in a particular column.\n",
      "348/36:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "348/37:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "# imputing Catagorical values\n",
      "#  -> Frequency Imputation/Mode imputation.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0], inplace=True)\n",
      "348/38: titanic[\"Embarked\"].unique()\n",
      "348/39:\n",
      "# Number of person survived and number of person dinot survived.\n",
      "print(titanic['Survived'].value_counts())\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "348/40: sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "348/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)\n",
      "348/42: titanic[\"Pclass\"].unique()\n",
      "348/43: sns.countplot(x='Survived',hue=\"Pclass\", data = titanic)\n",
      "348/44:\n",
      "##plt.figure(figsize=(12,8))\n",
      "##sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "348/45: titanic.corr()\n",
      "348/46: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)\n",
      "348/47: #sns.pairplot(titanic, hue=\"Survived\", vars=[\"Pclass\",\"Age\"])\n",
      "348/48: titanic.head()\n",
      "348/49:\n",
      "# Handeling Categorical features\n",
      "\n",
      "# ---> One hot encoding\n",
      "#  ---> Ordinal number encoding\n",
      "#  ---> Label encoder\n",
      "348/50:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"],drop_first=True)\n",
      "Embarked1\n",
      "\n",
      "\n",
      "titanic = pd.concat([titanic,Embarked1],axis=1)\n",
      "348/51: Embarked1\n",
      "348/52: titanic.head()\n",
      "348/53: ()\n",
      "348/54:\n",
      "sex1 = pd.get_dummies(titanic[\"Sex\"],drop_first=True)\n",
      "\n",
      "titanic = pd.concat([titanic,sex1],axis=1)\n",
      "titanic.head()\n",
      "348/55:\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "labl_encoder = LabelEncoder()\n",
      "titanic[\"Embarked\"] = labl_encoder.fit_transform(titanic[\"Embarked\"])\n",
      "titanic.head()\n",
      "348/56: titanic.drop([\"Sex\",\"Embarked\"],axis=1,inplace=true)\n",
      "348/57: titanic.drop([\"Sex\",\"Embarked\"],axis=1,inplace=True)\n",
      "348/58: titanic.head()\n",
      "348/59:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_values = titanic[\"Survived\"]\n",
      "x_value = titanic.drop([\"Survived\"])\n",
      "\n",
      "\n",
      "print(y_value)\n",
      "348/60:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_values = titanic[\"Survived\"]\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)\n",
      "\n",
      "\n",
      "print(y_value)\n",
      "348/61:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_value = titanic[\"Survived\"]\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)\n",
      "\n",
      "\n",
      "print(y_value)\n",
      "348/62:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_value = titanic[\"Survived\"]\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)\n",
      "\n",
      "\n",
      "print(x_value)\n",
      "348/63:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_value = titanic[\"Survived\"]\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)\n",
      "\n",
      "\n",
      "x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, randon_state = 42)\n",
      "348/64:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_value = titanic[\"Survived\"]\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)\n",
      "\n",
      "\n",
      "x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)\n",
      "348/65:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_value = titanic[\"Survived\"]\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)\n",
      "\n",
      "\n",
      "x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)\n",
      "x_train()\n",
      "348/66:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_value = titanic[\"Survived\"]\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)\n",
      "\n",
      "\n",
      "x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)\n",
      "x_train\n",
      "348/67: 80/100\n",
      "348/68: 0.8*891\n",
      "349/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "349/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                 # overhere titanic is basically a pandas dataframe\n",
      "349/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "349/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "349/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/6: titanic.columns     # to know all the labes, features, columns\n",
      "349/7: titanic.info()\n",
      "349/8: titanic.describe()\n",
      "349/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "349/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "349/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "349/12:\n",
      "titanic.head()\n",
      "titanic[\"Boarded\"].unique()\n",
      "349/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "349/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "349/15: titanic.isnull().mean()*100\n",
      "349/16: titanic.info()\n",
      "349/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "349/18: titanic.isnull().sum()\n",
      "349/19: titanic.info()\n",
      "349/20: titanic.shape\n",
      "349/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "349/22: titanic[\"Age\"].std()\n",
      "349/23: titanic[\"Age_median\"].std()\n",
      "349/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "349/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "349/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "#titanic.isnull().sum()\n",
      "349/27: 891-177\n",
      "349/28: titanic[\"Age\"].std()\n",
      "349/29: titanic[\"Age_random\"].std()\n",
      "349/30: titanic[\"Age_median\"].std()\n",
      "349/31:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "349/32: titanic.isnull().sum()\n",
      "349/33:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "349/34: 2/891\n",
      "349/35: titanic[\"Cabin\"].unique()    # this show the unique values in a particular column.\n",
      "349/36:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "349/37:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "# imputing Catagorical values\n",
      "#  -> Frequency Imputation/Mode imputation.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0], inplace=True)\n",
      "349/38: titanic[\"Embarked\"].unique()\n",
      "349/39:\n",
      "# Number of person survived and number of person dinot survived.\n",
      "print(titanic['Survived'].value_counts())\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "349/40: sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "349/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)\n",
      "349/42: titanic[\"Pclass\"].unique()\n",
      "349/43: sns.countplot(x='Survived',hue=\"Pclass\", data = titanic)\n",
      "349/44:\n",
      "##plt.figure(figsize=(12,8))\n",
      "##sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "349/45: titanic.corr()\n",
      "349/46: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)\n",
      "349/47: #sns.pairplot(titanic, hue=\"Survived\", vars=[\"Pclass\",\"Age\"])\n",
      "349/48: titanic.head()\n",
      "349/49:\n",
      "# Handeling Categorical features\n",
      "\n",
      "# ---> One hot encoding\n",
      "#  ---> Ordinal number encoding\n",
      "#  ---> Label encoder\n",
      "349/50:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"],drop_first=True)\n",
      "Embarked1\n",
      "\n",
      "\n",
      "titanic = pd.concat([titanic,Embarked1],axis=1)\n",
      "349/51: Embarked1\n",
      "349/52: titanic.head()\n",
      "349/53:\n",
      "sex1 = pd.get_dummies(titanic[\"Sex\"],drop_first=True)\n",
      "\n",
      "titanic = pd.concat([titanic,sex1],axis=1)\n",
      "titanic.head()\n",
      "349/54: titanic.drop([\"Sex\",\"Embarked\"],axis=1,inplace=True)\n",
      "349/55: titanic.head()\n",
      "349/56:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_value = titanic[\"Survived\"]\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)\n",
      "\n",
      "\n",
      "x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)\n",
      "350/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "350/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                 # overhere titanic is basically a pandas dataframe\n",
      "350/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "350/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "350/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "350/6: titanic.columns     # to know all the labes, features, columns\n",
      "350/7: titanic.info()\n",
      "350/8: titanic.describe()\n",
      "350/9: titanic.isnull()  # count's the number of missing values or NAN values in each column\n",
      "350/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "350/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "350/12:\n",
      "titanic.head()\n",
      "titanic[\"Boarded\"].unique()\n",
      "350/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "350/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "350/15: titanic.isnull().mean()*100\n",
      "350/16: titanic.info()\n",
      "350/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "350/18: titanic.isnull().sum()\n",
      "350/19: titanic.info()\n",
      "350/20: titanic.shape\n",
      "350/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "350/22: titanic[\"Age\"].std()\n",
      "350/23: titanic[\"Age_median\"].std()\n",
      "350/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "350/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "350/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "#titanic.isnull().sum()\n",
      "350/27: 891-177\n",
      "350/28: titanic[\"Age\"].std()\n",
      "350/29: titanic[\"Age_random\"].std()\n",
      "350/30: titanic[\"Age_median\"].std()\n",
      "350/31:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "350/32: titanic.isnull().sum()\n",
      "350/33:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "350/34: 2/891\n",
      "350/35: titanic[\"Cabin\"].unique()    # this show the unique values in a particular column.\n",
      "350/36:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "350/37:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "# imputing Catagorical values\n",
      "#  -> Frequency Imputation/Mode imputation.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0], inplace=True)\n",
      "350/38: titanic[\"Embarked\"].unique()\n",
      "350/39:\n",
      "# Number of person survived and number of person dinot survived.\n",
      "print(titanic['Survived'].value_counts())\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "350/40: sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "350/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)\n",
      "350/42: titanic[\"Pclass\"].unique()\n",
      "350/43: sns.countplot(x='Survived',hue=\"Pclass\", data = titanic)\n",
      "350/44:\n",
      "##plt.figure(figsize=(12,8))\n",
      "##sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "350/45: titanic.corr()\n",
      "350/46: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)\n",
      "350/47: #sns.pairplot(titanic, hue=\"Survived\", vars=[\"Pclass\",\"Age\"])\n",
      "350/48: titanic.head()\n",
      "350/49:\n",
      "# Handeling Categorical features\n",
      "\n",
      "# ---> One hot encoding\n",
      "#  ---> Ordinal number encoding\n",
      "#  ---> Label encoder\n",
      "350/50:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"],drop_first=True)\n",
      "Embarked1\n",
      "\n",
      "\n",
      "titanic = pd.concat([titanic,Embarked1],axis=1)\n",
      "350/51: Embarked1\n",
      "350/52: titanic.head()\n",
      "350/53:\n",
      "sex1 = pd.get_dummies(titanic[\"Sex\"],drop_first=True)\n",
      "\n",
      "titanic = pd.concat([titanic,sex1],axis=1)\n",
      "titanic.head()\n",
      "350/54: titanic.drop([\"Sex\",\"Embarked\"],axis=1,inplace=True)\n",
      "350/55: titanic.head()\n",
      "350/56:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_value = titanic[\"Survived\"]\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)\n",
      "\n",
      "\n",
      "x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)\n",
      "350/57: titanic.describe().sum()\n",
      "351/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "351/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                 # overhere titanic is basically a pandas dataframe\n",
      "351/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "351/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "351/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "351/6: titanic.columns     # to know all the labes, features, columns\n",
      "351/7: titanic.info()\n",
      "351/8: titanic.describe().sum()\n",
      "351/9: titanic.isnull()  # count's the number of missing values or NAN values in each column\n",
      "351/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "351/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "351/12:\n",
      "titanic.head()\n",
      "titanic[\"Boarded\"].unique()\n",
      "351/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "351/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "351/15: titanic.isnull().mean()*100\n",
      "351/16: titanic.info()\n",
      "351/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "351/18: titanic.isnull().sum()\n",
      "351/19: titanic.info()\n",
      "351/20: titanic.shape\n",
      "351/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "351/22: titanic[\"Age\"].std()\n",
      "351/23: titanic[\"Age_median\"].std()\n",
      "351/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "351/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "351/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "#titanic.isnull().sum()\n",
      "351/27: 891-177\n",
      "351/28: titanic[\"Age\"].std()\n",
      "351/29: titanic[\"Age_random\"].std()\n",
      "351/30: titanic[\"Age_median\"].std()\n",
      "351/31:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "351/32: titanic.isnull().sum()\n",
      "351/33:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "351/34: 2/891\n",
      "351/35: titanic[\"Cabin\"].unique()    # this show the unique values in a particular column.\n",
      "351/36:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "351/37:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "# imputing Catagorical values\n",
      "#  -> Frequency Imputation/Mode imputation.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0], inplace=True)\n",
      "351/38: titanic[\"Embarked\"].unique()\n",
      "351/39:\n",
      "# Number of person survived and number of person dinot survived.\n",
      "print(titanic['Survived'].value_counts())\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "351/40: sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "351/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)\n",
      "351/42: titanic[\"Pclass\"].unique()\n",
      "351/43: sns.countplot(x='Survived',hue=\"Pclass\", data = titanic)\n",
      "351/44:\n",
      "##plt.figure(figsize=(12,8))\n",
      "##sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "351/45: titanic.corr()\n",
      "351/46: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)\n",
      "351/47: #sns.pairplot(titanic, hue=\"Survived\", vars=[\"Pclass\",\"Age\"])\n",
      "351/48: titanic.head()\n",
      "351/49:\n",
      "# Handeling Categorical features\n",
      "\n",
      "# ---> One hot encoding\n",
      "#  ---> Ordinal number encoding\n",
      "#  ---> Label encoder\n",
      "351/50:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"],drop_first=True)\n",
      "Embarked1\n",
      "\n",
      "\n",
      "titanic = pd.concat([titanic,Embarked1],axis=1)\n",
      "351/51: Embarked1\n",
      "351/52: titanic.head()\n",
      "351/53:\n",
      "sex1 = pd.get_dummies(titanic[\"Sex\"],drop_first=True)\n",
      "\n",
      "titanic = pd.concat([titanic,sex1],axis=1)\n",
      "titanic.head()\n",
      "351/54: titanic.drop([\"Sex\",\"Embarked\"],axis=1,inplace=True)\n",
      "351/55: titanic.head()\n",
      "351/56:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_value = titanic[\"Survived\"]\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)\n",
      "\n",
      "\n",
      "x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)\n",
      "352/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "352/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                 # overhere titanic is basically a pandas dataframe\n",
      "352/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "352/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "352/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "352/6: titanic.columns     # to know all the labes, features, columns\n",
      "352/7: titanic.info()\n",
      "352/8: titanic.describe()\n",
      "352/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "352/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "352/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "352/12:\n",
      "titanic.head()\n",
      "titanic[\"Boarded\"].unique()\n",
      "352/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "352/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "352/15: titanic.isnull().mean()*100\n",
      "352/16: titanic.info()\n",
      "352/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "352/18: titanic.isnull().sum()\n",
      "352/19: titanic.info()\n",
      "352/20: titanic.shape\n",
      "352/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "352/22: titanic[\"Age\"].std()\n",
      "352/23: titanic[\"Age_median\"].std()\n",
      "352/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "352/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "352/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "#titanic.isnull().sum()\n",
      "352/27: 891-177\n",
      "352/28: titanic[\"Age\"].std()\n",
      "352/29: titanic[\"Age_random\"].std()\n",
      "352/30: titanic[\"Age_median\"].std()\n",
      "352/31:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "352/32: titanic.isnull().sum()\n",
      "352/33:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "352/34: 2/891\n",
      "352/35: titanic[\"Cabin\"].unique()    # this show the unique values in a particular column.\n",
      "352/36:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "352/37:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "# imputing Catagorical values\n",
      "#  -> Frequency Imputation/Mode imputation.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0], inplace=True)\n",
      "352/38: titanic[\"Embarked\"].unique()\n",
      "352/39:\n",
      "# Number of person survived and number of person dinot survived.\n",
      "print(titanic['Survived'].value_counts())\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "352/40: sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "352/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)\n",
      "352/42: titanic[\"Pclass\"].unique()\n",
      "352/43: sns.countplot(x='Survived',hue=\"Pclass\", data = titanic)\n",
      "352/44:\n",
      "##plt.figure(figsize=(12,8))\n",
      "##sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "352/45: titanic.corr()\n",
      "352/46: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)\n",
      "352/47: #sns.pairplot(titanic, hue=\"Survived\", vars=[\"Pclass\",\"Age\"])\n",
      "352/48: titanic.head()\n",
      "352/49:\n",
      "# Handeling Categorical features\n",
      "\n",
      "# ---> One hot encoding\n",
      "#  ---> Ordinal number encoding\n",
      "#  ---> Label encoder\n",
      "352/50:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"],drop_first=True)\n",
      "Embarked1\n",
      "\n",
      "\n",
      "titanic = pd.concat([titanic,Embarked1],axis=1)\n",
      "352/51: Embarked1\n",
      "352/52: titanic.head()\n",
      "352/53:\n",
      "sex1 = pd.get_dummies(titanic[\"Sex\"],drop_first=True)\n",
      "\n",
      "titanic = pd.concat([titanic,sex1],axis=1)\n",
      "titanic.head()\n",
      "352/54: titanic.drop([\"Sex\",\"Embarked\"],axis=1,inplace=True)\n",
      "352/55: titanic.head()\n",
      "352/56:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_value = titanic[\"Survived\"]\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)\n",
      "\n",
      "\n",
      "x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)\n",
      "353/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "353/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                 # overhere titanic is basically a pandas dataframe\n",
      "353/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "353/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "353/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "353/6: titanic.columns     # to know all the labes, features, columns\n",
      "353/7: titanic.info()\n",
      "353/8: titanic.describe()\n",
      "353/9: titanic[\"Survived\"].isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "353/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "353/11: titanic.drop(\"Class\",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "353/12:\n",
      "titanic.head()\n",
      "titanic[\"Boarded\"].unique()\n",
      "353/13:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "# to drop multiple column at a time.\n",
      "353/14: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "353/15: titanic.isnull().mean()*100\n",
      "353/16: titanic.info()\n",
      "353/17: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "353/18: titanic.isnull().sum()\n",
      "353/19: titanic.info()\n",
      "353/20: titanic.shape\n",
      "353/21:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "353/22: titanic[\"Age\"].std()\n",
      "353/23: titanic[\"Age_median\"].std()\n",
      "353/24:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "353/25:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "353/26:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "#titanic.isnull().sum()\n",
      "353/27: 891-177\n",
      "353/28: titanic[\"Age\"].std()\n",
      "353/29: titanic[\"Age_random\"].std()\n",
      "353/30: titanic[\"Age_median\"].std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/31:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "353/32: titanic.isnull().sum()\n",
      "353/33:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "353/34: 2/891\n",
      "353/35: titanic[\"Cabin\"].unique()    # this show the unique values in a particular column.\n",
      "353/36:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "353/37:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "# imputing Catagorical values\n",
      "#  -> Frequency Imputation/Mode imputation.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0], inplace=True)\n",
      "353/38: titanic[\"Embarked\"].unique()\n",
      "353/39:\n",
      "# Number of person survived and number of person dinot survived.\n",
      "print(titanic['Survived'].value_counts())\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "353/40: sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "353/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)\n",
      "353/42: titanic[\"Pclass\"].unique()\n",
      "353/43: sns.countplot(x='Survived',hue=\"Pclass\", data = titanic)\n",
      "353/44:\n",
      "##plt.figure(figsize=(12,8))\n",
      "##sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "353/45: titanic.corr()\n",
      "353/46: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)\n",
      "353/47: #sns.pairplot(titanic, hue=\"Survived\", vars=[\"Pclass\",\"Age\"])\n",
      "353/48: titanic.head()\n",
      "353/49:\n",
      "# Handeling Categorical features\n",
      "\n",
      "# ---> One hot encoding\n",
      "#  ---> Ordinal number encoding\n",
      "#  ---> Label encoder\n",
      "353/50:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"],drop_first=True)\n",
      "Embarked1\n",
      "\n",
      "\n",
      "titanic = pd.concat([titanic,Embarked1],axis=1)\n",
      "353/51: Embarked1\n",
      "353/52: titanic.head()\n",
      "353/53:\n",
      "sex1 = pd.get_dummies(titanic[\"Sex\"],drop_first=True)\n",
      "\n",
      "titanic = pd.concat([titanic,sex1],axis=1)\n",
      "titanic.head()\n",
      "353/54: titanic.drop([\"Sex\",\"Embarked\"],axis=1,inplace=True)\n",
      "353/55: titanic.head()\n",
      "353/56:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_value = titanic[\"Survived\"]\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)\n",
      "\n",
      "\n",
      "x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)\n",
      "354/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "354/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                 # overhere titanic is basically a pandas dataframe\n",
      "354/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "354/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "354/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "354/6: titanic.columns     # to know all the labes, features, columns\n",
      "354/7: titanic.info()\n",
      "354/8: titanic.describe()\n",
      "354/9: titanic[\"Survived\"].isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "354/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "354/11: titanic.drop()   # to drop any column or you can say to remove any colummn.\n",
      "354/12: titanic.drop(\"Class\", axis=1)   # to drop any column or you can say to remove any colummn.\n",
      "354/13: titanic.head()\n",
      "354/14: titanic.drop(\"Class\", axis=1, inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "354/15: titanic.head()\n",
      "354/16: #titanic.head()\n",
      "354/17:\n",
      "#titanic.head()\n",
      "titanic[\"Boarded\"].unique()\n",
      "354/18:\n",
      "'''titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      " to drop multiple column at a time.'''\n",
      "354/19:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "#to drop multiple column at a time.\n",
      "titanic.head()\n",
      "354/20: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "354/21: titanic.isnull().mean()*100\n",
      "354/22: titanic.isnull().sum()\n",
      "354/23: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "354/24:\n",
      "#\n",
      "titanic.isnull().sum()\n",
      "354/25: titanic.isnull().sum()\n",
      "354/26:\n",
      "#\n",
      "titanic.info()\n",
      "354/27:\n",
      "#\n",
      "titanic.isnull().mean()*100\n",
      "355/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "355/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                 # overhere titanic is basically a pandas dataframe\n",
      "355/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "355/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "355/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "355/6: titanic.columns     # to know all the labes, features, columns\n",
      "355/7: titanic.info()\n",
      "355/8: titanic.describe()\n",
      "355/9: titanic[\"Survived\"].isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "355/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "355/11: titanic.drop(\"Class\", axis=1, inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "355/12: #titanic.head()\n",
      "355/13:\n",
      "#titanic.head()\n",
      "titanic[\"Boarded\"].unique()  # to see all the diffrent type of discrit or categorical values prasent inside a particular column.\n",
      "355/14:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "#to drop multiple column at a time.\n",
      "titanic.head()\n",
      "355/15: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "355/16: titanic.isnull().sum()\n",
      "355/17: #titanic.info()\n",
      "355/18: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "355/19:\n",
      "#\n",
      "titanic.isnull().mean()*100\n",
      "355/20:\n",
      "#\n",
      "titanic.info()\n",
      "355/21: #titanic.shape\n",
      "355/22:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "355/23: titanic[\"Age\"].std()\n",
      "355/24: titanic[\"Age_median\"].std()\n",
      "355/25:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "355/26:\n",
      "titanic[\"Age_random\"].dropna().sample()\n",
      "titanic[\"Age_random\"].isnull().sum()\n",
      "356/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "356/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                 # overhere titanic is basically a pandas dataframe\n",
      "356/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "356/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "356/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "356/6: titanic.columns     # to know all the labes, features, columns\n",
      "356/7: titanic.info()\n",
      "356/8: titanic.describe()\n",
      "356/9: titanic[\"Survived\"].isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "356/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "356/11: titanic.drop(\"Class\", axis=1, inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "356/12: #titanic.head()\n",
      "356/13:\n",
      "#titanic.head()\n",
      "titanic[\"Boarded\"].unique()  # to see all the diffrent type of discrit or categorical values prasent inside a particular column.\n",
      "356/14:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "#to drop multiple column at a time.\n",
      "titanic.head()\n",
      "356/15: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "356/16: titanic.isnull().sum()\n",
      "356/17: #titanic.info()\n",
      "356/18: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "356/19:\n",
      "#\n",
      "titanic.isnull().mean()*100\n",
      "356/20:\n",
      "#\n",
      "titanic.info()\n",
      "356/21: #titanic.shape\n",
      "356/22:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "356/23: titanic[\"Age\"].std()\n",
      "356/24: titanic[\"Age_median\"].std()\n",
      "356/25:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "356/26:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "356/27:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "titanic.isnull().sum()\n",
      "356/28: 891-177\n",
      "356/29: titanic[\"Age\"].std()\n",
      "356/30: titanic[\"Age_random\"].std()\n",
      "356/31: titanic[\"Age_median\"].std()\n",
      "356/32:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "356/33: #titanic.isnull().sum()\n",
      "356/34:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "356/35: 2/891\n",
      "356/36: titanic[\"Cabin\"].unique()    # this show the unique values in a particular column.\n",
      "356/37:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "356/38:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "# imputing Catagorical values\n",
      "#  -> Frequency Imputation/Mode imputation.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0], inplace=True)\n",
      "356/39: titanic[\"Embarked\"].unique()\n",
      "356/40:\n",
      "# Number of person survived and number of person dinot survived.\n",
      "print(titanic['Survived'].value_counts())\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "356/41: sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "356/42: sns.countplot(x='Survived', hue='Embarked', data=titanic)\n",
      "356/43: titanic[\"Pclass\"].unique()\n",
      "356/44: sns.countplot(x='Survived',hue=\"Pclass\", data = titanic)\n",
      "356/45:\n",
      "plt.figure(figsize=(12,8))\n",
      "sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "356/46: titanic.corr()\n",
      "356/47: sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)\n",
      "356/48: sns.pairplot(titanic, hue=\"Survived\", vars=[\"Pclass\",\"Age\"])\n",
      "356/49: titanic.head()\n",
      "356/50:\n",
      "# Handeling Categorical features\n",
      "\n",
      "# ---> One hot encoding\n",
      "#  ---> Ordinal number encoding\n",
      "#  ---> Label encoder\n",
      "356/51:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"],drop_first=True)\n",
      "Embarked1\n",
      "\n",
      "\n",
      "#titanic = pd.concat([titanic,Embarked1],axis=1)\n",
      "356/52: Embarked1\n",
      "356/53: titanic.head()\n",
      "356/54:\n",
      "sex1 = pd.get_dummies(titanic[\"Sex\"],drop_first=True)\n",
      "\n",
      "titanic = pd.concat([titanic,sex1],axis=1)\n",
      "titanic.head()\n",
      "356/55: titanic.drop([\"Sex\",\"Embarked\"],axis=1,inplace=True)\n",
      "356/56: titanic.head()\n",
      "356/57:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_value = titanic[\"Survived\"]\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)\n",
      "\n",
      "\n",
      "x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)\n",
      "358/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "358/2:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                 # overhere titanic is basically a pandas dataframe\n",
      "358/3:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "358/4: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "358/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "358/6: titanic.columns     # to know all the labes, features, columns\n",
      "358/7: titanic.info()\n",
      "358/8: titanic.describe()\n",
      "358/9: titanic[\"Survived\"].isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "358/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "358/11: titanic.drop(\"Class\", axis=1, inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "358/12: #titanic.head()\n",
      "358/13:\n",
      "#titanic.head()\n",
      "titanic[\"Boarded\"].unique()  # to see all the diffrent type of discrit or categorical values prasent inside a particular column.\n",
      "358/14:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "#to drop multiple column at a time.\n",
      "titanic.head()\n",
      "358/15: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "358/16: titanic.isnull().sum()\n",
      "358/17: #titanic.info()\n",
      "358/18: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "358/19:\n",
      "#\n",
      "titanic.isnull().mean()*100\n",
      "358/20:\n",
      "#\n",
      "titanic.info()\n",
      "358/21: #titanic.shape\n",
      "358/22:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "358/23: titanic[\"Age\"].std()\n",
      "358/24: titanic[\"Age_median\"].std()\n",
      "358/25:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "358/26:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "358/27:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "titanic.isnull().sum()\n",
      "358/28: 891-177\n",
      "358/29: titanic[\"Age\"].std()\n",
      "358/30: titanic[\"Age_random\"].std()\n",
      "358/31: titanic[\"Age_median\"].std()\n",
      "358/32:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "358/33: #titanic.isnull().sum()\n",
      "358/34:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "358/35: 2/891\n",
      "358/36: titanic[\"Cabin\"].unique()    # this show the unique values in a particular column.\n",
      "358/37:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "358/38:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "# imputing Catagorical values\n",
      "#  -> Frequency Imputation/Mode imputation.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0], inplace=True)\n",
      "358/39: titanic[\"Embarked\"].unique()\n",
      "358/40:\n",
      "# Number of person survived and number of person dinot survived.\n",
      "print(titanic['Survived'].value_counts())\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "358/41: sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "358/42: sns.countplot(x='Survived', hue='Embarked', data=titanic)\n",
      "358/43: titanic[\"Pclass\"].unique()\n",
      "358/44: sns.countplot(x='Survived',hue=\"Pclass\", data = titanic)\n",
      "358/45:\n",
      "plt.figure(figsize=(12,8))\n",
      "sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "358/46: titanic.corr()\n",
      "358/47: sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)\n",
      "358/48: sns.pairplot(titanic, hue=\"Survived\", vars=[\"Pclass\",\"Age\"])\n",
      "358/49: titanic.head()\n",
      "358/50:\n",
      "# Handeling Categorical features\n",
      "\n",
      "# ---> One hot encoding\n",
      "#  ---> Ordinal number encoding\n",
      "#  ---> Label encoder\n",
      "358/51:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"],drop_first=True)\n",
      "Embarked1\n",
      "\n",
      "\n",
      "#titanic = pd.concat([titanic,Embarked1],axis=1)\n",
      "358/52: Embarked1\n",
      "358/53: titanic.head()\n",
      "358/54:\n",
      "sex1 = pd.get_dummies(titanic[\"Sex\"],drop_first=True)\n",
      "\n",
      "titanic = pd.concat([titanic,sex1,Embarked1],axis=1)\n",
      "titanic.head()\n",
      "358/55: titanic.drop([\"Sex\",\"Embarked\"],axis=1,inplace=True)\n",
      "358/56: titanic.head()\n",
      "358/57:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_value = titanic[\"Survived\"]\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)\n",
      "\n",
      "\n",
      "x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)\n",
      "358/58: titanic.isnull().sum()\n",
      "358/59: titanic.head()\n",
      "358/60:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "log_model = LogisticRegression()\n",
      "log_model.fit_transform(x_train,y_train)\n",
      "y_predicted = log_model.predict(x_test)\n",
      "358/61:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "log_model = LogisticRegression()\n",
      "log_model.fit(x_train,y_train)\n",
      "y_predicted = log_model.predict(x_test)\n",
      "358/62:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_value = titanic[\"Survived\"] # depandent variable or to be predicted variable    # 80:20 or 70:30\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)  # indepandent variable\n",
      "\n",
      "\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)\n",
      "358/63:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "log_model = LogisticRegression()\n",
      "log_model.fit(x_train,y_train)\n",
      "y_predicted = log_model.predict(x_test)\n",
      "358/64:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import warning\n",
      "358/65:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import warnings\n",
      "358/66: warning.filterwarnings(\"ignore\")\n",
      "358/67: warnings.filterwarnings(\"ignore\")\n",
      "358/68:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "log_model = LogisticRegression()\n",
      "log_model.fit(x_train,y_train)\n",
      "y_predicted = log_model.predict(x_test)\n",
      "358/69:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "log_model = LogisticRegression()\n",
      "log_model.fit(x_train,y_train)\n",
      "y_predicted = log_model.predict(x_test)\n",
      "y_predict\n",
      "358/70:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "log_model = LogisticRegression()\n",
      "log_model.fit(x_train,y_train)\n",
      "y_predicted = log_model.predict(x_test)\n",
      "y_predicted\n",
      "358/71:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "log_model = LogisticRegression()\n",
      "log_model.fit(x_train,y_train)\n",
      "y_predicted = log_model.predict(x_test)\n",
      "y_predicted\n",
      "358/72:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "log_model = LogisticRegression()\n",
      "log_model.fit(x_train,y_train)\n",
      "y_predicted = log_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(y_predicted,y_test))\n",
      "358/73:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "\n",
      "log_model = LogisticRegression()\n",
      "log_model.fit(x_train,y_train)\n",
      "y_predicted = log_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(y_predicted,y_test))\n",
      "print(confusion_matrix)\n",
      "358/74:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "\n",
      "log_model = LogisticRegression()\n",
      "log_model.fit(x_train,y_train)\n",
      "y_predicted = log_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(y_predicted,y_test))\n",
      "print(confusion_matrix(y_predicted,y_test))\n",
      "358/75:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "\n",
      "log_model = LogisticRegression()\n",
      "log_model.fit(x_train,y_train)\n",
      "y_predicted = log_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(y_predicted,y_test))*100\n",
      "print(confusion_matrix(y_predicted,y_test))\n",
      "358/76:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "\n",
      "log_model = LogisticRegression()\n",
      "log_model.fit(x_train,y_train)\n",
      "y_predicted = log_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(y_predicted,y_test)*100)\n",
      "print(confusion_matrix(y_predicted,y_test))\n",
      "358/77: (89+54)/(89+20+16+54)\n",
      "358/78: ((89+54)/(89+20+16+54))*100\n",
      "358/79:\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "dt_model = DecisionTreeClassifier()\n",
      "dt_model.fit(x_train,y_train)\n",
      "dt_y_predicted = dt_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(dt_y_predicted,y_test)*100)\n",
      "print(confusion_matrix(dt_y_predicted,y_test))\n",
      "358/80:\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "dt_model = DecisionTreeClassifier()\n",
      "dt_model.fit(x_train,y_train)\n",
      "dt_y_predicted = dt_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(dt_y_predicted,y_test)*100)\n",
      "print(confusion_matrix(dt_y_predicted,y_test))\n",
      "358/81:\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "dt_model = DecisionTreeClassifier()\n",
      "dt_model.fit(x_train,y_train)\n",
      "dt_y_predicted = dt_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(dt_y_predicted,y_test)*100)\n",
      "print(confusion_matrix(dt_y_predicted,y_test))\n",
      "358/82:\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "dt_model = DecisionTreeClassifier()\n",
      "dt_model.fit(x_train,y_train)\n",
      "dt_y_predicted = dt_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(dt_y_predicted,y_test)*100)\n",
      "print(confusion_matrix(dt_y_predicted,y_test))\n",
      "358/83:\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "dt_model = DecisionTreeClassifier()\n",
      "dt_model.fit(x_train,y_train)\n",
      "dt_y_predicted = dt_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(dt_y_predicted,y_test)*100)\n",
      "print(confusion_matrix(dt_y_predicted,y_test))\n",
      "358/84:\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_starte = 42)\n",
      "dt_model.fit(x_train,y_train)\n",
      "dt_y_predicted = dt_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(dt_y_predicted,y_test)*100)\n",
      "print(confusion_matrix(dt_y_predicted,y_test))\n",
      "358/85:\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state=42)\n",
      "dt_model.fit(x_train,y_train)\n",
      "dt_y_predicted = dt_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(dt_y_predicted,y_test)*100)\n",
      "print(confusion_matrix(dt_y_predicted,y_test))\n",
      "360/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "360/2: warnings.filterwarnings(\"ignore\")\n",
      "360/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "360/4: data.head()\n",
      "360/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "360/6:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "360/7: data['Insulin'].replace(0, np.nan, inplace=True)\n",
      "360/8: data.isnull().sum()\n",
      "360/9: data.head()\n",
      "360/10: data[\"Insulin\"].fillna(value=data[\"Insulin\"].median(), inplace=True)\n",
      "360/11: data.info()\n",
      "360/12: data.head()\n",
      "360/13: data.describe()\n",
      "360/14:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "360/15:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "360/16: #sns.pairplot(data, hue=\"Outcome\")\n",
      "360/17: data.corr()\n",
      "360/18:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "360/19:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "360/20:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "360/21:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "x_data\n",
      "360/22:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "360/23:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "360/24:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "360/25:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "360/26:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "360/27:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "360/28:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "360/29:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Hyperparameter optimization    \n",
      "    \n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "360/30:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "360/31:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "360/32: rf_random.fit(std_x_train,y_train)\n",
      "360/33: rf_random.best_params_\n",
      "360/34: predictions=rf_random.predict(std_x_test)\n",
      "360/35: accuracy_score(y_test,predictions)\n",
      "360/36:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "360/37:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "360/38:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "360/39:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "358/86:\n",
      "# randomize search cv\n",
      "\n",
      "criterion=['gini','entropy']\n",
      "splitter=['best', 'random']\n",
      "max_depth=[1,3,5,None]\n",
      "min_samples_split=[2, 5, 10, 15, 100]\n",
      "min_samples_leaf=[1, 2, 5, 10]\n",
      "random_state=42 \n",
      "    \n",
      "    \n",
      "    \n",
      "random_grid = {'criterion': criterion,\n",
      "               'splitter': splitter,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf,\n",
      "                'random_state'=42 }\n",
      "\n",
      "print(random_grid)\n",
      "358/87:\n",
      "# randomize search cv\n",
      "\n",
      "criterion=['gini','entropy']\n",
      "splitter=['best', 'random']\n",
      "max_depth=[1,3,5,None]\n",
      "min_samples_split=[2, 5, 10, 15, 100]\n",
      "min_samples_leaf=[1, 2, 5, 10]\n",
      "random_state=42 \n",
      "    \n",
      "    \n",
      "    \n",
      "random_grid = {'criterion': criterion,\n",
      "               'splitter': splitter,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf,\n",
      "                'random_state'= 42 }\n",
      "\n",
      "print(random_grid)\n",
      "358/88:\n",
      "# randomize search cv\n",
      "\n",
      "criterion=['gini','entropy']\n",
      "splitter=['best', 'random']\n",
      "max_depth=[1,3,5,None]\n",
      "min_samples_split=[2, 5, 10, 15, 100]\n",
      "min_samples_leaf=[1, 2, 5, 10]\n",
      "random_state=42 \n",
      "    \n",
      "    \n",
      "    \n",
      "random_grid = {'criterion': criterion,\n",
      "               'splitter': splitter,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf,\n",
      "                'random_state': 42 }\n",
      "\n",
      "print(random_grid)\n",
      "358/89:\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "\n",
      "tree = DecisionTreeClassifier()\n",
      "\n",
      "# Instantiate the RandomizedSearchCV object: tree_cv\n",
      "#tree_cv = RandomizedSearchCV(tree, param_dist, cv=5,  n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "\n",
      "# Fit it to the data\n",
      "358/90:\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "\n",
      "tree = DecisionTreeClassifier()\n",
      "\n",
      "# Instantiate the RandomizedSearchCV object: tree_cv\n",
      "tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=random_grid, cv=5,  n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "\n",
      "# Fit it to the data\n",
      "358/91:\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "\n",
      "tree = DecisionTreeClassifier()\n",
      "\n",
      "# Instantiate the RandomizedSearchCV object: tree_cv\n",
      "tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=random_grid,n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "\n",
      "# Fit it to the data\n",
      "358/92: tree_cv.fit(x_train,y_train)\n",
      "358/93:\n",
      "# randomize search cv\n",
      "\n",
      "criterion=['gini','entropy']\n",
      "splitter=['best', 'random']\n",
      "max_depth=[1,3,5,None]\n",
      "min_samples_split=[2, 5, 10, 15, 100]\n",
      "min_samples_leaf=[1, 2, 5, 10]\n",
      "random_state=42 \n",
      "    \n",
      "    \n",
      "    \n",
      "random_grid = {'criterion': criterion,\n",
      "               'splitter': splitter,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf,\n",
      "                }\n",
      "\n",
      "print(random_grid)\n",
      "358/94:\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "\n",
      "tree = DecisionTreeClassifier(random_state=42)\n",
      "\n",
      "# Instantiate the RandomizedSearchCV object: tree_cv\n",
      "tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=random_grid,n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "\n",
      "# Fit it to the data\n",
      "358/95: tree_cv.fit(x_train,y_train)\n",
      "358/96: tree_cv.best_params_\n",
      "358/97: predictions=tree_cv.predict(x_test)\n",
      "358/98: accuracy_score(y_test,predictions)\n",
      "358/99:\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "\n",
      "tree = DecisionTreeClassifier()\n",
      "\n",
      "# Instantiate the RandomizedSearchCV object: tree_cv\n",
      "tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=random_grid,n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "\n",
      "# Fit it to the data\n",
      "358/100: tree_cv.fit(x_train,y_train)\n",
      "358/101: tree_cv.best_params_\n",
      "358/102: predictions=tree_cv.predict(x_test)\n",
      "358/103: accuracy_score(y_test,predictions)\n",
      "361/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import warnings\n",
      "361/2: warnings.filterwarnings(\"ignore\")\n",
      "361/3:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                 # overhere titanic is basically a pandas dataframe\n",
      "361/4:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "361/5: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "361/6: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "361/7: titanic.columns     # to know all the labes, features, columns\n",
      "361/8: titanic.info()\n",
      "361/9: titanic.describe()\n",
      "361/10: titanic[\"Survived\"].isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "361/11: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "361/12: titanic.drop(\"Class\", axis=1, inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "361/13: #titanic.head()\n",
      "361/14:\n",
      "#titanic.head()\n",
      "titanic[\"Boarded\"].unique()  # to see all the diffrent type of discrit or categorical values prasent inside a particular column.\n",
      "361/15:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "#to drop multiple column at a time.\n",
      "titanic.head()\n",
      "361/16: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "361/17: titanic.isnull().sum()\n",
      "361/18: #titanic.info()\n",
      "361/19: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "361/20:\n",
      "#\n",
      "titanic.isnull().mean()*100\n",
      "361/21:\n",
      "#\n",
      "titanic.info()\n",
      "361/22: #titanic.shape\n",
      "361/23:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "361/24: titanic[\"Age\"].std()\n",
      "361/25: titanic[\"Age_median\"].std()\n",
      "361/26:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "361/27:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "361/28:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "titanic.isnull().sum()\n",
      "361/29: 891-177\n",
      "361/30: titanic[\"Age\"].std()\n",
      "361/31: titanic[\"Age_random\"].std()\n",
      "361/32: titanic[\"Age_median\"].std()\n",
      "361/33:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "361/34: #titanic.isnull().sum()\n",
      "361/35:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "361/36: 2/891\n",
      "361/37: titanic[\"Cabin\"].unique()    # this show the unique values in a particular column.\n",
      "361/38:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "361/39:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "# imputing Catagorical values\n",
      "#  -> Frequency Imputation/Mode imputation.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0], inplace=True)\n",
      "361/40: titanic[\"Embarked\"].unique()\n",
      "361/41:\n",
      "# Number of person survived and number of person dinot survived.\n",
      "print(titanic['Survived'].value_counts())\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "361/42: sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "361/43: sns.countplot(x='Survived', hue='Embarked', data=titanic)\n",
      "361/44: titanic[\"Pclass\"].unique()\n",
      "361/45: sns.countplot(x='Survived',hue=\"Pclass\", data = titanic)\n",
      "361/46:\n",
      "plt.figure(figsize=(12,8))\n",
      "sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "361/47: titanic.corr()\n",
      "361/48: sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)\n",
      "361/49: sns.pairplot(titanic, hue=\"Survived\", vars=[\"Pclass\",\"Age\"])\n",
      "361/50: titanic.head()\n",
      "361/51:\n",
      "# Handeling Categorical features\n",
      "\n",
      "# ---> One hot encoding\n",
      "#  ---> Ordinal number encoding\n",
      "#  ---> Label encoder\n",
      "361/52:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"],drop_first=True)\n",
      "Embarked1\n",
      "\n",
      "\n",
      "#titanic = pd.concat([titanic,Embarked1],axis=1)\n",
      "361/53: Embarked1\n",
      "361/54: titanic.head()\n",
      "361/55:\n",
      "sex1 = pd.get_dummies(titanic[\"Sex\"],drop_first=True)\n",
      "\n",
      "titanic = pd.concat([titanic,sex1,Embarked1],axis=1)\n",
      "titanic.head()\n",
      "361/56: titanic.drop([\"Sex\",\"Embarked\"],axis=1,inplace=True)\n",
      "361/57: titanic.head()\n",
      "361/58:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "y_value = titanic[\"Survived\"] # depandent variable or to be predicted variable    # 80:20 or 70:30\n",
      "x_value = titanic.drop([\"Survived\"],axis=1)  # indepandent variable\n",
      "\n",
      "\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)\n",
      "361/59:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "\n",
      "log_model = LogisticRegression()\n",
      "log_model.fit(x_train,y_train)\n",
      "y_predicted = log_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(y_predicted,y_test)*100)\n",
      "print(confusion_matrix(y_predicted,y_test))\n",
      "361/60: ((89+54)/(89+20+16+54))*100\n",
      "361/61:\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state=42)\n",
      "dt_model.fit(x_train,y_train)\n",
      "dt_y_predicted = dt_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(dt_y_predicted,y_test)*100)\n",
      "print(confusion_matrix(dt_y_predicted,y_test))\n",
      "361/62:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "rf_model = RandomForestClassifier(random_state=42)\n",
      "rf_model.fit(x_train,y_train)\n",
      "rf_y_predicted = rf_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(rf_y_predicted,y_test)*100)\n",
      "print(confusion_matrix(rf_y_predicted,y_test))\n",
      "361/63:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100,random_state=42)\n",
      "rf_model.fit(x_train,y_train)\n",
      "rf_y_predicted = rf_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(rf_y_predicted,y_test)*100)\n",
      "print(confusion_matrix(rf_y_predicted,y_test))\n",
      "361/64:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=200,random_state=42)\n",
      "rf_model.fit(x_train,y_train)\n",
      "rf_y_predicted = rf_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(rf_y_predicted,y_test)*100)\n",
      "print(confusion_matrix(rf_y_predicted,y_test))\n",
      "361/65:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=500,random_state=42)\n",
      "rf_model.fit(x_train,y_train)\n",
      "rf_y_predicted = rf_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(rf_y_predicted,y_test)*100)\n",
      "print(confusion_matrix(rf_y_predicted,y_test))\n",
      "361/66:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=1000,random_state=42)\n",
      "rf_model.fit(x_train,y_train)\n",
      "rf_y_predicted = rf_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(rf_y_predicted,y_test)*100)\n",
      "print(confusion_matrix(rf_y_predicted,y_test))\n",
      "361/67:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100,random_state=42)\n",
      "rf_model.fit(x_train,y_train)\n",
      "rf_y_predicted = rf_model.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(rf_y_predicted,y_test)*100)\n",
      "print(confusion_matrix(rf_y_predicted,y_test))\n",
      "361/68:\n",
      "# Randomized searched cv\n",
      "\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "criterion=['gini', 'entropy']\n",
      "splitter=['best','random']\n",
      "max_depth=[2,5,6,7,8,9,None]\n",
      "min_samples_split=[2,4,6,8]\n",
      "min_samples_leaf=[1,3,5,7]\n",
      "\n",
      "\n",
      "dt_random_grid = {\n",
      "    'criterion':criterion,\n",
      "    'splitter':splitter\n",
      "    'max_depth':max_depth\n",
      "    'min_samples_split':min_samples_split\n",
      "    'min_samples_leaf':min_samples_leaf\n",
      "    \n",
      "}\n",
      "\n",
      "print(dt_random_grid)\n",
      "361/69:\n",
      "# Randomized searched cv\n",
      "\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "criterion=['gini', 'entropy']\n",
      "splitter=['best','random']\n",
      "max_depth=[2,5,6,7,8,9,None]\n",
      "min_samples_split=[2,4,6,8]\n",
      "min_samples_leaf=[1,3,5,7]\n",
      "\n",
      "\n",
      "dt_random_grid = {\n",
      "    'criterion':criterion,\n",
      "    'splitter':splitter,\n",
      "    'max_depth':max_depth,\n",
      "    'min_samples_split':min_samples_split,\n",
      "    'min_samples_leaf':min_samples_leaf,\n",
      "    \n",
      "}\n",
      "\n",
      "print(dt_random_grid)\n",
      "361/70:\n",
      "tree = DecisionTreeClassifier()\n",
      "\n",
      "tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=dt_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)\n",
      "361/71: tree_cv.fit(x_train,y_train)\n",
      "361/72: tree_cv.best_params_\n",
      "361/73:\n",
      "prediction = tree_cv.predict(x_test)\n",
      "print(\"Accuracy \",accuracy_score(y_test,prediction))\n",
      "361/74:\n",
      "prediction = tree_cv.predict(x_test)\n",
      "print(\"Accuracy \",accuracy_score(y_test,prediction)*100)\n",
      "361/75:\n",
      "# Randomized searched cv\n",
      "\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "criterion=['gini', 'entropy']\n",
      "splitter=['best','random']\n",
      "max_depth=[2,5,6,8,9,None]\n",
      "min_samples_split=[2,4,6,8,10,12]\n",
      "min_samples_leaf=[1,3,5,7,8,9,10]\n",
      "\n",
      "\n",
      "dt_random_grid = {\n",
      "    'criterion':criterion,\n",
      "    'splitter':splitter,\n",
      "    'max_depth':max_depth,\n",
      "    'min_samples_split':min_samples_split,\n",
      "    'min_samples_leaf':min_samples_leaf,\n",
      "    \n",
      "}\n",
      "\n",
      "print(dt_random_grid)\n",
      "361/76:\n",
      "tree = DecisionTreeClassifier()\n",
      "\n",
      "tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=dt_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)\n",
      "361/77: tree_cv.fit(x_train,y_train)\n",
      "361/78: tree_cv.best_params_\n",
      "361/79:\n",
      "prediction = tree_cv.predict(x_test)\n",
      "print(\"Accuracy \",accuracy_score(y_test,prediction)*100)\n",
      "361/80:\n",
      "# Randomized searched cv\n",
      "\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "criterion=['gini', 'entropy']\n",
      "splitter=['best','random']\n",
      "max_depth=[1,3,5,None]\n",
      "min_samples_split=[2,4,5,10,15,100]\n",
      "min_samples_leaf=[1,3,5,7,8,9,10]\n",
      "\n",
      "\n",
      "dt_random_grid = {\n",
      "    'criterion':criterion,\n",
      "    'splitter':splitter,\n",
      "    'max_depth':max_depth,\n",
      "    'min_samples_split':min_samples_split,\n",
      "    'min_samples_leaf':min_samples_leaf,\n",
      "    \n",
      "}\n",
      "\n",
      "print(dt_random_grid)\n",
      "361/81:\n",
      "tree = DecisionTreeClassifier()\n",
      "\n",
      "tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=dt_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)\n",
      "361/82: tree_cv.fit(x_train,y_train)\n",
      "361/83: tree_cv.best_params_\n",
      "361/84:\n",
      "prediction = tree_cv.predict(x_test)\n",
      "print(\"Accuracy \",accuracy_score(y_test,prediction)*100)\n",
      "361/85:\n",
      "# Randomized searched cv\n",
      "\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "criterion=['gini', 'entropy']\n",
      "splitter=['best','random']\n",
      "max_depth=[1,3,5,None]\n",
      "min_samples_split=[2,4,5,10,15,100]\n",
      "min_samples_leaf=[1,2,5,10]\n",
      "\n",
      "\n",
      "dt_random_grid = {\n",
      "    'criterion':criterion,\n",
      "    'splitter':splitter,\n",
      "    'max_depth':max_depth,\n",
      "    'min_samples_split':min_samples_split,\n",
      "    'min_samples_leaf':min_samples_leaf,\n",
      "    \n",
      "}\n",
      "\n",
      "print(dt_random_grid)\n",
      "361/86:\n",
      "tree = DecisionTreeClassifier()\n",
      "\n",
      "tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=dt_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)\n",
      "361/87: tree_cv.fit(x_train,y_train)\n",
      "361/88: tree_cv.best_params_\n",
      "361/89:\n",
      "prediction = tree_cv.predict(x_test)\n",
      "print(\"Accuracy \",accuracy_score(y_test,prediction)*100)\n",
      "361/90:\n",
      "# Randomized searched cv\n",
      "\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "criterion=['gini', 'entropy']\n",
      "splitter=['best','random']\n",
      "max_depth=[1,3,5,None]\n",
      "min_samples_split=[2,5,10,15,100]\n",
      "min_samples_leaf=[1,2,5,10]\n",
      "\n",
      "\n",
      "dt_random_grid = {\n",
      "    'criterion':criterion,\n",
      "    'splitter':splitter,\n",
      "    'max_depth':max_depth,\n",
      "    'min_samples_split':min_samples_split,\n",
      "    'min_samples_leaf':min_samples_leaf,\n",
      "    \n",
      "}\n",
      "\n",
      "print(dt_random_grid)\n",
      "361/91:\n",
      "tree = DecisionTreeClassifier()\n",
      "\n",
      "tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=dt_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)\n",
      "361/92: tree_cv.fit(x_train,y_train)\n",
      "361/93: tree_cv.best_params_\n",
      "361/94:\n",
      "prediction = tree_cv.predict(x_test)\n",
      "print(\"Accuracy \",accuracy_score(y_test,prediction)*100)\n",
      "361/95:\n",
      "# Randomized searched cv\n",
      "\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "criterion=['gini', 'entropy']\n",
      "splitter=['best','random']\n",
      "max_depth=[1,3,5,None]\n",
      "min_samples_split=[2,5,10,15,100,1000]\n",
      "min_samples_leaf=[1,2,5,10,15]\n",
      "\n",
      "\n",
      "dt_random_grid = {\n",
      "    'criterion':criterion,\n",
      "    'splitter':splitter,\n",
      "    'max_depth':max_depth,\n",
      "    'min_samples_split':min_samples_split,\n",
      "    'min_samples_leaf':min_samples_leaf,\n",
      "    \n",
      "}\n",
      "\n",
      "print(dt_random_grid)\n",
      "361/96:\n",
      "tree = DecisionTreeClassifier()\n",
      "\n",
      "tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=dt_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)\n",
      "361/97: tree_cv.fit(x_train,y_train)\n",
      "361/98: tree_cv.best_params_\n",
      "361/99:\n",
      "prediction = tree_cv.predict(x_test)\n",
      "print(\"Accuracy \",accuracy_score(y_test,prediction)*100)\n",
      "361/100: li = for x in np.linespace(start=100,stop=2000,num=15)\n",
      "361/101: li = [int(x) for x in np.linespace(start=100,stop=2000,num=15)]\n",
      "361/102: li = [int(x) for x in np.linspace(start=100,stop=2000,num=15)]\n",
      "361/103: li\n",
      "361/104: 371-235\n",
      "361/105: 507-371\n",
      "361/106:\n",
      "n_estimators=[int(x) for x in np.linspace(start=100,stop=2000,num=15)]\n",
      "criterion=['gini','entropy']\n",
      "max_depth=[int(x) for x in np.linspace(start=2,stop=30,num=10)]\n",
      "min_samples_split=[2,5,10,15,100,1000]\n",
      "min_samples_leaf=[1,2,5,10,15]\n",
      "max_features=['auto','sqrt','log2']\n",
      "\n",
      "\n",
      "rf_random_grid = {\n",
      "    'n_estimators':n_estimators,\n",
      "    'criterion':criterion,\n",
      "    'splitter':splitter,\n",
      "    'max_depth':max_depth,\n",
      "    'min_samples_split':min_samples_split,\n",
      "    'min_samples_leaf':min_samples_leaf,\n",
      "    'max_features':max_features\n",
      "}\n",
      "\n",
      "print(rf_random_grid)\n",
      "361/107:\n",
      "rf_trees = RandomForestClassifier()\n",
      "\n",
      "tree_cv = RandomizedSearchCV(estimator=rf_trees, param_distributions=rf_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)\n",
      "361/108: rf_trees.fit(x_train,y_train)\n",
      "361/109: rf_trees_cv.fit(x_train,y_train)\n",
      "361/110:\n",
      "rf_trees = RandomForestClassifier()\n",
      "\n",
      "rf_tree_cv = RandomizedSearchCV(estimator=rf_trees, param_distributions=rf_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)\n",
      "361/111: rf_trees_cv.fit(x_train,y_train)\n",
      "361/112: rf_tree_cv.fit(x_train,y_train)\n",
      "361/113:\n",
      "n_estimators=[int(x) for x in np.linspace(start=100,stop=2000,num=15)]\n",
      "criterion=['gini','entropy']\n",
      "max_depth=[int(x) for x in np.linspace(start=2,stop=30,num=10)]\n",
      "min_samples_split=[2,5,10,15,100,1000]\n",
      "min_samples_leaf=[1,2,5,10,15]\n",
      "max_features=['auto','sqrt','log2']\n",
      "\n",
      "\n",
      "rf_random_grid = {\n",
      "    'n_estimators':n_estimators,\n",
      "    'criterion':criterion,\n",
      "    'max_depth':max_depth,\n",
      "    'min_samples_split':min_samples_split,\n",
      "    'min_samples_leaf':min_samples_leaf,\n",
      "    'max_features':max_features\n",
      "}\n",
      "\n",
      "print(rf_random_grid)\n",
      "361/114:\n",
      "rf_trees = RandomForestClassifier()\n",
      "\n",
      "rf_tree_cv = RandomizedSearchCV(estimator=rf_trees, param_distributions=rf_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)\n",
      "361/115: rf_tree_cv.fit(x_train,y_train)\n",
      "361/116: rf_tree_cv.best_params_\n",
      "361/117:\n",
      "rf_prediction = rf_tree_cv.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(rf_prediction,y_test)*100)\n",
      "361/118:\n",
      "n_estimators=[int(x) for x in np.linspace(start=100,stop=2000,num=20)]\n",
      "criterion=['gini','entropy']\n",
      "max_depth=[int(x) for x in np.linspace(start=2,stop=30,num=10)]\n",
      "min_samples_split=[2,5,10,15,100,1000]\n",
      "min_samples_leaf=[1,2,5,10,15]\n",
      "max_features=['auto','sqrt','log2']\n",
      "\n",
      "\n",
      "rf_random_grid = {\n",
      "    'n_estimators':n_estimators,\n",
      "    'criterion':criterion,\n",
      "    'max_depth':max_depth,\n",
      "    'min_samples_split':min_samples_split,\n",
      "    'min_samples_leaf':min_samples_leaf,\n",
      "    'max_features':max_features\n",
      "}\n",
      "\n",
      "print(rf_random_grid)\n",
      "361/119:\n",
      "rf_trees = RandomForestClassifier()\n",
      "\n",
      "rf_tree_cv = RandomizedSearchCV(estimator=rf_trees, param_distributions=rf_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)\n",
      "361/120: rf_tree_cv.fit(x_train,y_train)\n",
      "361/121: rf_tree_cv.best_params_\n",
      "361/122:\n",
      "rf_prediction = rf_tree_cv.predict(x_test)\n",
      "print(\"Accuracy = \",accuracy_score(rf_prediction,y_test)*100)\n",
      "364/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import warnings\n",
      "364/2: warnings.filterwarnings(\"ignore\")\n",
      "364/3:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                 # overhere titanic is basically a pandas dataframe\n",
      "364/4:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "364/5: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "364/6: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "364/7: titanic.columns     # to know all the labes, features, columns\n",
      "364/8: titanic.info()\n",
      "364/9: titanic.describe()\n",
      "364/10: titanic[\"Survived\"].isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "364/11: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "364/12: titanic.drop(\"Class\", axis=1, inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "364/13: #titanic.head()\n",
      "364/14:\n",
      "#titanic.head()\n",
      "titanic[\"Boarded\"].unique()  # to see all the diffrent type of discrit or categorical values prasent inside a particular column.\n",
      "364/15:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "#to drop multiple column at a time.\n",
      "titanic.head()\n",
      "364/16: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "364/17: titanic.isnull().sum()\n",
      "364/18: #titanic.info()\n",
      "364/19: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "364/20:\n",
      "#\n",
      "titanic.isnull().mean()*100\n",
      "364/21:\n",
      "#\n",
      "titanic.info()\n",
      "364/22: #titanic.shape\n",
      "364/23:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "364/24: titanic[\"Age\"].std()\n",
      "364/25: titanic[\"Age_median\"].std()\n",
      "364/26:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "364/27:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "364/28:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "titanic.isnull().sum()\n",
      "364/29: 891-177\n",
      "364/30: titanic[\"Age\"].std()\n",
      "364/31: titanic[\"Age_random\"].std()\n",
      "364/32: titanic[\"Age_median\"].std()\n",
      "364/33:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "364/34: #titanic.isnull().sum()\n",
      "364/35:\n",
      "import seaborn as sns\n",
      "sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "364/36: 2/891\n",
      "364/37: titanic[\"Cabin\"].unique()    # this show the unique values in a particular column.\n",
      "364/38:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "364/39:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "# imputing Catagorical values\n",
      "#  -> Frequency Imputation/Mode imputation.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0], inplace=True)\n",
      "364/40: titanic[\"Embarked\"].unique()\n",
      "364/41:\n",
      "# Number of person survived and number of person dinot survived.\n",
      "print(titanic['Survived'].value_counts())\n",
      "sns.countplot(x='Survived',data=titanic)\n",
      "364/42: sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "364/43: sns.countplot(x='Survived', hue='Embarked', data=titanic)\n",
      "364/44: titanic[\"Pclass\"].unique()\n",
      "364/45: sns.countplot(x='Survived',hue=\"Pclass\", data = titanic)\n",
      "364/46:\n",
      "plt.figure(figsize=(12,8))\n",
      "sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "364/47: titanic.corr()\n",
      "364/48: sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)\n",
      "364/49: sns.pairplot(titanic, hue=\"Survived\", vars=[\"Pclass\",\"Age\"])\n",
      "364/50: titanic.head()\n",
      "364/51:\n",
      "# Handeling Categorical features\n",
      "\n",
      "# ---> One hot encoding\n",
      "#  ---> Ordinal number encoding\n",
      "#  ---> Label encoder\n",
      "364/52:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"],drop_first=True)\n",
      "Embarked1\n",
      "\n",
      "titanic[\"Embarked2\"] = titanic[\"Embarked\"]\n",
      "\n",
      "\n",
      "#titanic = pd.concat([titanic,Embarked1],axis=1)\n",
      "364/53: Embarked1\n",
      "364/54: titanic.head()\n",
      "364/55:\n",
      "sex1 = pd.get_dummies(titanic[\"Sex\"],drop_first=True)\n",
      "\n",
      "titanic = pd.concat([titanic,sex1,Embarked1],axis=1)\n",
      "titanic.head()\n",
      "364/56: titanic.drop([\"Sex\",\"Embarked\"],axis=1,inplace=True)\n",
      "364/57: titanic.head()\n",
      "365/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import warnings\n",
      "365/2: warnings.filterwarnings(\"ignore\")\n",
      "365/3:\n",
      "# .csv -> comma seperated values\n",
      "\n",
      "titanic = pd.read_csv(\"full.csv\")     # to load the dataset\n",
      "                                 # overhere titanic is basically a pandas dataframe\n",
      "365/4:\n",
      "#print(type(titanic))\n",
      "titanic.head()        # head() returns first first five rows of the whole data.\n",
      "365/5: titanic.tail()     # tail() returns last five rows of the whole data.\n",
      "365/6: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.\n",
      "365/7: titanic.columns     # to know all the labes, features, columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/8: titanic.info()\n",
      "365/9: titanic.describe()\n",
      "365/10: titanic[\"Survived\"].isnull().sum()  # count's the number of missing values or NAN values in each column\n",
      "365/11: titanic.isnull().mean()*100      # to find the missing value percentage in each column.\n",
      "365/12: titanic.drop(\"Class\", axis=1, inplace=True)   # to drop any column or you can say to remove any colummn.\n",
      "365/13: #titanic.head()\n",
      "365/14:\n",
      "#titanic.head()\n",
      "titanic[\"Boarded\"].unique()  # to see all the diffrent type of discrit or categorical values prasent inside a particular column.\n",
      "365/15:\n",
      "titanic.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\",\"Body\"],axis=1,inplace=True) \n",
      "#to drop multiple column at a time.\n",
      "titanic.head()\n",
      "365/16: titanic.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\n",
      "365/17: titanic.isnull().sum()\n",
      "365/18: #titanic.info()\n",
      "365/19: titanic.dropna(subset=[\"Survived\"],inplace=True)\n",
      "365/20:\n",
      "#\n",
      "titanic.isnull().mean()*100\n",
      "365/21:\n",
      "#\n",
      "titanic.info()\n",
      "365/22: #titanic.shape\n",
      "365/23:\n",
      "## Mean/ Midean Imputation\n",
      "\n",
      "# we try to replace the missing values or we impute the missing values using the mean or the meandian.\n",
      "\n",
      "titanic[\"Age_median\"] = titanic['Age'] \n",
      "titanic[\"Age_median\"].fillna(titanic[\"Age_median\"].median(),inplace=True) # i am replacing the missing values with the median()\n",
      "\n",
      "titanic.head()\n",
      "365/24: titanic[\"Age\"].std()\n",
      "365/25: titanic[\"Age_median\"].std()\n",
      "365/26:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "365/27:\n",
      "#titanic[\"Age_random\"].dropna().sample()\n",
      "#titanic[\"Age_random\"].isnull().sum()\n",
      "365/28:\n",
      "## Random sample Imputation\n",
      "\n",
      "titanic[\"Age_random\"] = titanic[\"Age\"] # we are copying all the values present inside the age column into the age_random column\n",
      "random_samples = titanic[\"Age_random\"].dropna().sample(titanic[\"Age_random\"].isnull().sum(), random_state=42)\n",
      "random_samples.index = titanic[titanic[\"Age_random\"].isnull()].index\n",
      "titanic.loc[titanic[\"Age_random\"].isnull(),\"Age_random\"]=random_samples\n",
      "\n",
      "titanic.isnull().sum()\n",
      "365/29: 891-177\n",
      "365/30: titanic[\"Age\"].std()\n",
      "365/31: titanic[\"Age_random\"].std()\n",
      "365/32: titanic[\"Age_median\"].std()\n",
      "365/33:\n",
      "#fig = plt.figure()\n",
      "#ax = fig.add_subplot(111)\n",
      "#titanic[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "#titanic[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
      "#titanic[\"Age_random\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
      "#line, labels = ax.get_legend_handles_labels()\n",
      "#ax.legend(line,labels,loc = \"best\")\n",
      "365/34: #titanic.isnull().sum()\n",
      "365/35:\n",
      "import seaborn as sns\n",
      "#sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap=\"brg\")\n",
      "365/36: 2/891\n",
      "365/37: titanic[\"Cabin\"].unique()    # this show the unique values in a particular column.\n",
      "365/38:\n",
      "titanic[\"Age\"]=titanic[\"Age_random\"]\n",
      "titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)\n",
      "365/39:\n",
      "# Imputing the Embarked column.\n",
      "\n",
      "# imputing Catagorical values\n",
      "#  -> Frequency Imputation/Mode imputation.\n",
      "\n",
      "titanic[\"Embarked\"].unique()\n",
      "titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0], inplace=True)\n",
      "365/40: titanic[\"Embarked\"].unique()\n",
      "365/41:\n",
      "# Number of person survived and number of person dinot survived.\n",
      "#print(titanic['Survived'].value_counts())\n",
      "#sns.countplot(x='Survived',data=titanic)\n",
      "365/42: #sns.countplot(x='Survived', hue='Sex', data=titanic)\n",
      "365/43: #sns.countplot(x='Survived', hue='Embarked', data=titanic)\n",
      "365/44: titanic[\"Pclass\"].unique()\n",
      "365/45: #sns.countplot(x='Survived',hue=\"Pclass\", data = titanic)\n",
      "365/46:\n",
      "#plt.figure(figsize=(12,8))\n",
      "#sns.boxplot(x='Pclass',y='Age',data=titanic)\n",
      "365/47: #titanic.corr()\n",
      "365/48: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)\n",
      "365/49: sns.pairplot(titanic, hue=\"Survived\", vars=[\"Pclass\",\"Age\"])\n",
      "365/50: titanic.head()\n",
      "365/51:\n",
      "# Handeling Categorical features\n",
      "\n",
      "# ---> One hot encoding\n",
      "#  ---> Ordinal number encoding\n",
      "#  ---> Label encoder\n",
      "365/52:\n",
      "# one hot encodding\n",
      "\n",
      "\n",
      "Embarked1 = pd.get_dummies(titanic[\"Embarked\"],drop_first=True)\n",
      "Embarked1\n",
      "\n",
      "titanic[\"Embarked2\"] = titanic[\"Embarked\"]\n",
      "\n",
      "\n",
      "#titanic = pd.concat([titanic,Embarked1],axis=1)\n",
      "365/53: Embarked1\n",
      "365/54: titanic.head()\n",
      "365/55:\n",
      "sex1 = pd.get_dummies(titanic[\"Sex\"],drop_first=True)\n",
      "\n",
      "titanic = pd.concat([titanic,sex1,Embarked1],axis=1)\n",
      "titanic.head()\n",
      "365/56: #titanic.drop([\"Sex\",\"Embarked\"],axis=1,inplace=True)\n",
      "365/57: titanic.head()\n",
      "365/58: from sklearn.preprocessing import OrdinalEncoder\n",
      "365/59:\n",
      "o_encoder = OrdinalEncoder\n",
      "result = o_encoder.fit_transform(titanic[\"Embarked2\"])\n",
      "result\n",
      "365/60:\n",
      "o_encoder = OrdinalEncoder\n",
      "x = np.array(titanic[\"Embarked2\"])\n",
      "#result = o_encoder.fit_transform()\n",
      "#result\n",
      "x\n",
      "365/61:\n",
      "o_encoder = OrdinalEncoder\n",
      "x = np.array(titanic[\"Embarked2\"])\n",
      "result = o_encoder.fit_transform(x)\n",
      "result\n",
      "365/62:\n",
      "o_encoder = OrdinalEncoder\n",
      "x = np.array(titanic[\"Embarked2\"]).reshape(1,-1)\n",
      "print(x)\n",
      "result = o_encoder.fit_transform(x)\n",
      "result\n",
      "365/63:\n",
      "o_encoder = OrdinalEncoder\n",
      "x = np.array(titanic[\"Embarked2\"]).reshape(-1,1)\n",
      "print(x)\n",
      "result = o_encoder.fit_transform(x)\n",
      "result\n",
      "365/64:\n",
      "o_encoder = OrdinalEncoder\n",
      "x = np.array(titanic[\"Embarked2\"]).reshape(-1,1)\n",
      "#print(x)\n",
      "result = o_encoder.fit_transform(x)\n",
      "result\n",
      "365/65:\n",
      "o_encoder = OrdinalEncoder\n",
      "x = np.array(titanic[\"Embarked2\"]).reshape(-1,1)\n",
      "print(x)\n",
      "result = o_encoder.fit_transform(x)\n",
      "result\n",
      "365/66:\n",
      "o_encoder = OrdinalEncoder\n",
      "x = np.array(titanic[\"Embarked2\"]).reshape(-1,1)\n",
      "#print(x)\n",
      "result = o_encoder.fit_transform(x)\n",
      "print(result)\n",
      "365/67:\n",
      "o_encoder = OrdinalEncoder()\n",
      "x = np.array(titanic[\"Embarked2\"]).reshape(-1,1)\n",
      "#print(x)\n",
      "result = o_encoder.fit_transform(x)\n",
      "print(result)\n",
      "365/68:\n",
      "o_encoder = OrdinalEncoder()\n",
      "x = np.array(titanic[\"Embarked2\"]).reshape(-1,1)\n",
      "#print(x)\n",
      "embarked2 = o_encoder.fit_transform(x)\n",
      "\n",
      "embarked2 = pd.DataFrame(data=embarked2,columns=['embarked2'])\n",
      "\n",
      "titanic = pd.concat([titanic,embarked2],axis=1)\n",
      "365/69: titanic.head()\n",
      "367/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "import pickle\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "367/2: warnings.filterwarnings(\"ignore\")\n",
      "367/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "367/4: data.head()\n",
      "367/5: data.info()\n",
      "367/6: data.describe()\n",
      "367/7: data.columns\n",
      "367/8: data.info()\n",
      "367/9: data.isnull().sum()\n",
      "367/10: data.corr()\n",
      "367/11:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "367/12:\n",
      "sns.kdeplot(data[data['target']==1]['chol'],shade=True,color=\"orange\", label=\"Unwell\", alpha=.7)\n",
      "sns.kdeplot(data[data['target']==0]['chol'],shade=True,color=\"dodgerblue\", label=\"Healthy\", alpha=.7)\n",
      "plt.title('Cholesterol in mg/d for both case')\n",
      "plt.show()\n",
      "367/13:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "367/14: #sns.pairplot(data, hue=\"target\")\n",
      "367/15:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "367/16:\n",
      "data_2 = data.drop([\"target\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))\n",
      "367/17: sns.pairplot(data, hue = \"target\", vars = [\"cp\",\"restecg\",\"thalach\",\"slope\"])\n",
      "367/18:\n",
      "# Selecting all the features\n",
      "\n",
      "all_x = data.drop((\"target\"), axis=1)\n",
      "all_x = np.array(all_x)\n",
      "\n",
      "# Selecting only importent features\n",
      "\n",
      "imp_x = data[[\"cp\",\"restecg\",\"thalach\",\"slope\"]]\n",
      "imp_x = np.array(imp_x)\n",
      "\n",
      "\n",
      "y = data[\"target\"]\n",
      "y = np.array(y).reshape(-1,1)\n",
      "\n",
      "print(all_x)\n",
      "print(imp_x)\n",
      "print(y)\n",
      "367/19:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "all_x = std_scl.fit_transform(all_x)\n",
      "imp_x = std_scl.fit_transform(imp_x)\n",
      "\n",
      "all_x\n",
      "367/20:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(all_x)\n",
      "367/21:\n",
      "all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)\n",
      "all_x_train\n",
      "#all_y_train\n",
      "367/22:\n",
      "imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)\n",
      "imp_x_train\n",
      "#imp_y_train\n",
      "367/23:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction)*100)\n",
      "print(\"R-square on all features: \", r2_score(all_y_test,lg_model_prediction))\n",
      "367/24:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "lg_model1 = LogisticRegression()\n",
      "lg_model1.fit(imp_x_train,imp_y_train)\n",
      "lg_model1_prediction = lg_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on Selected features: \", accuracy_score(imp_y_test,lg_model1_prediction)*100)\n",
      "print(\"Accuracy on Selected features: \", r2_score(imp_y_test,lg_model1_prediction))\n",
      "367/25:\n",
      "confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "367/26:\n",
      "# Training and testing on all features.\n",
      "\n",
      "dt_model = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model.fit(all_x_train,all_y_train)\n",
      "dt_model_prediction = dt_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,dt_model_prediction)*100)\n",
      "print(\"Accuracy on all features: \", r2_score(all_y_test,dt_model_prediction))\n",
      "367/27:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "dt_model1 = DecisionTreeClassifier(random_state = 42)\n",
      "dt_model1.fit(imp_x_train,imp_y_train)\n",
      "dt_model1_prediction = dt_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test,dt_model1_prediction)*100)\n",
      "print(\"Accuracy on selected features: \", r2_score(imp_y_test,dt_model1_prediction))\n",
      "367/28:\n",
      "confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')\n",
      "plt.show()\n",
      "367/29:\n",
      "# Training and testing on all features.\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model.fit(all_x_train,all_y_train)\n",
      "rf_model_prediction = rf_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, rf_model_prediction)*100)\n",
      "367/30:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction)*100)\n",
      "367/31:\n",
      "confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')\n",
      "plt.show()\n",
      "367/32:\n",
      "# Training and testing on all features.\n",
      "\n",
      "nb_model = GaussianNB()\n",
      "nb_model.fit(all_x_train,all_y_train)\n",
      "nb_model_prediction = nb_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, nb_model_prediction)*100)\n",
      "367/33:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "nb_model1 = GaussianNB()\n",
      "nb_model1.fit(imp_x_train,imp_y_train)\n",
      "nb_model1_prediction = nb_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, nb_model1_prediction)*100)\n",
      "367/34:\n",
      "confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "367/35:\n",
      "# Training and testing on all features.\n",
      "\n",
      "svm_model = SVC()\n",
      "svm_model.fit(all_x_train,all_y_train)\n",
      "svm_model_prediction = svm_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test, svm_model_prediction)*100)\n",
      "367/36:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "svm_model1 = SVC()\n",
      "svm_model1.fit(imp_x_train,imp_y_train)\n",
      "svm_model1_prediction = svm_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(imp_y_test, svm_model1_prediction)*100)\n",
      "367/37:\n",
      "confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "367/38:\n",
      "# save model\n",
      "pickle.dump(rf_model,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "heart_disease_prediction_model = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = heart_disease_prediction_model.predict(all_x_test)\n",
      "\n",
      "print(\"confusion matrix: \\n\", confusion_matrix(all_y_test,y_predict))\n",
      "accuracy_score(all_y_test,y_predict)\n",
      "367/39:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "import pickle\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "367/40:\n",
      "# Training and testing on selected features.\n",
      "\n",
      "rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_model1.fit(imp_x_train,imp_y_train)\n",
      "rf_model1_prediction = rf_model1.predict(imp_x_test)\n",
      "print(\"Accuracy on selected features: \", accuracy_score(imp_y_test, rf_model1_prediction)*100)\n",
      "367/41:\n",
      "cross_val = cross_val_score(estimator=rf_model, X=all_x_train, y=all_y_train)\n",
      "print(\"Cross validation accuracy of Random Forest Classifier: \",cross_val)\n",
      "367/42:\n",
      "cross_val = cross_val_score(estimator=rf_model, X=all_x_train, y=all_y_train)\n",
      "print(\"Cross validation accuracy of Random Forest Classifier: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Random Forest Classifier: \", cross_val.mean())\n",
      "369/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "369/2: data = pd.read_csv(\"Breast_cancer_dataset.csv\")\n",
      "369/3: data.head()\n",
      "369/4: data.columns\n",
      "369/5: data.describe()\n",
      "369/6: data.info()\n",
      "369/7: data.isnull().sum() # looking for null values\n",
      "369/8: data.drop([\"id\",\"Unnamed: 32\"], axis=1, inplace = True) # droping the unwanted columns\n",
      "369/9: data.head()\n",
      "369/10: #sns.countplot(data[\"diagnosis\"])\n",
      "369/11: #sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "369/12:\n",
      "print(data[\"diagnosis\"].unique())\n",
      "label_encoder = preprocessing.LabelEncoder()\n",
      "data[\"diagnosis\"] = label_encoder.fit_transform(data[\"diagnosis\"])\n",
      "print(data[\"diagnosis\"].unique())\n",
      "369/13: sns.countplot(data[\"diagnosis\"])\n",
      "369/14:\n",
      "data.head()      # M = 1 (It means the the tumor is cancerous or having breast cancer.)\n",
      "                 # B = 0 ( It means the tumor is normal and not a cancerous type.)\n",
      "369/15:\n",
      "plt.figure(figsize=(15,8))              #HEAT MAP\n",
      "sns.heatmap(data)\n",
      "369/16: #sns.pairplot(data,hue = \"diagnosis\")\n",
      "369/17:\n",
      "plt.figure(figsize=(20,8))           \n",
      "sns.countplot(data[\"radius_mean\"])\n",
      "369/18: data.corr()\n",
      "369/19:\n",
      "f,ax = plt.subplots(figsize=(18, 18))    #CORRELATION HEAT MAP\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
      "369/20:\n",
      "sns.pairplot(data, hue = \"diagnosis\", vars = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\"])\n",
      "# 1 = M, 0 = B\n",
      "369/21:\n",
      "data_x_labels = data.drop((\"diagnosis\"), axis=1)\n",
      "#data_x_labels.head()\n",
      "\n",
      "data_y_label = data[\"diagnosis\"]\n",
      "369/22:\n",
      "stnd_scl = StandardScaler()\n",
      "data_x_labels = stnd_scl.fit_transform(data_x_labels)\n",
      "369/23: x_train,x_test,y_train,y_test = train_test_split(data_x_labels, data_y_label, test_size = 0.2, random_state = 42)\n",
      "369/24:\n",
      "#print(x_train)\n",
      "# print(y_train.head())\n",
      "369/25:\n",
      "model_1 = LogisticRegression()\n",
      "model_1.fit(x_train,y_train)\n",
      "model_1_y_prediction = model_1.predict(x_test)\n",
      "print(\"Accuracy of Logistic Regrassion: \",accuracy_score(y_test, model_1_y_prediction))\n",
      "369/26:\n",
      "model_2 = DecisionTreeClassifier(random_state = 45)\n",
      "model_2.fit(x_train,y_train)\n",
      "model_2_y_prediction = model_2.predict(x_test)\n",
      "print(\"Accuracy of Decission Tree Classifier: \",accuracy_score(y_test, model_2_y_prediction))\n",
      "369/27:\n",
      "model_3 = RandomForestClassifier(n_estimators=100, random_state=45)\n",
      "model_3.fit(x_train, y_train)\n",
      "model_3_y_prediction = model_3.predict(x_test)\n",
      "print(\"Accuracy of Random Forest Classifier: \",accuracy_score(y_test, model_1_y_prediction))\n",
      "369/28:\n",
      "confu_matrix = confusion_matrix(y_test, model_3_y_prediction)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "369/29:\n",
      "cross_val = cross_val_score(estimator=model_3, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Random Forest Classifier: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Random Forest Classifier: \", cross_val.mean())\n",
      "369/30:\n",
      "stnd_scl = StandardScaler()\n",
      "data_x_labels = stnd_scl.fit_transform(data_x_labels)\n",
      "\n",
      "data_x_labels_labels\n",
      "369/31:\n",
      "stnd_scl = StandardScaler()\n",
      "data_x_labels = stnd_scl.fit_transform(data_x_labels)\n",
      "\n",
      "data_x_labels\n",
      "369/32:\n",
      "stnd_scl = StandardScaler()\n",
      "data_x_labels = stnd_scl.fit_transform(data_x_labels)\n",
      "\n",
      "plt.figure(figsize=(15,8))              #HEAT MAP\n",
      "sns.heatmap(data_x_labels)\n",
      "370/1: import pandas as pd\n",
      "370/2: df = pd.read_csv(\"full.csv\", usecols = [\"Cabin\",\"Survived\"])\n",
      "370/3:\n",
      "df = pd.read_csv(\"full.csv\", usecols = [\"Cabin\",\"Survived\"])\n",
      "df.head()\n",
      "370/4: df[\"Survived\"] = df[\"Survived\"].astype(int)\n",
      "370/5: df[\"Survived\"] = df[\"Survived\"].astype(int32)\n",
      "370/6: df[\"Survived\"] = df[\"Survived\"].astype('int')\n",
      "370/7: df[\"Survived\"] = df[\"Survived\"].astype('int64')\n",
      "370/8: df[\"Survived\"] = df[\"Survived\"].astype('int64').dtypes\n",
      "370/9: df[\"Survived\"] = df[\"Survived\"].astype('int32').dtypes\n",
      "370/10: df.isnull().sum()\n",
      "370/11: df.dropna()\n",
      "370/12: df.isnull().sum()\n",
      "370/13: df[\"Survived\"].dropna(inplace = True)\n",
      "370/14: df.isnull().sum()\n",
      "370/15: df[\"Survived\"].dropna(inplace = True)\n",
      "370/16: df.isnull().sum()\n",
      "370/17: df.dropna(subset = [\"Survived\"], inplace = True)\n",
      "370/18: df.isnull().sum()\n",
      "370/19: df[\"Survived\"] = df[\"Survived\"].astype(\"int64\").dtypes\n",
      "370/20: df.head()\n",
      "370/21:\n",
      "df = pd.read_csv(\"full.csv\", usecols = [\"Cabin\",\"Survived\"])\n",
      "df.head()\n",
      "370/22: #df.isnull().sum()\n",
      "370/23: df.dropna(subset = [\"Survived\"], inplace = True)\n",
      "370/24: df.head()\n",
      "370/25: df[\"Cabin\"].fillna(\"Missing\",inplace=True)\n",
      "370/26: df.head()\n",
      "370/27: sad\n",
      "370/28:\n",
      "df[\"Cabin\"].fillna(\"Missing\",inplace=True)\n",
      "df.head()\n",
      "370/29: dtype(df[\"Cabin\"])\n",
      "370/30: dtypes(df[\"Cabin\"])\n",
      "370/31: types(df[\"Cabin\"])\n",
      "370/32: type(df[\"Cabin\"])\n",
      "370/33: df.Cabin.dtype\n",
      "370/34: df.Cabin.dtypes\n",
      "370/35: df.dtypes\n",
      "370/36: df[\"Cabin\"] = df[\"Cabin\"].astype(str)\n",
      "370/37: df.dtypes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/38: df[\"Cabin\"] = df[\"Cabin\"].astype(str).str[0]\n",
      "370/39: df.head()\n",
      "370/40: df.dtypes\n",
      "370/41: df.Cabin.Unique()\n",
      "370/42: df.Cabin.Unique\n",
      "370/43: df.Cabin.unique\n",
      "370/44: df[\"Cabin\"].unique\n",
      "370/45: df[\"Cabin\"].unique\n",
      "370/46: df[\"Cabin\"].unique()\n",
      "370/47: df.groupby([\"Cabin\"])\n",
      "370/48: df.groupby([\"Cabin\"])[\"Survived\"]\n",
      "370/49: df.groupby([\"Cabin\"])[\"Survived\"].mean()\n",
      "370/50: df.groupby([\"Cabin\"])[\"Survived\"].mean().value_sort()\n",
      "370/51: df.groupby([\"Cabin\"])[\"Survived\"].mean().sort_values()\n",
      "370/52: df[\"Cabin\"] = df[\"Cabin\"].astype(str).str[0]\n",
      "370/53:\n",
      "df[\"Cabin\"] = df[\"Cabin\"].astype(str).str[0]\n",
      "df.head()\n",
      "370/54: labes = df.groupby([\"Cabin\"])[\"Survived\"].mean().sort_values().index\n",
      "370/55:\n",
      "labes = df.groupby([\"Cabin\"])[\"Survived\"].mean().sort_values().index\n",
      "labels\n",
      "370/56:\n",
      "labels = df.groupby([\"Cabin\"])[\"Survived\"].mean().sort_values().index\n",
      "labels\n",
      "370/57: ordinal_labels = {key:value for key,value in enumerate(labels,0)}\n",
      "370/58:\n",
      "ordinal_labels = {key:value for key,value in enumerate(labels,0)}\n",
      "ordinal_labels\n",
      "370/59:\n",
      "ordinal_labels = {key:value for value,key in enumerate(labels,0)}\n",
      "ordinal_labels\n",
      "370/60: df[\"Encoded_value\"] = df[\"Cabin\"].map(ordinal_labels)\n",
      "370/61: df.head()\n",
      "371/1: import pandas as pd\n",
      "371/2:\n",
      "df = pd.read_csv(\"full.csv\", usecols = [\"Cabin\",\"Survived\"])\n",
      "df.head()\n",
      "371/3: #df.isnull().sum()\n",
      "371/4: df.dropna(subset = [\"Survived\"], inplace = True)\n",
      "371/5:\n",
      "df[\"Cabin\"].fillna(\"Missing\",inplace=True)\n",
      "df.head()\n",
      "371/6: df.dtypes\n",
      "371/7:\n",
      "df[\"Cabin\"] = df[\"Cabin\"].astype(str).str[0]\n",
      "df.head()\n",
      "371/8: df[\"Cabin\"].unique()\n",
      "371/9: df.groupby([\"Cabin\"])[\"Survived\"].mean()\n",
      "371/10: df.groupby([\"Cabin\"])[\"Survived\"].mean().sort_values()\n",
      "371/11:\n",
      "labels = df.groupby([\"Cabin\"])[\"Survived\"].mean().sort_values().index\n",
      "labels\n",
      "371/12:\n",
      "ordinal_labels = {key:value for value,key in enumerate(labels,0)}\n",
      "ordinal_labels\n",
      "371/13: df[\"Encoded_value\"] = df[\"Cabin\"].map(ordinal_labels)\n",
      "371/14: df.head()\n",
      "372/1: import pandas as pd\n",
      "372/2:\n",
      "df = pd.read_csv(\"full.csv\", usecols = [\"Sex\"])\n",
      "df.head()\n",
      "372/3: pd.getdummies(df.[\"Sex\"])\n",
      "372/4: pd.getdummies(df[\"Sex\"])\n",
      "372/5: pd.get_dummies(df[\"Sex\"])\n",
      "372/6: one_hot = pd.get_dummies(df[\"Sex\"],drop_first=True)\n",
      "372/7:\n",
      "one_hot = pd.get_dummies(df[\"Sex\"],drop_first=True)\n",
      "one_hot\n",
      "372/8:\n",
      "one_hot = pd.get_dummies(df[\"Sex\"],drop_first=True)\n",
      "#one_hot\n",
      "df = df.concat([df,one_hot],axis=1)\n",
      "372/9:\n",
      "one_hot = pd.get_dummies(df[\"Sex\"],drop_first=True)\n",
      "#one_hot\n",
      "df = pd.concat([df,one_hot],axis=1)\n",
      "372/10:\n",
      "one_hot = pd.get_dummies(df[\"Sex\"],drop_first=True)\n",
      "#one_hot\n",
      "df = pd.concat([df,one_hot],axis=1)\n",
      "df.head()\n",
      "373/1: import pandas as pd\n",
      "373/2:\n",
      "df = pd.read_csv(\"full.csv\", usecols = [\"Sex\"])\n",
      "df.head()\n",
      "373/3: pd.get_dummies(df[\"Sex\"])\n",
      "373/4:\n",
      "one_hot = pd.get_dummies(df[\"Sex\"],drop_first=True)\n",
      "#one_hot\n",
      "df = pd.concat([df,one_hot],axis=1)\n",
      "df.head()\n",
      "373/5:\n",
      "one_hot = pd.get_dummies(df[\"Sex\"],drop_first=True)\n",
      "#one_hot\n",
      "df = pd.concat([df,one_hot],axis=1)\n",
      "df.head()\n",
      "374/1: import pandas as pd\n",
      "374/2:\n",
      "df = pd.read_csv(\"full.csv\", usecols = [\"Sex\"])\n",
      "df.head()\n",
      "374/3: pd.get_dummies(df[\"Sex\"])\n",
      "374/4:\n",
      "one_hot = pd.get_dummies(df[\"Sex\"],drop_first=True)\n",
      "#one_hot\n",
      "df = pd.concat([df,one_hot],axis=1)\n",
      "df.head()\n",
      "374/5: df.rename(columns={\"male\":\"One_hot_encoded\"})\n",
      "374/6:\n",
      "df2 = pd.read_csv(\"full.csv\",usecols=[\"Sex\"])\n",
      "df2.head()\n",
      "374/7:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "374/8:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "encoder = OneHotEncoder(drop='first',spares=False)\n",
      "sex = np.array(df2[\"Sex\"]).reshape(-1,1)\n",
      "sex = encoder.fit_transform(sex)\n",
      "sex = pd.DataFrame(data=sex,columns=[\"One_hot_encoded\"])\n",
      "df2 = pd.concat([df2,sex],axis=1)\n",
      "374/9:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "encoder = OneHotEncoder(drop='first',sparse=False)\n",
      "sex = np.array(df2[\"Sex\"]).reshape(-1,1)\n",
      "sex = encoder.fit_transform(sex)\n",
      "sex = pd.DataFrame(data=sex,columns=[\"One_hot_encoded\"])\n",
      "df2 = pd.concat([df2,sex],axis=1)\n",
      "374/10:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "encoder = OneHotEncoder(drop='first',sparse=False)\n",
      "sex = np.array(df2[\"Sex\"]).reshape(-1,1)\n",
      "sex = encoder.fit_transform(sex)\n",
      "sex = pd.DataFrame(data=sex,columns=[\"One_hot_encoded\"])\n",
      "df2 = pd.concat([df2,sex],axis=1)\n",
      "\n",
      "\n",
      "df2.head()\n",
      "375/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "375/2:\n",
      "df = pd.read_csv(\"full.csv\", usecols = [\"Sex\"])\n",
      "df.head()\n",
      "375/3: pd.get_dummies(df[\"Sex\"])\n",
      "375/4:\n",
      "one_hot = pd.get_dummies(df[\"Sex\"],drop_first=True)\n",
      "#one_hot\n",
      "df = pd.concat([df,one_hot],axis=1)\n",
      "df.head()\n",
      "375/5: df.rename(columns={\"male\":\"One_hot_encoded\"})    # to rename any columns in dataframe\n",
      "375/6:\n",
      "df2 = pd.read_csv(\"full.csv\",usecols=[\"Sex\"])\n",
      "df2.head()\n",
      "375/7:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "encoder = OneHotEncoder(drop='first',sparse=False)\n",
      "sex = np.array(df2[\"Sex\"]).reshape(-1,1)\n",
      "sex = encoder.fit_transform(sex)\n",
      "sex = pd.DataFrame(data=sex,columns=[\"One_hot_encoded\"])\n",
      "df2 = pd.concat([df2,sex],axis=1)\n",
      "\n",
      "\n",
      "df2.head()\n",
      "375/8: df2.head()\n",
      "376/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "376/2:\n",
      "df = pd.read_csv(\"full.csv\", usecols = [\"Sex\"])\n",
      "df.head()\n",
      "376/3: pd.get_dummies(df[\"Sex\"])\n",
      "376/4:\n",
      "one_hot = pd.get_dummies(df[\"Sex\"],drop_first=True)\n",
      "#one_hot\n",
      "df = pd.concat([df,one_hot],axis=1)\n",
      "df.head()\n",
      "376/5: df.rename(columns={\"male\":\"One_hot_encoded\"})    # to rename any columns in dataframe\n",
      "376/6:\n",
      "df2 = pd.read_csv(\"full.csv\",usecols=[\"Sex\"])\n",
      "df2.head()\n",
      "376/7:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "encoder = OneHotEncoder(drop='first',sparse=False)\n",
      "sex = np.array(df2[\"Sex\"]).reshape(-1,1)\n",
      "sex = encoder.fit_transform(sex)\n",
      "sex = pd.DataFrame(data=sex,columns=[\"One_hot_encoded\"])\n",
      "df2 = pd.concat([df2,sex],axis=1)\n",
      "376/8: df2.head()\n",
      "376/9:\n",
      "df.rename(columns={\"male\":\"One_hot_encoded\"})  # to rename any columns in dataframe\n",
      "df.head()\n",
      "376/10:\n",
      "df.rename(columns={\"male\":\"One_hot_encoded\"}, inplace=True)  # to rename any columns in dataframe\n",
      "df.head()\n",
      "377/1:\n",
      "import pandas as pd\n",
      "import datetime\n",
      "377/2: today = datetime.datetime.today()\n",
      "377/3:\n",
      "today = datetime.datetime.today()\n",
      "today\n",
      "377/4: datetime.datetime.today()\n",
      "377/5: datetime.datetime.today() - datetime.timedelta(3)\n",
      "377/6:\n",
      "days = [datetime.datetime.today() - datetime.timedelta(i) for i in range(20)]\n",
      "days\n",
      "377/7:\n",
      "df = pd.DataFrame(data=days,columns=[\"Date\"])\n",
      "df.head\n",
      "377/8:\n",
      "df = pd.DataFrame(data=days,columns=[\"Date\"])\n",
      "df.head()\n",
      "377/9:\n",
      "df = pd.DataFrame(data=days,columns=[\"Date\"])\n",
      "df.head()\n",
      "377/10:\n",
      "df = pd.DataFrame(data=days,columns=[\"Date\"])\n",
      "df.head()\n",
      "377/11: df[\"Week_days\"] = df[\"Date\"].dt.weekday_name\n",
      "377/12: df[\"Week_days\"] = df[\"Date\"].dt.day_name()\n",
      "377/13: df.head()\n",
      "377/14: df[\"Week_days\"] = df[\"Date\"].dt.day_name()\n",
      "377/15: df.head()\n",
      "377/16: df[\"Week_days\"] = df[\"Date\"].dt.day_name()\n",
      "377/17: df.head()\n",
      "377/18: df[\"Week_days\"] = df[\"Date\"].dt.day_name()\n",
      "377/19: df.head()\n",
      "377/20: days = df[\"Week_days\"].to_dict()\n",
      "377/21: days\n",
      "377/22: days = {\"Monday\":1, \"Tuesday\":2, \"Wednesday\":3, \"Thursday\":4, \"Friday\":5, \"Saturday\":6, \"Sunday\":7}\n",
      "377/23: days\n",
      "377/24:\n",
      "days = {\"Monday\":1, \"Tuesday\":2, \"Wednesday\":3, \"Thursday\":4, \"Friday\":5, \"Saturday\":6, \"Sunday\":7}\n",
      "days\n",
      "377/25:\n",
      "df[\"Ordinal_encoded\"] = df[\"Week_days\"].map(days)\n",
      "df.head()\n",
      "378/1:\n",
      "import pandas as pd\n",
      "import datetime\n",
      "378/2: datetime.datetime.today()\n",
      "378/3: datetime.datetime.today() - datetime.timedelta(3) # to find the three days back date information\n",
      "378/4:\n",
      "days = [datetime.datetime.today() - datetime.timedelta(i) for i in range(20)]\n",
      "days\n",
      "378/5:\n",
      "df = pd.DataFrame(data=days,columns=[\"Date\"])\n",
      "df.head()\n",
      "378/6: df[\"Week_days\"] = df[\"Date\"].dt.day_name() # to get day name for all days\n",
      "378/7: df.head()\n",
      "378/8:\n",
      "days_name = {\"Monday\":1, \"Tuesday\":2, \"Wednesday\":3, \"Thursday\":4, \"Friday\":5, \"Saturday\":6, \"Sunday\":7}\n",
      "days_name\n",
      "378/9:\n",
      "df[\"Ordinal_encoded\"] = df[\"Week_days\"].map(days_name)\n",
      "df.head()\n",
      "378/10:\n",
      "days = [datetime.datetime.today() - datetime.timedelta(i) for i in range(20)]\n",
      "days\n",
      "378/11:\n",
      "days = [datetime.datetime.today() - datetime.timedelta(i) for i in range(20)]\n",
      "#days\n",
      "378/12:\n",
      "df2 = pd.DataFrame(data=days,columns=[\"Date\"])\n",
      "df2.head()\n",
      "378/13:\n",
      "df2 = pd.DataFrame(data=days,columns=[\"Date\"])\n",
      "df[\"Week_days\"] = df[\"Date\"].dt.day_name()\n",
      "df2.head()\n",
      "378/14:\n",
      "df2 = pd.DataFrame(data=days,columns=[\"Date\"])\n",
      "df2[\"Week_days\"] = df[\"Date\"].dt.day_name()\n",
      "df2.head()\n",
      "378/15: from sklearn.preprocessing import OrdinalEncoder\n",
      "378/16:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "378/17:\n",
      "o-encoder = OrdinalEncoder()\n",
      "x = np.array(df2[\"Week_daysk\"]).reshape(-1,1)\n",
      "378/18:\n",
      "o-encoder = OrdinalEncoder()\n",
      "x = np.array(df2[\"Week_days\"]).reshape(-1,1)\n",
      "378/19:\n",
      "o_encoder = OrdinalEncoder()\n",
      "x = np.array(df2[\"Week_days\"]).reshape(-1,1)\n",
      "378/20:\n",
      "o_encoder = OrdinalEncoder()\n",
      "x = np.array(df2[\"Week_days\"]).reshape(-1,1)\n",
      "days_column = o_encoder.fit_transform(x)\n",
      "days_column = pd.DataFrame(data=days_column, columns=[\"Ordinal_encoded\"])\n",
      "df2 = pd.concat([df2,days_column],axis=1)\n",
      "378/21: df2.head()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/22: df2.head(20)\n",
      "378/23: df2[\"Week_days\"].unique()\n",
      "378/24: df2[\"Week_days\"].unique().sort()\n",
      "378/25: df2[\"Week_days\"].unique().sort()\n",
      "378/26:\n",
      "c = df2[\"Week_days\"].unique().sort()\n",
      "c\n",
      "378/27:\n",
      "c = df2[\"Week_days\"].unique()\n",
      "c\n",
      "378/28:\n",
      "c = df2[\"Week_days\"].unique().value_sort()\n",
      "c\n",
      "378/29:\n",
      "c = df2[\"Week_days\"].unique().values_sort()\n",
      "c\n",
      "378/30:\n",
      "c = df2[\"Week_days\"].unique().sort_values()\n",
      "c\n",
      "378/31:\n",
      "c = list(df2[\"Week_days\"].unique())\n",
      "c\n",
      "378/32:\n",
      "c = list(df2[\"Week_days\"].unique())\n",
      "c.sort()\n",
      "378/33:\n",
      "c = list(df2[\"Week_days\"].unique())\n",
      "c.sort()\n",
      "378/34:\n",
      "c = list(df2[\"Week_days\"].unique())\n",
      "c.sort\n",
      "378/35:\n",
      "c = list(df2[\"Week_days\"].unique())\n",
      "c.sort()\n",
      "print(c)\n",
      "379/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "379/2: datetime.datetime.today()\n",
      "379/3: datetime.datetime.today() - datetime.timedelta(3) # to find the three days back date information\n",
      "379/4:\n",
      "days = [datetime.datetime.today() - datetime.timedelta(i) for i in range(20)]\n",
      "days\n",
      "379/5:\n",
      "df = pd.DataFrame(data=days,columns=[\"Date\"])\n",
      "df.head()\n",
      "379/6: df[\"Week_days\"] = df[\"Date\"].dt.day_name() # to get day name for all days\n",
      "379/7: df.head()\n",
      "379/8:\n",
      "days_name = {\"Monday\":1, \"Tuesday\":2, \"Wednesday\":3, \"Thursday\":4, \"Friday\":5, \"Saturday\":6, \"Sunday\":7}\n",
      "days_name\n",
      "379/9:\n",
      "df[\"Ordinal_encoded\"] = df[\"Week_days\"].map(days_name)\n",
      "df.head()\n",
      "379/10:\n",
      "days = [datetime.datetime.today() - datetime.timedelta(i) for i in range(20)]\n",
      "#days\n",
      "379/11:\n",
      "df2 = pd.DataFrame(data=days,columns=[\"Date\"])\n",
      "df2[\"Week_days\"] = df[\"Date\"].dt.day_name()\n",
      "df2.head()\n",
      "379/12:\n",
      "c = list(df2[\"Week_days\"].unique())\n",
      "c.sort()\n",
      "print(c)\n",
      "379/13: from sklearn.preprocessing import OrdinalEncoder\n",
      "379/14:\n",
      "o_encoder = OrdinalEncoder()\n",
      "x = np.array(df2[\"Week_days\"]).reshape(-1,1)\n",
      "days_column = o_encoder.fit_transform(x)\n",
      "days_column = pd.DataFrame(data=days_column, columns=[\"Ordinal_encoded\"])\n",
      "df2 = pd.concat([df2,days_column],axis=1)\n",
      "379/15: df2.head(20)\n",
      "380/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "380/2:\n",
      "df = pd.read_csv(\"adult.csv\")\n",
      "df.head()\n",
      "381/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "381/2:\n",
      "df = pd.read_csv(\"adult.csv\")\n",
      "df.head()\n",
      "381/3:\n",
      "for i in range(1,16):\n",
      "    print(df[i].unique)\n",
      "381/4: df[1].unique\n",
      "381/5: df[1].unique()\n",
      "381/6: df[1]\n",
      "381/7: df[1]\n",
      "381/8: df[\"1\"]\n",
      "381/9:\n",
      "for i in range(1,16):\n",
      "    print(df[str(i)].unique)\n",
      "381/10:\n",
      "for i in range(1,16):\n",
      "    print(df[str(i)].unique())\n",
      "381/11: df.info()\n",
      "381/12:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        print(df[str(i)].unique())\n",
      "381/13:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        print(\"Column {}\".format(i,df[str(i)].unique()))\n",
      "381/14:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        print(\"Column {} {}\".format(i,df[str(i)].unique()))\n",
      "381/15:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        print(\"Column {} {} /n\".format(i,df[str(i)].unique()))\n",
      "381/16:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        print(\"Column {} {} \\n\".format(i,df[str(i)].unique()))\n",
      "381/17: df[1].unique\n",
      "381/18: df[\"1\"].unique\n",
      "381/19: df[\"1\"].unique()\n",
      "381/20:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "       df[str(i)].replace(\"?\",\"Unknown\")\n",
      "381/21:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        print(\"Column {} {} \\n\".format(i,df[str(i)].unique()))\n",
      "381/22:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "       df[str(i)].replace(\" ?\",\"Unknown\")\n",
      "381/23:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        print(\"Column {} {} \\n\".format(i,df[str(i)].unique()))\n",
      "381/24: df[\"1\"].replace(\"?\",\"Unknown\")\n",
      "381/25: df[\"2\"].replace(\"?\",\"Unknown\")\n",
      "381/26: df[\"2\"].unique()\n",
      "381/27: df[\"2\"].replace(\" ?\",\"Unknown\")\n",
      "381/28: df[\"2\"].unique()\n",
      "381/29:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "       df[str(i)].replace(\" ?\",\"Unknown\",inplace=True)\n",
      "381/30:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        print(\"Column {} {} \\n\".format(i,df[str(i)].unique()))\n",
      "381/31: df[\"2\"].unique()\n",
      "381/32:\n",
      "columns = []\n",
      "for in in range(1,16):\n",
      "    columns.append(str(i))\n",
      "    \n",
      "columns\n",
      "381/33:\n",
      "columns = []\n",
      "for i in range(1,16):\n",
      "    columns.append(str(i))\n",
      "    \n",
      "columns\n",
      "381/34: df = df[columns]\n",
      "381/35:\n",
      "column = []\n",
      "for i in range(1,16):\n",
      "    column.append(str(i))\n",
      "column\n",
      "381/36:\n",
      "df = df[column]\n",
      "df.columns\n",
      "381/37:\n",
      "column = []\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "       column.append(str(i))\n",
      "381/38:\n",
      "column = []\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "       column.append(str(i))\n",
      "column\n",
      "381/39:\n",
      "column = []\n",
      "for i in range(1,15):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        column.append(str(i))\n",
      "column\n",
      "381/40:\n",
      "df = df[column]\n",
      "df.columns=[\"Employe\",\"Degree\",\"Status\",\"Post\",\"Family_status\",\"Races\",\"Gender\",\"Country\"]\n",
      "381/41:\n",
      "df = df[column]\n",
      "df.columns=[\"Employe\",\"Degree\",\"Status\",\"Post\",\"Family_status\",\"Races\",\"Gender\",\"Country\"]\n",
      "df.head()\n",
      "381/42:\n",
      "df = df[column]\n",
      "df.column=[\"Employe\",\"Degree\",\"Status\",\"Post\",\"Family_status\",\"Races\",\"Gender\",\"Country\"]\n",
      "df.head()\n",
      "381/43:\n",
      "column = []\n",
      "for i in range(1,15):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        column.append(str(i))\n",
      "column\n",
      "381/44:\n",
      "df = df[column]\n",
      "df.column=[\"Employe\",\"Degree\",\"Status\",\"Post\",\"Family_status\",\"Races\",\"Gender\",\"Country\"]\n",
      "df.head()\n",
      "381/45:\n",
      "df = df[column]\n",
      "df.columns=[\"Employe\",\"Degree\",\"Status\",\"Post\",\"Family_status\",\"Races\",\"Gender\",\"Country\"]\n",
      "df.head()\n",
      "381/46:\n",
      "column = []\n",
      "for i in range(1,15):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        column.append(str(i))\n",
      "column\n",
      "382/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "382/2:\n",
      "df = pd.read_csv(\"adult.csv\")\n",
      "df.head()\n",
      "382/3: df.info()\n",
      "382/4:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        print(\"Column {} {} \\n\".format(i,df[str(i)].unique()))\n",
      "382/5:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "       df[str(i)].replace(\" ?\",\"Unknown\",inplace=True)\n",
      "382/6:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        print(\"Column {} {} \\n\".format(i,df[str(i)].unique()))\n",
      "382/7:\n",
      "column = []\n",
      "for i in range(1,15):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        column.append(str(i))\n",
      "column\n",
      "382/8:\n",
      "df = df[column]\n",
      "df.columns=[\"Employe\",\"Degree\",\"Status\",\"Post\",\"Family_status\",\"Races\",\"Gender\",\"Country\"]\n",
      "df.head()\n",
      "382/9:\n",
      "for column in df.columns[:]:\n",
      "    print(\"{}, : ,{}, values\".format(column,len(df[column].unique())))\n",
      "382/10:\n",
      "for column in df.columns[:]:\n",
      "    print(\"{}, : {}, values\".format(column,len(df[column].unique())))\n",
      "382/11:\n",
      "for column in df.columns[:]:\n",
      "    print(\"{}, : {} values\".format(column,len(df[column].unique())))\n",
      "382/12:\n",
      "for column in df.columns[:]:\n",
      "    print(\"{} : {} values\".format(column,len(df[column].unique())))\n",
      "382/13: df[\"Country\"].unique()\n",
      "382/14: df[\"Country\"].value_count()\n",
      "382/15: df[\"Country\"].values_count()\n",
      "382/16: df[\"Country\"].count_values()\n",
      "382/17: df[\"Country\"].value_counts()\n",
      "382/18: dictt = df[\"Country\"].value_counts().to_dict()\n",
      "382/19: dictt\n",
      "382/20:\n",
      "dictt = df[\"Country\"].value_counts().to_dict()\n",
      "dictt\n",
      "382/21:\n",
      "df[\"Country\"] = df[\"Country\"].map(dictt)\n",
      "df.head()\n",
      "383/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "383/2:\n",
      "df = pd.read_csv(\"adult.csv\")\n",
      "df.head()\n",
      "383/3: df.info()\n",
      "383/4:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        print(\"Column {} {} \\n\".format(i,df[str(i)].unique()))\n",
      "383/5:\n",
      "for i in range(1,16):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "       df[str(i)].replace(\" ?\",\"Unknown\",inplace=True)\n",
      "383/6:\n",
      "column = []\n",
      "for i in range(1,15):\n",
      "    if df[str(i)].dtype!=\"int64\":\n",
      "        column.append(str(i))\n",
      "column\n",
      "383/7:\n",
      "df = df[column]\n",
      "df.columns=[\"Employe\",\"Degree\",\"Status\",\"Post\",\"Family_status\",\"Races\",\"Gender\",\"Country\"]\n",
      "df.head()\n",
      "383/8:\n",
      "for column in df.columns[:]:\n",
      "    print(\"{} : {} values\".format(column,len(df[column].unique())))\n",
      "383/9: df[\"Country\"].unique()\n",
      "383/10: df[\"Country\"].value_counts()\n",
      "383/11:\n",
      "dictt = df[\"Country\"].value_counts().to_dict()\n",
      "dictt\n",
      "383/12:\n",
      "df[\"Country\"] = df[\"Country\"].map(dictt)\n",
      "df.head()\n",
      "387/1:\n",
      "import pandas as pd\n",
      "df = read_csv(\"full.csv\", use)\n",
      "387/2:\n",
      "import pandas as pd\n",
      "df = read_csv(\"full.csv\")\n",
      "387/3:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\")\n",
      "387/4:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\")\n",
      "df.head()\n",
      "387/5:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Faire\"])\n",
      "df.head()\n",
      "387/6:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Fare\"])\n",
      "df.head()\n",
      "387/7: df.isnull().sum()\n",
      "387/8: df.fillna(df.median(),inplace=True)\n",
      "387/9: df.isnull().sum()\n",
      "387/10: df.head()\n",
      "388/1:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Fare\"])\n",
      "df.head()\n",
      "388/2: df.fillna(df.median(),inplace=True)\n",
      "388/3: df.head()\n",
      "388/4: from sklearn.pre\n",
      "388/5: from sklearn.preprocessing import StandardScaler\n",
      "388/6:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df = std_scaler.fit_transform(df)\n",
      "df.head()\n",
      "388/7:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df = std_scaler.fit_transform(df)\n",
      "df\n",
      "388/8:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df = std_scaler.fit_transform(df)\n",
      "pd.DataFrame(df)\n",
      "389/1:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Fare\"])\n",
      "df.head()\n",
      "389/2: df.fillna(df.median(),inplace=True)\n",
      "389/3: df.head()\n",
      "389/4:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df_scal = std_scaler.fit_transform(df)\n",
      "pd.DataFrame(df_scal)\n",
      "390/1: import matplotlib.pyplot as plt\n",
      "391/1:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Fare\"])\n",
      "df.head()\n",
      "391/2: df.fillna(df.median(),inplace=True)\n",
      "391/3: df.head()\n",
      "391/4:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df_scal = std_scaler.fit_transform(df)\n",
      "pd.DataFrame(df_scal)\n",
      "391/5: import matplotlib.pyplot as plt\n",
      "391/6: plt.hist(df[\"2\"])\n",
      "391/7: plt.hist(df[\"Fare\"],bins=50)\n",
      "391/8: plt.hist(df[\"Fare\"],bins=20)\n",
      "391/9: plt.hist(df_scal[\"Fare\"],bins=20)\n",
      "391/10: plt.hist(df_scal[2],bins=20)\n",
      "391/11: plt.hist(df_scal[\"2\"],bins=20)\n",
      "391/12: plt.hist(df_scal[:,\"2\"],bins=20)\n",
      "391/13: plt.hist(df_scal[1],bins=20)\n",
      "391/14: plt.hist(df_scal[0],bins=20)\n",
      "391/15: plt.hist(df[\"Age\"],bins=20)\n",
      "391/16: plt.hist(df[\"Age\"],bins=20,density=True)\n",
      "391/17: plt.hist(df[\"Age\"],bins=20)\n",
      "391/18:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "391/19:\n",
      "sns.distplot(df['Age'], hist = False, kde = True,\n",
      "                 kde_kws = {'shade': True, 'linewidth': 3}, \n",
      "                  label = airline)\n",
      "391/20: sns.distplot(df['Age'], hist = False, kde = True,kde_kws = {'shade': True, 'linewidth': 3})\n",
      "391/21: sns.distplot(df['Age'],bins=20, hist = True, kde = True,kde_kws = {'shade': True, 'linewidth': 3})\n",
      "391/22: sns.distplot(df['Age'],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "392/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "392/2:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Fare\"])\n",
      "df.head()\n",
      "392/3: df.fillna(df.median(),inplace=True)\n",
      "392/4: df.head()\n",
      "392/5:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df_scal = std_scaler.fit_transform(df)\n",
      "pd.DataFrame(df_scal)\n",
      "392/6: plt.hist(df[\"Age\"],bins=20)\n",
      "392/7: sns.distplot(df['Age'],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "392/8: sns.distplot(df_scal[2],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "392/9: sns.distplot(df_scal[\"2\"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "392/10: sns.distplot(df_scal[0],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "392/11: sns.distplot(df_scal[:,2],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "392/12: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "392/13: sns.distplot(df[\"Age\"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "392/14: from sklearn.preprocessing import MinMaxScaler\n",
      "392/15:\n",
      "mm_scal = MinMaxScaler()\n",
      "df_minmax = mm_scal.fit_transform(df)\n",
      "pd.DataFrame(df_minmax)\n",
      "392/16: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "392/17: from sklearn.preprocessing import RobustScaler\n",
      "392/18:\n",
      "r_scal = RobustScaler()\n",
      "df_r_scal = r_scal.fit_transform(df)\n",
      "pd.DataFrame(df_r_scal)\n",
      "df_r_scal.head()\n",
      "392/19:\n",
      "r_scal = RobustScaler()\n",
      "df_r_scal = r_scal.fit_transform(df)\n",
      "df_r_scal = pd.DataFrame(df_r_scal)\n",
      "df_r_scal.head()\n",
      "392/20:\n",
      "mm_scal = MinMaxScaler()\n",
      "df_minmax = mm_scal.fit_transform(df)\n",
      "df_minmax = pd.DataFrame(df_minmax)\n",
      "df_minmax.head()\n",
      "393/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "393/2:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Fare\"])\n",
      "df.head()\n",
      "393/3: df.fillna(df.median(),inplace=True)\n",
      "393/4: df.head()\n",
      "393/5:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df_scal = std_scaler.fit_transform(df)\n",
      "pd.DataFrame(df_scal)\n",
      "393/6: plt.hist(df[\"Age\"],bins=20)\n",
      "393/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "393/8: sns.distplot(df[\"Age\"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "393/9: from sklearn.preprocessing import MinMaxScaler\n",
      "393/10:\n",
      "mm_scal = MinMaxScaler()\n",
      "df_minmax = mm_scal.fit_transform(df)\n",
      "df_minmax = pd.DataFrame(df_minmax)\n",
      "df_minmax.head()\n",
      "393/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "394/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "394/2:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Fare\"])\n",
      "df.head()\n",
      "394/3: df.fillna(df.median(),inplace=True)\n",
      "394/4: df.head()\n",
      "394/5:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df_scal = std_scaler.fit_transform(df)\n",
      "pd.DataFrame(df_scal)\n",
      "394/6: plt.hist(df[\"Age\"],bins=20)\n",
      "394/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "394/8: sns.distplot(df[\"Age\"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "394/9: from sklearn.preprocessing import MinMaxScaler\n",
      "394/10:\n",
      "mm_scal = MinMaxScaler()\n",
      "df_minmax = mm_scal.fit_transform(df)\n",
      "pd.DataFrame(df_minmax)\n",
      "394/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "394/12: from sklearn.preprocessing import RobustScaler\n",
      "394/13:\n",
      "r_scal = RobustScaler()\n",
      "df_r_scal = r_scal.fit_transform(df)\n",
      "pd.DataFrame(df_r_scal)\n",
      "394/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "394/15: sns.distplot(df_r_scal[:,2],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "394/16: sns.distplot(df_minmax[:,2],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "394/17: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "394/18: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "395/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "395/2:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Fare\"])\n",
      "df.head()\n",
      "395/3: df.fillna(df.median(),inplace=True)\n",
      "395/4: df.head()\n",
      "395/5:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df_scal = std_scaler.fit_transform(df)\n",
      "pd.DataFrame(df_scal)\n",
      "395/6: plt.hist(df[\"Age\"],bins=20)\n",
      "395/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "395/8: sns.distplot(df[\"Age\"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "395/9: from sklearn.preprocessing import MinMaxScaler\n",
      "395/10:\n",
      "mm_scal = MinMaxScaler()\n",
      "df_minmax = mm_scal.fit_transform(df)\n",
      "pd.DataFrame(df_minmax)\n",
      "395/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "395/12: from sklearn.preprocessing import RobustScaler\n",
      "395/13:\n",
      "r_scal = RobustScaler()\n",
      "df_r_scal = r_scal.fit_transform(df)\n",
      "pd.DataFrame(df_r_scal)\n",
      "395/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "397/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "397/2:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Fare\"])\n",
      "df.head()\n",
      "397/3: df.fillna(df.median(),inplace=True)\n",
      "397/4: df.head()\n",
      "397/5:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df_scal = std_scaler.fit_transform(df)\n",
      "pd.DataFrame(df_scal)\n",
      "397/6: plt.hist(df[\"Age\"],bins=20)\n",
      "397/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "397/8: sns.distplot(df[\"Age\"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "397/9: from sklearn.preprocessing import MinMaxScaler\n",
      "397/10:\n",
      "mm_scal = MinMaxScaler()\n",
      "df_minmax = mm_scal.fit_transform(df)\n",
      "pd.DataFrame(df_minmax)\n",
      "397/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "397/12: from sklearn.preprocessing import RobustScaler\n",
      "397/13:\n",
      "r_scal = RobustScaler()\n",
      "df_r_scal = r_scal.fit_transform(df)\n",
      "pd.DataFrame(df_r_scal)\n",
      "397/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "397/15: dic = {\"Jamshedpur\":[\"ranchi\",\"jharkhand\",\"jugasalai\"]}\n",
      "397/16: dic[\"Jamshedpur\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397/17: dic[\"Jamshedpur\"][0]\n",
      "397/18: dic[\"Jamshedpur\"][1]\n",
      "397/19: dic[\"Jamshedpur\"][2]\n",
      "397/20:\n",
      "jdf = pd.read_csv(\"Book1.csv\")\n",
      "jdf.head()\n",
      "397/21:\n",
      "jdf = pd.read_csv(\"Book1.csv\",encoding= 'unicode_escape')\n",
      "jdf.head()\n",
      "397/22: jdf.to_dict()\n",
      "397/23: ddf = pd.read_excel(\"vidal-hosp-list.xlsx\")\n",
      "397/24: ddf = pd.read_csv(\"vidal-hosp-list.xlsx\")\n",
      "397/25: ddf = pd.read_csv(\"vidal-hosp-list.csv\")\n",
      "397/26:\n",
      "ddf = pd.read_csv(\"vidal-hosp-list.csv\",encoding= 'unicode_escape')\n",
      "ddf.head()\n",
      "397/27: ddf[\"State\"].Unique()\n",
      "397/28: ddf[\"State\"].Unique\n",
      "397/29: ddf[\"State\"].unique\n",
      "397/30: ddf[\"State\"].unique()\n",
      "397/31: ddf[\"State\"][\"Jharkhand\"].unique()\n",
      "397/32: ddf[\"State\"].unique()\n",
      "397/33: ddf.duplicated()\n",
      "397/34: df = ddf.duplicated()\n",
      "397/35: pd.DataFrame(df)\n",
      "397/36: ddf[ddf.duplicated()]\n",
      "398/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "398/2:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Fare\"])\n",
      "df.head()\n",
      "398/3: df.fillna(df.median(),inplace=True)\n",
      "398/4: df.head()\n",
      "398/5:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df_scal = std_scaler.fit_transform(df)\n",
      "pd.DataFrame(df_scal)\n",
      "398/6: plt.hist(df[\"Age\"],bins=20)\n",
      "398/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "398/8: sns.distplot(df[\"Age\"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "398/9: from sklearn.preprocessing import MinMaxScaler\n",
      "398/10:\n",
      "mm_scal = MinMaxScaler()\n",
      "df_minmax = mm_scal.fit_transform(df)\n",
      "pd.DataFrame(df_minmax)\n",
      "398/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "398/12: from sklearn.preprocessing import RobustScaler\n",
      "398/13:\n",
      "r_scal = RobustScaler()\n",
      "df_r_scal = r_scal.fit_transform(df)\n",
      "pd.DataFrame(df_r_scal)\n",
      "398/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "398/15: dic = {\"Jamshedpur\":[\"ranchi\",\"jharkhand\",\"jugasalai\"]}\n",
      "398/16: dic[\"Jamshedpur\"][2]\n",
      "398/17:\n",
      "jdf = pd.read_csv(\"Book1.csv\",encoding= 'unicode_escape')\n",
      "jdf.head()\n",
      "398/18:\n",
      "ddf = pd.read_csv(\"vidal-hosp-list.csv\",encoding= 'unicode_escape')\n",
      "ddf.head()\n",
      "398/19: ddf[\"State\"].unique()\n",
      "398/20:\n",
      "df = ddf[ddf.duplicated()]\n",
      "df\n",
      "398/21:\n",
      "df = ddf[ddf.duplicated(\"State\")]\n",
      "df\n",
      "398/22:\n",
      "df = ddf[ddf.duplicated(\"State\"==\"Jharkhand\")]\n",
      "df\n",
      "398/23:\n",
      "df = ddf[ddf.duplicated(\"Jharkhand\")]\n",
      "df\n",
      "398/24:\n",
      "df = ddf[ddf.duplicated(ddf[\"State\"][\"Jharkhand\"])]\n",
      "df\n",
      "398/25:\n",
      "df = ddf[ddf.duplicated(ddf[\"State\"])]\n",
      "df\n",
      "398/26:\n",
      "df = ddf[ddf.duplicated(\"State\")]\n",
      "df\n",
      "398/27: d = ddf.groupby(\"State\")\n",
      "398/28: d\n",
      "398/29:\n",
      "for stet, stt in d:\n",
      "    print(stet)\n",
      "398/30:\n",
      "for stet, stt in d:\n",
      "    print(stet)\n",
      "    print(stt)\n",
      "398/31: d.get_group(\"Jharkhand\")\n",
      "398/32: d.get_group(\"Bihar\")\n",
      "398/33: ddf = d.get_group(\"Bihar\")\n",
      "398/34: ddf.head()\n",
      "398/35: ddf[\"City\"].unique()\n",
      "398/36: ddf.to_csv(\"Bihar.csv\")\n",
      "398/37:\n",
      "ddf[\"City\"].unique()\n",
      "a = \"Jharkhand\"\n",
      "398/38: ddf.to_csv(a+\".csv\")\n",
      "398/39: import folium\n",
      "398/40: import folium\n",
      "398/41: ddf.head()\n",
      "398/42:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "map = folium.Map(location=[20.5937, 78.9629] )\n",
      "398/43: map\n",
      "398/44: map.save(\"map1.html\")\n",
      "398/45:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "map = folium.Map(location=[28.70, 77.10] )\n",
      "398/46: map.save(\"map1.html\")\n",
      "398/47:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "map = folium.Map(location=[28.70, 77.10], zoom_start=4)\n",
      "398/48: map.save(\"map1.html\")\n",
      "398/49:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "map = folium.Map(location=[28.70, 77.10], zoom_start=5)\n",
      "398/50: map.save(\"map1.html\")\n",
      "398/51:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "map = folium.Map(location=[28.70, 77.10], zoom_start=15)\n",
      "398/52: map.save(\"map1.html\")\n",
      "398/53:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "map = folium.Map(location=[28.70, 77.10], zoom_start=10)\n",
      "398/54: map.save(\"map1.html\")\n",
      "398/55:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "map = folium.Map(location=[28.70, 77.10], zoom_start=10,width=50)\n",
      "398/56: map.save(\"map1.html\")\n",
      "398/57:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "map = folium.Map(location=[28.70, 77.10], zoom_start=10,width=50%)\n",
      "398/58:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "map = folium.Map(location=[28.70, 77.10], zoom_start=10,width='50%'')\n",
      "398/59:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "map = folium.Map(location=[28.70, 77.10], zoom_start=10,width='50%')\n",
      "398/60: map.save(\"map1.html\")\n",
      "398/61:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "map = folium.Map(location=[28.70, 77.10], zoom_start=10,width='50%',height='50%')\n",
      "398/62: map.save(\"map1.html\")\n",
      "398/63:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "map = folium.Map(location=[28.70, 77.10], zoom_start=10,width='50%',height='50%',top='10%')\n",
      "398/64: map.save(\"map1.html\")\n",
      "398/65: from IPython.display import HTML\n",
      "398/66: HTML(filename='map.html')\n",
      "398/67: HTML(filename='map1.html')\n",
      "398/68:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "map = folium.Map(location=[28.70, 77.10], zoom_start=10,width='50%',height='50%',top='10%')\n",
      "398/69: map.save(\"map1.html\")\n",
      "398/70: from IPython.display import HTML\n",
      "398/71: HTML(filename='map1.html')\n",
      "398/72: map.save(\"map1.html\")\n",
      "398/73:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "map = folium.Map(location=[28.70, 77.10], zoom_start=10)\n",
      "398/74: map.save(\"map1.html\")\n",
      "398/75:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for i,j in zip(lon,lat):\n",
      "    fg.add_child(folium.Marker(location=[i,j],icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "398/76:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "map = folium.Map(location=[28.70, 77.10], zoom_start=10)\n",
      "398/77:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for i,j in zip(lon,lat):\n",
      "    fg.add_child(folium.Marker(location=[i,j],icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "398/78:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for i,j in zip(lon,lat):\n",
      "    fg.add_child(folium.Marker(location=[j,i],icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "398/79:\n",
      "lat = ddf[\"LATITUDE\"]\n",
      "lon = ddf[\"LONGITUDE\"]\n",
      "print(lat)\n",
      "map = folium.Map(location=[28.70, 77.10], zoom_start=10)\n",
      "399/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "399/2:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Fare\"])\n",
      "df.head()\n",
      "399/3: df.fillna(df.median(),inplace=True)\n",
      "399/4: df.head()\n",
      "399/5:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df_scal = std_scaler.fit_transform(df)\n",
      "pd.DataFrame(df_scal)\n",
      "399/6: plt.hist(df[\"Age\"],bins=20)\n",
      "399/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "399/8: sns.distplot(df[\"Age\"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "399/9: from sklearn.preprocessing import MinMaxScaler\n",
      "399/10:\n",
      "mm_scal = MinMaxScaler()\n",
      "df_minmax = mm_scal.fit_transform(df)\n",
      "pd.DataFrame(df_minmax)\n",
      "399/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "399/12: from sklearn.preprocessing import RobustScaler\n",
      "399/13:\n",
      "r_scal = RobustScaler()\n",
      "df_r_scal = r_scal.fit_transform(df)\n",
      "pd.DataFrame(df_r_scal)\n",
      "399/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "399/15: dic = {\"Jamshedpur\":[\"ranchi\",\"jharkhand\",\"jugasalai\"]}\n",
      "399/16: dic[\"Jamshedpur\"][2]\n",
      "399/17:\n",
      "jdf = pd.read_csv(\"Book1.csv\",encoding= 'unicode_escape')\n",
      "jdf.head()\n",
      "399/18:\n",
      "ddf = pd.read_csv(\"vidal-hosp-list.csv\",encoding= 'unicode_escape')\n",
      "ddf.head()\n",
      "399/19: ddf[\"State\"].unique()\n",
      "399/20: d = ddf.groupby(\"State\")\n",
      "399/21:\n",
      "for stet, stt in d:\n",
      "    print(stet)\n",
      "    print(stt)\n",
      "399/22: ddf = d.get_group(\"Bihar\")\n",
      "399/23:\n",
      "ddf[\"City\"].unique()\n",
      "a = \"Jharkhand\"\n",
      "399/24: ddf.to_csv(a+\".csv\")\n",
      "399/25: import folium\n",
      "399/26: ddf.head()\n",
      "399/27:\n",
      "lat = list(ddf[\"LATITUDE\"])\n",
      "lon = list(ddf[\"LONGITUDE\"])\n",
      "#print(lat)\n",
      "map = folium.Map(location=[28.70, 77.10], zoom_start=10)\n",
      "399/28:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for i,j in zip(lon,lat):\n",
      "    fg.add_child(folium.Marker(location=[i,j],icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/29: map.save(\"map1.html\")\n",
      "399/30:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for i,j in zip(lat,lon):\n",
      "    fg.add_child(folium.Marker(location=[i,j],icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/31:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for i,j in zip(lat,lon):\n",
      "    fg.add_child(folium.Marker(location=[i,j],popup=\"Hey there\",icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/32:\n",
      "lat = list(ddf[\"LATITUDE\"])\n",
      "lon = list(ddf[\"LONGITUDE\"])\n",
      "#print(lat)\n",
      "map = folium.Map(location=[25.09, 85.31], zoom_start=10)\n",
      "399/33:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for i,j in zip(lat,lon):\n",
      "    fg.add_child(folium.Marker(location=[i,j],popup=\"Hey there\",icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/34:\n",
      "lat = list(ddf[\"LATITUDE\"])\n",
      "lon = list(ddf[\"LONGITUDE\"])\n",
      "h_name = list(ddf[\"Hospital Name\"])\n",
      "#print(lat)\n",
      "map = folium.Map(location=[25.09, 85.31], zoom_start=10)\n",
      "399/35:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for i,j,k in zip(lat,lon,h_name):\n",
      "    fg.add_child(folium.Marker(location=[i,j],popup=k,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/36: ddf.head(20)\n",
      "399/37:\n",
      "lat = list(ddf[\"LATITUDE\"])\n",
      "lon = list(ddf[\"LONGITUDE\"])\n",
      "h_name = list(ddf[\"Hospital Name\"])\n",
      "#print(lat)\n",
      "map = folium.Map(location=[lat[0], lon[0]], zoom_start=10)\n",
      "399/38:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for i,j,k in zip(lat,lon,h_name):\n",
      "    fg.add_child(folium.Marker(location=[i,j],popup=k,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/39:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for i,j,k in zip(lat,lon,h_name):\n",
      "    fg.add_child(folium.Marker(location=[i,j],popup=k +str(i)+str(j),icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/40:\n",
      "lat = list(ddf[\"LATITUDE\"])\n",
      "lon = list(ddf[\"LONGITUDE\"])\n",
      "h_name = list(ddf[\"Hospital Name\"])\n",
      "print(lat[0],lon[0])\n",
      "map = folium.Map(location=[lat[0], lon[0]], zoom_start=10)\n",
      "399/41:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for i,j,k in zip(lat,lon,h_name):\n",
      "    fg.add_child(folium.Marker(location=[i,j],popup=k +\"\\n\"+str(i)+str(j),icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/42: ddf.head(2)\n",
      "399/43: h = list(ddf[\"Hospital Name\"])\n",
      "399/44:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "h\n",
      "399/45:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf[\"Contact Perso\"],ddf[\"Email\"])\n",
      "    \n",
      "dic\n",
      "399/46:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf[\"Contact Perso\",\"Email\"])\n",
      "    \n",
      "dic\n",
      "399/47:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf[\"Contact Perso\"])\n",
      "    \n",
      "dic\n",
      "399/48:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf[\"Phone No.\"])\n",
      "    \n",
      "dic\n",
      "399/49:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf[591])\n",
      "    \n",
      "dic\n",
      "399/50:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf[\"591\"])\n",
      "    \n",
      "dic\n",
      "399/51:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[591])\n",
      "    \n",
      "dic\n",
      "399/52:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[1])\n",
      "    \n",
      "dic\n",
      "399/53:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[1])\n",
      "    i+=1\n",
      "dic\n",
      "399/54:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "count=1\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[count])\n",
      "    count+=1\n",
      "dic\n",
      "399/55:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "count=1\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[1])\n",
      "    count+=1\n",
      "dic\n",
      "399/56:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "count=1\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[0])\n",
      "    count+=1\n",
      "dic\n",
      "399/57:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index])\n",
      "    \n",
      "dic\n",
      "399/58:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index])\n",
      "    index+=1\n",
      "dic\n",
      "399/59:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10]])\n",
      "    index+=1\n",
      "dic\n",
      "399/60:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10]])\n",
      "    index+=1\n",
      "399/61:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "dic\n",
      "399/62:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "dic['Bishnupur Multispeciality Hospital']\n",
      "399/63:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "dic['Bishnupur Multispeciality Hospital'][0]\n",
      "399/64:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "dic['Bishnupur Multispeciality Hospital']\n",
      "399/65:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,12,11]])\n",
      "    index+=1\n",
      "dic['Bishnupur Multispeciality Hospital']\n",
      "399/66:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "dic['Bishnupur Multispeciality Hospital']\n",
      "399/67:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "dic['Bishnupur Multispeciality Hospital']\n",
      "dic.keys\n",
      "399/68:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "dic['Bishnupur Multispeciality Hospital']\n",
      "dic.keys()\n",
      "399/69:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "dic['Bishnupur Multispeciality Hospital']\n",
      "dic.keys()\n",
      "399/70:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "dic['Bishnupur Multispeciality Hospital']\n",
      "399/71:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "dic['Bishnupur Multispeciality Hospital']\n",
      "399/72: map = folium.Map(location=[lat[0], lon[0]], zoom_start=10)\n",
      "399/73:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name + df[h_name][0] + df[h_name][1]\n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/74:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/75:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name + dic[h_name][1] \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/76:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name + str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/77:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name + str(dic[h_name][0]) + str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/78:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name+\"\\n\" +str(dic[h_name][0])\"\\n\" + str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/79:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name+\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/80:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name+\"\\n\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/81:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name+\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/82:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=str(h_name)+\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/83:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],\n",
      "                               popup=h_name +\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/84: map = folium.Map(location=[lat[0], lon[0]], zoom_start=7)\n",
      "399/85:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],\n",
      "                               popup=h_name +\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "399/86:\n",
      "states = list(ddf[\"State\"].unique())\n",
      "states\n",
      "400/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "400/2:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Fare\"])\n",
      "df.head()\n",
      "400/3: df.fillna(df.median(),inplace=True)\n",
      "400/4: df.head()\n",
      "400/5:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df_scal = std_scaler.fit_transform(df)\n",
      "pd.DataFrame(df_scal)\n",
      "400/6: plt.hist(df[\"Age\"],bins=20)\n",
      "400/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "400/8: sns.distplot(df[\"Age\"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "400/9: from sklearn.preprocessing import MinMaxScaler\n",
      "400/10:\n",
      "mm_scal = MinMaxScaler()\n",
      "df_minmax = mm_scal.fit_transform(df)\n",
      "pd.DataFrame(df_minmax)\n",
      "400/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "400/12: from sklearn.preprocessing import RobustScaler\n",
      "400/13:\n",
      "r_scal = RobustScaler()\n",
      "df_r_scal = r_scal.fit_transform(df)\n",
      "pd.DataFrame(df_r_scal)\n",
      "400/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "400/15: dic = {\"Jamshedpur\":[\"ranchi\",\"jharkhand\",\"jugasalai\"]}\n",
      "400/16: dic[\"Jamshedpur\"][2]\n",
      "400/17:\n",
      "jdf = pd.read_csv(\"Book1.csv\",encoding= 'unicode_escape')\n",
      "jdf.head()\n",
      "400/18:\n",
      "ddf = pd.read_csv(\"vidal-hosp-list.csv\",encoding= 'unicode_escape')\n",
      "ddf.head()\n",
      "400/19:\n",
      "states = list(ddf[\"State\"].unique())\n",
      "states\n",
      "400/20: d = ddf.groupby(\"State\")\n",
      "400/21:\n",
      "for stet, stt in d:\n",
      "    print(stet)\n",
      "    print(stt)\n",
      "400/22: ddf = d.get_group(\"Bihar\")\n",
      "400/23:\n",
      "ddf[\"City\"].unique()\n",
      "a = \"Jharkhand\"\n",
      "400/24: ddf.to_csv(a+\".csv\")\n",
      "400/25: import folium\n",
      "400/26: ddf.head(2)\n",
      "400/27:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "dic['Bishnupur Multispeciality Hospital']\n",
      "400/28: map = folium.Map(location=[lat[0], lon[0]], zoom_start=7)\n",
      "400/29: map = folium.Map(location=[25.45, 86.27], zoom_start=7)\n",
      "400/30:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],\n",
      "                               popup=h_name +\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "400/31:\n",
      "for state in states:\n",
      "    if state == \"Jharkhand\":\n",
      "        ddf = d.get_group(state)\n",
      "400/32:\n",
      "ddf[\"City\"].unique()\n",
      "a = \"Jharkhand\"\n",
      "400/33: ddf.to_csv(a+\".csv\")\n",
      "400/34: import folium\n",
      "400/35: ddf.head(2)\n",
      "400/36:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "dic['Bishnupur Multispeciality Hospital']\n",
      "400/37: map = folium.Map(location=[25.45, 86.27], zoom_start=7)\n",
      "400/38:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "400/39: map = folium.Map(location=[25.45, 86.27], zoom_start=7)\n",
      "400/40:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],\n",
      "                               popup=h_name +\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "400/41: ddf\n",
      "401/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "401/2:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Fare\"])\n",
      "df.head()\n",
      "401/3: df.fillna(df.median(),inplace=True)\n",
      "401/4: df.head()\n",
      "401/5:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df_scal = std_scaler.fit_transform(df)\n",
      "pd.DataFrame(df_scal)\n",
      "401/6: plt.hist(df[\"Age\"],bins=20)\n",
      "401/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "401/8: sns.distplot(df[\"Age\"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "401/9: from sklearn.preprocessing import MinMaxScaler\n",
      "401/10:\n",
      "mm_scal = MinMaxScaler()\n",
      "df_minmax = mm_scal.fit_transform(df)\n",
      "pd.DataFrame(df_minmax)\n",
      "401/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "401/12: from sklearn.preprocessing import RobustScaler\n",
      "401/13:\n",
      "r_scal = RobustScaler()\n",
      "df_r_scal = r_scal.fit_transform(df)\n",
      "pd.DataFrame(df_r_scal)\n",
      "401/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "401/15: dic = {\"Jamshedpur\":[\"ranchi\",\"jharkhand\",\"jugasalai\"]}\n",
      "401/16: dic[\"Jamshedpur\"][2]\n",
      "401/17:\n",
      "jdf = pd.read_csv(\"Book1.csv\",encoding= 'unicode_escape')\n",
      "jdf.head()\n",
      "401/18:\n",
      "ddf = pd.read_csv(\"vidal-hosp-list.csv\",encoding= 'unicode_escape')\n",
      "ddf.head()\n",
      "401/19:\n",
      "states = list(ddf[\"State\"].unique())\n",
      "states\n",
      "401/20: d = ddf.groupby(\"State\")\n",
      "401/21:\n",
      "for state in states:\n",
      "    if state == \"Jharkhand\":\n",
      "        ddf = d.get_group(state)\n",
      "401/22:\n",
      "ddf[\"City\"].unique()\n",
      "a = \"Jharkhand\"\n",
      "401/23: ddf.to_csv(a+\".csv\")\n",
      "401/24: import folium\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401/25: ddf\n",
      "401/26:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "401/27: map = folium.Map(location=[25.45, 86.27], zoom_start=7)\n",
      "401/28:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],\n",
      "                               popup=h_name +\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "401/29: map.save(\"map1.html\")\n",
      "401/30:\n",
      "for state in states:\n",
      "    if state == \"West Bengal\":\n",
      "        ddf = d.get_group(state)\n",
      "401/31: ddf[\"City\"].unique()\n",
      "401/32: ddf.to_csv(a+\".csv\")\n",
      "401/33: import folium\n",
      "401/34: ddf\n",
      "401/35:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "401/36: map = folium.Map(location=[25.45, 86.27], zoom_start=7)\n",
      "401/37:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],\n",
      "                               popup=h_name +\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "401/38:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],\n",
      "                               popup=h_name +\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "401/39: len(ddf[\"Hospital Name\"].unique())\n",
      "401/40:\n",
      "for state in states:\n",
      "    if state == \"Bihar\":\n",
      "        ddf = d.get_group(state)\n",
      "401/41: ddf[\"City\"].unique()\n",
      "401/42: ddf.to_csv(a+\".csv\")\n",
      "401/43: import folium\n",
      "401/44: len(ddf[\"Hospital Name\"].unique())\n",
      "401/45:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "401/46: map = folium.Map(location=[25.45, 86.27], zoom_start=7)\n",
      "401/47:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],\n",
      "                               popup=h_name +\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "401/48:\n",
      "for state in states:\n",
      "    if state == \"Jharkhand\":\n",
      "        ddf = d.get_group(state)\n",
      "401/49: ddf[\"City\"].unique()\n",
      "401/50: ddf.to_csv(a+\".csv\")\n",
      "401/51: import folium\n",
      "401/52: len(ddf[\"Hospital Name\"].unique())\n",
      "401/53:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "401/54: map = folium.Map(location=[25.45, 86.27], zoom_start=7)\n",
      "401/55:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],\n",
      "                               popup=h_name +\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "401/56:\n",
      "for state in states:\n",
      "    if state == \"Odisha\":\n",
      "        ddf = d.get_group(state)\n",
      "401/57: ddf[\"City\"].unique()\n",
      "401/58: ddf.to_csv(a+\".csv\")\n",
      "401/59: import folium\n",
      "401/60: len(ddf[\"Hospital Name\"].unique())\n",
      "401/61:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "401/62: map = folium.Map(location=[25.45, 86.27], zoom_start=7)\n",
      "401/63:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],\n",
      "                               popup=h_name +\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "401/64:\n",
      "for state in states:\n",
      "    if state == \"Jharkhand\":\n",
      "        ddf = d.get_group(state)\n",
      "401/65: ddf[\"City\"].unique()\n",
      "401/66: ddf.to_csv(a+\".csv\")\n",
      "401/67: import folium\n",
      "401/68: len(ddf[\"Hospital Name\"].unique())\n",
      "401/69:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "401/70: map = folium.Map(location=[25.45, 86.27], zoom_start=7)\n",
      "401/71:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],\n",
      "                               popup=h_name +\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "402/1: dic = {\"Jamshedpur\":[\"ranchi\",\"jharkhand\",\"jugasalai\"]}\n",
      "402/2: dic[\"Jamshedpur\"][2]\n",
      "402/3:\n",
      "jdf = pd.read_csv(\"Book1.csv\",encoding= 'unicode_escape')\n",
      "jdf.head()\n",
      "402/4:\n",
      "ddf = pd.read_csv(\"vidal-hosp-list.csv\",encoding= 'unicode_escape')\n",
      "ddf.head()\n",
      "402/5:\n",
      "states = list(ddf[\"State\"].unique())\n",
      "states\n",
      "402/6: d = ddf.groupby(\"State\")\n",
      "402/7:\n",
      "for state in states:\n",
      "    if state == \"Jharkhand\":\n",
      "        ddf = d.get_group(state)\n",
      "403/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "403/2:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Fare\"])\n",
      "df.head()\n",
      "403/3: df.fillna(df.median(),inplace=True)\n",
      "403/4: df.head()\n",
      "403/5:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df_scal = std_scaler.fit_transform(df)\n",
      "pd.DataFrame(df_scal)\n",
      "403/6: plt.hist(df[\"Age\"],bins=20)\n",
      "403/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "403/8: sns.distplot(df[\"Age\"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "403/9: from sklearn.preprocessing import MinMaxScaler\n",
      "403/10:\n",
      "mm_scal = MinMaxScaler()\n",
      "df_minmax = mm_scal.fit_transform(df)\n",
      "pd.DataFrame(df_minmax)\n",
      "403/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "403/12: from sklearn.preprocessing import RobustScaler\n",
      "403/13:\n",
      "r_scal = RobustScaler()\n",
      "df_r_scal = r_scal.fit_transform(df)\n",
      "pd.DataFrame(df_r_scal)\n",
      "403/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "404/1: import pandas as pd\n",
      "404/2: dic = {\"Jamshedpur\":[\"ranchi\",\"jharkhand\",\"jugasalai\"]}\n",
      "404/3: dic[\"Jamshedpur\"][2]\n",
      "404/4:\n",
      "jdf = pd.read_csv(\"Book1.csv\",encoding= 'unicode_escape')\n",
      "jdf.head()\n",
      "404/5:\n",
      "ddf = pd.read_csv(\"vidal-hosp-list.csv\",encoding= 'unicode_escape')\n",
      "ddf.head()\n",
      "404/6:\n",
      "states = list(ddf[\"State\"].unique())\n",
      "states\n",
      "404/7: d = ddf.groupby(\"State\")\n",
      "404/8:\n",
      "for state in states:\n",
      "    if state == \"Jharkhand\":\n",
      "        ddf = d.get_group(state)\n",
      "404/9: ddf[\"City\"].unique()\n",
      "404/10: ddf.to_csv(a+\".csv\")\n",
      "405/1: import pandas as pd\n",
      "405/2: dic = {\"Jamshedpur\":[\"ranchi\",\"jharkhand\",\"jugasalai\"]}\n",
      "405/3: dic[\"Jamshedpur\"][2]\n",
      "405/4:\n",
      "jdf = pd.read_csv(\"Book1.csv\",encoding= 'unicode_escape')\n",
      "jdf.head()\n",
      "405/5:\n",
      "ddf = pd.read_csv(\"vidal-hosp-list.csv\",encoding= 'unicode_escape')\n",
      "ddf.head()\n",
      "405/6:\n",
      "states = list(ddf[\"State\"].unique())\n",
      "states\n",
      "405/7: d = ddf.groupby(\"State\")\n",
      "405/8:\n",
      "for state in states:\n",
      "    if state == \"Jharkhand\":\n",
      "        ddf = d.get_group(state)\n",
      "405/9: ddf[\"City\"].unique()\n",
      "405/10: import folium\n",
      "405/11: len(ddf[\"Hospital Name\"].unique())\n",
      "405/12:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "405/13: map = folium.Map(location=[25.45, 86.27], zoom_start=7)\n",
      "405/14:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],\n",
      "                               popup=h_name +\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "403/15: 24*60\n",
      "403/16: 3000/1440\n",
      "403/17: 30*70\n",
      "406/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "406/2:\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"full.csv\",usecols=[\"Pclass\",\"Age\",\"Fare\"])\n",
      "df.head()\n",
      "406/3: df.fillna(df.median(),inplace=True)\n",
      "406/4: df.head()\n",
      "406/5:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "std_scaler = StandardScaler()\n",
      "df_scal = std_scaler.fit_transform(df)\n",
      "pd.DataFrame(df_scal)\n",
      "406/6: plt.hist(df[\"Age\"],bins=20)\n",
      "406/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "406/8: sns.distplot(df[\"Age\"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "406/9: from sklearn.preprocessing import MinMaxScaler\n",
      "406/10:\n",
      "mm_scal = MinMaxScaler()\n",
      "df_minmax = mm_scal.fit_transform(df)\n",
      "pd.DataFrame(df_minmax)\n",
      "406/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "406/12: from sklearn.preprocessing import RobustScaler\n",
      "406/13:\n",
      "r_scal = RobustScaler()\n",
      "df_r_scal = r_scal.fit_transform(df)\n",
      "pd.DataFrame(df_r_scal)\n",
      "406/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})\n",
      "406/15: 24*60\n",
      "406/16: 30*70\n",
      "407/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "407/2: df = pd.read_csv(\"full.csv\")\n",
      "407/3: df.head()\n",
      "407/4:\n",
      "df = pd.read_csv(\"full.csv\")\n",
      "df.head()\n",
      "407/5:\n",
      "df = pd.read_csv(\"full.csv\")\n",
      "df.head(5)\n",
      "407/6: df[\"Age\"].isnull().sum()\n",
      "407/7: df[\"Age\"].fillna(100)\n",
      "407/8: import seaborn as sns\n",
      "407/9: sns.distplot(df)\n",
      "407/10: sns.distplot(df[\"Age\"])\n",
      "407/11: df[\"Age\"].fillna(100,inplace=True)\n",
      "407/12: import seaborn as sns\n",
      "407/13: sns.distplot(df[\"Age\"])\n",
      "407/14: df.boxplot(column=\"Age\")\n",
      "407/15: sns.distplot(df[\"Age\"].fillna(100))\n",
      "408/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "408/2:\n",
      "df = pd.read_csv(\"full.csv\")\n",
      "df.head(5)\n",
      "408/3: df[\"Age\"].isnull().sum()\n",
      "408/4: import seaborn as sns\n",
      "408/5: sns.distplot(df[\"Age\"].fillna(100))\n",
      "408/6: df.boxplot(column=\"Age\")\n",
      "406/17:\n",
      "from flask import Flask, render_template\n",
      "from flask_googlemaps import GoogleMaps\n",
      "from flask_googlemaps import Map\n",
      "\n",
      "app = Flask(__name__, template_folder=\".\")\n",
      "GoogleMaps(app)\n",
      "\n",
      "@app.route(\"/\")\n",
      "def mapview():\n",
      "    # creating a map in the view\n",
      "    mymap = Map(\n",
      "        identifier=\"view-side\",\n",
      "        lat=37.4419,\n",
      "        lng=-122.1419,\n",
      "        markers=[(37.4419, -122.1419)]\n",
      "    )\n",
      "    sndmap = Map(\n",
      "        identifier=\"sndmap\",\n",
      "        lat=37.4419,\n",
      "        lng=-122.1419,\n",
      "        markers=[\n",
      "          {\n",
      "             'icon': 'http://maps.google.com/mapfiles/ms/icons/green-dot.png',\n",
      "             'lat': 37.4419,\n",
      "             'lng': -122.1419,\n",
      "             'infobox': \"<b>Hello World</b>\"\n",
      "          },\n",
      "          {\n",
      "             'icon': 'http://maps.google.com/mapfiles/ms/icons/blue-dot.png',\n",
      "             'lat': 37.4300,\n",
      "             'lng': -122.1400,\n",
      "             'infobox': \"<b>Hello World from other place</b>\"\n",
      "          }\n",
      "        ]\n",
      "    )\n",
      "    return render_template('example.html', mymap=mymap, sndmap=sndmap)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    app.run(debug=True)\n",
      "410/1: import pandas as pd\n",
      "410/2: dic = {\"Jamshedpur\":[\"ranchi\",\"jharkhand\",\"jugasalai\"]}\n",
      "410/3: dic[\"Jamshedpur\"][2]\n",
      "410/4:\n",
      "jdf = pd.read_csv(\"Book1.csv\",encoding= 'unicode_escape')\n",
      "jdf.head()\n",
      "410/5:\n",
      "ddf = pd.read_csv(\"vidal-hosp-list.csv\",encoding= 'unicode_escape')\n",
      "ddf.head()\n",
      "410/6:\n",
      "states = list(ddf[\"State\"].unique())\n",
      "states\n",
      "410/7: d = ddf.groupby(\"State\")\n",
      "410/8:\n",
      "for state in states:\n",
      "    if state == \"Jharkhand\":\n",
      "        ddf = d.get_group(state)\n",
      "410/9: ddf[\"City\"].unique()\n",
      "410/10: import folium\n",
      "410/11: len(ddf[\"Hospital Name\"].unique())\n",
      "410/12:\n",
      "h = list(ddf[\"Hospital Name\"])\n",
      "index=0\n",
      "dic = dict()\n",
      "for i in h:\n",
      "    dic[i] = list()\n",
      "    dic[i].extend(ddf.iloc[index,[6,10,11,12]])\n",
      "    index+=1\n",
      "410/13: map = folium.Map(location=[25.45, 86.27], zoom_start=7)\n",
      "410/14:\n",
      "fg = folium.FeatureGroup(name=\"My map\")\n",
      "for h_name in dic.keys():\n",
      "    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],\n",
      "                               popup=h_name +\"\\n\"+str(dic[h_name][0])+\"\\n\"+str(dic[h_name][1]) \n",
      "                               ,icon=folium.Icon(color=\"green\")))\n",
      "map.add_child(fg)\n",
      "410/15:\n",
      "d = ['abcdefghijklmnopqrstuvwxyz']\n",
      "alpha = list(set(d))\n",
      "alpha\n",
      "410/16:\n",
      "d = ['abcdefghijklmnopqrstuvwxyz']\n",
      "alpha = list(d.set())\n",
      "alpha\n",
      "410/17:\n",
      "d = ['abcdefghijklmnopqrstuvwxyz']\n",
      "alpha = (d.set())\n",
      "alpha\n",
      "410/18:\n",
      "d = 'abcdefghijklmnopqrstuvwxyz'\n",
      "alpha = list(d.set())\n",
      "alpha\n",
      "410/19:\n",
      "d = 'abcdefghijklmnopqrstuvwxyz'\n",
      "alpha = list(set(d))\n",
      "alpha\n",
      "410/20:\n",
      "d = 'abcdefghijklmnopqrstuvwxyz'\n",
      "alpha = list(set(d))\n",
      "alpha.sort()\n",
      "410/21:\n",
      "d = 'abcdefghijklmnopqrstuvwxyz'\n",
      "alpha = list(set(d))\n",
      "alpha.sort()\n",
      "alpha\n",
      "410/22: alpha[::-1]\n",
      "410/23: alpha[::-2]\n",
      "410/24: alpha[::-3]\n",
      "410/25: alpha[:-1]\n",
      "410/26: alpha[:-2]\n",
      "410/27: alpha[:1:-1]\n",
      "410/28: alpha[:0:-1]\n",
      "410/29: alpha[::-1]\n",
      "410/30: alpha[0::-1]\n",
      "410/31: alpha[25::-1]\n",
      "410/32: alpha[24::-1]\n",
      "410/33: alpha[::-1]\n",
      "410/34: alpha[:1:-1]\n",
      "410/35: alpha[::-1]\n",
      "410/36: alpha[::]\n",
      "410/37: mat = alpha[0::]\n",
      "410/38:\n",
      "mat = alpha[0::]\n",
      "mat\n",
      "410/39:\n",
      "mat = alpha[1::]\n",
      "mat\n",
      "410/40:\n",
      "mat = alpha[1::]\n",
      "mat = mat.append(aplha[1])\n",
      "410/41:\n",
      "mat = alpha[1::]\n",
      "mat = mat.append(alpha[1])\n",
      "410/42:\n",
      "mat = alpha[1::]\n",
      "mat = mat.append(alpha[1])\n",
      "mat\n",
      "410/43:\n",
      "mat = alpha[1::]\n",
      "mat = mat.append(alpha[1])\n",
      "mat\n",
      "410/44:\n",
      "mat = alpha[1::]\n",
      "mat = mat.append(alpha[1])\n",
      "print(mat)\n",
      "410/45:\n",
      "mat = alpha[1::]\n",
      "print(type(mat))\n",
      "mat = mat.append(alpha[1])\n",
      "410/46:\n",
      "mat = alpha[1::]\n",
      "print(type(mat))\n",
      "mat.append(alpha[1])\n",
      "mat\n",
      "410/47:\n",
      "mat = alpha[1::]\n",
      "print(type(mat))\n",
      "mat.append(alpha[0])\n",
      "mat\n",
      "410/48:\n",
      "mat = alpha[1::]\n",
      "print(type(mat))\n",
      "mat.append(alpha[0:0])\n",
      "mat\n",
      "410/49:\n",
      "mat = alpha[1::]\n",
      "print(type(mat))\n",
      "mat.append(alpha[0:])\n",
      "mat\n",
      "410/50:\n",
      "mat = alpha[1::]\n",
      "print(type(mat))\n",
      "mat.append(alpha[0:1])\n",
      "mat\n",
      "410/51:\n",
      "mat = alpha[1::]\n",
      "print(type(mat))\n",
      "mat.extend(alpha[0:1])\n",
      "mat\n",
      "410/52:\n",
      "mat = alpha[1::]\n",
      "print(type(mat))\n",
      "mat.extend(alpha[0:2])\n",
      "mat\n",
      "410/53:\n",
      "matrix = []\n",
      "\n",
      "for i in range(26):\n",
      "    a = []\n",
      "    for j in range(26):\n",
      "        a = alpha[::]\n",
      "    matrix.append(a)\n",
      "410/54: matrix\n",
      "410/55:\n",
      "for i in range(26):\n",
      "    for j in range(26):\n",
      "        print(matrix[i][j], end=\" \")\n",
      "    print()\n",
      "410/56:\n",
      "matrix = []\n",
      "\n",
      "for i in range(26):\n",
      "    a = []\n",
      "    if i!=0:\n",
      "        a = alpha[::]\n",
      "        matrix.append(a)\n",
      "    else:\n",
      "        a = alpha[i::]\n",
      "        a.extend(alpha[0:i+1])\n",
      "        matrix.append(a)\n",
      "410/57:\n",
      "for i in range(26):\n",
      "    for j in range(26):\n",
      "        print(matrix[i][j], end=\" \")\n",
      "    print()\n",
      "410/58:\n",
      "d = 'abcdefghijklmnopqrstuvwxyz'\n",
      "alpha = list(set(d))\n",
      "alpha.sort()\n",
      "alpha\n",
      "410/59:\n",
      "mat = alpha[1::]\n",
      "print(type(mat))\n",
      "mat.extend(alpha[0:2])\n",
      "mat\n",
      "410/60:\n",
      "matrix = []\n",
      "\n",
      "for i in range(26):\n",
      "    a = []\n",
      "    if i!=0:\n",
      "        a = alpha[::]\n",
      "        matrix.append(a)\n",
      "    else:\n",
      "        a = alpha[i::]\n",
      "        a.extend(alpha[0:i+1])\n",
      "        matrix.append(a)\n",
      "410/61:\n",
      "for i in range(26):\n",
      "    for j in range(26):\n",
      "        print(matrix[i][j], end=\" \")\n",
      "    print()\n",
      "410/62:\n",
      "matrix = []\n",
      "\n",
      "for i in range(26):\n",
      "    a = []\n",
      "    if i==0:\n",
      "        a = alpha[::]\n",
      "        matrix.append(a)\n",
      "    else:\n",
      "        a = alpha[i::]\n",
      "        a.extend(alpha[0:i+1])\n",
      "        matrix.append(a)\n",
      "410/63:\n",
      "for i in range(26):\n",
      "    for j in range(26):\n",
      "        print(matrix[i][j], end=\" \")\n",
      "    print()\n",
      "410/64:\n",
      "plane_text = \"we are discovered save yourself\"\n",
      "key = \"deceptive\"\n",
      "len_key = len(key)\n",
      "410/65:\n",
      "plane_text = \"we are discovered save yourself\"\n",
      "key = \"deceptive\"\n",
      "len_key = len(key)\n",
      "print(len_key)\n",
      "flag = 0\n",
      "410/66:\n",
      "d = 'abcdefghijklmnopqrstuvwxyz'\n",
      "alpha = list(set(d))\n",
      "alpha.sort()\n",
      "alpha[\"a\"].index()\n",
      "410/67:\n",
      "d = 'abcdefghijklmnopqrstuvwxyz'\n",
      "alpha = list(set(d))\n",
      "alpha.sort()\n",
      "alpha.index(\"a\")\n",
      "410/68:\n",
      "cypher_text = []\n",
      "for i in plane_text:\n",
      "    if flag==len_key:\n",
      "        flag=0\n",
      "    cypher_text.append(matrix[alpha.index[i],alpha.index[key[flag]]])\n",
      "410/69:\n",
      "cypher_text = []\n",
      "for i in plane_text:\n",
      "    if flag==len_key:\n",
      "        flag=0\n",
      "    cypher_text.append(matrix[alpha.index[i],alpha.index[key[flag]]])\n",
      "    flag+=1\n",
      "410/70:\n",
      "for i in plane_text:\n",
      "    print(alpha.index[i])\n",
      "410/71:\n",
      "for i in plane_text:\n",
      "    if i!=\" \":\n",
      "        print(alpha.index[i])\n",
      "410/72:\n",
      "for i in plane_text:\n",
      "    print(i)\n",
      "    if i!=\" \":\n",
      "        print(alpha.index[i])\n",
      "410/73:\n",
      "for i in plane_text:\n",
      "    print(type(i))\n",
      "    if i!=\" \":\n",
      "        print(alpha.index[i])\n",
      "410/74:\n",
      "for i in plane_text:\n",
      "    print(type(\" \"))\n",
      "    if i!=\" \":\n",
      "        print(alpha.index[i])\n",
      "410/75:\n",
      "for i in plane_text:\n",
      "    print(type(i))\n",
      "    if i!=\" \":\n",
      "        print(alpha.index[i])\n",
      "410/76:\n",
      "for i in plane_text:\n",
      "    print(type(i))\n",
      "    if i!=\" \":\n",
      "        print(alpha.index[\"w\"])\n",
      "410/77:\n",
      "for i in plane_text:\n",
      "    print(type(i))\n",
      "    if i!=\" \":\n",
      "        print(alpha.index(i))\n",
      "410/78:\n",
      "cypher_text = []\n",
      "for i in plane_text:\n",
      "    if i!=\" \":\n",
      "        if flag==len_key:\n",
      "            flag=0\n",
      "            cypher_text.append(matrix[alpha.index(i),alpha.index(key[flag]))\n",
      "            flag+=1\n",
      "410/79:\n",
      "cypher_text = []\n",
      "for i in plane_text:\n",
      "    if i!=\" \":\n",
      "        if flag==len_key:\n",
      "            flag=0\n",
      "            cypher_text.append(matrix[alpha.index(i),alpha.index(key[flag]])\n",
      "            flag+=1\n",
      "410/80:\n",
      "cypher_text = []\n",
      "for ele in plane_text:\n",
      "    if ele!=\" \":\n",
      "        if flag==len_key:\n",
      "            flag=0\n",
      "        i = alpha.index(ele)\n",
      "        j = alpha.index(key[flag])\n",
      "        cypher_text.append(matrix[i][j])\n",
      "        flag+=1\n",
      "410/81: cypher_text\n",
      "410/82: cypher_text = \"\".join(cypher_text)\n",
      "410/83:\n",
      "cypher_text = \"\".join(cypher_text)\n",
      "cypher_text\n",
      "414/1: df[\"Age\"].isnull().sum()\n",
      "414/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "414/3:\n",
      "df = pd.read_csv(\"full.csv\")\n",
      "df.head(5)\n",
      "414/4: df[\"Age\"].isnull().sum()\n",
      "414/5: import seaborn as sns\n",
      "414/6: sns.distplot(df[\"Age\"].fillna(100))\n",
      "414/7: df.boxplot(column=\"Age\")\n",
      "414/8:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "414/9:\n",
      "df = pd.read_csv(\"full.csv\")\n",
      "df.head(5)\n",
      "414/10: df[\"Age\"].isnull().sum()\n",
      "414/11: import seaborn as sns\n",
      "414/12: sns.distplot(df[\"Age\"].fillna(100))\n",
      "414/13: df.boxplot(column=\"Age\")\n",
      "414/14:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "414/15:\n",
      "df = pd.read_csv(\"full.csv\")\n",
      "df.head(5)\n",
      "414/16: df[\"Age\"].isnull().sum()\n",
      "414/17: import seaborn as sns\n",
      "414/18: sns.distplot(df[\"Age\"].fillna(100))\n",
      "414/19: df.boxplot(column=\"Age\")\n",
      "414/20:\n",
      "import pandas as pd\n",
      "import numpy as np\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414/21:\n",
      "df = pd.read_csv(\"full.csv\")\n",
      "df.head(5)\n",
      "414/22: df[\"Age\"].isnull().sum()\n",
      "414/23: import seaborn as sns\n",
      "414/24: sns.distplot(df[\"Age\"].fillna(100))\n",
      "414/25: df.boxplot(column=\"Age\")\n",
      "415/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "415/2:\n",
      "df = pd.read_csv(\"full.csv\")\n",
      "df.head(5)\n",
      "415/3: df[\"Age\"].isnull().sum()\n",
      "415/4: import seaborn as sns\n",
      "415/5: sns.distplot(df[\"Age\"].fillna(100))\n",
      "415/6: df.boxplot(column=\"Age\")\n",
      "415/7: sns.distplot(df[\"Age\"].fillna(100))\n",
      "416/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "416/2:\n",
      "df = pd.read_csv(\"full.csv\")\n",
      "df.head(5)\n",
      "416/3: df[\"Age\"].isnull().sum()\n",
      "416/4: import seaborn as sns\n",
      "416/5: sns.distplot(df[\"Age\"].fillna(100))\n",
      "416/6: df.boxplot(column=\"Age\")\n",
      "417/1: import this\n",
      "420/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "420/2: warnings.filterwarnings(\"ignore\")\n",
      "420/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "420/4: data.head()\n",
      "420/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "420/6:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "420/7: data['Insulin'].replace(0, np.nan, inplace=True)\n",
      "420/8: data.isnull().sum()\n",
      "420/9: data.head()\n",
      "420/10: data[\"Insulin\"].fillna(value=data[\"Insulin\"].median(), inplace=True)\n",
      "420/11: data.info()\n",
      "420/12: data.head()\n",
      "420/13: data.describe()\n",
      "420/14:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "420/15:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "420/16: #sns.pairplot(data, hue=\"Outcome\")\n",
      "420/17: data.corr()\n",
      "420/18:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "420/19:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "420/20:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "420/21:\n",
      "impute = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
      "x_data = impute.fit_transform(x_data)\n",
      "x_data\n",
      "420/22:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "420/23:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "420/24:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "420/25:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "420/26:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "420/27:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "420/28:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "420/29:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Hyperparameter optimization    \n",
      "    \n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "420/30:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "420/31:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "420/32: rf_random.fit(std_x_train,y_train)\n",
      "420/33: rf_random.best_params_\n",
      "420/34: predictions=rf_random.predict(std_x_test)\n",
      "420/35: accuracy_score(y_test,predictions)\n",
      "420/36:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "420/37:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "420/38:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "420/39:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "421/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "421/2: warnings.filterwarnings(\"ignore\")\n",
      "421/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "421/4: data.head()\n",
      "421/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "421/6:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "421/7: data['Insulin'].replace(0, np.nan, inplace=True)\n",
      "421/8: data.isnull().sum()\n",
      "421/9: data.head()\n",
      "421/10: data[\"Insulin\"].fillna(value=data[\"Insulin\"].median(), inplace=True)\n",
      "421/11: data.info()\n",
      "421/12: data.head()\n",
      "421/13: data.describe()\n",
      "421/14:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "421/15:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "421/16: #sns.pairplot(data, hue=\"Outcome\")\n",
      "421/17: data.corr()\n",
      "421/18:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "421/19:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "421/20:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "421/21:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "421/22:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "421/23:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "421/24:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "421/25:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "421/26:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "421/27:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "421/28:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Hyperparameter optimization    \n",
      "    \n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "421/29:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "421/30:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "421/31: rf_random.fit(std_x_train,y_train)\n",
      "421/32: rf_random.best_params_\n",
      "421/33: predictions=rf_random.predict(std_x_test)\n",
      "421/34: accuracy_score(y_test,predictions)\n",
      "421/35:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "421/36:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "421/37:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "421/38:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "421/39:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3, random_state=42)\n",
      "x_train\n",
      "421/40:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.3, random_state=42)\n",
      "std_x_train\n",
      "421/41:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "421/42:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "421/43:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "421/44:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "421/45:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Hyperparameter optimization    \n",
      "    \n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "421/46:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "421/47:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "421/48: rf_random.fit(std_x_train,y_train)\n",
      "421/49: rf_random.best_params_\n",
      "421/50: predictions=rf_random.predict(std_x_test)\n",
      "421/51: accuracy_score(y_test,predictions)\n",
      "421/52:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "421/53:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "421/54:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "421/55:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "421/56:\n",
      "data['Insulin'].replace(0, np.nan, inplace=True)\n",
      "data['SkinThickness'].replace(0, np.nan, inplace=True)\n",
      "421/57: data = pd.read_csv(\"diabetes.csv\")\n",
      "421/58: data.head()\n",
      "421/59:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "421/60:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "421/61:\n",
      "data['Insulin'].replace(0, np.nan, inplace=True)\n",
      "data['SkinThickness'].replace(0, np.nan, inplace=True)\n",
      "421/62: data.isnull().sum()\n",
      "421/63:\n",
      "columns_name = data.columns\n",
      "columns_name\n",
      "421/64:\n",
      "columns_name = data.columns\n",
      "columns_name[0]\n",
      "421/65:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    print(data[ele].unique())\n",
      "421/66:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    print(ele,data[ele].unique())\n",
      "421/67:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    print(ele,\"/n\",data[ele].unique())\n",
      "421/68:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    print(ele,\"\\n\",data[ele].unique())\n",
      "421/69:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    print(data[ele].unique().count(0))\n",
      "421/70:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    print(ele,\"\\n\",data[ele].unique())\n",
      "421/71: data = pd.read_csv(\"diabetes.csv\")\n",
      "421/72: data.head()\n",
      "421/73:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "421/74:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "421/75:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    print(ele,\"\\n\",data[ele].unique())\n",
      "421/76:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    if 0 in data[ele].unique():\n",
      "        print(ele,\":\",\"True\")\n",
      "421/77:\n",
      "for ele in columns_name:\n",
      "    if ele!=\"Pregnancies\" or ele!=\"Outcome\":\n",
      "        data[ele].replace(0, np.nan, inplace=True)\n",
      "421/78: data.isnull().sum()\n",
      "421/79:\n",
      "for ele in columns_name:\n",
      "    if ele!=\"Pregnancies\" and ele!=\"Outcome\":\n",
      "        data[ele].replace(0, np.nan, inplace=True)\n",
      "421/80: data.isnull().sum()\n",
      "421/81: data = pd.read_csv(\"diabetes.csv\")\n",
      "421/82: data.head()\n",
      "421/83:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "421/84:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "421/85:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    if 0 in data[ele].unique():\n",
      "        print(ele,\":\",\"True\")\n",
      "421/86:\n",
      "for ele in columns_name:\n",
      "    if ele!=\"Pregnancies\" and ele!=\"Outcome\":\n",
      "        data[ele].replace(0, np.nan, inplace=True)\n",
      "421/87: data.isnull().sum()\n",
      "421/88: data.head()\n",
      "421/89:\n",
      "for ele in columns_name:\n",
      "    data[ele].fillna(value=data[\"Insulin\"].median(), inplace=True)\n",
      "421/90: data.head()\n",
      "421/91: data.info()\n",
      "421/92: data.head()\n",
      "421/93: data.describe()\n",
      "421/94:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "421/95:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "421/96: #sns.pairplot(data, hue=\"Outcome\")\n",
      "421/97: data.corr()\n",
      "421/98:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "421/99:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "421/100:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "421/101:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "421/102:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3, random_state=42)\n",
      "x_train\n",
      "421/103:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.3, random_state=42)\n",
      "std_x_train\n",
      "421/104:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "421/105:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "421/106:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "421/107:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "422/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "422/2: warnings.filterwarnings(\"ignore\")\n",
      "422/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "422/4: data.head()\n",
      "422/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "422/6:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "422/7:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    if 0 in data[ele].unique():\n",
      "        print(ele,\":\",\"True\")\n",
      "422/8:\n",
      "for ele in columns_name:\n",
      "    if ele!=\"Pregnancies\" and ele!=\"Outcome\":\n",
      "        data[ele].replace(0, np.nan, inplace=True)\n",
      "422/9: data.isnull().sum()\n",
      "422/10: data.head()\n",
      "422/11:\n",
      "for ele in columns_name:\n",
      "    data[ele].fillna(value=data[ele].median(), inplace=True)\n",
      "422/12: data.info()\n",
      "422/13: data.head()\n",
      "422/14: data.describe()\n",
      "422/15:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "422/16:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "422/17: #sns.pairplot(data, hue=\"Outcome\")\n",
      "422/18: data.corr()\n",
      "422/19:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "422/20:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "422/21:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "422/22:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "422/23:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3, random_state=42)\n",
      "x_train\n",
      "422/24:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.3, random_state=42)\n",
      "std_x_train\n",
      "422/25:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "422/26:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "422/27:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "422/28:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "422/29:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Hyperparameter optimization    \n",
      "    \n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "422/30:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "422/31:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "422/32: rf_random.fit(std_x_train,y_train)\n",
      "422/33: rf_random.best_params_\n",
      "422/34: predictions=rf_random.predict(std_x_test)\n",
      "422/35: accuracy_score(y_test,predictions)\n",
      "422/36:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "422/37:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "422/38:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "422/39:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "422/40: sns.boxplot(data[\"Age\"])\n",
      "422/41: data[\"Age\"].unique()\n",
      "422/42: sns.boxplot(data[\"SkinThickness\"])\n",
      "423/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "423/2: warnings.filterwarnings(\"ignore\")\n",
      "423/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "423/4: data.head()\n",
      "423/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "423/6:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "423/7:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    if 0 in data[ele].unique():\n",
      "        print(ele,\":\",\"True\")\n",
      "423/8:\n",
      "for ele in columns_name:\n",
      "    if ele!=\"Pregnancies\" and ele!=\"Outcome\":\n",
      "        data[ele].replace(0, np.nan, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423/9: data.isnull().sum()\n",
      "423/10: data.head()\n",
      "423/11:\n",
      "for ele in columns_name:\n",
      "    data[ele].fillna(value=data[ele].median(), inplace=True)\n",
      "423/12: data[\"Age\"].unique()\n",
      "423/13: sns.boxplot(data[\"SkinThickness\"])\n",
      "423/14: data.info()\n",
      "423/15: data.head()\n",
      "423/16: data.describe()\n",
      "423/17:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "423/18:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "423/19: #sns.pairplot(data, hue=\"Outcome\")\n",
      "423/20: data.corr()\n",
      "423/21:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "423/22:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "423/23:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "423/24:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "423/25:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3, random_state=42)\n",
      "x_train\n",
      "423/26:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.3, random_state=42)\n",
      "std_x_train\n",
      "423/27:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "423/28:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "423/29:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "423/30:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "423/31:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Hyperparameter optimization    \n",
      "    \n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "423/32:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "423/33:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "423/34: rf_random.fit(std_x_train,y_train)\n",
      "423/35: rf_random.best_params_\n",
      "423/36: predictions=rf_random.predict(std_x_test)\n",
      "423/37: accuracy_score(y_test,predictions)\n",
      "423/38:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "423/39:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "423/40:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "423/41:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "424/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "424/2: warnings.filterwarnings(\"ignore\")\n",
      "424/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "424/4: data.head()\n",
      "424/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "424/6:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "424/7:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    if 0 in data[ele].unique():\n",
      "        print(ele,\":\",\"True\")\n",
      "424/8:\n",
      "for ele in columns_name:\n",
      "    if ele!=\"Pregnancies\" and ele!=\"Outcome\":\n",
      "        data[ele].replace(0, np.nan, inplace=True)\n",
      "424/9: data.isnull().sum()\n",
      "424/10: data.head()\n",
      "424/11:\n",
      "for ele in columns_name:\n",
      "    data[ele].fillna(value=data[ele].median(), inplace=True)\n",
      "424/12: data[\"Age\"].unique()\n",
      "424/13: sns.boxplot(data[\"SkinThickness\"])\n",
      "424/14: data.info()\n",
      "424/15: data.head()\n",
      "424/16: data.describe()\n",
      "424/17:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "424/18:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "424/19: #sns.pairplot(data, hue=\"Outcome\")\n",
      "424/20: data.corr()\n",
      "424/21:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "424/22:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "424/23:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "424/24:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "424/25:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3, random_state=42)\n",
      "x_train\n",
      "424/26:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.3, random_state=42)\n",
      "std_x_train\n",
      "424/27:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "424/28:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "424/29:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "424/30:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "424/31:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Hyperparameter optimization    \n",
      "    \n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "424/32:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "424/33:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "424/34: rf_random.fit(std_x_train,y_train)\n",
      "424/35: rf_random.best_params_\n",
      "424/36: predictions=rf_random.predict(std_x_test)\n",
      "424/37: accuracy_score(y_test,predictions)\n",
      "424/38:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "424/39:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "424/40:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "424/41:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "424/42:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "data[\"Age\"].plot(kind = \"kde\", ax = ax)\n",
      "\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "424/43:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "data[\"SkinThickness\"].plot(kind = \"kde\", ax = ax)\n",
      "\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "424/44:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "data[\"Glucose\"].plot(kind = \"kde\", ax = ax)\n",
      "\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "425/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "425/2: warnings.filterwarnings(\"ignore\")\n",
      "425/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "425/4: data.head()\n",
      "425/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "425/6:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "425/7:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    if 0 in data[ele].unique():\n",
      "        print(ele,\":\",\"True\")\n",
      "425/8:\n",
      "for ele in columns_name:\n",
      "    if ele!=\"Pregnancies\" and ele!=\"Outcome\":\n",
      "        data[ele].replace(0, np.nan, inplace=True)\n",
      "425/9: data.isnull().sum()\n",
      "425/10: data.head()\n",
      "425/11:\n",
      "for ele in columns_name:\n",
      "    data[ele].fillna(value=data[ele].median(), inplace=True)\n",
      "425/12: data[\"Age\"].unique()\n",
      "425/13:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "data[\"Glucose\"].plot(kind = \"kde\", ax = ax)\n",
      "\n",
      "line, labels = ax.get_legend_handles_labels()\n",
      "ax.legend(line,labels,loc = \"best\")\n",
      "425/14: data.info()\n",
      "425/15: data.head()\n",
      "425/16: data.describe()\n",
      "425/17:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "425/18:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "425/19: #sns.pairplot(data, hue=\"Outcome\")\n",
      "425/20: data.corr()\n",
      "425/21:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "425/22:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "425/23:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "425/24:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "425/25:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "425/26:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "425/27:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "425/28:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "425/29:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "425/30:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "425/31:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Hyperparameter optimization    \n",
      "    \n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "425/32:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "425/33:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "425/34: rf_random.fit(std_x_train,y_train)\n",
      "425/35: rf_random.best_params_\n",
      "425/36: predictions=rf_random.predict(std_x_test)\n",
      "425/37: accuracy_score(y_test,predictions)\n",
      "425/38:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "425/39:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "425/40:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "425/41:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "425/42:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "425/43:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(9,9))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "425/44:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "425/45:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(9,9))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "425/46:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "425/47:\n",
      "from sklearn.utils import resample\n",
      "\n",
      "df_majority = data.loc[data.Outcome == 0].copy()\n",
      "df_minority = data.loc[data.Outcome == 1].copy()\n",
      "df_minority_upsampled = resample(df_minority,\n",
      "                             replace=True,  # sample with replacement\n",
      "                            n_samples=500,  # to match majority class\n",
      "                            random_state=123) \n",
      "data = pd.concat([df_majority, df_minority_upsampled])\n",
      "425/48:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "425/49: #### Now our data set look balanced\n",
      "425/50:\n",
      "data1=data.drop('Outcome',axis=1)\n",
      "data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))\n",
      "plt.show()\n",
      "425/51:\n",
      "for ele in columns_name:\n",
      "    data[ele].fillna(value=data[ele].mean(), inplace=True)\n",
      "426/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "426/2: warnings.filterwarnings(\"ignore\")\n",
      "426/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "426/4: data.head()\n",
      "426/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "426/6:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "426/7:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    if 0 in data[ele].unique():\n",
      "        print(ele,\":\",\"True\")\n",
      "426/8:\n",
      "for ele in columns_name:\n",
      "    if ele!=\"Pregnancies\" and ele!=\"Outcome\":\n",
      "        data[ele].replace(0, np.nan, inplace=True)\n",
      "426/9: data.isnull().sum()\n",
      "426/10: data.head()\n",
      "426/11:\n",
      "for ele in columns_name:\n",
      "    data[ele].fillna(value=data[ele].mean(), inplace=True)\n",
      "426/12: data[\"Age\"].unique()\n",
      "426/13:\n",
      "# import matplotlib.pyplot as plt\n",
      "\n",
      "# fig = plt.figure()\n",
      "# ax = fig.add_subplot(111)\n",
      "# data[\"Glucose\"].plot(kind = \"kde\", ax = ax)\n",
      "\n",
      "# line, labels = ax.get_legend_handles_labels()\n",
      "# ax.legend(line,labels,loc = \"best\")\n",
      "426/14: data.info()\n",
      "426/15: data.head()\n",
      "426/16: data.describe()\n",
      "426/17:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "426/18:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "426/19:\n",
      "from sklearn.utils import resample\n",
      "\n",
      "df_majority = data.loc[data.Outcome == 0].copy()\n",
      "df_minority = data.loc[data.Outcome == 1].copy()\n",
      "df_minority_upsampled = resample(df_minority,\n",
      "                             replace=True,  # sample with replacement\n",
      "                            n_samples=500,  # to match majority class\n",
      "                            random_state=123) \n",
      "data = pd.concat([df_majority, df_minority_upsampled])\n",
      "426/20:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "426/21:\n",
      "data1=data.drop('Outcome',axis=1)\n",
      "data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))\n",
      "plt.show()\n",
      "426/22:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "426/23: #sns.pairplot(data, hue=\"Outcome\")\n",
      "426/24: data.corr()\n",
      "426/25:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "426/26:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "426/27:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "426/28:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "426/29:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "426/30:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "426/31:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "426/32:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "426/33:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "426/34:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "426/35:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Hyperparameter optimization    \n",
      "    \n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "426/36:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "426/37:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "426/38: rf_random.fit(std_x_train,y_train)\n",
      "426/39: rf_random.best_params_\n",
      "426/40: predictions=rf_random.predict(std_x_test)\n",
      "426/41: accuracy_score(y_test,predictions)\n",
      "426/42:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "426/43:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "426/44:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "426/45:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "426/46:\n",
      "from sklearn.ensemble import IsolationForest\n",
      "from collections import Counter\n",
      "rs=np.random.RandomState(0)\n",
      "clf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) \n",
      "clf.fit(data)\n",
      "y_pred_train = clf.predict(data)\n",
      "sayÄ± = Counter(y_pred_train)\n",
      "print(sayÄ±)\n",
      "426/47:\n",
      "from collections import Counter\n",
      "def detect_outliers(data,features):\n",
      "    outlier_indices = []\n",
      "    for c in features:\n",
      "        # 1st quartile\n",
      "        Q1 = np.percentile(data[c],25)\n",
      "        # 3rd quartile\n",
      "        Q3 = np.percentile(data[c],75)\n",
      "        # IQR\n",
      "        IQR = Q3 - Q1\n",
      "        # Outlier step\n",
      "        outlier_step = IQR * 1.5\n",
      "        # detect outlier and their indeces\n",
      "        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index\n",
      "        # store indeces\n",
      "        outlier_indices.extend(outlier_list_col)\n",
      "    \n",
      "    outlier_indices = Counter(outlier_indices)\n",
      "    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n",
      "    \n",
      "    return multiple_outliers\n",
      "426/48:\n",
      "data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age'])]\n",
      "426/49: multiple_outliers\n",
      "426/50:\n",
      "from collections import Counter\n",
      "def detect_outliers(data,features):\n",
      "    outlier_indices = []\n",
      "    for c in features:\n",
      "        # 1st quartile\n",
      "        Q1 = np.percentile(data[c],25)\n",
      "        # 3rd quartile\n",
      "        Q3 = np.percentile(data[c],75)\n",
      "        # IQR\n",
      "        IQR = Q3 - Q1\n",
      "        # Outlier step\n",
      "        outlier_step = IQR * 1.5\n",
      "        # detect outlier and their indeces\n",
      "        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index\n",
      "        # store indeces\n",
      "        outlier_indices.extend(outlier_list_col)\n",
      "    \n",
      "    outlier_indices = Counter(outlier_indices)\n",
      "    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n",
      "    print(type(multiple_outliers))\n",
      "    return multiple_outliers\n",
      "426/51:\n",
      "data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age'])]\n",
      "426/52:\n",
      "from collections import Counter\n",
      "def detect_outliers(data,features):\n",
      "    outlier_indices = []\n",
      "    for c in features:\n",
      "        # 1st quartile\n",
      "        Q1 = np.percentile(data[c],25)\n",
      "        # 3rd quartile\n",
      "        Q3 = np.percentile(data[c],75)\n",
      "        # IQR\n",
      "        IQR = Q3 - Q1\n",
      "        # Outlier step\n",
      "        outlier_step = IQR * 1.5\n",
      "        # detect outlier and their indeces\n",
      "        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index\n",
      "        # store indeces\n",
      "        outlier_indices.extend(outlier_list_col)\n",
      "    \n",
      "    outlier_indices = Counter(outlier_indices)\n",
      "    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n",
      "    print(type(multiple_outliers))\n",
      "    print(multiple_outliers)\n",
      "    return multiple_outliers\n",
      "426/53:\n",
      "data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age'])]\n",
      "426/54:\n",
      "from collections import Counter\n",
      "def detect_outliers(data,features):\n",
      "    outlier_indices = []\n",
      "    for c in features:\n",
      "        # 1st quartile\n",
      "        Q1 = np.percentile(data[c],25)\n",
      "        # 3rd quartile\n",
      "        Q3 = np.percentile(data[c],75)\n",
      "        # IQR\n",
      "        IQR = Q3 - Q1\n",
      "        # Outlier step\n",
      "        outlier_step = IQR * 1.5\n",
      "        # detect outlier and their indeces\n",
      "        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index\n",
      "        # store indeces\n",
      "        outlier_indices.extend(outlier_list_col)\n",
      "    \n",
      "    outlier_indices = Counter(outlier_indices)\n",
      "    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n",
      "\n",
      "    print(multiple_outliers)\n",
      "    return multiple_outliers\n",
      "426/55:\n",
      "data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age'])]\n",
      "426/56:\n",
      "data = data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, \n",
      "                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,\n",
      "                  748, 655, 584, 485, 618, 661, 45, 243])\n",
      "426/57:\n",
      "data1=data.drop('Outcome',axis=1)\n",
      "data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))\n",
      "plt.show()\n",
      "426/58:\n",
      "data = data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, \n",
      "                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,\n",
      "                  748, 655, 584, 485, 618, 661, 45, 243], axis=0)\n",
      "427/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "427/2: warnings.filterwarnings(\"ignore\")\n",
      "427/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "427/4: data.head()\n",
      "427/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "427/6:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "427/7:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    if 0 in data[ele].unique():\n",
      "        print(ele,\":\",\"True\")\n",
      "427/8:\n",
      "for ele in columns_name:\n",
      "    if ele!=\"Pregnancies\" and ele!=\"Outcome\":\n",
      "        data[ele].replace(0, np.nan, inplace=True)\n",
      "427/9: data.isnull().sum()\n",
      "427/10: data.head()\n",
      "427/11:\n",
      "for ele in columns_name:\n",
      "    data[ele].fillna(value=data[ele].mean(), inplace=True)\n",
      "427/12: data[\"Age\"].unique()\n",
      "427/13:\n",
      "# import matplotlib.pyplot as plt\n",
      "\n",
      "# fig = plt.figure()\n",
      "# ax = fig.add_subplot(111)\n",
      "# data[\"Glucose\"].plot(kind = \"kde\", ax = ax)\n",
      "\n",
      "# line, labels = ax.get_legend_handles_labels()\n",
      "# ax.legend(line,labels,loc = \"best\")\n",
      "427/14: data.info()\n",
      "427/15: data.head()\n",
      "427/16: data.describe()\n",
      "427/17:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "427/18:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "427/19:\n",
      "from sklearn.utils import resample\n",
      "\n",
      "df_majority = data.loc[data.Outcome == 0].copy()\n",
      "df_minority = data.loc[data.Outcome == 1].copy()\n",
      "df_minority_upsampled = resample(df_minority,\n",
      "                             replace=True,  # sample with replacement\n",
      "                            n_samples=500,  # to match majority class\n",
      "                            random_state=123) \n",
      "data = pd.concat([df_majority, df_minority_upsampled])\n",
      "427/20:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "427/21:\n",
      "data1=data.drop('Outcome',axis=1)\n",
      "data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))\n",
      "plt.show()\n",
      "427/22:\n",
      "from sklearn.ensemble import IsolationForest\n",
      "from collections import Counter\n",
      "rs=np.random.RandomState(0)\n",
      "clf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) \n",
      "clf.fit(data)\n",
      "y_pred_train = clf.predict(data)\n",
      "sayÄ± = Counter(y_pred_train)\n",
      "print(sayÄ±)\n",
      "\n",
      "## -1 = Outliers\n",
      "427/23:\n",
      "from collections import Counter\n",
      "def detect_outliers(data,features):\n",
      "    outlier_indices = []\n",
      "    for c in features:\n",
      "        # 1st quartile\n",
      "        Q1 = np.percentile(data[c],25)\n",
      "        # 3rd quartile\n",
      "        Q3 = np.percentile(data[c],75)\n",
      "        # IQR\n",
      "        IQR = Q3 - Q1\n",
      "        # Outlier step\n",
      "        outlier_step = IQR * 1.5\n",
      "        # detect outlier and their indeces\n",
      "        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index\n",
      "        # store indeces\n",
      "        outlier_indices.extend(outlier_list_col)\n",
      "    \n",
      "    outlier_indices = Counter(outlier_indices)\n",
      "    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n",
      "\n",
      "    print(multiple_outliers)\n",
      "    return multiple_outliers\n",
      "427/24:\n",
      "data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age'])]\n",
      "427/25:\n",
      "data = data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, \n",
      "                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,\n",
      "                  748, 655, 584, 485, 618, 661, 45, 243], axis=0)\n",
      "427/26:\n",
      "data1=data.drop('Outcome',axis=1)\n",
      "data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))\n",
      "plt.show()\n",
      "427/27:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "427/28: #sns.pairplot(data, hue=\"Outcome\")\n",
      "427/29: data.corr()\n",
      "427/30:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "427/31:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "427/32:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "427/33:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "427/34:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "427/35:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "427/36:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "427/37:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "427/38:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "427/39:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "427/40:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Hyperparameter optimization    \n",
      "    \n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "427/41:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "427/42:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "427/43: rf_random.fit(std_x_train,y_train)\n",
      "427/44: rf_random.best_params_\n",
      "427/45: predictions=rf_random.predict(std_x_test)\n",
      "427/46: accuracy_score(y_test,predictions)\n",
      "427/47:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "427/48:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "427/49:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "427/50:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "427/51: data.shape()\n",
      "428/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "428/2: warnings.filterwarnings(\"ignore\")\n",
      "428/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "428/4: data.head()\n",
      "428/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "428/6:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "428/7:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    if 0 in data[ele].unique():\n",
      "        print(ele,\":\",\"True\")\n",
      "428/8:\n",
      "for ele in columns_name:\n",
      "    if ele!=\"Pregnancies\" and ele!=\"Outcome\":\n",
      "        data[ele].replace(0, np.nan, inplace=True)\n",
      "428/9: data.isnull().sum()\n",
      "428/10: data.head()\n",
      "428/11:\n",
      "for ele in columns_name:\n",
      "    data[ele].fillna(value=data[ele].mean(), inplace=True)\n",
      "428/12: data[\"Age\"].unique()\n",
      "428/13:\n",
      "# import matplotlib.pyplot as plt\n",
      "\n",
      "# fig = plt.figure()\n",
      "# ax = fig.add_subplot(111)\n",
      "# data[\"Glucose\"].plot(kind = \"kde\", ax = ax)\n",
      "\n",
      "# line, labels = ax.get_legend_handles_labels()\n",
      "# ax.legend(line,labels,loc = \"best\")\n",
      "428/14: data.info()\n",
      "428/15: data.head()\n",
      "428/16: data.describe()\n",
      "428/17:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "428/18:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "428/19:\n",
      "from sklearn.utils import resample\n",
      "\n",
      "df_majority = data.loc[data.Outcome == 0].copy()\n",
      "df_minority = data.loc[data.Outcome == 1].copy()\n",
      "df_minority_upsampled = resample(df_minority,\n",
      "                             replace=True,  # sample with replacement\n",
      "                            n_samples=500,  # to match majority class\n",
      "                            random_state=123) \n",
      "data = pd.concat([df_majority, df_minority_upsampled])\n",
      "428/20:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "428/21:\n",
      "data1=data.drop('Outcome',axis=1)\n",
      "data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))\n",
      "plt.show()\n",
      "428/22:\n",
      "from sklearn.ensemble import IsolationForest\n",
      "from collections import Counter\n",
      "rs=np.random.RandomState(0)\n",
      "clf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) \n",
      "clf.fit(data)\n",
      "y_pred_train = clf.predict(data)\n",
      "sayÄ± = Counter(y_pred_train)\n",
      "print(sayÄ±)\n",
      "\n",
      "## -1 = Outliers\n",
      "428/23:\n",
      "from collections import Counter\n",
      "def detect_outliers(data,features):\n",
      "    outlier_indices = []\n",
      "    for c in features:\n",
      "        # 1st quartile\n",
      "        Q1 = np.percentile(data[c],25)\n",
      "        # 3rd quartile\n",
      "        Q3 = np.percentile(data[c],75)\n",
      "        # IQR\n",
      "        IQR = Q3 - Q1\n",
      "        # Outlier step\n",
      "        outlier_step = IQR * 1.5\n",
      "        # detect outlier and their indeces\n",
      "        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index\n",
      "        # store indeces\n",
      "        outlier_indices.extend(outlier_list_col)\n",
      "    \n",
      "    outlier_indices = Counter(outlier_indices)\n",
      "    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n",
      "\n",
      "    print(multiple_outliers)\n",
      "    return multiple_outliers\n",
      "428/24:\n",
      "data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age'])]\n",
      "428/25:\n",
      "data = data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, \n",
      "                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,\n",
      "                  748, 655, 584, 485, 618, 661, 45, 243], axis=0)\n",
      "428/26: data.shape()\n",
      "428/27:\n",
      "data = data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, \n",
      "                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,\n",
      "                  748, 655, 584, 485, 618, 661, 45, 243], axis=0,inplace=True)\n",
      "429/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "429/2: warnings.filterwarnings(\"ignore\")\n",
      "429/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "429/4: data.head()\n",
      "429/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "429/6:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "429/7:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    if 0 in data[ele].unique():\n",
      "        print(ele,\":\",\"True\")\n",
      "429/8:\n",
      "for ele in columns_name:\n",
      "    if ele!=\"Pregnancies\" and ele!=\"Outcome\":\n",
      "        data[ele].replace(0, np.nan, inplace=True)\n",
      "429/9: data.isnull().sum()\n",
      "429/10: data.head()\n",
      "429/11:\n",
      "for ele in columns_name:\n",
      "    data[ele].fillna(value=data[ele].mean(), inplace=True)\n",
      "429/12: data[\"Age\"].unique()\n",
      "429/13:\n",
      "# import matplotlib.pyplot as plt\n",
      "\n",
      "# fig = plt.figure()\n",
      "# ax = fig.add_subplot(111)\n",
      "# data[\"Glucose\"].plot(kind = \"kde\", ax = ax)\n",
      "\n",
      "# line, labels = ax.get_legend_handles_labels()\n",
      "# ax.legend(line,labels,loc = \"best\")\n",
      "429/14: data.info()\n",
      "429/15: data.head()\n",
      "429/16: data.describe()\n",
      "429/17:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "429/18:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "429/19:\n",
      "from sklearn.utils import resample\n",
      "\n",
      "df_majority = data.loc[data.Outcome == 0].copy()\n",
      "df_minority = data.loc[data.Outcome == 1].copy()\n",
      "df_minority_upsampled = resample(df_minority,\n",
      "                             replace=True,  # sample with replacement\n",
      "                            n_samples=500,  # to match majority class\n",
      "                            random_state=123) \n",
      "data = pd.concat([df_majority, df_minority_upsampled])\n",
      "429/20:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "429/21:\n",
      "data1=data.drop('Outcome',axis=1)\n",
      "data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))\n",
      "plt.show()\n",
      "429/22:\n",
      "from sklearn.ensemble import IsolationForest\n",
      "from collections import Counter\n",
      "rs=np.random.RandomState(0)\n",
      "clf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) \n",
      "clf.fit(data)\n",
      "y_pred_train = clf.predict(data)\n",
      "sayÄ± = Counter(y_pred_train)\n",
      "print(sayÄ±)\n",
      "\n",
      "## -1 = Outliers\n",
      "429/23:\n",
      "from collections import Counter\n",
      "def detect_outliers(data,features):\n",
      "    outlier_indices = []\n",
      "    for c in features:\n",
      "        # 1st quartile\n",
      "        Q1 = np.percentile(data[c],25)\n",
      "        # 3rd quartile\n",
      "        Q3 = np.percentile(data[c],75)\n",
      "        # IQR\n",
      "        IQR = Q3 - Q1\n",
      "        # Outlier step\n",
      "        outlier_step = IQR * 1.5\n",
      "        # detect outlier and their indeces\n",
      "        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index\n",
      "        # store indeces\n",
      "        outlier_indices.extend(outlier_list_col)\n",
      "    \n",
      "    outlier_indices = Counter(outlier_indices)\n",
      "    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n",
      "\n",
      "    print(multiple_outliers)\n",
      "    return multiple_outliers\n",
      "429/24:\n",
      "data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age'])]\n",
      "429/25:\n",
      "data = data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, \n",
      "                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,\n",
      "                  748, 655, 584, 485, 618, 661, 45, 243], axis=0,inplace=True)\n",
      "429/26: data.shape()\n",
      "429/27: data\n",
      "429/28: data\n",
      "429/29: print(data)\n",
      "430/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "430/2: warnings.filterwarnings(\"ignore\")\n",
      "430/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "430/4: data.head()\n",
      "430/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "430/6:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "430/7:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    if 0 in data[ele].unique():\n",
      "        print(ele,\":\",\"True\")\n",
      "430/8:\n",
      "for ele in columns_name:\n",
      "    if ele!=\"Pregnancies\" and ele!=\"Outcome\":\n",
      "        data[ele].replace(0, np.nan, inplace=True)\n",
      "430/9: data.isnull().sum()\n",
      "430/10: data.head()\n",
      "430/11:\n",
      "for ele in columns_name:\n",
      "    data[ele].fillna(value=data[ele].mean(), inplace=True)\n",
      "430/12: data[\"Age\"].unique()\n",
      "430/13:\n",
      "# import matplotlib.pyplot as plt\n",
      "\n",
      "# fig = plt.figure()\n",
      "# ax = fig.add_subplot(111)\n",
      "# data[\"Glucose\"].plot(kind = \"kde\", ax = ax)\n",
      "\n",
      "# line, labels = ax.get_legend_handles_labels()\n",
      "# ax.legend(line,labels,loc = \"best\")\n",
      "430/14: data.info()\n",
      "430/15: data.head()\n",
      "430/16: data.describe()\n",
      "430/17:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "430/18:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "430/19:\n",
      "from sklearn.utils import resample\n",
      "\n",
      "df_majority = data.loc[data.Outcome == 0].copy()\n",
      "df_minority = data.loc[data.Outcome == 1].copy()\n",
      "df_minority_upsampled = resample(df_minority,\n",
      "                             replace=True,  # sample with replacement\n",
      "                            n_samples=500,  # to match majority class\n",
      "                            random_state=123) \n",
      "data = pd.concat([df_majority, df_minority_upsampled])\n",
      "430/20:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "430/21:\n",
      "data1=data.drop('Outcome',axis=1)\n",
      "data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))\n",
      "plt.show()\n",
      "430/22:\n",
      "from sklearn.ensemble import IsolationForest\n",
      "from collections import Counter\n",
      "rs=np.random.RandomState(0)\n",
      "clf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) \n",
      "clf.fit(data)\n",
      "y_pred_train = clf.predict(data)\n",
      "sayÄ± = Counter(y_pred_train)\n",
      "print(sayÄ±)\n",
      "\n",
      "## -1 = Outliers\n",
      "430/23:\n",
      "from collections import Counter\n",
      "def detect_outliers(data,features):\n",
      "    outlier_indices = []\n",
      "    for c in features:\n",
      "        # 1st quartile\n",
      "        Q1 = np.percentile(data[c],25)\n",
      "        # 3rd quartile\n",
      "        Q3 = np.percentile(data[c],75)\n",
      "        # IQR\n",
      "        IQR = Q3 - Q1\n",
      "        # Outlier step\n",
      "        outlier_step = IQR * 1.5\n",
      "        # detect outlier and their indeces\n",
      "        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index\n",
      "        # store indeces\n",
      "        outlier_indices.extend(outlier_list_col)\n",
      "    \n",
      "    outlier_indices = Counter(outlier_indices)\n",
      "    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n",
      "\n",
      "    print(multiple_outliers)\n",
      "    return multiple_outliers\n",
      "430/24:\n",
      "data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age'])]\n",
      "430/25:\n",
      "data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, \n",
      "                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,\n",
      "                  748, 655, 584, 485, 618, 661, 45, 243], axis=0,inplace=True)\n",
      "430/26: print(data)\n",
      "430/27:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "430/28: #sns.pairplot(data, hue=\"Outcome\")\n",
      "430/29: data.corr()\n",
      "430/30:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "430/31:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "430/32:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "430/33:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "430/34:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "430/35:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "430/36:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "430/37:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "430/38:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "430/39:\n",
      "#  standardize data\n",
      "sd_tree = DecisionTreeClassifier(random_state=42)\n",
      "sd_tree.fit(std_x_train,y_train)\n",
      "sd_predict2 = sd_tree.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,sd_predict2))\n",
      "430/40:\n",
      " #Randomized Search CV\n",
      "\n",
      "# Hyperparameter optimization    \n",
      "    \n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "430/41:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "430/42:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "430/43: rf_random.fit(std_x_train,y_train)\n",
      "430/44: rf_random.best_params_\n",
      "430/45: predictions=rf_random.predict(std_x_test)\n",
      "430/46: accuracy_score(y_test,predictions)\n",
      "430/47:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "430/48:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "430/49:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "430/50:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "430/51: data\n",
      "430/52: newdata=data.drop(['BloodPressure', 'SkinThickness', 'Insulin','DiabetesPedigreeFunction'],axis=1)\n",
      "430/53:\n",
      "newdata=data.drop(['BloodPressure', 'SkinThickness', 'Insulin','DiabetesPedigreeFunction'],axis=1)\n",
      "newdata\n",
      "430/54: data.drop(['BloodPressure', 'SkinThickness', 'Insulin','DiabetesPedigreeFunction'],axis=1,inplace=True)\n",
      "430/55:\n",
      "data.drop(['BloodPressure', 'SkinThickness', 'Insulin','DiabetesPedigreeFunction'],axis=1,inplace=True)\n",
      "data\n",
      "431/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "431/2: warnings.filterwarnings(\"ignore\")\n",
      "431/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "431/4: data.head()\n",
      "431/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "431/6:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "431/7:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    if 0 in data[ele].unique():\n",
      "        print(ele,\":\",\"True\")\n",
      "431/8:\n",
      "for ele in columns_name:\n",
      "    if ele!=\"Pregnancies\" and ele!=\"Outcome\":\n",
      "        data[ele].replace(0, np.nan, inplace=True)\n",
      "431/9: data.isnull().sum()\n",
      "431/10: data.head()\n",
      "431/11:\n",
      "for ele in columns_name:\n",
      "    data[ele].fillna(value=data[ele].mean(), inplace=True)\n",
      "431/12: data[\"Age\"].unique()\n",
      "431/13:\n",
      "# import matplotlib.pyplot as plt\n",
      "\n",
      "# fig = plt.figure()\n",
      "# ax = fig.add_subplot(111)\n",
      "# data[\"Glucose\"].plot(kind = \"kde\", ax = ax)\n",
      "\n",
      "# line, labels = ax.get_legend_handles_labels()\n",
      "# ax.legend(line,labels,loc = \"best\")\n",
      "431/14: data.info()\n",
      "431/15: data.head()\n",
      "431/16: data.describe()\n",
      "431/17:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "431/18:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "431/19:\n",
      "from sklearn.utils import resample\n",
      "\n",
      "df_majority = data.loc[data.Outcome == 0].copy()\n",
      "df_minority = data.loc[data.Outcome == 1].copy()\n",
      "df_minority_upsampled = resample(df_minority,\n",
      "                             replace=True,  # sample with replacement\n",
      "                            n_samples=500,  # to match majority class\n",
      "                            random_state=123) \n",
      "data = pd.concat([df_majority, df_minority_upsampled])\n",
      "431/20:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "431/21:\n",
      "data1=data.drop('Outcome',axis=1)\n",
      "data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))\n",
      "plt.show()\n",
      "431/22:\n",
      "from sklearn.ensemble import IsolationForest\n",
      "from collections import Counter\n",
      "rs=np.random.RandomState(0)\n",
      "clf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) \n",
      "clf.fit(data)\n",
      "y_pred_train = clf.predict(data)\n",
      "sayÄ± = Counter(y_pred_train)\n",
      "print(sayÄ±)\n",
      "\n",
      "## -1 = Outliers\n",
      "431/23:\n",
      "from collections import Counter\n",
      "def detect_outliers(data,features):\n",
      "    outlier_indices = []\n",
      "    for c in features:\n",
      "        # 1st quartile\n",
      "        Q1 = np.percentile(data[c],25)\n",
      "        # 3rd quartile\n",
      "        Q3 = np.percentile(data[c],75)\n",
      "        # IQR\n",
      "        IQR = Q3 - Q1\n",
      "        # Outlier step\n",
      "        outlier_step = IQR * 1.5\n",
      "        # detect outlier and their indeces\n",
      "        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index\n",
      "        # store indeces\n",
      "        outlier_indices.extend(outlier_list_col)\n",
      "    \n",
      "    outlier_indices = Counter(outlier_indices)\n",
      "    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n",
      "\n",
      "    print(multiple_outliers)\n",
      "    return multiple_outliers\n",
      "431/24:\n",
      "data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age'])]\n",
      "431/25:\n",
      "data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, \n",
      "                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,\n",
      "                  748, 655, 584, 485, 618, 661, 45, 243], axis=0,inplace=True)\n",
      "431/26: data\n",
      "431/27:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "431/28: #sns.pairplot(data, hue=\"Outcome\")\n",
      "431/29: data.corr()\n",
      "431/30:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "431/31:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "431/32:\n",
      "data.drop(['BloodPressure', 'SkinThickness', 'Insulin','DiabetesPedigreeFunction'],axis=1,inplace=True)\n",
      "data\n",
      "431/33:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "X = data.iloc[:, 0:4]\n",
      "Y = data.iloc[:, 4]\n",
      "nd = StandardScaler()\n",
      "nd.fit(X)\n",
      "X =nd.transform(X)\n",
      "print(Y)\n",
      "431/34:\n",
      "x_data = data.iloc[:,:8].values\n",
      "y_data = data.iloc[:,8:].values\n",
      "431/35:\n",
      "std_scl = StandardScaler()\n",
      "std_scl_x = std_scl.fit_transform(x_data)\n",
      "#std_scl_x\n",
      "431/36:\n",
      "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "431/37:\n",
      "std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)\n",
      "std_x_train\n",
      "431/38:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "431/39:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "x_data = data.iloc[:, 0:4]\n",
      "y_data = data.iloc[:, 4]\n",
      "nd = StandardScaler()\n",
      "nd.fit(x_data)\n",
      "x_data =nd.transform(x_data)\n",
      "print(y_data)\n",
      "431/40:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "X = data.iloc[:, 0:4]\n",
      "Y = data.iloc[:, 4]\n",
      "nd = StandardScaler()\n",
      "nd.fit(X)\n",
      "X =nd.transform(X)\n",
      "print(Y)\n",
      "431/41:\n",
      "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "431/42:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1))\n",
      "431/43:\n",
      "# standardize data\n",
      "slog_reg1 = LogisticRegression()\n",
      "slog_reg1.fit(std_x_train,y_train)\n",
      "slog_predict2 = slog_reg1.predict(std_x_test)\n",
      "print(accuracy_score(y_test,slog_predict2))\n",
      "431/44:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1))\n",
      "431/45:\n",
      "#  #Randomized Search CV\n",
      "\n",
      "# # Hyperparameter optimization    \n",
      "    \n",
      "# # Number of trees in random forest\n",
      "# n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# # Number of features to consider at every split\n",
      "# max_features = ['auto', 'sqrt']\n",
      "# # Maximum number of levels in tree\n",
      "# max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# # max_depth.append(None)\n",
      "# # Minimum number of samples required to split a node\n",
      "# min_samples_split = [2, 5, 10, 15, 100]\n",
      "# # Minimum number of samples required at each leaf node\n",
      "# min_samples_leaf = [1, 2, 5, 10]\n",
      "431/46:\n",
      "# # Create the random grid\n",
      "# random_grid = {'n_estimators': n_estimators,\n",
      "#                'max_features': max_features,\n",
      "#                'max_depth': max_depth,\n",
      "#                'min_samples_split': min_samples_split,\n",
      "#                'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "# print(random_grid)\n",
      "431/47:\n",
      "# rf = RandomForestClassifier()\n",
      "# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "431/48: # rf_random.fit(std_x_train,y_train)\n",
      "431/49: # rf_random.best_params_\n",
      "431/50: # predictions=rf_random.predict(std_x_test)\n",
      "431/51: # accuracy_score(y_test,predictions)\n",
      "431/52:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1))\n",
      "431/53:\n",
      "# standardize data\n",
      "sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "sr_forest.fit(std_x_train,y_train)\n",
      "sr_predict2 = sr_forest.predict(std_x_test)\n",
      "print(\"Accuracy of standardize data in Random Forest Classifier:\",accuracy_score(y_test,sr_predict2))\n",
      "431/54:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "431/55:\n",
      "cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "431/56:\n",
      "#  #Randomized Search CV\n",
      "\n",
      "# # Hyperparameter optimization    \n",
      "    \n",
      "# Number of trees in random forest\n",
      "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
      "# Number of features to consider at every split\n",
      "max_features = ['auto', 'sqrt']\n",
      "# Maximum number of levels in tree\n",
      "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
      "# max_depth.append(None)\n",
      "# Minimum number of samples required to split a node\n",
      "min_samples_split = [2, 5, 10, 15, 100]\n",
      "# Minimum number of samples required at each leaf node\n",
      "min_samples_leaf = [1, 2, 5, 10]\n",
      "431/57:\n",
      "# Create the random grid\n",
      "random_grid = {'n_estimators': n_estimators,\n",
      "               'max_features': max_features,\n",
      "               'max_depth': max_depth,\n",
      "               'min_samples_split': min_samples_split,\n",
      "               'min_samples_leaf': min_samples_leaf}\n",
      "\n",
      "print(random_grid)\n",
      "431/58:\n",
      "rf = RandomForestClassifier()\n",
      "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n",
      "431/59: rf_random.fit(std_x_train,y_train)\n",
      "431/60: rf_random.best_params_\n",
      "431/61: predictions=rf_random.predict(std_x_test)\n",
      "431/62: accuracy_score(y_test,predictions)\n",
      "431/63: predictions=rf_random.predict(x_test)\n",
      "431/64: rf_random.fit(x_train,y_train)\n",
      "431/65: rf_random.best_params_\n",
      "431/66: predictions=rf_random.predict(x_test)\n",
      "431/67: accuracy_score(y_test,predictions)\n",
      "431/68:\n",
      "confu_matrix = confusion_matrix(y_test, rf_random)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "431/69: r = accuracy_score(y_test,predictions)\n",
      "431/70:\n",
      "confu_matrix = confusion_matrix(y_test, r)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "431/71:\n",
      "confu_matrix = confusion_matrix(y_test, predictions)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "431/72:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "431/73:\n",
      "confu_matrix = confusion_matrix(y_test, r_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "431/74:\n",
      "confu_matrix = confusion_matrix(y_test, d_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "431/75:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1)*100)\n",
      "431/76:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1)*100)\n",
      "431/77:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1)*100)\n",
      "431/78:\n",
      "cross_val = cross_val_score(estimator=rf_random, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "431/79:\n",
      "cross_val = cross_val_score(estimator=rf, X=x_train, y=y_train)\n",
      "print(\"Cross validation accuracy of Logistic Regrassior: \",cross_val)\n",
      "print(\"Cross validation mean accuracy of Logistic Regrassion: \", cross_val.mean())\n",
      "431/80:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "import pickle\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "431/81:\n",
      "\n",
      "# save model\n",
      "pickle.dump(rf_random,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "dP = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = dp.predict(all_x_test)\n",
      "\n",
      "print(\"confusion matrix: \\n\", confusion_matrix(all_y_test,y_predict))\n",
      "accuracy_score(all_y_test,y_predict)\n",
      "431/82:\n",
      "\n",
      "# save model\n",
      "pickle.dump(rf_random,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "dp = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = dp.predict(all_x_test)\n",
      "\n",
      "print(\"confusion matrix: \\n\", confusion_matrix(all_y_test,y_predict))\n",
      "accuracy_score(all_y_test,y_predict)\n",
      "431/83:\n",
      "\n",
      "# save model\n",
      "pickle.dump(rf_random,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "dp = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = dp.predict(x_test)\n",
      "\n",
      "print(\"confusion matrix: \\n\", confusion_matrix(y_test,y_predict))\n",
      "accuracy_score(y_test,y_predict)\n",
      "431/84:\n",
      "\n",
      "# save model\n",
      "pickle.dump(rf_random,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "dp = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = dp.predict([1,89.0,28.1,21])\n",
      "print(y_predict)\n",
      "#print(\"confusion matrix: \\n\", confusion_matrix(y_test,y_predict))\n",
      "#accuracy_score(y_test,y_predict)\n",
      "431/85:\n",
      "\n",
      "# save model\n",
      "pickle.dump(rf_random,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "dp = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = dp.predict(np.array([1,89.0,28.1,21]))\n",
      "print(y_predict)\n",
      "#print(\"confusion matrix: \\n\", confusion_matrix(y_test,y_predict))\n",
      "#accuracy_score(y_test,y_predict)\n",
      "431/86:\n",
      "\n",
      "# save model\n",
      "pickle.dump(rf_random,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "dp = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = dp.predict(np.array([1,89.0,28.1,21].reshape(-1,-1)))\n",
      "print(y_predict)\n",
      "#print(\"confusion matrix: \\n\", confusion_matrix(y_test,y_predict))\n",
      "#accuracy_score(y_test,y_predict)\n",
      "431/87:\n",
      "\n",
      "# save model\n",
      "pickle.dump(rf_random,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "dp = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = dp.predict(np.array([1,89.0,28.1,21]).reshape(-1,-1))\n",
      "print(y_predict)\n",
      "#print(\"confusion matrix: \\n\", confusion_matrix(y_test,y_predict))\n",
      "#accuracy_score(y_test,y_predict)\n",
      "431/88:\n",
      "\n",
      "# save model\n",
      "pickle.dump(rf_random,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "dp = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = dp.predict(np.array([1,89.0,28.1,21]).reshape(1,-1))\n",
      "print(y_predict)\n",
      "#print(\"confusion matrix: \\n\", confusion_matrix(y_test,y_predict))\n",
      "#accuracy_score(y_test,y_predict)\n",
      "431/89:\n",
      "\n",
      "# save model\n",
      "pickle.dump(rf_random,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "dp = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = dp.predict(np.array([3,171,33.3,24]).reshape(1,-1))\n",
      "print(y_predict)\n",
      "#print(\"confusion matrix: \\n\", confusion_matrix(y_test,y_predict))\n",
      "#accuracy_score(y_test,y_predict)\n",
      "431/90:\n",
      "\n",
      "# save model\n",
      "pickle.dump(r_forest,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "dp = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = dp.predict(np.array([3,171,33.3,24]).reshape(1,-1))\n",
      "print(y_predict)\n",
      "#print(\"confusion matrix: \\n\", confusion_matrix(y_test,y_predict))\n",
      "#accuracy_score(y_test,y_predict)\n",
      "431/91:\n",
      "\n",
      "# save model\n",
      "pickle.dump(rf_random,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "dp = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = dp.predict(x_test)\n",
      "\n",
      "print(\"confusion matrix: \\n\", confusion_matrix(y_test,y_predict))\n",
      "accuracy_score(y_test,y_predict)\n",
      "431/92:\n",
      "\n",
      "# save model\n",
      "pickle.dump(r_forest,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "dp = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = dp.predict(x_test)\n",
      "\n",
      "print(\"confusion matrix: \\n\", confusion_matrix(y_test,y_predict))\n",
      "accuracy_score(y_test,y_predict)\n",
      "432/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "import warnings\n",
      "import pickle\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "432/2: warnings.filterwarnings(\"ignore\")\n",
      "432/3: data = pd.read_csv(\"diabetes.csv\")\n",
      "432/4: data.head()\n",
      "432/5:\n",
      "print(data.columns)\n",
      "data.shape\n",
      "432/6:\n",
      "data.isnull().sum()\n",
      "a = len(data.loc[data['Insulin']==0])\n",
      "a\n",
      "432/7:\n",
      "columns_name = data.columns\n",
      "for ele in columns_name:\n",
      "    if 0 in data[ele].unique():\n",
      "        print(ele,\":\",\"True\")\n",
      "432/8:\n",
      "for ele in columns_name:\n",
      "    if ele!=\"Pregnancies\" and ele!=\"Outcome\":\n",
      "        data[ele].replace(0, np.nan, inplace=True)\n",
      "432/9: data.isnull().sum()\n",
      "432/10: data.head()\n",
      "432/11:\n",
      "for ele in columns_name:\n",
      "    data[ele].fillna(value=data[ele].mean(), inplace=True)\n",
      "432/12: data[\"Age\"].unique()\n",
      "432/13:\n",
      "# import matplotlib.pyplot as plt\n",
      "\n",
      "# fig = plt.figure()\n",
      "# ax = fig.add_subplot(111)\n",
      "# data[\"Glucose\"].plot(kind = \"kde\", ax = ax)\n",
      "\n",
      "# line, labels = ax.get_legend_handles_labels()\n",
      "# ax.legend(line,labels,loc = \"best\")\n",
      "432/14: data.info()\n",
      "432/15: data.head()\n",
      "432/16: data.describe()\n",
      "432/17:\n",
      "print(data.groupby([\"Outcome\"]).count())\n",
      "sns.countplot(data[\"Outcome\"])\n",
      "\n",
      "# 0=No diabeties, 1=Diabeties\n",
      "432/18:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "432/19:\n",
      "from sklearn.utils import resample\n",
      "\n",
      "df_majority = data.loc[data.Outcome == 0].copy()\n",
      "df_minority = data.loc[data.Outcome == 1].copy()\n",
      "df_minority_upsampled = resample(df_minority,\n",
      "                             replace=True,  # sample with replacement\n",
      "                            n_samples=500,  # to match majority class\n",
      "                            random_state=123) \n",
      "data = pd.concat([df_majority, df_minority_upsampled])\n",
      "432/20:\n",
      "data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))\n",
      "plt.show\n",
      "varValue = data.Outcome.value_counts()\n",
      "print(varValue)\n",
      "432/21:\n",
      "data1=data.drop('Outcome',axis=1)\n",
      "data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))\n",
      "plt.show()\n",
      "432/22:\n",
      "from sklearn.ensemble import IsolationForest\n",
      "from collections import Counter\n",
      "rs=np.random.RandomState(0)\n",
      "clf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) \n",
      "clf.fit(data)\n",
      "y_pred_train = clf.predict(data)\n",
      "sayÄ± = Counter(y_pred_train)\n",
      "print(sayÄ±)\n",
      "\n",
      "## -1 = Outliers\n",
      "432/23:\n",
      "from collections import Counter\n",
      "def detect_outliers(data,features):\n",
      "    outlier_indices = []\n",
      "    for c in features:\n",
      "        # 1st quartile\n",
      "        Q1 = np.percentile(data[c],25)\n",
      "        # 3rd quartile\n",
      "        Q3 = np.percentile(data[c],75)\n",
      "        # IQR\n",
      "        IQR = Q3 - Q1\n",
      "        # Outlier step\n",
      "        outlier_step = IQR * 1.5\n",
      "        # detect outlier and their indeces\n",
      "        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index\n",
      "        # store indeces\n",
      "        outlier_indices.extend(outlier_list_col)\n",
      "    \n",
      "    outlier_indices = Counter(outlier_indices)\n",
      "    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n",
      "\n",
      "    print(multiple_outliers)\n",
      "    return multiple_outliers\n",
      "432/24:\n",
      "data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age'])]\n",
      "432/25:\n",
      "data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, \n",
      "                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,\n",
      "                  748, 655, 584, 485, 618, 661, 45, 243], axis=0,inplace=True)\n",
      "432/26: data\n",
      "432/27:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "432/28: #sns.pairplot(data, hue=\"Outcome\")\n",
      "432/29: data.corr()\n",
      "432/30:\n",
      "plt.figure(figsize=(18,8))\n",
      "sns.heatmap(data.corr(), annot=True, linewidths=1)\n",
      "432/31:\n",
      "data_2 = data.drop([\"Outcome\"],axis=1)\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))\n",
      "432/32:\n",
      "data.drop(['BloodPressure', 'SkinThickness', 'Insulin','DiabetesPedigreeFunction'],axis=1,inplace=True)\n",
      "data\n",
      "432/33:\n",
      "\n",
      "X = data.iloc[:, 0:4]\n",
      "Y = data.iloc[:, 4]\n",
      "432/34:\n",
      "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
      "x_train\n",
      "432/35:\n",
      "# Non standardize data\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(x_train,y_train)\n",
      "log_predict1 = log_reg.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Logistic regrassion:\",accuracy_score(y_test,log_predict1)*100)\n",
      "432/36:\n",
      "confu_matrix = confusion_matrix(y_test, log_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "432/37:\n",
      "# Non standardize data\n",
      "d_tree = DecisionTreeClassifier(random_state=42)\n",
      "d_tree.fit(x_train,y_train)\n",
      "d_predict1 = d_tree.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Deccission Tree Classifier:\",accuracy_score(y_test,d_predict1)*100)\n",
      "432/38:\n",
      "confu_matrix = confusion_matrix(y_test, d_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "432/39:\n",
      "# Non standardize data\n",
      "r_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "r_forest.fit(x_train,y_train)\n",
      "r_predict1 = r_forest.predict(x_test)\n",
      "print(\"Accuracy of Non standardize data in Random Forest Classifier:\",accuracy_score(y_test,r_predict1)*100)\n",
      "432/40:\n",
      "confu_matrix = confusion_matrix(y_test, r_predict1)\n",
      "plt.title(\"Confusion Matrix\", fontsize=10)\n",
      "sns.heatmap(confu_matrix, annot=True)\n",
      "plt.show()\n",
      "432/41:\n",
      "\n",
      "# save model\n",
      "pickle.dump(r_forest,open('model.pkl','wb'))\n",
      "\n",
      "# Load Model\n",
      "dp = pickle.load(open('model.pkl','rb'))\n",
      "\n",
      "y_predict = dp.predict(x_test)\n",
      "\n",
      "print(\"confusion matrix: \\n\", confusion_matrix(y_test,y_predict))\n",
      "accuracy_score(y_test,y_predict)\n",
      "433/1:\n",
      "from googleplaces import GooglePlaces, types, lang\n",
      "import requests\n",
      "import json\n",
      "  \n",
      "# This is the way to make api requests\n",
      "# using python requests library\n",
      "  \n",
      "# send_url = 'http://freegeoip.net/json'\n",
      "# r = requests.get(send_url)\n",
      "# j = json.loads(r.text)\n",
      "# print(j)\n",
      "# lat = j['latitude']\n",
      "# lon = j['longitude']\n",
      "  \n",
      "# Generate an API key by going to this location\n",
      "# https://cloud.google.com /maps-platform/places/?apis =\n",
      "# places in the google developers\n",
      "  \n",
      "# Use your own API key for making api request calls\n",
      "API_KEY = 'AIzaSyAU14VOIJDGbRPGzIpUXfKP40AmvLfcVIQ'\n",
      "  \n",
      "# Initialising the GooglePlaces constructor\n",
      "google_places = GooglePlaces(API_KEY)\n",
      "  \n",
      "# call the function nearby search with\n",
      "# the parameters as longitude, latitude,\n",
      "# radius and type of place which needs to be searched of \n",
      "# type can be HOSPITAL, CAFE, BAR, CASINO, etc\n",
      "query_result = google_places.nearby_search(\n",
      "        # lat_lng ={'lat': 46.1667, 'lng': -1.15},\n",
      "        lat_lng ={'lat': 28.4089, 'lng': 77.3178},\n",
      "        radius = 5000,\n",
      "        # types =[types.TYPE_HOSPITAL] or\n",
      "        # [types.TYPE_CAFE] or [type.TYPE_BAR]\n",
      "        # or [type.TYPE_CASINO])\n",
      "        types =[types.TYPE_HOSPITAL])\n",
      "  \n",
      "# If any attributions related \n",
      "# with search results print them\n",
      "if query_result.has_attributions:\n",
      "    print (query_result.html_attributions)\n",
      "  \n",
      "  \n",
      "# Iterate over the search results\n",
      "for place in query_result.places:\n",
      "    # print(type(place))\n",
      "    # place.get_details()\n",
      "    print (place.name)\n",
      "    print(\"Latitude\", place.geo_location['lat'])\n",
      "    print(\"Longitude\", place.geo_location['lng'])\n",
      "    print()\n",
      "433/2: !pip install googleplaces\n",
      "433/3: !pip install python-google-places\n",
      "433/4:\n",
      "from googleplaces import GooglePlaces, types, lang\n",
      "import requests\n",
      "import json\n",
      "  \n",
      "# This is the way to make api requests\n",
      "# using python requests library\n",
      "  \n",
      "# send_url = 'http://freegeoip.net/json'\n",
      "# r = requests.get(send_url)\n",
      "# j = json.loads(r.text)\n",
      "# print(j)\n",
      "# lat = j['latitude']\n",
      "# lon = j['longitude']\n",
      "  \n",
      "# Generate an API key by going to this location\n",
      "# https://cloud.google.com /maps-platform/places/?apis =\n",
      "# places in the google developers\n",
      "  \n",
      "# Use your own API key for making api request calls\n",
      "API_KEY = 'AIzaSyAU14VOIJDGbRPGzIpUXfKP40AmvLfcVIQ'\n",
      "  \n",
      "# Initialising the GooglePlaces constructor\n",
      "google_places = GooglePlaces(API_KEY)\n",
      "  \n",
      "# call the function nearby search with\n",
      "# the parameters as longitude, latitude,\n",
      "# radius and type of place which needs to be searched of \n",
      "# type can be HOSPITAL, CAFE, BAR, CASINO, etc\n",
      "query_result = google_places.nearby_search(\n",
      "        # lat_lng ={'lat': 46.1667, 'lng': -1.15},\n",
      "        lat_lng ={'lat': 28.4089, 'lng': 77.3178},\n",
      "        radius = 5000,\n",
      "        # types =[types.TYPE_HOSPITAL] or\n",
      "        # [types.TYPE_CAFE] or [type.TYPE_BAR]\n",
      "        # or [type.TYPE_CASINO])\n",
      "        types =[types.TYPE_HOSPITAL])\n",
      "  \n",
      "# If any attributions related \n",
      "# with search results print them\n",
      "if query_result.has_attributions:\n",
      "    print (query_result.html_attributions)\n",
      "  \n",
      "  \n",
      "# Iterate over the search results\n",
      "for place in query_result.places:\n",
      "    # print(type(place))\n",
      "    # place.get_details()\n",
      "    print (place.name)\n",
      "    print(\"Latitude\", place.geo_location['lat'])\n",
      "    print(\"Longitude\", place.geo_location['lng'])\n",
      "    print()\n",
      "435/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "import pickle\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "435/2: warnings.filterwarnings(\"ignore\")\n",
      "435/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data\n",
      "435/4:\n",
      "print(\"Shape:\")\n",
      "print(data.shape())\n",
      "print(\"-\"*100)\n",
      "data.info()\n",
      "435/5:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "data.info()\n",
      "435/6:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns:\")\n",
      "print(data.columns())\n",
      "print(\"-\"*100)\n",
      "data.info()\n",
      "435/7:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns:\")\n",
      "print(data.column())\n",
      "print(\"-\"*100)\n",
      "data.info()\n",
      "435/8:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns:\")\n",
      "columns = data.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "data.info()\n",
      "435/9:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns Name:\")\n",
      "columns = data.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data Information:\")\n",
      "data.info()\n",
      "print(\"-\"*100)\n",
      "print(\"Data Description: \")\n",
      "print(data.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Counting Null Values:\")\n",
      "print(data.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "435/10:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns Name:\")\n",
      "columns = data.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data Information:\")\n",
      "data.info()\n",
      "print(\"-\"*100)\n",
      "print(\"Data Description: \")\n",
      "print(data.describe)\n",
      "print(\"-\"*100)\n",
      "print(\"Counting Null Values:\")\n",
      "print(data.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "435/11:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data.head()\n",
      "435/12:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns Name:\")\n",
      "columns = data.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data Information:\")\n",
      "data.info()\n",
      "print(\"-\"*100)\n",
      "print(\"Data Description: \")\n",
      "print(data.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Counting Null Values:\")\n",
      "print(data.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "435/13:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns Name:\")\n",
      "columns = data.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data Information:\")\n",
      "data.info()\n",
      "print(\"-\"*100)\n",
      "print(\"Data Description: \")\n",
      "print(data.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Counting Null Values:\")\n",
      "print(data.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data correlation: \")\n",
      "print(data.corr)\n",
      "print(\"-\"*100)\n",
      "435/14:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns Name:\")\n",
      "columns = data.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data Information:\")\n",
      "data.info()\n",
      "print(\"-\"*100)\n",
      "print(\"Data Description: \")\n",
      "print(data.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Counting Null Values:\")\n",
      "print(data.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data correlation: \")\n",
      "print(data.corr())\n",
      "print(\"-\"*100)\n",
      "435/15:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='RdYlGn', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "435/16:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='RdYl', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "435/17:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='Blues', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "435/18:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='Blue', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "435/19:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='BuPu_r', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "\"\"\"'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',\n",
      "'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', \n",
      "'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', \n",
      "'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', \n",
      "'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', \n",
      "'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',\n",
      "'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', \n",
      "'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',\n",
      "'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',\n",
      "'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', \n",
      "'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', \n",
      "'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',\n",
      "'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',\n",
      "'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', \n",
      "'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',\n",
      "'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r\"\"\"\n",
      "435/20:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='BrBG', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "\"\"\"'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',\n",
      "'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', \n",
      "'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', \n",
      "'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', \n",
      "'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', \n",
      "'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',\n",
      "'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', \n",
      "'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',\n",
      "'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',\n",
      "'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', \n",
      "'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', \n",
      "'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',\n",
      "'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',\n",
      "'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', \n",
      "'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',\n",
      "'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r\"\"\"\n",
      "435/21:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='rainbow', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "\"\"\"'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',\n",
      "'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', \n",
      "'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', \n",
      "'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', \n",
      "'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', \n",
      "'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',\n",
      "'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', \n",
      "'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',\n",
      "'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',\n",
      "'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', \n",
      "'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', \n",
      "'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',\n",
      "'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',\n",
      "'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', \n",
      "'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',\n",
      "'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r\"\"\"\n",
      "435/22:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='rainbow', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "\"\"\"'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',\n",
      "'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', \n",
      "'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', \n",
      "'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', \n",
      "'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', \n",
      "'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',\n",
      "'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', \n",
      "'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',\n",
      "'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',\n",
      "'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', \n",
      "'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', \n",
      "'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',\n",
      "'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',\n",
      "'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', \n",
      "'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',\n",
      "'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r'\"\"\"\n",
      "435/23:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='RdYlGn', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "\"\"\"'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',\n",
      "'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', \n",
      "'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', \n",
      "'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', \n",
      "'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', \n",
      "'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',\n",
      "'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', \n",
      "'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',\n",
      "'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',\n",
      "'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', \n",
      "'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', \n",
      "'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',\n",
      "'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',\n",
      "'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', \n",
      "'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',\n",
      "'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r'\"\"\"\n",
      "435/24:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='viridis', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "\"\"\"'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',\n",
      "'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', \n",
      "'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', \n",
      "'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', \n",
      "'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', \n",
      "'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',\n",
      "'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', \n",
      "'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',\n",
      "'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',\n",
      "'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', \n",
      "'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', \n",
      "'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',\n",
      "'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',\n",
      "'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', \n",
      "'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',\n",
      "'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r'\"\"\"\n",
      "435/25:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='vlag', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "\"\"\"'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',\n",
      "'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', \n",
      "'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', \n",
      "'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', \n",
      "'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', \n",
      "'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',\n",
      "'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', \n",
      "'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',\n",
      "'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',\n",
      "'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', \n",
      "'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', \n",
      "'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',\n",
      "'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',\n",
      "'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', \n",
      "'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',\n",
      "'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r'\"\"\"\n",
      "435/26:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='viridis_r', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "\"\"\"'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',\n",
      "'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', \n",
      "'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', \n",
      "'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', \n",
      "'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', \n",
      "'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',\n",
      "'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', \n",
      "'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',\n",
      "'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',\n",
      "'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', \n",
      "'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', \n",
      "'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',\n",
      "'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',\n",
      "'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', \n",
      "'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',\n",
      "'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r'\"\"\"\n",
      "435/27:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "\"\"\"'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',\n",
      "'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', \n",
      "'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', \n",
      "'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', \n",
      "'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', \n",
      "'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',\n",
      "'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', \n",
      "'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',\n",
      "'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',\n",
      "'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', \n",
      "'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', \n",
      "'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',\n",
      "'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',\n",
      "'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', \n",
      "'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',\n",
      "'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r'\"\"\"\n",
      "435/28:\n",
      "data.hit(figure(20,20))\n",
      "plt.show\n",
      "435/29:\n",
      "data.hit(figure(20,20))\n",
      "plt.show()\n",
      "435/30:\n",
      "data.hist(figure(20,20))\n",
      "plt.show()\n",
      "435/31:\n",
      "data.hist(figsize(20,20))\n",
      "plt.show()\n",
      "435/32:\n",
      "data.hist(figsize=(20,20))\n",
      "plt.show()\n",
      "435/33: sns.pairplot(data, hue=\"target\")\n",
      "435/34:\n",
      "X = data.drop(\"target\",axis=1)\n",
      "y = data(\"target\")\n",
      "435/35:\n",
      "X = data.drop(\"target\",axis=1)\n",
      "y = data[\"target\"]\n",
      "435/36:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "X = std_scl.fit_transform(X)\n",
      "435/37:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "X = std_scl.fit_transform(X)\n",
      "print(X)\n",
      "435/38:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(X)\n",
      "435/39: x_train,x_test,y_tarin,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
      "435/40:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "import pickle\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "435/41: !pip install xgboost\n",
      "436/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "import pickle\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "436/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, cross_val_score\n",
      "436/3: warnings.filterwarnings(\"ignore\")\n",
      "436/4:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, cross_val_score\n",
      "436/5:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "436/6: warnings.filterwarnings(\"ignore\")\n",
      "436/7:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data.head()\n",
      "436/8:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns Name:\")\n",
      "columns = data.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data Information:\")\n",
      "data.info()\n",
      "print(\"-\"*100)\n",
      "print(\"Data Description: \")\n",
      "print(data.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Counting Null Values:\")\n",
      "print(data.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data correlation: \")\n",
      "print(data.corr())\n",
      "print(\"-\"*100)\n",
      "436/9:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "436/10:\n",
      "data.hist(figsize=(20,20))\n",
      "plt.show()\n",
      "436/11:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "436/12:\n",
      "sns.kdeplot(data[data['target']==1]['chol'],shade=True,color=\"orange\", label=\"Unwell\", alpha=.7)\n",
      "sns.kdeplot(data[data['target']==0]['chol'],shade=True,color=\"dodgerblue\", label=\"Healthy\", alpha=.7)\n",
      "plt.title('Cholesterol in mg/d for both case')\n",
      "plt.show()\n",
      "436/13:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "436/14: sns.pairplot(data, hue=\"target\")\n",
      "436/15:\n",
      "X = data.drop(\"target\",axis=1)\n",
      "y = data[\"target\"]\n",
      "436/16:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "X = std_scl.fit_transform(X)\n",
      "print(X)\n",
      "436/17:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(X)\n",
      "436/18: x_train,x_test,y_tarin,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
      "436/19:\n",
      "lr = LogisticRegression(solver=\"liblinear\").fit(x_train,y_train)\n",
      "gnb = GaussianNB().fit(x_train,y_train)\n",
      "knnc = KNeighborsClassifier().fit(x_train,y_train)\n",
      "dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)\n",
      "rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)\n",
      "gbc = GradientBoostingClassifier(verbose=False).fit(x_train,y_train)\n",
      "xgbc = XGBClassifier().fit(x_train,y_train)\n",
      "svmc = SVC().fit(x_train,y_train)\n",
      "catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)\n",
      "436/20: x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
      "436/21:\n",
      "lr = LogisticRegression(solver=\"liblinear\").fit(x_train,y_train)\n",
      "gnb = GaussianNB().fit(x_train,y_train)\n",
      "knnc = KNeighborsClassifier().fit(x_train,y_train)\n",
      "dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)\n",
      "rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)\n",
      "gbc = GradientBoostingClassifier(verbose=False).fit(x_train,y_train)\n",
      "xgbc = XGBClassifier().fit(x_train,y_train)\n",
      "svmc = SVC().fit(x_train,y_train)\n",
      "catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)\n",
      "436/22:\n",
      "lr = LogisticRegression(solver=\"liblinear\").fit(x_train,y_train)\n",
      "gnb = GaussianNB().fit(x_train,y_train)\n",
      "knnc = KNeighborsClassifier().fit(x_train,y_train)\n",
      "dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)\n",
      "rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)\n",
      "xgbc = XGBClassifier().fit(x_train,y_train)\n",
      "svmc = SVC().fit(x_train,y_train)\n",
      "catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436/23: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,svmc,catbc]\n",
      "436/24:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    error = -cross_val_score(model,x_test,y_test,cv=10,scoring=\"neg_mean_squared_error\",verbose=False).mean()\n",
      "    print(name + \": \")\n",
      "    print(\"-\" * 50)\n",
      "    print(\"Accuracy Score: \",accuracy_score(y_test,predict))\n",
      "    print(\"Cross Validation Score: \",CV)\n",
      "    print(np.sqrt(error))\n",
      "    print(\"R-square value: \",r2_score(y_test,predict))\n",
      "    print(\"Confusion matrix: \")\n",
      "    print(confusion_matrix(y_test,predict))\n",
      "    print(\"-\" * 30)\n",
      "436/25:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    error = -cross_val_score(model,x_test,y_test,cv=10,scoring=\"neg_mean_squared_error\",verbose=False).mean()\n",
      "    print(name + \": \")\n",
      "    print(\"-\" * 50)\n",
      "    print(\"Accuracy Score: \",accuracy_score(y_test,predict))\n",
      "    print(\"Cross Validation Score: \",CV)\n",
      "    print(np.sqrt(error))\n",
      "    print(\"R-square value: \",r2_score(y_test,predict))\n",
      "    print(\"Confusion matrix: \")\n",
      "    print(confusion_matrix(y_test,predict))\n",
      "    print(\"-\" * 100)\n",
      "437/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "437/2: warnings.filterwarnings(\"ignore\")\n",
      "437/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data.head()\n",
      "437/4:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns Name:\")\n",
      "columns = data.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data Information:\")\n",
      "data.info()\n",
      "print(\"-\"*100)\n",
      "print(\"Data Description: \")\n",
      "print(data.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Counting Null Values:\")\n",
      "print(data.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data correlation: \")\n",
      "print(data.corr())\n",
      "print(\"-\"*100)\n",
      "437/5:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "437/6:\n",
      "data.hist(figsize=(20,20))\n",
      "plt.show()\n",
      "437/7:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "437/8:\n",
      "sns.kdeplot(data[data['target']==1]['chol'],shade=True,color=\"orange\", label=\"Unwell\", alpha=.7)\n",
      "sns.kdeplot(data[data['target']==0]['chol'],shade=True,color=\"dodgerblue\", label=\"Healthy\", alpha=.7)\n",
      "plt.title('Cholesterol in mg/d for both case')\n",
      "plt.show()\n",
      "437/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "437/10: sns.pairplot(data, hue=\"target\")\n",
      "437/11:\n",
      "X = data.drop(\"target\",axis=1)\n",
      "y = data[\"target\"]\n",
      "437/12:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "X = std_scl.fit_transform(X)\n",
      "print(X)\n",
      "437/13:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(X)\n",
      "437/14: x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
      "437/15:\n",
      "lr = LogisticRegression(solver=\"liblinear\").fit(x_train,y_train)\n",
      "gnb = GaussianNB().fit(x_train,y_train)\n",
      "knnc = KNeighborsClassifier().fit(x_train,y_train)\n",
      "dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)\n",
      "rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)\n",
      "xgbc = XGBClassifier().fit(x_train,y_train)\n",
      "svmc = SVC().fit(x_train,y_train)\n",
      "catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)\n",
      "437/16: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,svmc,catbc]\n",
      "437/17:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    error = -cross_val_score(model,x_test,y_test,cv=10,scoring=\"neg_mean_squared_error\",verbose=False).mean()\n",
      "    print(name + \": \")\n",
      "    print(\"-\" * 50)\n",
      "    print(\"Accuracy Score: \",accuracy_score(y_test,predict))\n",
      "    print(\"Cross Validation Score: \",CV)\n",
      "    print(\"Error: \",np.sqrt(error))\n",
      "    print(\"R-square value: \",r2_score(y_test,predict))\n",
      "    print(\"Confusion matrix: \")\n",
      "    print(confusion_matrix(y_test,predict))\n",
      "    print(\"-\" * 100)\n",
      "437/18:\n",
      "# Training and testing on all features.\n",
      "\n",
      "lg_model = LogisticRegression()\n",
      "lg_model.fit(all_x_train,all_y_train)\n",
      "lg_model_prediction = lg_model.predict(all_x_test)\n",
      "print(\"Accuracy on all features: \", accuracy_score(all_y_test,lg_model_prediction)*100)\n",
      "print(\"R-square on all features: \", r2_score(all_y_test,lg_model_prediction))\n",
      "437/19: r = pd.DataFrame(columns=[\"MODELS\",\"R2CV\"])\n",
      "437/20:\n",
      "r = pd.DataFrame(columns=[\"MODELS\",\"R2CV\"])\n",
      "r\n",
      "437/21:\n",
      "r = pd.DataFrame(columns=[\"MODELS\",\"Accuracy\"])\n",
      "r\n",
      "437/22:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"R2CV\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    accuracy = accuracy_score(y_test,predict)\n",
      "    result = pd.DataFrame([[name,accuracy*100]],columns=[\"MODELS\",\"Accuracy\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"Accuracy\",y=\"MODELS\",data=r,color=\"k\")\n",
      "plt.xlabel(\"ACCURACY\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
      "plt.show()\n",
      "437/23:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Accuracy\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    accuracy = accuracy_score(y_test,predict)\n",
      "    result = pd.DataFrame([[name,accuracy*100]],columns=[\"MODELS\",\"Accuracy\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"Accuracy\",y=\"MODELS\",data=r,color=\"k\")\n",
      "plt.xlabel(\"ACCURACY\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
      "plt.show()\n",
      "437/24:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Accuracy\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    accuracy = accuracy_score(y_test,predict)\n",
      "    result = pd.DataFrame([[name,accuracy*100]],columns=[\"MODELS\",\"Accuracy\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"Accuracy\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"ACCURACY\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
      "plt.show()\n",
      "438/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
      "438/2: warnings.filterwarnings(\"ignore\")\n",
      "438/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data.head()\n",
      "438/4:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns Name:\")\n",
      "columns = data.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data Information:\")\n",
      "data.info()\n",
      "print(\"-\"*100)\n",
      "print(\"Data Description: \")\n",
      "print(data.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Counting Null Values:\")\n",
      "print(data.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data correlation: \")\n",
      "print(data.corr())\n",
      "print(\"-\"*100)\n",
      "438/5:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "438/6:\n",
      "data.hist(figsize=(20,20))\n",
      "plt.show()\n",
      "438/7:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "438/8:\n",
      "sns.kdeplot(data[data['target']==1]['chol'],shade=True,color=\"orange\", label=\"Unwell\", alpha=.7)\n",
      "sns.kdeplot(data[data['target']==0]['chol'],shade=True,color=\"dodgerblue\", label=\"Healthy\", alpha=.7)\n",
      "plt.title('Cholesterol in mg/d for both case')\n",
      "plt.show()\n",
      "438/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "438/10: sns.pairplot(data, hue=\"target\")\n",
      "438/11:\n",
      "X = data.drop(\"target\",axis=1)\n",
      "y = data[\"target\"]\n",
      "438/12:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "X = std_scl.fit_transform(X)\n",
      "print(X)\n",
      "438/13:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(X)\n",
      "438/14: x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
      "438/15:\n",
      "lr = LogisticRegression(solver=\"liblinear\").fit(x_train,y_train)\n",
      "gnb = GaussianNB().fit(x_train,y_train)\n",
      "knnc = KNeighborsClassifier().fit(x_train,y_train)\n",
      "dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)\n",
      "rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)\n",
      "xgbc = XGBClassifier().fit(x_train,y_train)\n",
      "svmc = SVC().fit(x_train,y_train)\n",
      "catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)\n",
      "438/16: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,svmc,catbc]\n",
      "438/17:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    error = -cross_val_score(model,x_test,y_test,cv=10,scoring=\"neg_mean_squared_error\",verbose=False).mean()\n",
      "    print(name + \": \")\n",
      "    print(\"-\" * 50)\n",
      "    print(\"Accuracy Score: \",accuracy_score(y_test,predict))\n",
      "    print(\"Cross Validation Score: \",CV)\n",
      "    print(\"Error: \",np.sqrt(error))\n",
      "    print(\"R-square value: \",r2_score(y_test,predict))\n",
      "    print(\"Confusion matrix: \")\n",
      "    print(confusion_matrix(y_test,predict))\n",
      "    print(\"-\" * 100)\n",
      "438/18:\n",
      "r = pd.DataFrame(columns=[\"MODELS\",\"Accuracy\"])\n",
      "r\n",
      "438/19:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Accuracy\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    accuracy = accuracy_score(y_test,predict)\n",
      "    result = pd.DataFrame([[name,accuracy*100]],columns=[\"MODELS\",\"Accuracy\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"Accuracy\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"ACCURACY\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
      "plt.show()\n",
      "438/20: df\n",
      "438/21:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Accuracy\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    accuracy = accuracy_score(y_test,predict)\n",
      "    result = pd.DataFrame([[name,accuracy*100]],columns=[\"MODELS\",\"Accuracy\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"Accuracy\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"ACCURACY\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
      "plt.show()\n",
      "438/22: df\n",
      "438/23:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"CV\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    result = pd.DataFrame([[name,CV*100]],columns=[\"MODELS\",\"CV\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"CV\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"CV\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL CROSS VALIDATION COMPARISON\")\n",
      "plt.show()\n",
      "438/24:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "438/25: r_prob = [0 for _ in range(len(y_test))]\n",
      "438/26:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "r_prob\n",
      "438/27: r_prob = [0 for _ in range(len(y_test))]\n",
      "438/28:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "\n",
      "rf = rf.predict_proba(x_test)\n",
      "rf\n",
      "438/29:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "\n",
      "rfc = rfc.predict_proba(x_test)\n",
      "rfc\n",
      "438/30: rfc_prob = rfc_prob[:.1]\n",
      "438/31: rfc = rfc[:.1]\n",
      "438/32: rfc = rfc[:,1]\n",
      "438/33:\n",
      "rfc = rfc[:,1]\n",
      "rfc\n",
      "438/34: rfc\n",
      "438/35:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "\n",
      "rfc = rfc.predict_proba(x_test)[1]\n",
      "rfc\n",
      "438/36:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "\n",
      "rfc = rfc.predict_proba(x_test)[:,1]\n",
      "rfc\n",
      "438/37:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "\n",
      "rfc = rfc.predict_proba(x_test)\n",
      "rfc\n",
      "439/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "439/2: warnings.filterwarnings(\"ignore\")\n",
      "439/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data.head()\n",
      "439/4:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns Name:\")\n",
      "columns = data.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data Information:\")\n",
      "data.info()\n",
      "print(\"-\"*100)\n",
      "print(\"Data Description: \")\n",
      "print(data.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Counting Null Values:\")\n",
      "print(data.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data correlation: \")\n",
      "print(data.corr())\n",
      "print(\"-\"*100)\n",
      "439/5:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "439/6:\n",
      "data.hist(figsize=(20,20))\n",
      "plt.show()\n",
      "439/7:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "439/8:\n",
      "sns.kdeplot(data[data['target']==1]['chol'],shade=True,color=\"orange\", label=\"Unwell\", alpha=.7)\n",
      "sns.kdeplot(data[data['target']==0]['chol'],shade=True,color=\"dodgerblue\", label=\"Healthy\", alpha=.7)\n",
      "plt.title('Cholesterol in mg/d for both case')\n",
      "plt.show()\n",
      "439/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "439/10: sns.pairplot(data, hue=\"target\")\n",
      "439/11:\n",
      "X = data.drop(\"target\",axis=1)\n",
      "y = data[\"target\"]\n",
      "439/12:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "X = std_scl.fit_transform(X)\n",
      "print(X)\n",
      "439/13:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(X)\n",
      "439/14: x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
      "439/15:\n",
      "lr = LogisticRegression(solver=\"liblinear\").fit(x_train,y_train)\n",
      "gnb = GaussianNB().fit(x_train,y_train)\n",
      "knnc = KNeighborsClassifier().fit(x_train,y_train)\n",
      "dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)\n",
      "rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)\n",
      "xgbc = XGBClassifier().fit(x_train,y_train)\n",
      "svmc = SVC().fit(x_train,y_train)\n",
      "catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)\n",
      "439/16: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,svmc,catbc]\n",
      "439/17:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    error = -cross_val_score(model,x_test,y_test,cv=10,scoring=\"neg_mean_squared_error\",verbose=False).mean()\n",
      "    print(name + \": \")\n",
      "    print(\"-\" * 50)\n",
      "    print(\"Accuracy Score: \",accuracy_score(y_test,predict))\n",
      "    print(\"Cross Validation Score: \",CV)\n",
      "    print(\"Error: \",np.sqrt(error))\n",
      "    print(\"R-square value: \",r2_score(y_test,predict))\n",
      "    print(\"Confusion matrix: \")\n",
      "    print(confusion_matrix(y_test,predict))\n",
      "    print(\"-\" * 100)\n",
      "439/18:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Accuracy\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    accuracy = accuracy_score(y_test,predict)\n",
      "    result = pd.DataFrame([[name,accuracy*100]],columns=[\"MODELS\",\"Accuracy\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"Accuracy\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"ACCURACY\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
      "plt.show()\n",
      "439/19:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"CV\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    result = pd.DataFrame([[name,CV*100]],columns=[\"MODELS\",\"CV\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"CV\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"CV\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL CROSS VALIDATION COMPARISON\")\n",
      "plt.show()\n",
      "439/20:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "\n",
      "rfc = rfc.predict_proba(x_test)\n",
      "rfc\n",
      "439/21: rfc = rfc[:,1]\n",
      "439/22: rfc\n",
      "439/23:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "\n",
      "rfcc = rfc.predict_proba(x_test)[:,1]\n",
      "rfcc\n",
      "439/24:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "\n",
      "rfcca = rfc.predict_proba(x_test)[:,1]\n",
      "rfcca\n",
      "439/25: r_prob = [0 for _ in range(len(y_test))]\n",
      "439/26:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "\n",
      "rfcc = rfc.predict_proba(x_test)\n",
      "439/27:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    error = -cross_val_score(model,x_test,y_test,cv=10,scoring=\"neg_mean_squared_error\",verbose=False).mean()\n",
      "    print(name + \": \")\n",
      "    print(\"-\" * 50)\n",
      "    print(\"Accuracy Score: \",accuracy_score(y_test,predict))\n",
      "    print(\"Cross Validation Score: \",CV)\n",
      "    print(\"Error: \",np.sqrt(error))\n",
      "    print(\"R-square value: \",r2_score(y_test,predict))\n",
      "    print(\"Confusion matrix: \")\n",
      "    print(confusion_matrix(y_test,predict))\n",
      "    print(\"-\" * 100)\n",
      "439/28:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Accuracy\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    accuracy = accuracy_score(y_test,predict)\n",
      "    result = pd.DataFrame([[name,accuracy*100]],columns=[\"MODELS\",\"Accuracy\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"Accuracy\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"ACCURACY\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
      "plt.show()\n",
      "439/29:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"CV\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    result = pd.DataFrame([[name,CV*100]],columns=[\"MODELS\",\"CV\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"CV\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"CV\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL CROSS VALIDATION COMPARISON\")\n",
      "plt.show()\n",
      "439/30:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "\n",
      "rfcc = rfc.predict_proba(x_test)\n",
      "439/31:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "\n",
      "rfcc = rfc.predict_proba(x_test)\n",
      "439/32: rfcc\n",
      "439/33:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "\n",
      "rfcc = rfc.predict_proba(x_test)\n",
      "440/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "440/2: warnings.filterwarnings(\"ignore\")\n",
      "440/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data.head()\n",
      "440/4:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns Name:\")\n",
      "columns = data.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data Information:\")\n",
      "data.info()\n",
      "print(\"-\"*100)\n",
      "print(\"Data Description: \")\n",
      "print(data.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Counting Null Values:\")\n",
      "print(data.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data correlation: \")\n",
      "print(data.corr())\n",
      "print(\"-\"*100)\n",
      "440/5:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "440/6:\n",
      "data.hist(figsize=(20,20))\n",
      "plt.show()\n",
      "440/7:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "440/8:\n",
      "sns.kdeplot(data[data['target']==1]['chol'],shade=True,color=\"orange\", label=\"Unwell\", alpha=.7)\n",
      "sns.kdeplot(data[data['target']==0]['chol'],shade=True,color=\"dodgerblue\", label=\"Healthy\", alpha=.7)\n",
      "plt.title('Cholesterol in mg/d for both case')\n",
      "plt.show()\n",
      "440/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "440/10: sns.pairplot(data, hue=\"target\")\n",
      "440/11:\n",
      "X = data.drop(\"target\",axis=1)\n",
      "y = data[\"target\"]\n",
      "440/12:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "X = std_scl.fit_transform(X)\n",
      "print(X)\n",
      "440/13:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(X)\n",
      "440/14: x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
      "440/15:\n",
      "lr = LogisticRegression(solver=\"liblinear\").fit(x_train,y_train)\n",
      "gnb = GaussianNB().fit(x_train,y_train)\n",
      "knnc = KNeighborsClassifier().fit(x_train,y_train)\n",
      "dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)\n",
      "rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)\n",
      "xgbc = XGBClassifier().fit(x_train,y_train)\n",
      "svmc = SVC().fit(x_train,y_train)\n",
      "catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)\n",
      "440/16: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,svmc,catbc]\n",
      "440/17:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    error = -cross_val_score(model,x_test,y_test,cv=10,scoring=\"neg_mean_squared_error\",verbose=False).mean()\n",
      "    print(name + \": \")\n",
      "    print(\"-\" * 50)\n",
      "    print(\"Accuracy Score: \",accuracy_score(y_test,predict))\n",
      "    print(\"Cross Validation Score: \",CV)\n",
      "    print(\"Error: \",np.sqrt(error))\n",
      "    print(\"R-square value: \",r2_score(y_test,predict))\n",
      "    print(\"Confusion matrix: \")\n",
      "    print(confusion_matrix(y_test,predict))\n",
      "    print(\"-\" * 100)\n",
      "440/18:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Accuracy\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    accuracy = accuracy_score(y_test,predict)\n",
      "    result = pd.DataFrame([[name,accuracy*100]],columns=[\"MODELS\",\"Accuracy\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"Accuracy\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"ACCURACY\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
      "plt.show()\n",
      "440/19:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"CV\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    result = pd.DataFrame([[name,CV*100]],columns=[\"MODELS\",\"CV\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"CV\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"CV\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL CROSS VALIDATION COMPARISON\")\n",
      "plt.show()\n",
      "440/20: r_prob = [0 for _ in range(len(y_test))]\n",
      "440/21: rfcc\n",
      "440/22:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "rcc = rfc.predict_proba(x_test)\n",
      "440/23: rcc\n",
      "440/24:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "rcc = rfc.predict_proba(x_test)\n",
      "440/25:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "rcc = rfc.predict_proba(x_test)[:,1]\n",
      "440/26: rcc\n",
      "440/27:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Roc_auc_score\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)\n",
      "    auroc_score = roc_auc_score(y_test,predict)\n",
      "    print(name,\"score: \",auroc_score)\n",
      "    print(\"-\"*50)\n",
      "440/28:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Roc_auc_score\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)\n",
      "    auroc_score = roc_auc_score(y_test,predict)\n",
      "    print(name+\"score: \",auroc_score)\n",
      "    print(\"-\"*50)\n",
      "440/29:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Roc_auc_score\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    auroc_score = roc_auc_score(y_test,predict)\n",
      "    print(name+\"score: \",auroc_score)\n",
      "    print(\"-\"*50)\n",
      "441/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "441/2: warnings.filterwarnings(\"ignore\")\n",
      "441/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data.head()\n",
      "441/4:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns Name:\")\n",
      "columns = data.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data Information:\")\n",
      "data.info()\n",
      "print(\"-\"*100)\n",
      "print(\"Data Description: \")\n",
      "print(data.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Counting Null Values:\")\n",
      "print(data.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data correlation: \")\n",
      "print(data.corr())\n",
      "print(\"-\"*100)\n",
      "441/5:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "441/6:\n",
      "data.hist(figsize=(20,20))\n",
      "plt.show()\n",
      "441/7:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "441/8:\n",
      "sns.kdeplot(data[data['target']==1]['chol'],shade=True,color=\"orange\", label=\"Unwell\", alpha=.7)\n",
      "sns.kdeplot(data[data['target']==0]['chol'],shade=True,color=\"dodgerblue\", label=\"Healthy\", alpha=.7)\n",
      "plt.title('Cholesterol in mg/d for both case')\n",
      "plt.show()\n",
      "441/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "441/10: sns.pairplot(data, hue=\"target\")\n",
      "441/11:\n",
      "X = data.drop(\"target\",axis=1)\n",
      "y = data[\"target\"]\n",
      "441/12:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "X = std_scl.fit_transform(X)\n",
      "print(X)\n",
      "441/13:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(X)\n",
      "441/14: x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
      "441/15:\n",
      "lr = LogisticRegression(solver=\"liblinear\").fit(x_train,y_train)\n",
      "gnb = GaussianNB().fit(x_train,y_train)\n",
      "knnc = KNeighborsClassifier().fit(x_train,y_train)\n",
      "dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)\n",
      "rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)\n",
      "xgbc = XGBClassifier().fit(x_train,y_train)\n",
      "catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)\n",
      "441/16: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,catbc]\n",
      "441/17:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    error = -cross_val_score(model,x_test,y_test,cv=10,scoring=\"neg_mean_squared_error\",verbose=False).mean()\n",
      "    print(name + \": \")\n",
      "    print(\"-\" * 50)\n",
      "    print(\"Accuracy Score: \",accuracy_score(y_test,predict))\n",
      "    print(\"Cross Validation Score: \",CV)\n",
      "    print(\"Error: \",np.sqrt(error))\n",
      "    print(\"R-square value: \",r2_score(y_test,predict))\n",
      "    print(\"Confusion matrix: \")\n",
      "    print(confusion_matrix(y_test,predict))\n",
      "    print(\"-\" * 100)\n",
      "441/18:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Accuracy\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    accuracy = accuracy_score(y_test,predict)\n",
      "    result = pd.DataFrame([[name,accuracy*100]],columns=[\"MODELS\",\"Accuracy\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"Accuracy\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"ACCURACY\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
      "plt.show()\n",
      "441/19:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"CV\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    result = pd.DataFrame([[name,CV*100]],columns=[\"MODELS\",\"CV\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"CV\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"CV\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL CROSS VALIDATION COMPARISON\")\n",
      "plt.show()\n",
      "441/20:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "r_auc = roc_auc_score(y_test,r_prob)\n",
      "441/21:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    auroc_score = roc_auc_score(y_test,predict)\n",
      "    print(name+\"score: \",auroc_score)\n",
      "    print(\"-\"*50)\n",
      "441/22: rfc\n",
      "441/23:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    auroc_score = roc_auc_score(y_test,predict)\n",
      "    print(name+\" score: \",auroc_score)\n",
      "    print(\"-\"*50)\n",
      "441/24:\n",
      "r_fpt,r_tpr,_= roc_curve(y_test,r_prob)\n",
      "model_dict={}\n",
      "\n",
      "for model in model_names:\n",
      "    name = model.__class__.name\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    fpt,tpr,_= roc_curve(y_test,predict)\n",
      "    model_dict[name]=[fpt,tpr]\n",
      "    \n",
      "model_dict\n",
      "441/25:\n",
      "r_fpt,r_tpr,_= roc_curve(y_test,r_prob)\n",
      "model_dict={}\n",
      "\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    fpt,tpr,_= roc_curve(y_test,predict)\n",
      "    model_dict[name]=[fpt,tpr]\n",
      "    \n",
      "model_dict\n",
      "441/26:\n",
      "r_fpt,r_tpr,_= roc_curve(y_test,r_prob)\n",
      "model_dict={}\n",
      "\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    fpt,tpr,_= roc_curve(y_test,predict)\n",
      "    model_dict[name]=[fpt,tpr]\n",
      "    \n",
      "model_dict[\"LogisticRegression\"][0]\n",
      "441/27:\n",
      "r_fpt,r_tpr,_= roc_curve(y_test,r_prob)\n",
      "model_dict={}\n",
      "\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    fpt,tpr,_= roc_curve(y_test,predict)\n",
      "    model_dict[name]=[fpt,tpr]\n",
      "    \n",
      "model_dict[\"LogisticRegression\"][1]\n",
      "441/28:\n",
      "r_fpt,r_tpr,_= roc_curve(y_test,r_prob)\n",
      "model_dict={}\n",
      "\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    fpt,tpr,_= roc_curve(y_test,predict)\n",
      "    model_dict[name]=[fpt,tpr]\n",
      "441/29: model_dict[\"LogisticRegression\"][1]\n",
      "441/30: model_dict[\"LogisticRegression\"][0]\n",
      "441/31:\n",
      "r_fpr,r_tpr,_= roc_curve(y_test,r_prob)\n",
      "model_dict={}\n",
      "\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    fpr,tpr,_= roc_curve(y_test,predict)\n",
      "    model_dict[name]=[fpt,tpr]\n",
      "441/32: model_dict[\"LogisticRegression\"][0]\n",
      "441/33:\n",
      "r_fpr,r_tpr,_= roc_curve(y_test,r_prob)\n",
      "model_dict={}\n",
      "\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    fpr,tpr,_= roc_curve(y_test,predict)\n",
      "    model_dict[name]=[fpt,tpr]\n",
      "441/34: model_dict[\"LogisticRegression\"][0]\n",
      "441/35: model_dict[\"LogisticRegression\"][1]\n",
      "441/36: model_dict[\"LogisticRegression\"][0]\n",
      "441/37:\n",
      "plt.plot(r_fpr,r_tpr,linestyle=\"--\")\n",
      "plt.title(\"ROC plot\")\n",
      "plt.xlabel(\"False positive rate.\")\n",
      "plt.ylabel(\"True positive rate.\")\n",
      "plt.show()\n",
      "441/38:\n",
      "plt.plot(r_fpr,r_tpr,linestyle=\"--\")\n",
      "plt.plot(model_dict[\"LogisticRegression\"][0],model_dict[\"LogisticRegression\"][1],linestyle='.')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.title(\"ROC plot\")\n",
      "plt.xlabel(\"False positive rate.\")\n",
      "plt.ylabel(\"True positive rate.\")\n",
      "plt.show()\n",
      "441/39:\n",
      "plt.plot(r_fpr,r_tpr,linestyle=\"--\")\n",
      "plt.plot(*model_dict[\"LogisticRegression\"][0],*model_dict[\"LogisticRegression\"][1],linestyle='.')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.title(\"ROC plot\")\n",
      "plt.xlabel(\"False positive rate.\")\n",
      "plt.ylabel(\"True positive rate.\")\n",
      "plt.show()\n",
      "441/40: *model_dict[\"LogisticRegression\"][0]\n",
      "441/41: model_dict[\"LogisticRegression\"][0]\n",
      "441/42: r_fpr\n",
      "441/43:\n",
      "plt.plot(r_fpr,r_tpr,linestyle=\"--\")\n",
      "plt.plot(model_dict[\"LogisticRegression\"][0],model_dict[\"LogisticRegression\"][1],linestyle='.')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.title(\"ROC plot\")\n",
      "plt.xlabel(\"False positive rate.\")\n",
      "plt.ylabel(\"True positive rate.\")\n",
      "plt.show()\n",
      "441/44: r_tpr\n",
      "441/45: model_dict[\"LogisticRegression\"][1]\n",
      "441/46: len(model_dict[\"LogisticRegression\"][1])\n",
      "441/47: len(model_dict[\"LogisticRegression\"][0])\n",
      "442/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "442/2: warnings.filterwarnings(\"ignore\")\n",
      "442/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data.head()\n",
      "442/4:\n",
      "print(\"Shape:\")\n",
      "print(data.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns Name:\")\n",
      "columns = data.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data Information:\")\n",
      "data.info()\n",
      "print(\"-\"*100)\n",
      "print(\"Data Description: \")\n",
      "print(data.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Counting Null Values:\")\n",
      "print(data.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data correlation: \")\n",
      "print(data.corr())\n",
      "print(\"-\"*100)\n",
      "442/5:\n",
      "figure = plt.figure(figsize=(10,8))\n",
      "sns.heatmap(data.corr(method=\"pearson\"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)\n",
      "plt.title(\"PEARSON\")\n",
      "plt.xlabel(\"COLUMNS\")\n",
      "plt.ylabel(\"COLUMNS\")\n",
      "plt.show()\n",
      "442/6:\n",
      "data.hist(figsize=(20,20))\n",
      "plt.show()\n",
      "442/7:\n",
      "sns.countplot(data[\"target\"])\n",
      "print(data.groupby([\"target\"]).count())\n",
      "\n",
      "# 0 -> No Heart Disease, 1 -> Heart Disease\n",
      "442/8:\n",
      "sns.kdeplot(data[data['target']==1]['chol'],shade=True,color=\"orange\", label=\"Unwell\", alpha=.7)\n",
      "sns.kdeplot(data[data['target']==0]['chol'],shade=True,color=\"dodgerblue\", label=\"Healthy\", alpha=.7)\n",
      "plt.title('Cholesterol in mg/d for both case')\n",
      "plt.show()\n",
      "442/9:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data)\n",
      "442/10: sns.pairplot(data, hue=\"target\")\n",
      "442/11:\n",
      "X = data.drop(\"target\",axis=1)\n",
      "y = data[\"target\"]\n",
      "442/12:\n",
      "std_scl = StandardScaler()\n",
      "\n",
      "X = std_scl.fit_transform(X)\n",
      "print(X)\n",
      "442/13:\n",
      "plt.figure(figsize=(15,8))\n",
      "sns.heatmap(X)\n",
      "442/14: x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
      "442/15:\n",
      "lr = LogisticRegression(solver=\"liblinear\").fit(x_train,y_train)\n",
      "gnb = GaussianNB().fit(x_train,y_train)\n",
      "knnc = KNeighborsClassifier().fit(x_train,y_train)\n",
      "dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)\n",
      "rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)\n",
      "xgbc = XGBClassifier().fit(x_train,y_train)\n",
      "catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)\n",
      "442/16: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,catbc]\n",
      "442/17:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    error = -cross_val_score(model,x_test,y_test,cv=10,scoring=\"neg_mean_squared_error\",verbose=False).mean()\n",
      "    print(name + \": \")\n",
      "    print(\"-\" * 50)\n",
      "    print(\"Accuracy Score: \",accuracy_score(y_test,predict))\n",
      "    print(\"Cross Validation Score: \",CV)\n",
      "    print(\"Error: \",np.sqrt(error))\n",
      "    print(\"R-square value: \",r2_score(y_test,predict))\n",
      "    print(\"Confusion matrix: \")\n",
      "    print(confusion_matrix(y_test,predict))\n",
      "    print(\"-\" * 100)\n",
      "442/18:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Accuracy\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    accuracy = accuracy_score(y_test,predict)\n",
      "    result = pd.DataFrame([[name,accuracy*100]],columns=[\"MODELS\",\"Accuracy\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"Accuracy\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"ACCURACY\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
      "plt.show()\n",
      "442/19:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"CV\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    result = pd.DataFrame([[name,CV*100]],columns=[\"MODELS\",\"CV\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"CV\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"CV\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL CROSS VALIDATION COMPARISON\")\n",
      "plt.show()\n",
      "442/20:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "r_auc = roc_auc_score(y_test,r_prob)\n",
      "442/21:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    auroc_score = roc_auc_score(y_test,predict)\n",
      "    print(name+\" score: \",auroc_score)\n",
      "    print(\"-\"*50)\n",
      "442/22:\n",
      "r_fpr,r_tpr,_= roc_curve(y_test,r_prob)\n",
      "model_dict={}\n",
      "\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    fpr,tpr,_= roc_curve(y_test,predict)\n",
      "    model_dict[name]=[fpt,tpr]\n",
      "442/23:\n",
      "r_fpr,r_tpr,_= roc_curve(y_test,r_prob)\n",
      "model_dict={}\n",
      "\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    fpr,tpr,_= roc_curve(y_test,predict)\n",
      "    model_dict[name]=[fpr,tpr]\n",
      "442/24:\n",
      "plt.plot(r_fpr,r_tpr,linestyle=\"--\")\n",
      "plt.plot(model_dict[\"LogisticRegression\"][0],model_dict[\"LogisticRegression\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"GaussianNB\"][0],model_dict[\"GaussianNB\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"KNeighborsClassifier\"][0],model_dict[\"KNeighborsClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"DecisionTreeClassifier\"][0],model_dict[\"DecisionTreeClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"RandomForestClassifier\"][0],model_dict[\"RandomForestClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"XGBClassifier\"][0],model_dict[\"XGBClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"CatBoostClassifier\"][0],model_dict[\"CatBoostClassifier\"][1],linestyle='dotted')\n",
      "\n",
      "plt.title(\"ROC plot\")\n",
      "plt.xlabel(\"False positive rate.\")\n",
      "plt.ylabel(\"True positive rate.\")\n",
      "plt.show()\n",
      "442/25:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "442/26: warnings.filterwarnings(\"ignore\")\n",
      "442/27:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data.head()\n",
      "442/28:\n",
      "from dataprep.datasets import load_dataset\n",
      "from dataprep.eda import create_report\n",
      "df = load_datatset(\"heart_disease\")\n",
      "create_report(df)\n",
      "442/29:\n",
      "import dtale\n",
      "import seaborn as sns\n",
      "442/30:\n",
      "df = sns.load_dataset(\"Heart_disease\")\n",
      "dtale.show(df)\n",
      "442/31:\n",
      "df = sns.load_dataset(\"heart_disease\")\n",
      "dtale.show(df)\n",
      "442/32:\n",
      "import dtale\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "442/33:\n",
      "df = pd.read_csv(\"heart_disease\")\n",
      "dtale.show(df)\n",
      "442/34:\n",
      "df = pd.read_csv(\"heart_disease.csv\")\n",
      "dtale.show(df)\n",
      "442/35:\n",
      "from pandas_profiling import ProfileReport\n",
      "profile=ProfileReport(df,explorative=True)\n",
      "profile.to_file(\"output.html\")\n",
      "442/36:\n",
      "import sweetviz as sv\n",
      "report = sv.analyze(df)\n",
      "report.show_html(\"report.html\")\n",
      "443/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "443/2: df = pd.read_csv(\"StudentsPerformance.csv\")\n",
      "443/3: df.head()\n",
      "443/4:\n",
      "print(\"Shape: \")\n",
      "print(df.shape)\n",
      "443/5:\n",
      "print(\"Shape: \")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "443/6:\n",
      "print(\"Shape: \")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns: \")\n",
      "columns = df.columns()\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Finding Null values: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data discription: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "443/7:\n",
      "print(\"Shape: \")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns: \")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Finding Null values: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data discription: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "443/8:\n",
      "def unique_values(columns):\n",
      "    print(columns+\": \",df[columns].unique())\n",
      "    print(\"-\"*100)\n",
      "443/9:\n",
      "for ele in columns:\n",
      "    unique_values(ele)\n",
      "443/10:\n",
      "for ele in columns:\n",
      "    unique_values(ele)\n",
      "443/11: df[\"Percentage\"] = round((df[\"math score\"]+df[\"reading score\"]+df[\"writing score\"])/3,2)\n",
      "443/12: df.head()\n",
      "443/13: df[\"CGPA\"] = round((df[\"Percentage\"])/9.5,2)\n",
      "443/14: df.head()\n",
      "443/15:\n",
      "# 8.0 - 10.0 --> A+\n",
      "# 7.0 - <8.0 --> A\n",
      "# 6.0 - <7.0 --> B+\n",
      "# 5.5 - <6.0 --> B\n",
      "# 4.5 - <5.5 --> C+\n",
      "# 4.0 - <4.5 --> C\n",
      "# 0.0 - <4.0 --> F\n",
      "\n",
      "df[\"CGPA\"][0]\n",
      "443/16:\n",
      "# 8.0 - 10.0 --> A+\n",
      "# 7.0 - <8.0 --> A\n",
      "# 6.0 - <7.0 --> B+\n",
      "# 5.5 - <6.0 --> B\n",
      "# 4.5 - <5.5 --> C+\n",
      "# 4.0 - <4.5 --> C\n",
      "# 0.0 - <4.0 --> F\n",
      "\n",
      "df[\"CGPA\"][1]\n",
      "443/17:\n",
      "# 8.0 - 10.0 --> A+\n",
      "# 7.0 - <8.0 --> A\n",
      "# 6.0 - <7.0 --> B+\n",
      "# 5.5 - <6.0 --> B\n",
      "# 4.5 - <5.5 --> C+\n",
      "# 4.0 - <4.5 --> C\n",
      "# 0.0 - <4.0 --> F\n",
      "\n",
      "df[\"CGPA\"][1000]\n",
      "443/18:\n",
      "# 8.0 - 10.0 --> A+\n",
      "# 7.0 - <8.0 --> A\n",
      "# 6.0 - <7.0 --> B+\n",
      "# 5.5 - <6.0 --> B\n",
      "# 4.5 - <5.5 --> C+\n",
      "# 4.0 - <4.5 --> C\n",
      "# 0.0 - <4.0 --> F\n",
      "\n",
      "df[\"CGPA\"][999]\n",
      "443/19: df.tail()\n",
      "443/20: df[\"Grade\"]\n",
      "443/21: df.head()\n",
      "443/22: df[\"Grade\"]=0\n",
      "443/23: df.head()\n",
      "443/24:\n",
      "# 8.0 - 10.0 --> A+\n",
      "# 7.0 - <8.0 --> A\n",
      "# 6.0 - <7.0 --> B+\n",
      "# 5.5 - <6.0 --> B\n",
      "# 4.5 - <5.5 --> C+\n",
      "# 4.0 - <4.5 --> C\n",
      "# 0.0 - <4.0 --> F\n",
      "def grade(i):\n",
      "    if df[\"CGPA\"][i]>=8.0:\n",
      "        df[\"Grade\"][i] = \"A+\"\n",
      "    elif df[\"CGPA\"][i]>=7.0 and df[\"CGPA\"][i]<8.0:\n",
      "        df[\"Grade\"][i] = \"A\"\n",
      "    elif df[\"CGPA\"][i]>=6.0 and df[\"CGPA\"][i]<7.0:\n",
      "        df[\"Grade\"][i] = \"B+\"\n",
      "    elif df[\"CGPA\"][i]>=5.5 and df[\"CGPA\"][i]<6.0:\n",
      "        df[\"Grade\"][i] = \"B\"\n",
      "    elif df[\"CGPA\"][i]>=4.5 and df[\"CGPA\"][i]<5.5:\n",
      "        df[\"Grade\"][i] = \"C+\"\n",
      "    elif df[\"CGPA\"][i]>=4.0 and df[\"CGPA\"][i]<4.5:\n",
      "        df[\"Grade\"][i] = \"C\"\n",
      "    else:\n",
      "        df[\"Grade\"] = \"F\"\n",
      "443/25:\n",
      "df[\"Grade\"]=\"nan\"\n",
      "for i in range(1000):\n",
      "    grade(i)\n",
      "443/26:\n",
      "df[\"Grade\"]=nan\n",
      "for i in range(1000):\n",
      "    grade(i)\n",
      "443/27:\n",
      "df[\"Grade\"]=0\n",
      "for i in range(1000):\n",
      "    grade(i)\n",
      "443/28:\n",
      "df[\"Grade\"]=\"nan\"\n",
      "for i in range(1000):\n",
      "    grade(i)\n",
      "444/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "444/2: df = pd.read_csv(\"StudentsPerformance.csv\")\n",
      "444/3: df.head()\n",
      "444/4:\n",
      "print(\"Shape: \")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns: \")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Finding Null values: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data discription: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "444/5:\n",
      "def unique_values(columns):\n",
      "    print(columns+\": \",df[columns].unique())\n",
      "    print(\"-\"*100)\n",
      "444/6:\n",
      "for ele in columns:\n",
      "    unique_values(ele)\n",
      "444/7: df[\"Percentage\"] = round((df[\"math score\"]+df[\"reading score\"]+df[\"writing score\"])/3,2)\n",
      "444/8: df.head()\n",
      "444/9: df[\"CGPA\"] = round((df[\"Percentage\"])/9.5,2)\n",
      "444/10:\n",
      "# 8.0 - 10.0 --> A+\n",
      "# 7.0 - <8.0 --> A\n",
      "# 6.0 - <7.0 --> B+\n",
      "# 5.5 - <6.0 --> B\n",
      "# 4.5 - <5.5 --> C+\n",
      "# 4.0 - <4.5 --> C\n",
      "# 0.0 - <4.0 --> F\n",
      "def grade(i):\n",
      "    if df[\"CGPA\"][i]>=8.0:\n",
      "        df[\"Grade\"][i] = \"A+\"\n",
      "    elif df[\"CGPA\"][i]>=7.0 and df[\"CGPA\"][i]<8.0:\n",
      "        df[\"Grade\"][i] = \"A\"\n",
      "    elif df[\"CGPA\"][i]>=6.0 and df[\"CGPA\"][i]<7.0:\n",
      "        df[\"Grade\"][i] = \"B+\"\n",
      "    elif df[\"CGPA\"][i]>=5.5 and df[\"CGPA\"][i]<6.0:\n",
      "        df[\"Grade\"][i] = \"B\"\n",
      "    elif df[\"CGPA\"][i]>=4.5 and df[\"CGPA\"][i]<5.5:\n",
      "        df[\"Grade\"][i] = \"C+\"\n",
      "    elif df[\"CGPA\"][i]>=4.0 and df[\"CGPA\"][i]<4.5:\n",
      "        df[\"Grade\"][i] = \"C\"\n",
      "    else:\n",
      "        df[\"Grade\"] = \"F\"\n",
      "444/11:\n",
      "df[\"Grade\"]=\"nan\"\n",
      "for i in range(1000):\n",
      "    grade(i)\n",
      "444/12: df.head()\n",
      "444/13:\n",
      "df[\"Grade\"]=0\n",
      "for i in range(1000):\n",
      "    grade(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/14:\n",
      "# 8.0 - 10.0 --> A+\n",
      "# 7.0 - <8.0 --> A\n",
      "# 6.0 - <7.0 --> B+\n",
      "# 5.5 - <6.0 --> B\n",
      "# 4.5 - <5.5 --> C+\n",
      "# 4.0 - <4.5 --> C\n",
      "# 0.0 - <4.0 --> F\n",
      "def grade(i):\n",
      "    if df[\"CGPA\"][i]>=8.0:\n",
      "        df[\"Grade\"][i] = \"A+\"\n",
      "    elif df[\"CGPA\"][i]>=7.0 and df[\"CGPA\"][i]<8.0:\n",
      "        df[\"Grade\"][i] = \"A\"\n",
      "    elif df[\"CGPA\"][i]>=6.0 and df[\"CGPA\"][i]<7.0:\n",
      "        df[\"Grade\"][i] = \"B+\"\n",
      "    elif df[\"CGPA\"][i]>=5.5 and df[\"CGPA\"][i]<6.0:\n",
      "        df[\"Grade\"][i] = \"B\"\n",
      "    elif df[\"CGPA\"][i]>=4.5 and df[\"CGPA\"][i]<5.5:\n",
      "        df[\"Grade\"][i] = \"C+\"\n",
      "    elif df[\"CGPA\"][i]>=4.0 and df[\"CGPA\"][i]<4.5:\n",
      "        df[\"Grade\"][i] = \"C\"\n",
      "    else:\n",
      "        df[\"Grade\"] = \"F\"\n",
      "444/15: df.head()\n",
      "445/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "445/2: df = pd.read_csv(\"StudentsPerformance.csv\")\n",
      "445/3: df.head()\n",
      "445/4:\n",
      "print(\"Shape: \")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns: \")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Finding Null values: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data discription: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "445/5:\n",
      "def unique_values(columns):\n",
      "    print(columns+\": \",df[columns].unique())\n",
      "    print(\"-\"*100)\n",
      "445/6:\n",
      "for ele in columns:\n",
      "    unique_values(ele)\n",
      "445/7: df[\"Percentage\"] = round((df[\"math score\"]+df[\"reading score\"]+df[\"writing score\"])/3,2)\n",
      "445/8: df.head()\n",
      "445/9: df[\"CGPA\"] = round((df[\"Percentage\"])/9.5,2)\n",
      "445/10:\n",
      "# 8.0 - 10.0 --> A+\n",
      "# 7.0 - <8.0 --> A\n",
      "# 6.0 - <7.0 --> B+\n",
      "# 5.5 - <6.0 --> B\n",
      "# 4.5 - <5.5 --> C+\n",
      "# 4.0 - <4.5 --> C\n",
      "# 0.0 - <4.0 --> F\n",
      "def grade(i):\n",
      "    if df[\"CGPA\"][i]>=8.0:\n",
      "        df[\"Grade\"][i] = \"A+\"\n",
      "    elif df[\"CGPA\"][i]>=7.0 and df[\"CGPA\"][i]<8.0:\n",
      "        df[\"Grade\"][i] = \"A\"\n",
      "    elif df[\"CGPA\"][i]>=6.0 and df[\"CGPA\"][i]<7.0:\n",
      "        df[\"Grade\"][i] = \"B+\"\n",
      "    elif df[\"CGPA\"][i]>=5.5 and df[\"CGPA\"][i]<6.0:\n",
      "        df[\"Grade\"][i] = \"B\"\n",
      "    elif df[\"CGPA\"][i]>=4.5 and df[\"CGPA\"][i]<5.5:\n",
      "        df[\"Grade\"][i] = \"C+\"\n",
      "    elif df[\"CGPA\"][i]>=4.0 and df[\"CGPA\"][i]<4.5:\n",
      "        df[\"Grade\"][i] = \"C\"\n",
      "    else:\n",
      "        df[\"Grade\"] = \"F\"\n",
      "445/11:\n",
      "df[\"Grade\"]=0\n",
      "for i in range(1000):\n",
      "    grade(i)\n",
      "445/12: df.head()\n",
      "445/13: df[\"Grade\"][0].replacee(\"F\",\"Jay\")\n",
      "445/14: df[\"Grade\"][0].replace(\"F\",\"Jay\")\n",
      "445/15: df.head()\n",
      "445/16: df[\"Grade\"][0].replace(\"F\",\"Jay\",inplace=True)\n",
      "445/17: df[\"Grade\"][0].replace(to_replace=\"F\",value=\"Jay\",inplace=True)\n",
      "445/18: df[\"Grade\"][1].replace(to_replace=\"F\",value=\"Jay\",inplace=True)\n",
      "445/19: df.head()\n",
      "445/20: df[\"Grade\"][1].replace(to_replace=\"F\",value=Jay,inplace=True)\n",
      "445/21: df[\"Grade\"][1].replace(to_replace=\"F\",value=\"Jay\",inplace=True)\n",
      "445/22: type(df[\"Grade\"])\n",
      "445/23: df.info()\n",
      "445/24: df[\"Grade\"][1]\n",
      "445/25: df[\"Grade\"][1].replace(to_replace=\"F\",value=\"Jay\")\n",
      "445/26: df[\"Grade\"][1].replace(to_replace=\"F\",value=\"Jay\")\n",
      "445/27: df[\"Grade\"][1]\n",
      "445/28: df[\"Grade\"][1].replace(to_replace=\"F\",value=\"Jay\")\n",
      "445/29: df[\"Grade\"][3].replace(to_replace=\"F\",value=\"Jay\")\n",
      "445/30: df[\"Grade\"][1].replace(to_replace=\"Jay\",value=\"F\",inplace=True)\n",
      "445/31: df.at[0,\"Grade\"]='jay'\n",
      "445/32: df.head()\n",
      "446/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "446/2: df = pd.read_csv(\"StudentsPerformance.csv\")\n",
      "446/3: df.head()\n",
      "446/4:\n",
      "print(\"Shape: \")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns: \")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Finding Null values: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data discription: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "446/5:\n",
      "def unique_values(columns):\n",
      "    print(columns+\": \",df[columns].unique())\n",
      "    print(\"-\"*100)\n",
      "446/6:\n",
      "for ele in columns:\n",
      "    unique_values(ele)\n",
      "446/7: df[\"Percentage\"] = round((df[\"math score\"]+df[\"reading score\"]+df[\"writing score\"])/3,2)\n",
      "446/8: df.head()\n",
      "446/9: df[\"CGPA\"] = round((df[\"Percentage\"])/9.5,2)\n",
      "446/10:\n",
      "# 8.0 - 10.0 --> A+\n",
      "# 7.0 - <8.0 --> A\n",
      "# 6.0 - <7.0 --> B+\n",
      "# 5.5 - <6.0 --> B\n",
      "# 4.5 - <5.5 --> C+\n",
      "# 4.0 - <4.5 --> C\n",
      "# 0.0 - <4.0 --> F\n",
      "def grade(i):\n",
      "    if df[\"CGPA\"][i]>=8.0:\n",
      "        df.at[i,\"Grade\"] = \"A+\"\n",
      "    elif df[\"CGPA\"][i]>=7.0 and df[\"CGPA\"][i]<8.0:\n",
      "        df.at[i,\"Grade\"] = \"A\"\n",
      "    elif df[\"CGPA\"][i]>=6.0 and df[\"CGPA\"][i]<7.0:\n",
      "        df.at[i,\"Grade\"] = \"B+\"\n",
      "    elif df[\"CGPA\"][i]>=5.5 and df[\"CGPA\"][i]<6.0:\n",
      "        df.at[i,\"Grade\"] = \"B\"\n",
      "    elif df[\"CGPA\"][i]>=4.5 and df[\"CGPA\"][i]<5.5:\n",
      "        df.at[i,\"Grade\"] = \"C+\"\n",
      "    elif df[\"CGPA\"][i]>=4.0 and df[\"CGPA\"][i]<4.5:\n",
      "        df.at[i,\"Grade\"] = \"C\"\n",
      "    else:\n",
      "        df.at[i,\"Grade\"] = \"F\"\n",
      "446/11:\n",
      "df[\"Grade\"]=\"nan\"\n",
      "for i in range(1000):\n",
      "    grade(i)\n",
      "446/12: df.head()\n",
      "446/13: df.shape\n",
      "446/14: df.shape()\n",
      "446/15: df.shape\n",
      "446/16: plt.bar(df[\"gender\"],df[\"Grade\"])\n",
      "446/17:\n",
      "plt.bar(df[\"gender\"],df[\"Grade\"])\n",
      "plt.show()\n",
      "446/18:\n",
      "plt.bar(df[\"Grade\"],df[\"gender\"])\n",
      "plt.show()\n",
      "446/19:\n",
      "sns.catplot(x='gender', kind='count', hue='Grade', data=df)\n",
      "plt.show()\n",
      "446/20:\n",
      "sns.catplot(x='gender', kind='bar', hue='Grade', data=df)\n",
      "plt.show()\n",
      "446/21:\n",
      "sns.catplot(x='gender', kind='count', hue='Grade', data=df)\n",
      "plt.show()\n",
      "446/22:\n",
      "sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df)\n",
      "plt.show()\n",
      "446/23:\n",
      "sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df)\n",
      "#plt.show()\n",
      "446/24:\n",
      "plt.figure(figsize=(12,12))\n",
      "sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df)\n",
      "#plt.show()\n",
      "446/25:\n",
      "plt.figure(figsize=(12,5))\n",
      "sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df)\n",
      "#plt.show()\n",
      "446/26:\n",
      "plt.figure(figsize=(12,5))\n",
      "sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df)\n",
      "plt.show()\n",
      "446/27:\n",
      "plt.figure(figsize=(12,15))\n",
      "sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df)\n",
      "plt.show()\n",
      "446/28:\n",
      "from pandas_profiling import ProfileReport\n",
      "profile=ProfileReport(df,explorative=True)\n",
      "profile.to_file(\"output.html\")\n",
      "446/29: df[\"gender\"].value_count()\n",
      "446/30: df[\"gender\"].values_count()\n",
      "446/31: df[\"gender\"].value_counts()\n",
      "446/32: df[\"Grade\"].value_counts()\n",
      "446/33:\n",
      "print(df[\"Grade\"].value_counts())\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",hue=\"Grade\")\n",
      "446/34:\n",
      "print(df[\"Grade\"].value_counts())\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",hue=\"Grade\",data=df)\n",
      "446/35:\n",
      "print(df[\"Grade\"].value_counts())\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df)\n",
      "446/36:\n",
      "print(df[\"Grade\"].value_counts())\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df)\n",
      "plt.subplot(2,2)\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df)\n",
      "446/37:\n",
      "print(df[\"Grade\"].value_counts())\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df)\n",
      "plt.subplot(1,2,2)\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df)\n",
      "446/38:\n",
      "print(df[\"Grade\"].value_counts())\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df)\n",
      "plt.subplot(1,2,1)\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df)\n",
      "446/39:\n",
      "print(df[\"Grade\"].value_counts())\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df)\n",
      "plt.subplot(1,1,2)\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df)\n",
      "446/40:\n",
      "print(df[\"Grade\"].value_counts())\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df)\n",
      "plt.subplot(1,1,1)\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df)\n",
      "446/41:\n",
      "print(df[\"Grade\"].value_counts())\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df)\n",
      "446/42:\n",
      "print(df[\"Grade\"].value_counts())\n",
      "sns.catplot(x=\"test preparation course\",kind=\"count\",hue=\"Grade\",data=df)\n",
      "446/43: sns.catplot(x=\"test preparation course\",kind=\"count\",hue=\"Grade\",data=df)\n",
      "446/44: sns.catplot(x=\"test preparation course\",kind=\"count\",hue=\"Grade\",data=df, palette=\"ch:.25\")\n",
      "446/45:\n",
      "print(df[\"Grade\"].value_counts())\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df,palette=\"ch:.25\")\n",
      "446/46: sns.catplot(x=\"lunch\",kind=\"bar\",hue=\"Grade\",data=df,palette=\"ch:.25\")\n",
      "446/47: sns.catplot(x=\"lunch\",kind=\"count\",hue=\"Grade\",data=df,palette=\"ch:.25\")\n",
      "446/48: sns.catplot(x=\"race/ethnicity\", kind=\"count\", hue=\"Grade\")\n",
      "446/49: sns.catplot(x=\"race/ethnicity\", kind=\"count\", hue=\"Grade\",data=df)\n",
      "446/50: sns.catplot(x=\"lunch\",kind=\"count\",hue=\"Grade\",data=df,palette=\"ch:.29\")\n",
      "446/51: sns.catplot(x=\"lunch\",kind=\"count\",hue=\"Grade\",data=df,palette=\"ch:.50\")\n",
      "446/52: sns.catplot(x=\"lunch\",kind=\"count\",hue=\"Grade\",data=df,palette=\"ch:1\")\n",
      "446/53: sns.catplot(x=\"lunch\",kind=\"count\",hue=\"Grade\",data=df,palette=\"ch:1.2\")\n",
      "446/54: sns.catplot(x=\"race/ethnicity\", kind=\"count\", hue=\"Grade\",data=df,palette=\"ch:1.5\")\n",
      "446/55: sns.catplot(x=\"race/ethnicity\", kind=\"count\", hue=\"Grade\",data=df,palette=\"ch:2.5\")\n",
      "446/56: sns.catplot(x=\"race/ethnicity\", kind=\"count\", hue=\"Grade\",data=df,palette=\"ch:2.5\", col=\"race/ethnicity\")\n",
      "446/57: sns.catplot(x=\"race/ethnicity\", kind=\"count\", hue=\"Grade\",data=df,palette=\"ch:2.5\", row=\"race/ethnicity\")\n",
      "446/58:\n",
      "sns.catplot(x=\"race/ethnicity\",\n",
      "            kind=\"count\", hue=\"Grade\",\n",
      "            data=df,palette=\"ch:2.5\", \n",
      "            col=\"race/ethnicity\",\n",
      "           col_order=[\"group A\",\"group B\",\"group C\",\"group D\"])\n",
      "446/59:\n",
      "plt.figure(figsize=(12,15))\n",
      "sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, col=\"parental level of education\")\n",
      "plt.show()\n",
      "446/60:\n",
      "plt.figure(figsize=(12,15))\n",
      "sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row=\"parental level of education\")\n",
      "plt.show()\n",
      "446/61:\n",
      "plt.figure(figsize=(12,15))\n",
      "sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row=\"parental level of education\")\n",
      "plt.show()\n",
      "446/62: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row=\"parental level of education\")\n",
      "446/63: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, col=\"parental level of education\")\n",
      "446/64: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, col=\"parental level of education\")\n",
      "446/65: sns.catplot(kind='count', hue='Grade', data=df, col=\"parental level of education\")\n",
      "446/66: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, col=\"parental level of education\")\n",
      "446/67: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row=\"parental level of education\")\n",
      "446/68: sns.catplot(x='parental level of education', kind='box', hue='Grade', data=df, row=\"parental level of education\")\n",
      "446/69: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row=\"parental level of education\")\n",
      "446/70: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row=\"parental level of education\",aspect=2:4)\n",
      "446/71: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row=\"parental level of education\",aspect=4)\n",
      "446/72: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row=\"parental level of education\",aspect=3)\n",
      "446/73: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)\n",
      "446/74:\n",
      "sns.set(style=\"darkgrid\",font_scale=2)\n",
      "sns.catplot(x='gender', kind='count', hue='Grade', data=df)\n",
      "plt.show()\n",
      "446/75: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)\n",
      "446/76:\n",
      "sns.set(style=\"darkgrid\",font_scale=1)\n",
      "sns.catplot(x='gender', kind='count', hue='Grade', data=df)\n",
      "plt.show()\n",
      "446/77: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)\n",
      "446/78:\n",
      "sns.set(style=\"darkgrid\",font_scale=1.2)\n",
      "sns.catplot(x='gender', kind='count', hue='Grade', data=df)\n",
      "plt.show()\n",
      "446/79: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)\n",
      "446/80:\n",
      "print(df[\"Grade\"].value_counts())\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df,palette=\"ch:.25\")\n",
      "446/81: sns.catplot(x=\"test preparation course\",kind=\"count\",hue=\"Grade\",data=df, palette=\"ch:.25\")\n",
      "446/82: sns.catplot(x=\"lunch\",kind=\"count\",hue=\"Grade\",data=df,palette=\"ch:1.2\")\n",
      "446/83:\n",
      "sns.catplot(x=\"race/ethnicity\",\n",
      "            kind=\"count\",\n",
      "            hue=\"Grade\",\n",
      "            data=df,\n",
      "            palette=\"ch:2.5\", \n",
      "            col=\"race/ethnicity\",\n",
      "           col_order=[\"group A\",\"group B\",\"group C\",\"group D\"])\n",
      "446/84:\n",
      "sns.catplot(x=\"race/ethnicity\",\n",
      "            kind=\"count\",\n",
      "            hue=\"Grade\",\n",
      "            data=df,\n",
      "            palette=\"ch:2.5\", \n",
      "            col=\"race/ethnicity\",\n",
      "           col_order=[\"group A\",\"group B\",\"group C\",\"group D\"],\n",
      "           aspect=3)\n",
      "446/85:\n",
      "sns.catplot(x=\"race/ethnicity\",\n",
      "            kind=\"count\",\n",
      "            hue=\"Grade\",\n",
      "            data=df,\n",
      "            palette=\"ch:2.5\", \n",
      "            row=\"race/ethnicity\",\n",
      "            col_order=[\"group A\",\"group B\",\"group C\",\"group D\"],\n",
      "            aspect=3)\n",
      "446/86: sns.countplot(x=\"race/ethnicity\",hue=\"parental level of education\",kind=\"count\")\n",
      "446/87: sns.countplot(x=\"race/ethnicity\",hue=\"parental level of education\",kind=\"count\",data=df)\n",
      "446/88: sns.catplot(x=\"race/ethnicity\",hue=\"parental level of education\",kind=\"count\",data=df)\n",
      "446/89: sns.catplot(x=\"race/ethnicity\",hue=\"parental level of education\",kind=\"count\",data=df,col=\"parental level of education\")\n",
      "446/90: sns.catplot(x=\"race/ethnicity\",hue=\"parental level of education\",kind=\"count\",data=df,col=\"parental level of education\",col_wrap=3)\n",
      "446/91: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3,col_wrap=3)\n",
      "446/92: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)\n",
      "446/93:\n",
      "sns.catplot(x=\"race/ethnicity\",\n",
      "            hue=\"parental level of education\",\n",
      "            kind=\"count\",\n",
      "            data=df,\n",
      "            col=\"parental level of education\",\n",
      "            col_wrap=3,\n",
      "           palette=\"ch:0.2\")\n",
      "446/94:\n",
      "sns.catplot(x=\"race/ethnicity\",\n",
      "            kind=\"count\",\n",
      "            hue=\"Grade\",\n",
      "            data=df,\n",
      "            palette=\"ch:2.5\", \n",
      "            row=\"race/ethnicity\",\n",
      "            row_order=[\"group A\",\"group B\",\"group C\",\"group D\"],\n",
      "            aspect=3)\n",
      "446/95: sns.catplot(x=\"math score\",data=df)\n",
      "446/96: sns.catplot(x=\"math score\",data=df,kind=\"bar\")\n",
      "446/97: sns.catplot(y=\"math score\",data=df,kind=\"bar\")\n",
      "446/98: sns.catplot(y=\"math score\",data=df,kind=\"hist\")\n",
      "446/99: sns.displot()\n",
      "446/100: sns.distplot()\n",
      "446/101: sns.distplot(df[\"math score\"])\n",
      "446/102: sns.distplot(df[\"math score\"],fit=norm)\n",
      "446/103:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from scipy.stats import norm\n",
      "446/104: sns.distplot(df[\"math score\"],fit=norm)\n",
      "446/105: sns.distplot(df[\"reading score\"],fit=norm)\n",
      "446/106: sns.distplot(df[\"writing score\"],fit=norm)\n",
      "446/107: sns.countplot(x=\"Grade\",data=df)\n",
      "446/108: sns.countplot(x=\"Percentage\",data=df)\n",
      "446/109: df.corr()\n",
      "446/110:\n",
      "corr = df.corr()\n",
      "\n",
      "# Generate a mask for the upper triangle\n",
      "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
      "\n",
      "# Set up the matplotlib figure\n",
      "f, ax = plt.subplots(figsize=(11, 9))\n",
      "\n",
      "# Generate a custom diverging colormap\n",
      "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
      "\n",
      "# Draw the heatmap with the mask and correct aspect ratio\n",
      "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
      "446/111: sns.heatmap(corr)\n",
      "446/112: sns.heatmap(corr,annot=True)\n",
      "446/113:\n",
      "corr = df.corr()\n",
      "\n",
      "# Generate a mask for the upper triangle\n",
      "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
      "\n",
      "# Set up the matplotlib figure\n",
      "f, ax = plt.subplots(figsize=(11, 9))\n",
      "\n",
      "# Generate a custom diverging colormap\n",
      "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
      "\n",
      "# Draw the heatmap with the mask and correct aspect ratio\n",
      "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annote=True)\n",
      "446/114:\n",
      "corr = df.corr()\n",
      "\n",
      "# Generate a mask for the upper triangle\n",
      "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
      "\n",
      "# Set up the matplotlib figure\n",
      "f, ax = plt.subplots(figsize=(11, 9))\n",
      "\n",
      "# Generate a custom diverging colormap\n",
      "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
      "\n",
      "# Draw the heatmap with the mask and correct aspect ratio\n",
      "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True)\n",
      "446/115:\n",
      "corr = df.corr(method='pearson')\n",
      "\n",
      "# Generate a mask for the upper triangle\n",
      "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
      "\n",
      "# Set up the matplotlib figure\n",
      "f, ax = plt.subplots(figsize=(11, 9))\n",
      "\n",
      "# Generate a custom diverging colormap\n",
      "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
      "\n",
      "# Draw the heatmap with the mask and correct aspect ratio\n",
      "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True)\n",
      "446/116: sns.heatmap(df.corr(method='pearson'),annot=True,)\n",
      "446/117: sns.heatmap(df.corr(method='spherman'),annot=True,)\n",
      "446/118: sns.heatmap(df.corr(method='spearman'),annot=True,)\n",
      "446/119: sns.heatmap(df.corr(method='pearson'),annot=True)\n",
      "446/120: sns.heatmap(df.corr(method='spearman'),annot=True)\n",
      "446/121: sns.heatmap(df.corr(method='pearson'),annot=True,linewidths=0.5)\n",
      "446/122: sns.heatmap(df.corr(method='spearman'),annot=True,linewidths=0.5)\n",
      "447/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from scipy.stats import norm\n",
      "447/2: df = pd.read_csv(\"StudentsPerformance.csv\")\n",
      "447/3: df.head()\n",
      "447/4:\n",
      "print(\"Shape: \")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns: \")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Finding Null values: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data discription: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "447/5:\n",
      "def unique_values(columns):\n",
      "    print(columns+\": \",df[columns].unique())\n",
      "    print(\"-\"*100)\n",
      "447/6:\n",
      "for ele in columns:\n",
      "    unique_values(ele)\n",
      "447/7: df[\"Percentage\"] = round((df[\"math score\"]+df[\"reading score\"]+df[\"writing score\"])/3,2)\n",
      "447/8: df.head()\n",
      "447/9: df[\"CGPA\"] = round((df[\"Percentage\"])/9.5,2)\n",
      "447/10:\n",
      "# 8.0 - 10.0 --> A+\n",
      "# 7.0 - <8.0 --> A\n",
      "# 6.0 - <7.0 --> B+\n",
      "# 5.5 - <6.0 --> B\n",
      "# 4.5 - <5.5 --> C+\n",
      "# 4.0 - <4.5 --> C\n",
      "# 0.0 - <4.0 --> F\n",
      "def grade(i):\n",
      "    if df[\"CGPA\"][i]>=8.0:\n",
      "        df.at[i,\"Grade\"] = \"A+\"\n",
      "    elif df[\"CGPA\"][i]>=7.0 and df[\"CGPA\"][i]<8.0:\n",
      "        df.at[i,\"Grade\"] = \"A\"\n",
      "    elif df[\"CGPA\"][i]>=6.0 and df[\"CGPA\"][i]<7.0:\n",
      "        df.at[i,\"Grade\"] = \"B+\"\n",
      "    elif df[\"CGPA\"][i]>=5.5 and df[\"CGPA\"][i]<6.0:\n",
      "        df.at[i,\"Grade\"] = \"B\"\n",
      "    elif df[\"CGPA\"][i]>=4.5 and df[\"CGPA\"][i]<5.5:\n",
      "        df.at[i,\"Grade\"] = \"C+\"\n",
      "    elif df[\"CGPA\"][i]>=4.0 and df[\"CGPA\"][i]<4.5:\n",
      "        df.at[i,\"Grade\"] = \"C\"\n",
      "    else:\n",
      "        df.at[i,\"Grade\"] = \"F\"\n",
      "447/11:\n",
      "df[\"Grade\"]=\"nan\"\n",
      "for i in range(1000):\n",
      "    grade(i)\n",
      "447/12:\n",
      "sns.set(style=\"darkgrid\",font_scale=1.2)\n",
      "sns.catplot(x='gender', kind='count', hue='Grade', data=df)\n",
      "plt.show()\n",
      "447/13: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)\n",
      "447/14:\n",
      "from pandas_profiling import ProfileReport\n",
      "profile=ProfileReport(df,explorative=True)\n",
      "profile.to_file(\"output.html\")\n",
      "447/15:\n",
      "print(df[\"Grade\"].value_counts())\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df,palette=\"ch:.25\")\n",
      "447/16: sns.catplot(x=\"test preparation course\",kind=\"count\",hue=\"Grade\",data=df, palette=\"ch:.25\")\n",
      "447/17: sns.catplot(x=\"lunch\",kind=\"count\",hue=\"Grade\",data=df,palette=\"ch:1.2\")\n",
      "447/18:\n",
      "sns.catplot(x=\"race/ethnicity\",\n",
      "            kind=\"count\",\n",
      "            hue=\"Grade\",\n",
      "            data=df,\n",
      "            palette=\"ch:2.5\", \n",
      "            row=\"race/ethnicity\",\n",
      "            row_order=[\"group A\",\"group B\",\"group C\",\"group D\"],\n",
      "            aspect=3)\n",
      "447/19:\n",
      "sns.catplot(x=\"race/ethnicity\",\n",
      "            hue=\"parental level of education\",\n",
      "            kind=\"count\",\n",
      "            data=df,\n",
      "            col=\"parental level of education\",\n",
      "            col_wrap=3,\n",
      "           palette=\"ch:0.2\")\n",
      "447/20: sns.distplot(df[\"math score\"],fit=norm)\n",
      "447/21: sns.distplot(df[\"reading score\"],fit=norm)\n",
      "447/22: sns.distplot(df[\"writing score\"],fit=norm)\n",
      "447/23: df.corr()\n",
      "447/24:\n",
      "corr = df.corr(method='pearson')\n",
      "\n",
      "# Generate a mask for the upper triangle\n",
      "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
      "\n",
      "# Set up the matplotlib figure\n",
      "f, ax = plt.subplots(figsize=(11, 9))\n",
      "\n",
      "# Generate a custom diverging colormap\n",
      "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
      "\n",
      "# Draw the heatmap with the mask and correct aspect ratio\n",
      "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True)\n",
      "447/25: sns.heatmap(df.corr(method='pearson'),annot=True,linewidths=0.5)\n",
      "447/26: sns.heatmap(df.corr(method='spearman'),annot=True,linewidths=0.5,)\n",
      "447/27: df.hist()\n",
      "447/28: df.hist(figsize=(20,20))\n",
      "447/29: df.head()\n",
      "447/30: sex=pd.get_dummies(df[\"gender\"],drop_first=True)\n",
      "447/31: sex\n",
      "447/32: df = pd.concat([df,sex],axis=1)\n",
      "447/33: df.head()\n",
      "447/34:\n",
      "li = []\n",
      "for i in range(1000):\n",
      "    value = df[\"parental level of education\"][i]\n",
      "    if value not in li:\n",
      "        li.append(value)\n",
      "        \n",
      "li\n",
      "447/35:\n",
      "li = []\n",
      "for i in range(1000):\n",
      "    value = df[\"race/ethnicity\"][i]\n",
      "    if value == \"goup A\" and value not in li:\n",
      "        li.append(value)\n",
      "        \n",
      "li\n",
      "447/36:\n",
      "li = []\n",
      "for i in range(1000):\n",
      "    value = df[\"race/ethnicity\"][i]\n",
      "    if value == \"goup A\":\n",
      "        education = df[\"parental level of education\"][i]\n",
      "        if education not in li:\n",
      "            li.append(value)\n",
      "        \n",
      "li\n",
      "447/37:\n",
      "li = []\n",
      "for i in range(1000):\n",
      "    value = df[\"race/ethnicity\"][i]\n",
      "    if value == \"group A\":\n",
      "        education = df[\"parental level of education\"][i]\n",
      "        if education not in li:\n",
      "            li.append(value)\n",
      "        \n",
      "li\n",
      "447/38:\n",
      "li = []\n",
      "for i in range(1000):\n",
      "    value = df[\"race/ethnicity\"][i]\n",
      "    if value == \"group A\":\n",
      "        education = df[\"parental level of education\"][i]\n",
      "        if education not in li:\n",
      "            li.append(education)\n",
      "        \n",
      "li\n",
      "447/39:\n",
      "li = []\n",
      "for i in range(1000):\n",
      "    value = df[\"race/ethnicity\"][i]\n",
      "    if value == \"group B\":\n",
      "        education = df[\"parental level of education\"][i]\n",
      "        if education not in li:\n",
      "            li.append(education)\n",
      "        \n",
      "li\n",
      "447/40:\n",
      "race = {\"group A\":0, \"group B\":1, \"group C\":2, \"group D\": 3, \"group E\": 4}\n",
      "df[\"Education\"] = df[\"race/ethnicity\"].map(race)\n",
      "df.head()\n",
      "447/41: df[\"parental level of education\"].unique()\n",
      "448/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from scipy.stats import norm\n",
      "448/2: df = pd.read_csv(\"StudentsPerformance.csv\")\n",
      "448/3: df.head()\n",
      "448/4:\n",
      "print(\"Shape: \")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns: \")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Finding Null values: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data discription: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "448/5:\n",
      "def unique_values(columns):\n",
      "    print(columns+\": \",df[columns].unique())\n",
      "    print(\"-\"*100)\n",
      "448/6:\n",
      "for ele in columns:\n",
      "    unique_values(ele)\n",
      "448/7: df[\"Percentage\"] = round((df[\"math score\"]+df[\"reading score\"]+df[\"writing score\"])/3,2)\n",
      "448/8: df.head()\n",
      "448/9: df[\"CGPA\"] = round((df[\"Percentage\"])/9.5,2)\n",
      "448/10:\n",
      "# 8.0 - 10.0 --> A+\n",
      "# 7.0 - <8.0 --> A\n",
      "# 6.0 - <7.0 --> B+\n",
      "# 5.5 - <6.0 --> B\n",
      "# 4.5 - <5.5 --> C+\n",
      "# 4.0 - <4.5 --> C\n",
      "# 0.0 - <4.0 --> F\n",
      "def grade(i):\n",
      "    if df[\"CGPA\"][i]>=8.0:\n",
      "        df.at[i,\"Grade\"] = \"A+\"\n",
      "    elif df[\"CGPA\"][i]>=7.0 and df[\"CGPA\"][i]<8.0:\n",
      "        df.at[i,\"Grade\"] = \"A\"\n",
      "    elif df[\"CGPA\"][i]>=6.0 and df[\"CGPA\"][i]<7.0:\n",
      "        df.at[i,\"Grade\"] = \"B+\"\n",
      "    elif df[\"CGPA\"][i]>=5.5 and df[\"CGPA\"][i]<6.0:\n",
      "        df.at[i,\"Grade\"] = \"B\"\n",
      "    elif df[\"CGPA\"][i]>=4.5 and df[\"CGPA\"][i]<5.5:\n",
      "        df.at[i,\"Grade\"] = \"C+\"\n",
      "    elif df[\"CGPA\"][i]>=4.0 and df[\"CGPA\"][i]<4.5:\n",
      "        df.at[i,\"Grade\"] = \"C\"\n",
      "    else:\n",
      "        df.at[i,\"Grade\"] = \"F\"\n",
      "448/11:\n",
      "df[\"Grade\"]=\"nan\"\n",
      "for i in range(1000):\n",
      "    grade(i)\n",
      "448/12:\n",
      "sns.set(style=\"darkgrid\",font_scale=1.2)\n",
      "sns.catplot(x='gender', kind='count', hue='Grade', data=df)\n",
      "plt.show()\n",
      "448/13: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)\n",
      "448/14:\n",
      "from pandas_profiling import ProfileReport\n",
      "profile=ProfileReport(df,explorative=True)\n",
      "profile.to_file(\"output.html\")\n",
      "448/15:\n",
      "print(df[\"Grade\"].value_counts())\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df,palette=\"ch:.25\")\n",
      "448/16: df.hist(figsize=(20,20))\n",
      "448/17: sns.catplot(x=\"test preparation course\",kind=\"count\",hue=\"Grade\",data=df, palette=\"ch:.25\")\n",
      "448/18: sns.catplot(x=\"lunch\",kind=\"count\",hue=\"Grade\",data=df,palette=\"ch:1.2\")\n",
      "448/19:\n",
      "sns.catplot(x=\"race/ethnicity\",\n",
      "            kind=\"count\",\n",
      "            hue=\"Grade\",\n",
      "            data=df,\n",
      "            palette=\"ch:2.5\", \n",
      "            row=\"race/ethnicity\",\n",
      "            row_order=[\"group A\",\"group B\",\"group C\",\"group D\"],\n",
      "            aspect=3)\n",
      "448/20:\n",
      "sns.catplot(x=\"race/ethnicity\",\n",
      "            hue=\"parental level of education\",\n",
      "            kind=\"count\",\n",
      "            data=df,\n",
      "            col=\"parental level of education\",\n",
      "            col_wrap=3,\n",
      "           palette=\"ch:0.2\")\n",
      "448/21: sns.distplot(df[\"math score\"],fit=norm)\n",
      "448/22: sns.distplot(df[\"reading score\"],fit=norm)\n",
      "448/23: sns.distplot(df[\"writing score\"],fit=norm)\n",
      "448/24: df.corr()\n",
      "448/25:\n",
      "corr = df.corr(method='pearson')\n",
      "\n",
      "# Generate a mask for the upper triangle\n",
      "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
      "\n",
      "# Set up the matplotlib figure\n",
      "f, ax = plt.subplots(figsize=(11, 9))\n",
      "\n",
      "# Generate a custom diverging colormap\n",
      "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
      "\n",
      "# Draw the heatmap with the mask and correct aspect ratio\n",
      "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True)\n",
      "448/26: sns.heatmap(df.corr(method='pearson'),annot=True,linewidths=0.5)\n",
      "448/27: sns.heatmap(df.corr(method='spearman'),annot=True,linewidths=0.5,)\n",
      "448/28: df.head()\n",
      "448/29: sex=pd.get_dummies(df[\"gender\"],drop_first=True)\n",
      "448/30:\n",
      "df = pd.concat([df,sex],axis=1)\n",
      "# female --> 0\n",
      "# male --> 1\n",
      "448/31:\n",
      "race = {\"group A\": 0, \"group B\": 1, \"group C\": 2, \"group D\": 3, \"group E\": 4}\n",
      "df[\"Group\"] = df[\"race/ethnicity\"].map(race)\n",
      "df.head()\n",
      "448/32:\n",
      "education = {\"bachelor's degree\": 0, \"some college\": 1, \"master's degree\": 2, \"master's degree\": 3, \"associate's degree\": 4, \n",
      "       \"high school\": 5, \"some high school\": 6}\n",
      "df[\"Education\"] = df[\"race/ethnicity\"].map(education)\n",
      "df.head()\n",
      "448/33: df[\"parental level of education\"].unique()\n",
      "449/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "449/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "449/3:\n",
      "profile = ProfileReport(df,explorative=True)\n",
      "profile.to_file(\"output.html\")\n",
      "452/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "452/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "452/3:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "print(df.columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Check for null values: \")\n",
      "print(df.isnull())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "452/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "print(df.columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe)\n",
      "print(\"-\"*100)\n",
      "print(\"Check for null values: \")\n",
      "print(df.isnull())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "452/5:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "print(df.columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Check for null values: \")\n",
      "print(df.isnull())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "452/6:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "print(df.columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Check for null values: \")\n",
      "print(df.isnull())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "452/7:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "print(df.columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Check for null values: \")\n",
      "print(df.isnull)\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "452/8:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "print(df.columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "452/9:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "452/10:\n",
      "for ele in columns:\n",
      "    if ele!=\"id\":\n",
      "        print(ele+\" : \",df[ele].unique)\n",
      "452/11:\n",
      "for ele in columns:\n",
      "    if ele!=\"id\":\n",
      "        print(ele+\" : \",df[ele].unique())\n",
      "452/12: df[\"age\"]\n",
      "452/13: type(df[\"age\"])\n",
      "452/14: df[\"age\"].dtypes\n",
      "452/15: df[\"gender\"].unique()\n",
      "452/16: df[\"gender\"].unique\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452/17: df[\"gender\"].unique()\n",
      "452/18: print(\"Gender: \"df[\"gender\"].unique())\n",
      "452/19: print(\"Gender: \",df[\"gender\"].unique())\n",
      "452/20:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "452/21:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "452/22:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "452/23:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "452/24:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "452/25:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "452/26:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "452/27: sns.catplot(x=df[\"gender\"], kind=\"count\", data=df)\n",
      "452/28: sns.catplot(x=df[\"gender\"], kind=\"count\", data=df)\n",
      "452/29: sns.catplot(x=df[\"gender\"], kind=\"count\", data=df, palette=\"ch:.25\")\n",
      "459/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "459/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "459/3:\n",
      "profile = ProfileReport(df,explorative=True)\n",
      "profile.to_file(\"output.html\")\n",
      "459/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "459/5:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "459/6: sns.catplot(x=df[\"gender\"], kind=\"count\", data=df, palette=\"ch:.25\")\n",
      "459/7:\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.catplot(x=df[\"gender\"], kind=\"count\", data=df, palette=\"ch:.25\")\n",
      "460/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "460/2: warnings.filterwarnings(\"ignore\")\n",
      "460/3:\n",
      "data = pd.read_csv(\"heart_disease.csv\")\n",
      "data.head()\n",
      "461/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from scipy.stats import norm\n",
      "461/2: df = pd.read_csv(\"StudentsPerformance.csv\")\n",
      "461/3: df.head()\n",
      "461/4:\n",
      "print(\"Shape: \")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns: \")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Finding Null values: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "print(\"Data discription: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "461/5:\n",
      "def unique_values(columns):\n",
      "    print(columns+\": \",df[columns].unique())\n",
      "    print(\"-\"*100)\n",
      "461/6:\n",
      "for ele in columns:\n",
      "    unique_values(ele)\n",
      "461/7: df[\"Percentage\"] = round((df[\"math score\"]+df[\"reading score\"]+df[\"writing score\"])/3,2)\n",
      "461/8: df.head()\n",
      "461/9: df[\"CGPA\"] = round((df[\"Percentage\"])/9.5,2)\n",
      "461/10:\n",
      "# 8.0 - 10.0 --> A+\n",
      "# 7.0 - <8.0 --> A\n",
      "# 6.0 - <7.0 --> B+\n",
      "# 5.5 - <6.0 --> B\n",
      "# 4.5 - <5.5 --> C+\n",
      "# 4.0 - <4.5 --> C\n",
      "# 0.0 - <4.0 --> F\n",
      "def grade(i):\n",
      "    if df[\"CGPA\"][i]>=8.0:\n",
      "        df.at[i,\"Grade\"] = \"A+\"\n",
      "    elif df[\"CGPA\"][i]>=7.0 and df[\"CGPA\"][i]<8.0:\n",
      "        df.at[i,\"Grade\"] = \"A\"\n",
      "    elif df[\"CGPA\"][i]>=6.0 and df[\"CGPA\"][i]<7.0:\n",
      "        df.at[i,\"Grade\"] = \"B+\"\n",
      "    elif df[\"CGPA\"][i]>=5.5 and df[\"CGPA\"][i]<6.0:\n",
      "        df.at[i,\"Grade\"] = \"B\"\n",
      "    elif df[\"CGPA\"][i]>=4.5 and df[\"CGPA\"][i]<5.5:\n",
      "        df.at[i,\"Grade\"] = \"C+\"\n",
      "    elif df[\"CGPA\"][i]>=4.0 and df[\"CGPA\"][i]<4.5:\n",
      "        df.at[i,\"Grade\"] = \"C\"\n",
      "    else:\n",
      "        df.at[i,\"Grade\"] = \"F\"\n",
      "461/11:\n",
      "df[\"Grade\"]=\"nan\"\n",
      "for i in range(1000):\n",
      "    grade(i)\n",
      "461/12:\n",
      "sns.set(style=\"darkgrid\",font_scale=1.2)\n",
      "sns.catplot(x='gender', kind='count', hue='Grade', data=df)\n",
      "plt.show()\n",
      "461/13: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)\n",
      "461/14:\n",
      "from pandas_profiling import ProfileReport\n",
      "profile=ProfileReport(df,explorative=True)\n",
      "profile.to_file(\"output.html\")\n",
      "461/15:\n",
      "print(df[\"Grade\"].value_counts())\n",
      "sns.catplot(x=\"Grade\",kind=\"count\",data=df,palette=\"ch:.25\")\n",
      "461/16: df.hist(figsize=(20,20))\n",
      "461/17: sns.catplot(x=\"test preparation course\",kind=\"count\",hue=\"Grade\",data=df, palette=\"ch:.25\")\n",
      "461/18: sns.catplot(x=\"lunch\",kind=\"count\",hue=\"Grade\",data=df,palette=\"ch:1.2\")\n",
      "461/19:\n",
      "sns.catplot(x=\"race/ethnicity\",\n",
      "            kind=\"count\",\n",
      "            hue=\"Grade\",\n",
      "            data=df,\n",
      "            palette=\"ch:2.5\", \n",
      "            row=\"race/ethnicity\",\n",
      "            row_order=[\"group A\",\"group B\",\"group C\",\"group D\"],\n",
      "            aspect=3)\n",
      "461/20:\n",
      "sns.catplot(x=\"race/ethnicity\",\n",
      "            hue=\"parental level of education\",\n",
      "            kind=\"count\",\n",
      "            data=df,\n",
      "            col=\"parental level of education\",\n",
      "            col_wrap=3,\n",
      "           palette=\"ch:0.2\")\n",
      "461/21: sns.distplot(df[\"math score\"],fit=norm)\n",
      "461/22: sns.distplot(df[\"reading score\"],fit=norm)\n",
      "461/23: sns.distplot(df[\"writing score\"],fit=norm)\n",
      "461/24: df.corr()\n",
      "461/25:\n",
      "corr = df.corr(method='pearson')\n",
      "\n",
      "# Generate a mask for the upper triangle\n",
      "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
      "\n",
      "# Set up the matplotlib figure\n",
      "f, ax = plt.subplots(figsize=(11, 9))\n",
      "\n",
      "# Generate a custom diverging colormap\n",
      "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
      "\n",
      "# Draw the heatmap with the mask and correct aspect ratio\n",
      "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True)\n",
      "461/26: sns.heatmap(df.corr(method='pearson'),annot=True,linewidths=0.5)\n",
      "461/27: sns.heatmap(df.corr(method='spearman'),annot=True,linewidths=0.5,)\n",
      "461/28: df.head()\n",
      "461/29: sex=pd.get_dummies(df[\"gender\"],drop_first=True)\n",
      "461/30:\n",
      "df = pd.concat([df,sex],axis=1)\n",
      "# female --> 0\n",
      "# male --> 1\n",
      "461/31:\n",
      "race = {\"group A\": 0, \"group B\": 1, \"group C\": 2, \"group D\": 3, \"group E\": 4}\n",
      "df[\"Group\"] = df[\"race/ethnicity\"].map(race)\n",
      "df.head()\n",
      "461/32:\n",
      "education = {\"bachelor's degree\": 0, \"some college\": 1, \"master's degree\": 2, \"master's degree\": 3, \"associate's degree\": 4, \n",
      "       \"high school\": 5, \"some high school\": 6}\n",
      "df[\"Education\"] = df[\"race/ethnicity\"].map(education)\n",
      "df.head()\n",
      "461/33: df[\"parental level of education\"].unique()\n",
      "461/34:\n",
      "sns.set(style=\"darkgrid\",font_scale=1.2)\n",
      "sns.catplot(x='gender', kind='count', data=df)\n",
      "plt.show()\n",
      "461/35:\n",
      "sns.set(style=\"darkgrid\",font_scale=1.2)\n",
      "sns.catplot(x='gender', kind='count', hue='Grade', data=df)\n",
      "plt.show()\n",
      "459/8:\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.catplot(x=df[\"gender\"], kind=\"count\", data=df, palette=\"ch:.25\")\n",
      "sns.catplot(x=df[\"gender\"], kind=\"count\", data=df, palette=\"ch:.25\")\n",
      "459/9:\n",
      "sns.set(style=\"darkgrid\")\n",
      "ptint(sns.catplot(x=df[\"gender\"], kind=\"count\", data=df, palette=\"ch:.25\"))\n",
      "sns.catplot(x=df[\"gender\"], kind=\"count\", data=df, palette=\"ch:.25\")\n",
      "459/10:\n",
      "sns.set(style=\"darkgrid\")\n",
      "print(sns.catplot(x=df[\"gender\"], kind=\"count\", data=df, palette=\"ch:.25\"))\n",
      "sns.catplot(x=df[\"gender\"], kind=\"count\", data=df, palette=\"ch:.25\")\n",
      "459/11:\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.catplot(x=df[\"gender\"], kind=\"count\", data=df, palette=\"ch:.25\")\n",
      "459/12: df[\"gender\"].values_count()\n",
      "459/13: df[\"gender\"].value_counts()\n",
      "459/14:\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.catplot(x=df[\"gender\"], kind=\"cout\", data=df, palette=\"ch:.25\")\n",
      "459/15:\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.catplot(x=df[\"gender\"], kind=\"count\", data=df, palette=\"ch:.25\")\n",
      "459/16:\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "459/17: sns.catplot(x=df[\"hypertension\"],kind=\"count\", hue=df[\"stroke\"], data=df)\n",
      "459/18: sns.countplot(x=df[\"hypertension\"],kind=\"count\", hue=df[\"stroke\"], data=df)\n",
      "459/19: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "463/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "463/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "463/3:\n",
      "profile = ProfileReport(df,explorative=True)\n",
      "profile.to_file(\"output.html\")\n",
      "463/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "463/5:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "463/6:\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "463/7: df[\"gender\"].value_counts()\n",
      "463/8: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "463/9: sns.contplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "463/10: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "463/11: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df, palette=\"ch:.24\")\n",
      "463/12: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df, palette=\"ch:.25\")\n",
      "463/13: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df, palette=\"ch:.50\")\n",
      "463/14: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df, palette=\"ch:1\")\n",
      "463/15: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df, palette=\"ch:1.26\")\n",
      "463/16: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "463/17: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "463/18: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "463/19: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "463/20: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "463/21: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "463/22: sns.distplot(x=df[\"age\"])\n",
      "463/23: sns.distplot(df[\"age\"])\n",
      "463/24:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "463/25: sns.distplot(df[\"age\"], fit=norm)\n",
      "463/26: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "463/27: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "463/28: sns.countplot(x=df[\"stroke\"], data=df)\n",
      "463/29: sns.boxenplot(x=df[\"age\"], data=df)\n",
      "463/30: sns.boxplot(x=df[\"age\"], data=df)\n",
      "463/31: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "463/32: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "463/33:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "463/34:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "463/35:\n",
      "print(df[\"heart_disease\"])\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "463/36:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "463/37:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "463/38:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "463/39:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "463/40:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "463/41: print(\"Length of bmi before: \",len(df[\"bmi\"]))\n",
      "463/42: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "463/43: df[\"bmi\"]=df[\"bmi\"].fillna(value=df[\"bmi\"].mean())\n",
      "463/44: df[\"bmi\"]=df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "463/45: df[\"bmi\"].isnull().sum()\n",
      "463/46: df.head()\n",
      "465/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "465/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "465/3:\n",
      "profile = ProfileReport(df,explorative=True)\n",
      "profile.to_file(\"output.html\")\n",
      "465/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "465/5:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "465/6:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "465/7:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "465/8:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "465/9:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "465/10:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "465/11:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "465/12:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "465/13: sns.countplot(x=df[\"stroke\"], data=df)\n",
      "465/14: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "465/15: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "465/16: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "465/17: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "465/18: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "465/19: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "465/20: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "465/21: sns.distplot(df[\"age\"], fit=norm)\n",
      "465/22: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "465/23: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "465/24: sns.boxplot(x=df[\"age\"], data=df)\n",
      "465/25: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "465/26: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "465/27: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "465/28: df[\"bmi\"]=df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "465/29: df[\"bmi\"].isnull().sum()\n",
      "465/30: df.head()\n",
      "465/31: df[\"agee\"].mean()\n",
      "465/32: df[\"age\"].mean()\n",
      "466/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "466/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "466/3:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "466/4:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "466/5:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "466/6:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "466/7:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "466/8:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "466/9:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "466/10:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "466/11:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "466/12: sns.countplot(x=df[\"stroke\"], data=df)\n",
      "466/13: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "466/14: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "466/15: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "466/16: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "466/17: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "466/18: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "466/19: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "466/20: sns.distplot(df[\"age\"], fit=norm)\n",
      "466/21: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "466/22: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "466/23: sns.boxplot(x=df[\"age\"], data=df)\n",
      "466/24: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "466/25: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "466/26: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "466/27: df[\"bmi\"].fillna(value=df[\"bmi\"].mean, inplace=True)\n",
      "466/28: df[\"bmi\"].isnull().sum()\n",
      "466/29: df.head()\n",
      "466/30: df[\"age\"].mean()\n",
      "466/31: df.head()\n",
      "466/32: df.head(5)\n",
      "466/33: df[\"age\"].mean()\n",
      "466/34: df.head()\n",
      "467/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "467/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "467/3:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "467/4:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "467/5:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "467/6:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "467/7:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "467/8:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "467/9:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "467/10:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "467/11:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "467/12: sns.countplot(x=df[\"stroke\"], data=df)\n",
      "467/13: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "467/14: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "467/15: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "467/16: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "467/17: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "467/18: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "467/19: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "467/20: sns.distplot(df[\"age\"], fit=norm)\n",
      "467/21: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "467/22: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "467/23: sns.boxplot(x=df[\"age\"], data=df)\n",
      "467/24: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "467/25: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "467/26: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "467/27: df[\"bmi\"].fillna(value=df[\"bmi\"].mean, inplace=True)\n",
      "467/28: df[\"bmi\"].isnull().sum()\n",
      "467/29: df[\"age\"].mean()\n",
      "467/30: df.head()\n",
      "467/31: df[\"age\"]\n",
      "467/32: df.head()\n",
      "467/33: df.tail()\n",
      "469/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "469/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "469/3:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "469/4:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "469/5:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "469/6:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "469/7:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "469/8:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/9:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "469/10:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "469/11:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "469/12: sns.countplot(x=df[\"stroke\"], data=df)\n",
      "469/13: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "469/14: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "469/15: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "469/16: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "469/17: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "469/18: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "469/19: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "469/20: sns.distplot(df[\"age\"], fit=norm)\n",
      "469/21: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "469/22: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "469/23: sns.boxplot(x=df[\"age\"], data=df)\n",
      "469/24: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "469/25: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "469/26: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "469/27: df[\"bmi\"].fillna(value=df[\"bmi\"].mean, inplace=True)\n",
      "469/28: df[\"bmi\"].isnull().sum()\n",
      "469/29: df[\"age\"].mean()\n",
      "469/30: df.head()\n",
      "470/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "470/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "470/3: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "470/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "470/5:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "470/6:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "470/7:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "470/8:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "470/9:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "470/10:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "470/11:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "470/12:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "470/13: sns.countplot(x=df[\"stroke\"], data=df)\n",
      "470/14: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "470/15: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "470/16: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "470/17: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "470/18: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "470/19: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "470/20: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "470/21: sns.distplot(df[\"age\"], fit=norm)\n",
      "470/22: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "470/23: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "470/24: sns.boxplot(x=df[\"age\"], data=df)\n",
      "470/25: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "470/26: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "470/27: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "470/28: df[\"bmi\"].fillna(value=df[\"bmi\"].mean, inplace=True)\n",
      "470/29: df[\"bmi\"].isnull().sum()\n",
      "470/30: df[\"age\"].mean()\n",
      "470/31: df.head()\n",
      "470/32: df[\"age\"].mean\n",
      "470/33: df[\"age\"].mean()\n",
      "471/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "471/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "471/3:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "471/4:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "471/5:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "471/6:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "471/7:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "471/8:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "471/9:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "471/10:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "471/11:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "471/12: sns.countplot(x=df[\"stroke\"], data=df)\n",
      "471/13: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "471/14: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "471/15: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "471/16: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "471/17: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "471/18: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "471/19: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "471/20: sns.distplot(df[\"age\"], fit=norm)\n",
      "471/21: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "471/22: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "471/23: sns.boxplot(x=df[\"age\"], data=df)\n",
      "471/24: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "471/25: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "471/26: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "471/27: df.head()\n",
      "471/28: df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "471/29: df[\"bmi\"].isnull().sum()\n",
      "471/30: df[\"age\"].mean()\n",
      "471/31: df.head()\n",
      "471/32: print(\"Length of bmi after: \",[\"bmi\"].isnull().sum())\n",
      "471/33: print(\"Length of bmi after: \",df[\"bmi\"].isnull().sum())\n",
      "471/34: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "471/35: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "471/36:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"stroke\"], data=df)\n",
      "471/37:\n",
      "print(df[\"stroke\"].value_counts())\n",
      "sns.countplot(x=df[\"stroke\"], data=df)\n",
      "471/38:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "471/39:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "471/40:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound {}, lower bound {}\".formate(upper_bound,lower_bound))\n",
      "471/41:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound {}, lower bound {}\".format(upper_bound,lower_bound))\n",
      "471/42:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound: {}, lower bound: {}\".format(upper_bound,lower_bound))\n",
      "471/43: df[\"bmi\"][0]\n",
      "471/44: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "471/45:\n",
      "for i in range(len(df[\"bmi\"])):\n",
      "    if df[\"bmi\"][i] > upper_bound or df[\"bmi\"][i] < lower_bound:\n",
      "        df.drop([df[\"bmi\"].index[i]], inplace=True)\n",
      "471/46: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "471/47: df.drop(df[df['bmi'] > upper_bound or df[\"bmi\"]<lower_bound].index, inplace = True)\n",
      "471/48:\n",
      "df.drop(df[df['bmi'] > upper_bound].index, inplace = True)\n",
      "df.drop(df[df['bmi'] < lower_bound].index, inplace = True)\n",
      "471/49: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "472/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "472/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "472/3:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "472/4:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "472/5:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "472/6:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "472/7:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "472/8:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "472/9:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "472/10:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "472/11:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "472/12:\n",
      "print(df[\"stroke\"].value_counts())\n",
      "sns.countplot(x=df[\"stroke\"], data=df)\n",
      "472/13: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "472/14: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "472/15: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "472/16: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "472/17: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "472/18: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "472/19: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "472/20: sns.distplot(df[\"age\"], fit=norm)\n",
      "472/21: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "472/22: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "472/23: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "472/24: sns.boxplot(x=df[\"age\"], data=df)\n",
      "472/25: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "472/26: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "472/27: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "472/28: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "472/29: df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "472/30: print(\"Length of bmi after: \",df[\"bmi\"].isnull().sum())\n",
      "472/31:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound: {}, lower bound: {}\".format(upper_bound,lower_bound))\n",
      "472/32:\n",
      "df.drop(df[df['bmi'] > upper_bound].index, inplace = True)\n",
      "df.drop(df[df['bmi'] < lower_bound].index, inplace = True)\n",
      "472/33: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "472/34: print(df[\"stroke\"].value_counts())\n",
      "472/35: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "472/36:\n",
      "print(\"After outlier removal\")\n",
      "sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "472/37: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "472/38:\n",
      "print(\"After outlier removal\")\n",
      "fig, ax = pplt.subplot(1,2)\n",
      "sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "sns.distplot(df[\"bmi\"], fit=norm)\n",
      "472/39:\n",
      "print(\"After outlier removal\")\n",
      "fig, ax = plt.subplot(1,2)\n",
      "sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "sns.distplot(df[\"bmi\"], fit=norm)\n",
      "472/40:\n",
      "print(\"After outlier removal\")\n",
      "fig, ax = plt.subplot(1,2)\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax=ax)\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax=ax)\n",
      "472/41:\n",
      "print(\"After outlier removal\")\n",
      "fig, ax = plt.subplot(1,2,figsize=(10,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax=ax)\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax=ax)\n",
      "472/42:\n",
      "print(\"After outlier removal\")\n",
      "fig, ax = plt.subplot(2,2,1)\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax=ax)\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax=ax)\n",
      "472/43:\n",
      "print(\"After outlier removal\")\n",
      "fig, ax = plt.subplot(2,2,1)\n",
      "sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "sns.distplot(df[\"bmi\"], fit=norm)\n",
      "472/44:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,5))\n",
      "axes[0].sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "axes[1].sns.distplot(df[\"bmi\"], fit=norm)\n",
      "472/45:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df)axes[0]\n",
      "axes[1].sns.distplot(df[\"bmi\"], fit=norm)\n",
      "472/46:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df).axes[0]\n",
      "axes[1].sns.distplot(df[\"bmi\"], fit=norm)\n",
      "472/47:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax=axes[0])\n",
      "axes[1].sns.distplot(df[\"bmi\"], fit=norm)\n",
      "472/48:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax = axes[0])\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax = axes[1])\n",
      "472/49:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,10))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax = axes[0])\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax = axes[1])\n",
      "472/50:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2,figsize=(15,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax = axes[0])\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax = axes[1])\n",
      "472/51:\n",
      "data = df.corr(method='pearson')\n",
      "sns.heatmap(data,annot=True,cbar=True)\n",
      "472/52:\n",
      "data = df.corr(method='pearson')\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n",
      "472/53:\n",
      "data = df.corr(method='pearson')\n",
      "fig = plt.figure(figsize=(10,5))\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n",
      "472/54:\n",
      "data = df.corr(method='pearson')\n",
      "fig = plt.figure(figsize=(15,5))\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n",
      "472/55:\n",
      "data = df.corr(method='pearson')\n",
      "fig = plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n",
      "472/56: df.head()\n",
      "472/57: df.describe()\n",
      "474/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "474/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "474/3:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "474/4:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "474/5:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "474/6:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "474/7:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "474/8:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "474/9:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "474/10:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "474/11:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "474/12:\n",
      "print(df[\"stroke\"].value_counts())\n",
      "sns.countplot(x=df[\"stroke\"], data=df)\n",
      "474/13: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474/14: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "474/15: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "474/16: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "474/17: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "474/18: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "474/19: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "474/20: sns.distplot(df[\"age\"], fit=norm)\n",
      "474/21: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "474/22: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "474/23: sns.boxplot(x=df[\"age\"], data=df)\n",
      "474/24: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "474/25: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "474/26:\n",
      "data = df.corr(method='pearson')\n",
      "fig = plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n",
      "474/27: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "474/28: df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "474/29: print(\"Length of bmi after: \",df[\"bmi\"].isnull().sum())\n",
      "474/30:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound: {}, lower bound: {}\".format(upper_bound,lower_bound))\n",
      "474/31:\n",
      "df.drop(df[df['bmi'] > upper_bound].index, inplace = True)\n",
      "df.drop(df[df['bmi'] < lower_bound].index, inplace = True)\n",
      "474/32:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2,figsize=(15,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax = axes[0])\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax = axes[1])\n",
      "474/33: print(df[\"stroke\"].value_counts())\n",
      "474/34:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df[\"gender\"].drop([df[\"gender\"]==\"other\"].index, inplace=True)\n",
      "474/35:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df[\"gender\"].drop(df[df[\"gender\"]==\"other\"].index, inplace=True)\n",
      "474/36:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "474/37:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df[\"gender\"].drop(df[df[\"gender\"]==\"other\"].index, inplace=True)\n",
      "474/38:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "474/39:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df[\"gender\"].drop(df[df[\"gender\"]==\"Other\"].index, inplace=True)\n",
      "474/40:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "474/41: df[\"gender\"].value_counts()\n",
      "474/42:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df.drop(df[df[\"gender\"]==\"Other\"].index, inplace=True)\n",
      "474/43: df[\"gender\"].value_counts()\n",
      "474/44:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "474/45:\n",
      "sex = pd.get_dummies(df[\"gender\"], drop_first=True)\n",
      "df = pd.concat([df,sex],axis=1)\n",
      "df.head()\n",
      "474/46:\n",
      "# For Ever_married colummn.\n",
      "married_status = pd.get_dummies(df[\"ever_married\"], drop_first=True)\n",
      "df = pd.concat([df, married_status], axis=1)\n",
      "474/47: df.head()\n",
      "474/48:\n",
      "# For Residence type column.\n",
      "residence = pd.get_dummies(df[\"Residence_type\"], drop_first=True)\n",
      "df = pd.concat([df, residence], axis=1)\n",
      "474/49: df.head()\n",
      "474/50: df[\"smoking_status\"].unique()\n",
      "474/51:\n",
      "df[\"Work_Type\"] = df[\"work_type\"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})\n",
      "df[\"Smoking_Status\"] = df[\"smoking_status\"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})\n",
      "474/52: df.head()\n",
      "474/53:\n",
      "# Now since we have converted all the categorical values to numerical, now we should drop all those columns.\n",
      "\n",
      "df = df.drop([\"id\",\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"],axis=1, inplace=True)\n",
      "df.head()\n",
      "474/54:\n",
      "# Now since we have converted all the categorical values to numerical, now we should drop all those columns.\n",
      "\n",
      "df.drop([\"id\",\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"],axis=1, inplace=True)\n",
      "df.head()\n",
      "475/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "475/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "475/3: df.head()\n",
      "475/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "475/5:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "475/6:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "475/7:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "475/8:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "475/9:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "475/10:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "475/11:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "475/12:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "475/13:\n",
      "print(df[\"stroke\"].value_counts())\n",
      "sns.countplot(x=df[\"stroke\"], data=df)\n",
      "475/14: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "475/15: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "475/16: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "475/17: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "475/18: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "475/19: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "475/20: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "475/21: sns.distplot(df[\"age\"], fit=norm)\n",
      "475/22: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "475/23: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "475/24: sns.boxplot(x=df[\"age\"], data=df)\n",
      "475/25: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "475/26: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "475/27:\n",
      "data = df.corr(method='pearson')\n",
      "fig = plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n",
      "475/28: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "475/29: df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "475/30: print(\"Length of bmi after: \",df[\"bmi\"].isnull().sum())\n",
      "475/31:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound: {}, lower bound: {}\".format(upper_bound,lower_bound))\n",
      "475/32:\n",
      "df.drop(df[df['bmi'] > upper_bound].index, inplace = True)\n",
      "df.drop(df[df['bmi'] < lower_bound].index, inplace = True)\n",
      "475/33:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2,figsize=(15,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax = axes[0])\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax = axes[1])\n",
      "475/34:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df.drop(df[df[\"gender\"]==\"Other\"].index, inplace=True)\n",
      "475/35:\n",
      "# For gender column.\n",
      "sex = pd.get_dummies(df[\"gender\"], drop_first=True)\n",
      "df = pd.concat([df,sex],axis=1)\n",
      "475/36:\n",
      "# For Ever_married colummn.\n",
      "married_status = pd.get_dummies(df[\"ever_married\"], drop_first=True)\n",
      "df = pd.concat([df, married_status], axis=1)\n",
      "475/37:\n",
      "# For Residence type column.\n",
      "residence = pd.get_dummies(df[\"Residence_type\"], drop_first=True)\n",
      "df = pd.concat([df, residence], axis=1)\n",
      "475/38:\n",
      "df[\"Work_Type\"] = df[\"work_type\"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})\n",
      "df[\"Smoking_Status\"] = df[\"smoking_status\"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})\n",
      "475/39: df.head()\n",
      "475/40:\n",
      "# Now since we have converted all the categorical values to numerical, now we should drop all those columns.\n",
      "\n",
      "df.drop([\"id\",\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"],axis=1, inplace=True)\n",
      "df.head()\n",
      "475/41: print(df[\"stroke\"].value_counts())\n",
      "475/42: df[\"gender\"].value_counts()\n",
      "475/43:\n",
      "df.rename(columns={\"Male\":\"Gender\",\"Yes\":\"Ever_Married\",\"Urban\":\"Residence_type\"})\n",
      "df.head()\n",
      "475/44:\n",
      "df.rename(columns={\"Male\":\"Gender\",\"Yes\":\"Ever_Married\",\"Urban\":\"Residence_type\"}, inplace=True)\n",
      "df.head()\n",
      "475/45: from imblearn.over_sampling import SMOTE\n",
      "475/46: !pip install imblearn\n",
      "475/47: from imblearn.over_sampling import SMOTE\n",
      "475/48: from imblearn.over_sampling import SMOTE\n",
      "475/49: from imblearn.over_sampling import SMOTE\n",
      "477/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "477/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "477/3: df.head()\n",
      "477/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "477/5:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "477/6:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "477/7:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "477/8:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "477/9:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "477/10:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "477/11:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "477/12:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "477/13:\n",
      "print(df[\"stroke\"].value_counts())\n",
      "sns.countplot(x=df[\"stroke\"], data=df)\n",
      "477/14: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "477/15: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "477/16: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "477/17: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "477/18: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "477/19: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "477/20: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "477/21: sns.distplot(df[\"age\"], fit=norm)\n",
      "477/22: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "477/23: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "477/24: sns.boxplot(x=df[\"age\"], data=df)\n",
      "477/25: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "477/26: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "477/27:\n",
      "data = df.corr(method='pearson')\n",
      "fig = plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n",
      "477/28: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "477/29: df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "477/30: print(\"Length of bmi after: \",df[\"bmi\"].isnull().sum())\n",
      "477/31:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound: {}, lower bound: {}\".format(upper_bound,lower_bound))\n",
      "477/32:\n",
      "df.drop(df[df['bmi'] > upper_bound].index, inplace = True)\n",
      "df.drop(df[df['bmi'] < lower_bound].index, inplace = True)\n",
      "477/33:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2,figsize=(15,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax = axes[0])\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax = axes[1])\n",
      "477/34:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df.drop(df[df[\"gender\"]==\"Other\"].index, inplace=True)\n",
      "477/35:\n",
      "# For gender column.\n",
      "sex = pd.get_dummies(df[\"gender\"], drop_first=True)\n",
      "df = pd.concat([df,sex],axis=1)\n",
      "477/36:\n",
      "# For Ever_married colummn.\n",
      "married_status = pd.get_dummies(df[\"ever_married\"], drop_first=True)\n",
      "df = pd.concat([df, married_status], axis=1)\n",
      "477/37:\n",
      "# For Residence type column.\n",
      "residence = pd.get_dummies(df[\"Residence_type\"], drop_first=True)\n",
      "df = pd.concat([df, residence], axis=1)\n",
      "477/38:\n",
      "df[\"Work_Type\"] = df[\"work_type\"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})\n",
      "df[\"Smoking_Status\"] = df[\"smoking_status\"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})\n",
      "477/39: df.head()\n",
      "477/40:\n",
      "df.drop([\"id\",\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"],axis=1, inplace=True)\n",
      "df.head()\n",
      "477/41:\n",
      "df.rename(columns={\"Male\":\"Gender\",\"Yes\":\"Ever_Married\",\"Urban\":\"Residence_type\"}, inplace=True)\n",
      "df.head()\n",
      "477/42: from imblearn.over_sampling import SMOTE\n",
      "478/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "478/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "478/3: df.head()\n",
      "478/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "478/5:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "478/6:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "478/7:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "478/8:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "478/9:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "478/10:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "478/11:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "478/12:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "478/13:\n",
      "print(df[\"stroke\"].value_counts())\n",
      "sns.countplot(x=df[\"stroke\"], data=df)\n",
      "478/14: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "478/15: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "478/16: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "478/17: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "478/18: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "478/19: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "478/20: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "478/21: sns.distplot(df[\"age\"], fit=norm)\n",
      "478/22: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "478/23: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "478/24: sns.boxplot(x=df[\"age\"], data=df)\n",
      "478/25: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "478/26: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "478/27:\n",
      "data = df.corr(method='pearson')\n",
      "fig = plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478/28: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "478/29: df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "478/30: print(\"Length of bmi after: \",df[\"bmi\"].isnull().sum())\n",
      "478/31:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound: {}, lower bound: {}\".format(upper_bound,lower_bound))\n",
      "478/32:\n",
      "df.drop(df[df['bmi'] > upper_bound].index, inplace = True)\n",
      "df.drop(df[df['bmi'] < lower_bound].index, inplace = True)\n",
      "478/33:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2,figsize=(15,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax = axes[0])\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax = axes[1])\n",
      "478/34:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df.drop(df[df[\"gender\"]==\"Other\"].index, inplace=True)\n",
      "478/35:\n",
      "# For gender column.\n",
      "sex = pd.get_dummies(df[\"gender\"], drop_first=True)\n",
      "df = pd.concat([df,sex],axis=1)\n",
      "478/36:\n",
      "# For Ever_married colummn.\n",
      "married_status = pd.get_dummies(df[\"ever_married\"], drop_first=True)\n",
      "df = pd.concat([df, married_status], axis=1)\n",
      "478/37:\n",
      "# For Residence type column.\n",
      "residence = pd.get_dummies(df[\"Residence_type\"], drop_first=True)\n",
      "df = pd.concat([df, residence], axis=1)\n",
      "478/38:\n",
      "df[\"Work_Type\"] = df[\"work_type\"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})\n",
      "df[\"Smoking_Status\"] = df[\"smoking_status\"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})\n",
      "478/39: df.head()\n",
      "478/40:\n",
      "df.drop([\"id\",\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"],axis=1, inplace=True)\n",
      "df.head()\n",
      "478/41:\n",
      "df.rename(columns={\"Male\":\"Gender\",\"Yes\":\"Ever_Married\",\"Urban\":\"Residence_type\"}, inplace=True)\n",
      "df.head()\n",
      "478/42: from imblearn.over_sampling import SMOTE\n",
      "478/43:\n",
      "from imblearn.over_sampling import SMOTE\n",
      "\n",
      "smote = SMOTE(random_state = 42)\n",
      "X = df.drop(['stroke'],axis=1)\n",
      "y = df[['stroke']]\n",
      "X,y= smote.fit_resample(X,y['stroke'].values.ravel())\n",
      "y = pd.DataFrame({'stroke':y})\n",
      "sns.countplot(data = y, x = 'stroke', y= None)\n",
      "plt.show()\n",
      "479/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "479/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "479/3: df.head()\n",
      "479/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "479/5:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "479/6:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "479/7:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "479/8:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "479/9:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "479/10:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "479/11:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "479/12:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "479/13:\n",
      "print(df[\"stroke\"].value_counts())\n",
      "sns.countplot(x=df[\"stroke\"], data=df)\n",
      "479/14: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "479/15: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "479/16: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "479/17: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "479/18: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "479/19: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "479/20: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "479/21: sns.distplot(df[\"age\"], fit=norm)\n",
      "479/22: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "479/23: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "479/24: sns.boxplot(x=df[\"age\"], data=df)\n",
      "479/25: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "479/26: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "479/27:\n",
      "data = df.corr(method='pearson')\n",
      "fig = plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n",
      "479/28: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "479/29: df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "479/30: print(\"Length of bmi after: \",df[\"bmi\"].isnull().sum())\n",
      "479/31:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound: {}, lower bound: {}\".format(upper_bound,lower_bound))\n",
      "479/32:\n",
      "df.drop(df[df['bmi'] > upper_bound].index, inplace = True)\n",
      "df.drop(df[df['bmi'] < lower_bound].index, inplace = True)\n",
      "479/33:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2,figsize=(15,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax = axes[0])\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax = axes[1])\n",
      "479/34:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df.drop(df[df[\"gender\"]==\"Other\"].index, inplace=True)\n",
      "479/35:\n",
      "# For gender column.\n",
      "sex = pd.get_dummies(df[\"gender\"], drop_first=True)\n",
      "df = pd.concat([df,sex],axis=1)\n",
      "479/36:\n",
      "# For Ever_married colummn.\n",
      "married_status = pd.get_dummies(df[\"ever_married\"], drop_first=True)\n",
      "df = pd.concat([df, married_status], axis=1)\n",
      "479/37:\n",
      "# For Residence type column.\n",
      "residence = pd.get_dummies(df[\"Residence_type\"], drop_first=True)\n",
      "df = pd.concat([df, residence], axis=1)\n",
      "479/38:\n",
      "df[\"Work_Type\"] = df[\"work_type\"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})\n",
      "df[\"Smoking_Status\"] = df[\"smoking_status\"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})\n",
      "479/39: df.head()\n",
      "479/40:\n",
      "df.drop([\"id\",\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"],axis=1, inplace=True)\n",
      "df.head()\n",
      "479/41:\n",
      "df.rename(columns={\"Male\":\"Gender\",\"Yes\":\"Ever_Married\",\"Urban\":\"Residence_type\"}, inplace=True)\n",
      "df.head()\n",
      "479/42:\n",
      "from imblearn.over_sampling import SMOTE\n",
      "\n",
      "smote = SMOTE(random_state = 42)\n",
      "X = df.drop(['stroke'],axis=1)\n",
      "y = df['stroke']\n",
      "X,y= smote.fit_resample(X,y['stroke'])\n",
      "sns.countplot(data = y, x = 'stroke', y= None)\n",
      "plt.show()\n",
      "479/43:\n",
      "from imblearn.over_sampling import SMOTE\n",
      "\n",
      "smote = SMOTE(random_state = 42)\n",
      "X = df.drop(['stroke'],axis=1)\n",
      "y = df['stroke']\n",
      "X,y= smote.fit_resample(X,y)\n",
      "sns.countplot(data = y, x = 'stroke', y= None)\n",
      "plt.show()\n",
      "480/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "480/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "480/3: df.head()\n",
      "480/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "480/5:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "480/6:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "480/7:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "480/8:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "480/9:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "480/10:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "480/11:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "480/12:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "480/13:\n",
      "print(df[\"stroke\"].value_counts())\n",
      "sns.countplot(x=df[\"stroke\"], data=df)\n",
      "480/14: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "480/15: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "480/16: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "480/17: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "480/18: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "480/19: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "480/20: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "480/21: sns.distplot(df[\"age\"], fit=norm)\n",
      "480/22: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "480/23: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "480/24: sns.boxplot(x=df[\"age\"], data=df)\n",
      "480/25: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "480/26: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "480/27:\n",
      "data = df.corr(method='pearson')\n",
      "fig = plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n",
      "480/28: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "480/29: df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "480/30: print(\"Length of bmi after: \",df[\"bmi\"].isnull().sum())\n",
      "480/31:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound: {}, lower bound: {}\".format(upper_bound,lower_bound))\n",
      "480/32:\n",
      "df.drop(df[df['bmi'] > upper_bound].index, inplace = True)\n",
      "df.drop(df[df['bmi'] < lower_bound].index, inplace = True)\n",
      "480/33:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2,figsize=(15,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax = axes[0])\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax = axes[1])\n",
      "480/34:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df.drop(df[df[\"gender\"]==\"Other\"].index, inplace=True)\n",
      "480/35:\n",
      "# For gender column.\n",
      "sex = pd.get_dummies(df[\"gender\"], drop_first=True)\n",
      "df = pd.concat([df,sex],axis=1)\n",
      "480/36:\n",
      "# For Ever_married colummn.\n",
      "married_status = pd.get_dummies(df[\"ever_married\"], drop_first=True)\n",
      "df = pd.concat([df, married_status], axis=1)\n",
      "480/37:\n",
      "# For Residence type column.\n",
      "residence = pd.get_dummies(df[\"Residence_type\"], drop_first=True)\n",
      "df = pd.concat([df, residence], axis=1)\n",
      "480/38:\n",
      "df[\"Work_Type\"] = df[\"work_type\"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})\n",
      "df[\"Smoking_Status\"] = df[\"smoking_status\"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})\n",
      "480/39: df.head()\n",
      "480/40:\n",
      "df.drop([\"id\",\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"],axis=1, inplace=True)\n",
      "df.head()\n",
      "480/41:\n",
      "df.rename(columns={\"Male\":\"Gender\",\"Yes\":\"Ever_Married\",\"Urban\":\"Residence_type\"}, inplace=True)\n",
      "df.head()\n",
      "480/42:\n",
      "from imblearn.over_sampling import SMOTE\n",
      "\n",
      "smote = SMOTE(random_state = 42)\n",
      "X = df.drop(['stroke'],axis=1)\n",
      "y = df['stroke']\n",
      "X,y= smote.fit_resample(X,y)\n",
      "y = pd.DataFrame({'stroke':y})\n",
      "sns.countplot(data = y, x = 'stroke', y= None)\n",
      "plt.show()\n",
      "481/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "481/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "481/3: df.head()\n",
      "481/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "481/5:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "481/6:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "481/7:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "481/8:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "481/9:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "481/10:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "481/11:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "481/12:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "481/13:\n",
      "print(df[\"stroke\"].value_counts())\n",
      "sns.countplot(x=df[\"stroke\"], data=df)\n",
      "481/14: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "481/15: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "481/16: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "481/17: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "481/18: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "481/19: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "481/20: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "481/21: sns.distplot(df[\"age\"], fit=norm)\n",
      "481/22: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "481/23: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "481/24: sns.boxplot(x=df[\"age\"], data=df)\n",
      "481/25: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "481/26: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "481/27:\n",
      "data = df.corr(method='pearson')\n",
      "fig = plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n",
      "481/28: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "481/29: df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "481/30: print(\"Length of bmi after: \",df[\"bmi\"].isnull().sum())\n",
      "481/31:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound: {}, lower bound: {}\".format(upper_bound,lower_bound))\n",
      "481/32:\n",
      "df.drop(df[df['bmi'] > upper_bound].index, inplace = True)\n",
      "df.drop(df[df['bmi'] < lower_bound].index, inplace = True)\n",
      "481/33:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2,figsize=(15,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax = axes[0])\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax = axes[1])\n",
      "481/34:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df.drop(df[df[\"gender\"]==\"Other\"].index, inplace=True)\n",
      "481/35:\n",
      "# For gender column.\n",
      "sex = pd.get_dummies(df[\"gender\"], drop_first=True)\n",
      "df = pd.concat([df,sex],axis=1)\n",
      "481/36:\n",
      "# For Ever_married colummn.\n",
      "married_status = pd.get_dummies(df[\"ever_married\"], drop_first=True)\n",
      "df = pd.concat([df, married_status], axis=1)\n",
      "481/37:\n",
      "# For Residence type column.\n",
      "residence = pd.get_dummies(df[\"Residence_type\"], drop_first=True)\n",
      "df = pd.concat([df, residence], axis=1)\n",
      "481/38:\n",
      "df[\"Work_Type\"] = df[\"work_type\"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})\n",
      "df[\"Smoking_Status\"] = df[\"smoking_status\"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})\n",
      "481/39: df.head()\n",
      "481/40:\n",
      "df.drop([\"id\",\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"],axis=1, inplace=True)\n",
      "df.head()\n",
      "481/41:\n",
      "df.rename(columns={\"Male\":\"Gender\",\"Yes\":\"Ever_Married\",\"Urban\":\"Residence_type\"}, inplace=True)\n",
      "df.head()\n",
      "481/42:\n",
      "from imblearn.over_sampling import SMOTE\n",
      "\n",
      "smote = SMOTE(random_state = 42)\n",
      "X = df.drop(['stroke'],axis=1)\n",
      "y = df['stroke']\n",
      "X,y= smote.fit_resample(X,y)\n",
      "y = pd.DataFrame({'stroke':y})\n",
      "sns.countplot(data = y, x = 'stroke', y= None)\n",
      "plt.show()\n",
      "print(y.value_counts())\n",
      "481/43: df[\"stroke\"].value_counts()\n",
      "481/44:\n",
      "df = pd.concat([X,y],axis = 1)\n",
      "df.head()\n",
      "481/45: df[\"stroke\"].value_counts()\n",
      "481/46:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "ss = StandardScaler()\n",
      "df = ss.fit_transform(df)\n",
      "481/47: df.head()\n",
      "481/48:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "ss = StandardScaler()\n",
      "df = ss.fit_transform(df)\n",
      "df = pd.Dataframe(df)\n",
      "481/49:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "ss = StandardScaler()\n",
      "df = ss.fit_transform(df)\n",
      "df = np.Dataframe(df)\n",
      "481/50: df.head()\n",
      "482/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "482/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "482/3: df.head()\n",
      "482/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "482/5:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "482/6:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "482/7:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "482/8:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "482/9:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "482/10:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "482/11:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "482/12:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "482/13:\n",
      "print(df[\"stroke\"].value_counts())\n",
      "sns.countplot(x=df[\"stroke\"], data=df)\n",
      "482/14: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "482/15: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "482/16: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "482/17: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "482/18: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "482/19: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "482/20: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "482/21: sns.distplot(df[\"age\"], fit=norm)\n",
      "482/22: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "482/23: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "482/24: sns.boxplot(x=df[\"age\"], data=df)\n",
      "482/25: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "482/26: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "482/27:\n",
      "data = df.corr(method='pearson')\n",
      "fig = plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n",
      "482/28: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "482/29: df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "482/30: print(\"Length of bmi after: \",df[\"bmi\"].isnull().sum())\n",
      "482/31:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound: {}, lower bound: {}\".format(upper_bound,lower_bound))\n",
      "482/32:\n",
      "df.drop(df[df['bmi'] > upper_bound].index, inplace = True)\n",
      "df.drop(df[df['bmi'] < lower_bound].index, inplace = True)\n",
      "482/33:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2,figsize=(15,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax = axes[0])\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax = axes[1])\n",
      "482/34:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df.drop(df[df[\"gender\"]==\"Other\"].index, inplace=True)\n",
      "482/35:\n",
      "# For gender column.\n",
      "sex = pd.get_dummies(df[\"gender\"], drop_first=True)\n",
      "df = pd.concat([df,sex],axis=1)\n",
      "482/36:\n",
      "# For Ever_married colummn.\n",
      "married_status = pd.get_dummies(df[\"ever_married\"], drop_first=True)\n",
      "df = pd.concat([df, married_status], axis=1)\n",
      "482/37:\n",
      "# For Residence type column.\n",
      "residence = pd.get_dummies(df[\"Residence_type\"], drop_first=True)\n",
      "df = pd.concat([df, residence], axis=1)\n",
      "482/38:\n",
      "df[\"Work_Type\"] = df[\"work_type\"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})\n",
      "df[\"Smoking_Status\"] = df[\"smoking_status\"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})\n",
      "482/39: df.head()\n",
      "482/40:\n",
      "df.drop([\"id\",\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"],axis=1, inplace=True)\n",
      "df.head()\n",
      "482/41:\n",
      "df.rename(columns={\"Male\":\"Gender\",\"Yes\":\"Ever_Married\",\"Urban\":\"Residence_type\"}, inplace=True)\n",
      "df.head()\n",
      "482/42:\n",
      "from imblearn.over_sampling import SMOTE\n",
      "\n",
      "smote = SMOTE(random_state = 42)\n",
      "X = df.drop(['stroke'],axis=1)\n",
      "y = df['stroke']\n",
      "X,y= smote.fit_resample(X,y)\n",
      "y = pd.DataFrame({'stroke':y})\n",
      "sns.countplot(data = y, x = 'stroke', y= None)\n",
      "plt.show()\n",
      "print(y.value_counts())\n",
      "482/43:\n",
      "df = pd.concat([X,y],axis = 1)\n",
      "df.head()\n",
      "482/44: X = df.drop([\"stroke\"], axis=1)\n",
      "482/45: x\n",
      "482/46: X\n",
      "482/47:\n",
      "X = df.drop([\"stroke\"], axis=1)\n",
      "y = df[\"stroke\"]\n",
      "482/48: X\n",
      "482/49: X.head()\n",
      "482/50:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "ss = StandardScaler()\n",
      "X = ss.fit_transform(X)\n",
      "482/51: X\n",
      "485/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "485/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "485/3: df.head()\n",
      "485/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "485/5:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "485/6:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "485/7:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "485/8:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "485/9:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "485/10:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "485/11:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "485/12:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "485/13:\n",
      "print(df[\"stroke\"].value_counts())\n",
      "sns.countplot(x=df[\"stroke\"], data=df)\n",
      "485/14: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "485/15: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "485/16: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "485/17: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "485/18: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "485/19: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "485/20: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "485/21: sns.distplot(df[\"age\"], fit=norm)\n",
      "485/22: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "485/23: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "485/24: sns.boxplot(x=df[\"age\"], data=df)\n",
      "485/25: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "485/26: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "485/27:\n",
      "data = df.corr(method='pearson')\n",
      "fig = plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n",
      "485/28: print(\"Length of bmi before: \",len(df[\"bmi\"])-df[\"bmi\"].isnull().sum())\n",
      "485/29: df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "485/30: print(\"Length of bmi after: \",df[\"bmi\"].isnull().sum())\n",
      "485/31:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound: {}, lower bound: {}\".format(upper_bound,lower_bound))\n",
      "485/32:\n",
      "df.drop(df[df['bmi'] > upper_bound].index, inplace = True)\n",
      "df.drop(df[df['bmi'] < lower_bound].index, inplace = True)\n",
      "485/33:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2,figsize=(15,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax = axes[0])\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax = axes[1])\n",
      "485/34:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df.drop(df[df[\"gender\"]==\"Other\"].index, inplace=True)\n",
      "485/35:\n",
      "# For gender column.\n",
      "sex = pd.get_dummies(df[\"gender\"], drop_first=True)\n",
      "df = pd.concat([df,sex],axis=1)\n",
      "485/36:\n",
      "# For Ever_married colummn.\n",
      "married_status = pd.get_dummies(df[\"ever_married\"], drop_first=True)\n",
      "df = pd.concat([df, married_status], axis=1)\n",
      "485/37:\n",
      "# For Residence type column.\n",
      "residence = pd.get_dummies(df[\"Residence_type\"], drop_first=True)\n",
      "df = pd.concat([df, residence], axis=1)\n",
      "485/38:\n",
      "df[\"Work_Type\"] = df[\"work_type\"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})\n",
      "df[\"Smoking_Status\"] = df[\"smoking_status\"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})\n",
      "485/39: df.head()\n",
      "485/40:\n",
      "df.drop([\"id\",\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"],axis=1, inplace=True)\n",
      "df.head()\n",
      "485/41:\n",
      "df.rename(columns={\"Male\":\"Gender\",\"Yes\":\"Ever_Married\",\"Urban\":\"Residence_type\"}, inplace=True)\n",
      "df.head()\n",
      "485/42:\n",
      "from imblearn.over_sampling import SMOTE\n",
      "\n",
      "smote = SMOTE(random_state = 42)\n",
      "X = df.drop(['stroke'],axis=1)\n",
      "y = df['stroke']\n",
      "X,y= smote.fit_resample(X,y)\n",
      "y = pd.DataFrame({'stroke':y})\n",
      "sns.countplot(data = y, x = 'stroke', y= None)\n",
      "plt.show()\n",
      "print(y.value_counts())\n",
      "485/43:\n",
      "df = pd.concat([X,y],axis = 1)\n",
      "df.head()\n",
      "485/44:\n",
      "X = df.drop([\"stroke\"], axis=1)\n",
      "y = df[\"stroke\"]\n",
      "485/45:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "ss = StandardScaler()\n",
      "X = ss.fit_transform(X)\n",
      "485/46:\n",
      "lr = LogisticRegression(solver=\"liblinear\").fit(x_train,y_train)\n",
      "gnb = GaussianNB().fit(x_train,y_train)\n",
      "knnc = KNeighborsClassifier().fit(x_train,y_train)\n",
      "dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)\n",
      "rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)\n",
      "xgbc = XGBClassifier().fit(x_train,y_train)\n",
      "catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)\n",
      "485/47: x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
      "485/48:\n",
      "lr = LogisticRegression(solver=\"liblinear\").fit(x_train,y_train)\n",
      "gnb = GaussianNB().fit(x_train,y_train)\n",
      "knnc = KNeighborsClassifier().fit(x_train,y_train)\n",
      "dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)\n",
      "rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)\n",
      "xgbc = XGBClassifier().fit(x_train,y_train)\n",
      "catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)\n",
      "485/49: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,catbc]\n",
      "485/50:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    error = -cross_val_score(model,x_test,y_test,cv=10,scoring=\"neg_mean_squared_error\",verbose=False).mean()\n",
      "    print(name + \": \")\n",
      "    print(\"-\" * 50)\n",
      "    print(\"Accuracy Score: \",accuracy_score(y_test,predict))\n",
      "    print(\"Cross Validation Score: \",CV)\n",
      "    print(\"Error: \",np.sqrt(error))\n",
      "    print(\"R-square value: \",r2_score(y_test,predict))\n",
      "    print(\"Confusion matrix: \")\n",
      "    print(confusion_matrix(y_test,predict))\n",
      "    print(\"-\" * 100)\n",
      "485/51:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Accuracy\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    accuracy = accuracy_score(y_test,predict)\n",
      "    result = pd.DataFrame([[name,accuracy*100]],columns=[\"MODELS\",\"Accuracy\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"Accuracy\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"ACCURACY\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
      "plt.show()\n",
      "485/52:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"CV\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    result = pd.DataFrame([[name,CV*100]],columns=[\"MODELS\",\"CV\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"CV\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"CV\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL CROSS VALIDATION COMPARISON\")\n",
      "plt.show()\n",
      "485/53:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "r_auc = roc_auc_score(y_test,r_prob)\n",
      "485/54:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    auroc_score = roc_auc_score(y_test,predict)\n",
      "    print(name+\" score: \",auroc_score)\n",
      "    print(\"-\"*50)\n",
      "485/55:\n",
      "r_fpr,r_tpr,_= roc_curve(y_test,r_prob)\n",
      "model_dict={}\n",
      "\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    fpr,tpr,_= roc_curve(y_test,predict)\n",
      "    model_dict[name]=[fpr,tpr]\n",
      "485/56:\n",
      "plt.plot(r_fpr,r_tpr,linestyle=\"--\")\n",
      "plt.plot(model_dict[\"LogisticRegression\"][0],model_dict[\"LogisticRegression\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"GaussianNB\"][0],model_dict[\"GaussianNB\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"KNeighborsClassifier\"][0],model_dict[\"KNeighborsClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"DecisionTreeClassifier\"][0],model_dict[\"DecisionTreeClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"RandomForestClassifier\"][0],model_dict[\"RandomForestClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"XGBClassifier\"][0],model_dict[\"XGBClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"CatBoostClassifier\"][0],model_dict[\"CatBoostClassifier\"][1],linestyle='dotted')\n",
      "\n",
      "plt.title(\"ROC plot\")\n",
      "plt.xlabel(\"False positive rate.\")\n",
      "plt.ylabel(\"True positive rate.\")\n",
      "plt.show()\n",
      "485/57:\n",
      "plt.plot(r_fpr,r_tpr,linestyle=\"--\")\n",
      "plt.plot(model_dict[\"LogisticRegression\"][0],model_dict[\"LogisticRegression\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"GaussianNB\"][0],model_dict[\"GaussianNB\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"KNeighborsClassifier\"][0],model_dict[\"KNeighborsClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"DecisionTreeClassifier\"][0],model_dict[\"DecisionTreeClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"RandomForestClassifier\"][0],model_dict[\"RandomForestClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"XGBClassifier\"][0],model_dict[\"XGBClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"CatBoostClassifier\"][0],model_dict[\"CatBoostClassifier\"][1],linestyle='dotted')\n",
      "\n",
      "plt.title(\"ROC plot\")\n",
      "plt.xlabel(\"False positive rate.\")\n",
      "plt.ylabel(\"True positive rate.\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "485/58:\n",
      "plt.plot(r_fpr,r_tpr,linestyle=\"--\")\n",
      "plt.plot(model_dict[\"LogisticRegression\"][0],model_dict[\"LogisticRegression\"][1],linestyle='dotted', label=\"LogisticRegression\")\n",
      "plt.plot(model_dict[\"GaussianNB\"][0],model_dict[\"GaussianNB\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"KNeighborsClassifier\"][0],model_dict[\"KNeighborsClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"DecisionTreeClassifier\"][0],model_dict[\"DecisionTreeClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"RandomForestClassifier\"][0],model_dict[\"RandomForestClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"XGBClassifier\"][0],model_dict[\"XGBClassifier\"][1],linestyle='dotted')\n",
      "plt.plot(model_dict[\"CatBoostClassifier\"][0],model_dict[\"CatBoostClassifier\"][1],linestyle='dotted')\n",
      "\n",
      "plt.title(\"ROC plot\")\n",
      "plt.xlabel(\"False positive rate.\")\n",
      "plt.ylabel(\"True positive rate.\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "485/59:\n",
      "plt.plot(r_fpr,r_tpr,linestyle=\"--\")\n",
      "plt.plot(model_dict[\"LogisticRegression\"][0],model_dict[\"LogisticRegression\"][1],linestyle='dotted', label=\"LogisticRegression\")\n",
      "plt.plot(model_dict[\"GaussianNB\"][0],model_dict[\"GaussianNB\"][1],linestyle='dotted',label=\"GaussianNB\")\n",
      "plt.plot(model_dict[\"KNeighborsClassifier\"][0],model_dict[\"KNeighborsClassifier\"][1],linestyle='dotted', label=\"KNeighborsClassifier\")\n",
      "plt.plot(model_dict[\"DecisionTreeClassifier\"][0],model_dict[\"DecisionTreeClassifier\"][1],linestyle='dotted', label=\"DecisionTreeClassifier\")\n",
      "plt.plot(model_dict[\"RandomForestClassifier\"][0],model_dict[\"RandomForestClassifier\"][1],linestyle='dotted', label=\"RandomForestClassifier\")\n",
      "plt.plot(model_dict[\"XGBClassifier\"][0],model_dict[\"XGBClassifier\"][1],linestyle='dotted', label=\"XGBClassifier\")\n",
      "plt.plot(model_dict[\"CatBoostClassifier\"][0],model_dict[\"CatBoostClassifier\"][1],linestyle='dotted', label=\"CatBoostClassifier\")\n",
      "\n",
      "plt.title(\"ROC plot\")\n",
      "plt.xlabel(\"False positive rate.\")\n",
      "plt.ylabel(\"True positive rate.\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "485/60:\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(r_fpr,r_tpr,linestyle=\"--\")\n",
      "plt.plot(model_dict[\"LogisticRegression\"][0],model_dict[\"LogisticRegression\"][1],linestyle='dotted', label=\"LogisticRegression\")\n",
      "plt.plot(model_dict[\"GaussianNB\"][0],model_dict[\"GaussianNB\"][1],linestyle='dotted',label=\"GaussianNB\")\n",
      "plt.plot(model_dict[\"KNeighborsClassifier\"][0],model_dict[\"KNeighborsClassifier\"][1],linestyle='dotted', label=\"KNeighborsClassifier\")\n",
      "plt.plot(model_dict[\"DecisionTreeClassifier\"][0],model_dict[\"DecisionTreeClassifier\"][1],linestyle='dotted', label=\"DecisionTreeClassifier\")\n",
      "plt.plot(model_dict[\"RandomForestClassifier\"][0],model_dict[\"RandomForestClassifier\"][1],linestyle='dotted', label=\"RandomForestClassifier\")\n",
      "plt.plot(model_dict[\"XGBClassifier\"][0],model_dict[\"XGBClassifier\"][1],linestyle='dotted', label=\"XGBClassifier\")\n",
      "plt.plot(model_dict[\"CatBoostClassifier\"][0],model_dict[\"CatBoostClassifier\"][1],linestyle='dotted', label=\"CatBoostClassifier\")\n",
      "\n",
      "plt.title(\"ROC plot\")\n",
      "plt.xlabel(\"False positive rate.\")\n",
      "plt.ylabel(\"True positive rate.\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "485/61: print(\"Missing values in bmi after: \",df[\"bmi\"].isnull().sum())\n",
      "486/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "486/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "486/3: df.head()\n",
      "486/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "486/5:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "486/6:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "486/7:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "486/8:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "486/9:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "486/10:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "486/11:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "486/12:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "486/13:\n",
      "print(df[\"stroke\"].value_counts())\n",
      "sns.countplot(x=df[\"stroke\"], data=df)\n",
      "486/14: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "486/15: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "486/16: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "486/17: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "486/18: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "486/19: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "486/20: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "486/21: sns.distplot(df[\"age\"], fit=norm)\n",
      "486/22: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "486/23: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "486/24: sns.boxplot(x=df[\"age\"], data=df)\n",
      "486/25: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "486/26: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "486/27:\n",
      "data = df.corr(method='pearson')\n",
      "fig = plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n",
      "486/28: print(\"Missing values in bmi before: \",df[\"bmi\"].isnull().sum())\n",
      "486/29: df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "486/30: print(\"Missing values in bmi after: \",df[\"bmi\"].isnull().sum())\n",
      "486/31:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound: {}, lower bound: {}\".format(upper_bound,lower_bound))\n",
      "486/32:\n",
      "df.drop(df[df['bmi'] > upper_bound].index, inplace = True)\n",
      "df.drop(df[df['bmi'] < lower_bound].index, inplace = True)\n",
      "486/33:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2,figsize=(15,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax = axes[0])\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax = axes[1])\n",
      "486/34:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df.drop(df[df[\"gender\"]==\"Other\"].index, inplace=True)\n",
      "486/35:\n",
      "# For gender column.\n",
      "sex = pd.get_dummies(df[\"gender\"], drop_first=True)\n",
      "df = pd.concat([df,sex],axis=1)\n",
      "486/36:\n",
      "# For Ever_married colummn.\n",
      "married_status = pd.get_dummies(df[\"ever_married\"], drop_first=True)\n",
      "df = pd.concat([df, married_status], axis=1)\n",
      "486/37:\n",
      "# For Residence type column.\n",
      "residence = pd.get_dummies(df[\"Residence_type\"], drop_first=True)\n",
      "df = pd.concat([df, residence], axis=1)\n",
      "486/38:\n",
      "df[\"Work_Type\"] = df[\"work_type\"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})\n",
      "df[\"Smoking_Status\"] = df[\"smoking_status\"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})\n",
      "486/39: df.head()\n",
      "486/40:\n",
      "df.drop([\"id\",\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"],axis=1, inplace=True)\n",
      "df.head()\n",
      "486/41:\n",
      "df.rename(columns={\"Male\":\"Gender\",\"Yes\":\"Ever_Married\",\"Urban\":\"Residence_type\"}, inplace=True)\n",
      "df.head()\n",
      "486/42:\n",
      "from imblearn.over_sampling import SMOTE\n",
      "\n",
      "smote = SMOTE(random_state = 42)\n",
      "X = df.drop(['stroke'],axis=1)\n",
      "y = df['stroke']\n",
      "X,y= smote.fit_resample(X,y)\n",
      "y = pd.DataFrame({'stroke':y})\n",
      "sns.countplot(data = y, x = 'stroke', y= None)\n",
      "plt.show()\n",
      "print(y.value_counts())\n",
      "486/43:\n",
      "df = pd.concat([X,y],axis = 1)\n",
      "df.head()\n",
      "486/44:\n",
      "X = df.drop([\"stroke\"], axis=1)\n",
      "y = df[\"stroke\"]\n",
      "486/45:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "ss = StandardScaler()\n",
      "X = ss.fit_transform(X)\n",
      "486/46: x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
      "486/47:\n",
      "lr = LogisticRegression(solver=\"liblinear\").fit(x_train,y_train)\n",
      "gnb = GaussianNB().fit(x_train,y_train)\n",
      "knnc = KNeighborsClassifier().fit(x_train,y_train)\n",
      "dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)\n",
      "rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)\n",
      "xgbc = XGBClassifier().fit(x_train,y_train)\n",
      "catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)\n",
      "486/48: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,catbc]\n",
      "486/49:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    error = -cross_val_score(model,x_test,y_test,cv=10,scoring=\"neg_mean_squared_error\",verbose=False).mean()\n",
      "    print(name + \": \")\n",
      "    print(\"-\" * 50)\n",
      "    print(\"Accuracy Score: \",accuracy_score(y_test,predict))\n",
      "    print(\"Cross Validation Score: \",CV)\n",
      "    print(\"Error: \",np.sqrt(error))\n",
      "    print(\"R-square value: \",r2_score(y_test,predict))\n",
      "    print(\"Confusion matrix: \")\n",
      "    print(confusion_matrix(y_test,predict))\n",
      "    print(\"-\" * 100)\n",
      "486/50:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Accuracy\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    accuracy = accuracy_score(y_test,predict)\n",
      "    result = pd.DataFrame([[name,accuracy*100]],columns=[\"MODELS\",\"Accuracy\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"Accuracy\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"ACCURACY\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
      "plt.show()\n",
      "486/51:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"CV\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    result = pd.DataFrame([[name,CV*100]],columns=[\"MODELS\",\"CV\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"CV\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"CV\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL CROSS VALIDATION COMPARISON\")\n",
      "plt.show()\n",
      "486/52:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "r_auc = roc_auc_score(y_test,r_prob)\n",
      "486/53:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    auroc_score = roc_auc_score(y_test,predict)\n",
      "    print(name+\" score: \",auroc_score)\n",
      "    print(\"-\"*50)\n",
      "486/54:\n",
      "r_fpr,r_tpr,_= roc_curve(y_test,r_prob)\n",
      "model_dict={}\n",
      "\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    fpr,tpr,_= roc_curve(y_test,predict)\n",
      "    model_dict[name]=[fpr,tpr]\n",
      "486/55:\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(r_fpr,r_tpr,linestyle=\"--\")\n",
      "plt.plot(model_dict[\"LogisticRegression\"][0],model_dict[\"LogisticRegression\"][1],linestyle='dotted', label=\"LogisticRegression\")\n",
      "plt.plot(model_dict[\"GaussianNB\"][0],model_dict[\"GaussianNB\"][1],linestyle='dotted',label=\"GaussianNB\")\n",
      "plt.plot(model_dict[\"KNeighborsClassifier\"][0],model_dict[\"KNeighborsClassifier\"][1],linestyle='dotted', label=\"KNeighborsClassifier\")\n",
      "plt.plot(model_dict[\"DecisionTreeClassifier\"][0],model_dict[\"DecisionTreeClassifier\"][1],linestyle='dotted', label=\"DecisionTreeClassifier\")\n",
      "plt.plot(model_dict[\"RandomForestClassifier\"][0],model_dict[\"RandomForestClassifier\"][1],linestyle='dotted', label=\"RandomForestClassifier\")\n",
      "plt.plot(model_dict[\"XGBClassifier\"][0],model_dict[\"XGBClassifier\"][1],linestyle='dotted', label=\"XGBClassifier\")\n",
      "plt.plot(model_dict[\"CatBoostClassifier\"][0],model_dict[\"CatBoostClassifier\"][1],linestyle='dotted', label=\"CatBoostClassifier\")\n",
      "\n",
      "plt.title(\"ROC plot\")\n",
      "plt.xlabel(\"False positive rate.\")\n",
      "plt.ylabel(\"True positive rate.\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "489/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/List/TM2_list_q1_20846636.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/List')\n",
      "489/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/List/TM2_list_q1_20846636.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/List')\n",
      "489/3: clear\n",
      "492/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/List/TM2_list_q1_20846636.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/List')\n",
      "492/2: clear\n",
      "494/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments')\n",
      "494/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments')\n",
      "494/3: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')\n",
      "494/4: clear\n",
      "494/5: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')\n",
      "494/6: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')\n",
      "494/7: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')\n",
      "494/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')\n",
      "494/9: clear\n",
      "494/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')\n",
      "494/11: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')\n",
      "494/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')\n",
      "494/13: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')\n",
      "494/14: clear\n",
      "494/15: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')\n",
      "494/17: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')\n",
      "494/18: runcell(0, 'C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q5.py')\n",
      "494/19:\n",
      "for i in range(len(Sample_list)):\r\n",
      "    temp = list(Sample_list[i])\r\n",
      "    temp[-1] = 100\r\n",
      "    temp = tuple(temp)\r\n",
      "    Sample_list[i]=temp\n",
      "494/20: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')\n",
      "494/21: clear\n",
      "494/22: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')\n",
      "494/23: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')\n",
      "495/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/3: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/4: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/5: clear\n",
      "495/6: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/7: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/9: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/11: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/13: clear\n",
      "495/14: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/15: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/17: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/18: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/19: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/20: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/21: clear\n",
      "495/22: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/23: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')\n",
      "495/24: clear\n",
      "498/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/3: clear\n",
      "498/4: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/5: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/6: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/7: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/9: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/11: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/13: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/14: clear\n",
      "498/15: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/17: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/18: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/19: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/20: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/21: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/22: clear\n",
      "498/23: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/24: clear\n",
      "498/25: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/26: clear\n",
      "498/27: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/28: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/29: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/30: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/31: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/32: clear\n",
      "498/33: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/34: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/35: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/36: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/37: clear\n",
      "498/38: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/39: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/40: clear\n",
      "498/41: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/42: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/43: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/44: clear\n",
      "498/45: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/46: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/47: clear\n",
      "498/48: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/49: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/50: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/51: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/52: clear\n",
      "498/53: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/54: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/55: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/56: clear\n",
      "498/57: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "498/58: clear\n",
      "500/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "500/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/3: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "500/4: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "500/5: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "500/6: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "500/7: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "500/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "500/9: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "500/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "500/11: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "500/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')\n",
      "501/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/3: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/4: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/5: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/6: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/7: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/9: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/11: runcell(0, 'C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled1.py')\n",
      "501/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/13: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/14: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/15: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/17: clear\n",
      "501/18: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/19: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/20: clear\n",
      "501/21: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/22: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/23: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/24: clear\n",
      "501/25: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/26: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')\n",
      "501/27: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/28: clear\n",
      "501/29: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/30: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/31: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/32: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled7.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/33: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled7.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/34: clear\n",
      "501/35: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled7.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/36: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled7.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/37: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled7.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/38: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled7.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/39: clear\n",
      "501/40: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled8.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/41: clear\n",
      "501/42: clear\n",
      "501/43: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled8.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/44: clear\n",
      "501/45: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled8.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/46: clear\n",
      "501/47: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled9.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/48: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled9.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/49: clear\n",
      "501/50: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled9.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/51: runcell(0, 'C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py')\n",
      "501/52: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/53: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/54: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/55: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/56: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/57: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/58: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/59: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/60: clear\n",
      "501/61: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled11.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/62: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled11.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/63: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled11.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/64: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/65: clear\n",
      "501/66: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/67: clear\n",
      "501/68: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/69: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/70: clear\n",
      "501/71: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/72: clear\n",
      "501/73: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/74: clear\n",
      "501/75: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/76: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/77: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/78: clear\n",
      "501/79: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/80: clear\n",
      "501/81: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/82: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/83: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "501/84: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/Question_6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "502/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/Project_2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "502/2: clear\n",
      "502/3: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/Project_2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')\n",
      "502/4: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument/Question_1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument')\n",
      "502/5: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument/Question_1.py', args='10 20', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument')\n",
      "502/6: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument/Question_1.py', args='10 20', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument')\n",
      "502/7: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument/Question_1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument')\n",
      "502/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "502/9: clear\n",
      "502/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "502/11: clear\n",
      "502/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/Question_1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "502/13: clear\n",
      "502/14: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "502/15: clear\n",
      "502/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "502/17: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "502/18: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "502/19: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "502/20: clear\n",
      "503/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/3: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/4: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/5: clear\n",
      "503/6: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/7: clear\n",
      "503/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/9: clear\n",
      "503/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/11: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/12: clear\n",
      "503/13: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/14: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/15: clear\n",
      "503/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/17: clear\n",
      "503/18: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/19: clear\n",
      "503/20: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/21: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/22: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/23: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/24: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/25: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/26: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/27: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/28: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/29: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/30: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/31: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/32: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/33: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/34: clear\n",
      "503/35: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/36: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/37: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/38: clear\n",
      "503/39: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/40: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/41: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/42: clear\n",
      "503/43: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/44: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/45: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/46: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/47: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "503/48: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "505/1: runfile('C:/Users/Jay/untitled0.py', wdir='C:/Users/Jay')\n",
      "505/2: runfile('C:/Users/Jay/untitled0.py', wdir='C:/Users/Jay')\n",
      "505/3: runfile('C:/Users/Jay/untitled0.py', wdir='C:/Users/Jay')\n",
      "505/4: runfile('C:/Users/Jay/untitled0.py', wdir='C:/Users/Jay')\n",
      "505/5: runfile('C:/Users/Jay/untitled0.py', wdir='C:/Users/Jay')\n",
      "505/6: runfile('C:/Users/Jay/untitled0.py', wdir='C:/Users/Jay')\n",
      "505/7: runfile('C:/Users/Jay/untitled0.py', wdir='C:/Users/Jay')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/8: clear\n",
      "505/9: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/11: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/13: clear\n",
      "505/14: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/15: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/17: clear\n",
      "505/18: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/19: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/20: clear\n",
      "505/21: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/22: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/23: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/24: clear\n",
      "505/25: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/26: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/27: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/28: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/29: clear\n",
      "505/30: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/31: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/32: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/33: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/Question_6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')\n",
      "505/34: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/35: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/36: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/37: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/38: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/39: clear\n",
      "505/40: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "505/41: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/3: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/4: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/5: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/6: clear\n",
      "506/7: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/9: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/11: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/13: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/14: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/15: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/17: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/18: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/19: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/20: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/21: clear\n",
      "506/22: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/23: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/24: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/25: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/26: clear\n",
      "506/27: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/28: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/29: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/30: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/31: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/32: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/33: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/34: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/35: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/36: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/37: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/38: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/39: clear\n",
      "506/40: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/41: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/42: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/43: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/44: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/45: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/46: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/47: clear\n",
      "506/48: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')\n",
      "506/49: clear\n",
      "507/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "507/2: data1 = pd.read_csv(\"country_wise_latest.csv\")\n",
      "507/3: data1.head()\n",
      "507/4:\n",
      "data1 = pd.read_csv(\"country_wise_latest.csv\")\n",
      "data2 = pd.read_csv(\"covid_19_clean_complete.csv\")\n",
      "507/5: data2.head()\n",
      "507/6: data2\n",
      "507/7: data2[\"Country/Region\"] == \"India\"\n",
      "507/8: data2\n",
      "507/9:\n",
      "data1 = pd.read_csv(\"country_wise_latest.csv\")\n",
      "data2 = pd.read_csv(\"covid_19_clean_complete.csv\")\n",
      "data3 = pd.read_csv(\"full_grouped.csv\")\n",
      "507/10: data3.head()\n",
      "507/11:\n",
      "data1 = pd.read_csv(\"country_wise_latest.csv\")\n",
      "data2 = pd.read_csv(\"covid_19_clean_complete.csv\")\n",
      "data3 = pd.read_csv(\"full_grouped.csv\")\n",
      "data4 = pd.read_csv(\"day_wise.csv\")\n",
      "507/12: data4.head()\n",
      "507/13: data3.head(186)\n",
      "507/14: data3.head(188)\n",
      "507/15: data1.head(188)\n",
      "507/16: data1.size()\n",
      "507/17: data1.size\n",
      "507/18: data1.head(10)\n",
      "507/19: data1.shape()\n",
      "507/20: data1.shape\n",
      "507/21: data1.head(188)\n",
      "507/22: data1.isna().sum()\n",
      "507/23: data2.head(188)\n",
      "507/24: data2.tail()\n",
      "507/25: data2.shape\n",
      "507/26: data2.head()\n",
      "507/27: data2[\"Country/Region\"].isunique()\n",
      "507/28: data2[\"Country/Region\"].unique()\n",
      "507/29: data2[\"Country/Region\"].unique().sum()\n",
      "507/30: data2[\"Country/Region\"].unique()\n",
      "507/31: len(data2[\"Country/Region\"].unique())\n",
      "507/32: data2[\"Country/Region\"].unique()\n",
      "507/33: data2.head()\n",
      "507/34: data2.isna().sum()\n",
      "507/35: data3.shape\n",
      "507/36: data2.head().shape\n",
      "507/37: data2.head()\n",
      "507/38: data2.shape\n",
      "507/39: new_data = pd.read_csv(\"https://github.com/owid/covid-19-data/blob/master/public/data/latest/owid-covid-latest.csv\")\n",
      "507/40:\n",
      "vactin_data = https://github.com/owid/covid-19-data/blob/master/public/data/latest/owid-covid-latest.csv\n",
      "new_data = pd.read_csv(\"vactin_data\")\n",
      "507/41:\n",
      "vactin_data = 'https://github.com/owid/covid-19-data/blob/master/public/data/latest/owid-covid-latest.csv'\n",
      "new_data = pd.read_csv(\"vactin_data\")\n",
      "507/42:\n",
      "vactin_data = 'https://github.com/owid/covid-19-data/blob/master/public/data/latest/owid-covid-latest.csv'\n",
      "#new_data = pd.read_csv(\"vactin_data\")\n",
      "507/43: vactin_data\n",
      "507/44:\n",
      "vactin_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.csv'\n",
      "#new_data = pd.read_csv(\"vactin_data\")\n",
      "507/45: vactin_data\n",
      "507/46:\n",
      "vactin_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.csv'\n",
      "new_data = pd.read_csv(\"vactin_data\")\n",
      "507/47:\n",
      "vactin_data = \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.csv\"\n",
      "new_data = pd.read_csv(\"vactin_data\")\n",
      "507/48:\n",
      "vactin_data = \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.csv\"\n",
      "new_data = pd.read_csv(vactin_data)\n",
      "507/49:\n",
      "vactin_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.csv'\n",
      "new_data = pd.read_csv(vactin_data)\n",
      "507/50:\n",
      "vactin_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.csv'\n",
      "new_data = pd.read_csv(vactin_data)\n",
      "new_data\n",
      "507/51:\n",
      "v_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv'\n",
      "pd.read_csv(v_data)\n",
      "507/52:\n",
      "v_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv'\n",
      "pd.read_csv(v_data)\n",
      "507/53: v_data['date'].unique()\n",
      "507/54: v_data['date'].unique\n",
      "507/55: v_data['location'].unique\n",
      "507/56:\n",
      "v_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv'\n",
      "pd.read_csv(v_data)\n",
      "print(type(v_data))\n",
      "507/57:\n",
      "v_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv'\n",
      "pd.read_csv(v_data)\n",
      "507/58: v_data['location']\n",
      "507/59:\n",
      "v_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv'\n",
      "v_data = pd.read_csv(v_data)\n",
      "v_data.head()\n",
      "507/60: v_data['location'].unique\n",
      "507/61: v_data['location'].unique()\n",
      "507/62: v_data['date'].unique()\n",
      "507/63: columns = v_data['date'].unique()\n",
      "507/64:\n",
      "columns = v_data['date'].unique()\n",
      "columns\n",
      "507/65: data_frame = pd.DataFrame(0, columns=[date_columns])\n",
      "507/66: date_columns = v_data['date'].unique()\n",
      "507/67: data_frame = pd.DataFrame(0, columns=[date_columns])\n",
      "507/68: data_frame = pd.DataFrame(0, columns=[for col in date_colmns])\n",
      "507/69: data_frame = pd.DataFrame(0, columns=[col, for col in date_colmns])\n",
      "507/70: data_frame = pd.DataFrame(0, columns=[col, for col in date_colmns:])\n",
      "507/71: data_frame = pd.DataFrame(0, columns=date_columns)\n",
      "507/72: date_columns = v_data['date'].unique()\n",
      "507/73:\n",
      "date_columns = v_data['date'].unique()\n",
      "date_columns\n",
      "507/74:\n",
      "v_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv'\n",
      "v_data = pd.read_csv(v_data)\n",
      "v_data\n",
      "507/75:\n",
      "date_columns = v_data['date'].unique()\n",
      "date_columns.sort()\n",
      "date_column\n",
      "507/76:\n",
      "date_columns = v_data['date'].unique()\n",
      "date_columns.sort()\n",
      "date_columns\n",
      "507/77: data_frame[date_columns]\n",
      "507/78: v_data_1 = v_data.pivot_table('total_vaccinations','location','date')\n",
      "507/79: v_data_1\n",
      "507/80: v_data_1 = v_data.pivot_table('total_vaccinations'*10000,'location','date')\n",
      "507/81: v_data_1 = v_data.pivot_table('total_vaccinations','location','date')\n",
      "507/82: v_data_1\n",
      "507/83: v_data_1['Afghanistan']['2021-09-16']\n",
      "507/84: v_data_1\n",
      "507/85: v_data['2021-09-16']['Afghanistan']\n",
      "507/86: v_data['Zimbabwe']\n",
      "507/87: v_data['location']['Zimbabwe']\n",
      "507/88: v_data['location'].iloc[1]\n",
      "507/89: v_data['location'].iloc[1:]\n",
      "507/90: v_data['location'].iloc[1]\n",
      "507/91: v_data[:].iloc[1]\n",
      "507/92: v_data[:].iloc[50237]\n",
      "507/93: v_df_1 = pd.DataFrame()\n",
      "507/94:\n",
      "v_df_1 = pd.DataFrame()\n",
      "v_df_1\n",
      "507/95:\n",
      "v_df_1 = pd.DataFrame()\n",
      "v_df_1[date_columns]\n",
      "507/96: v_df_1 = pd.DataFrame(columns=date_columns)\n",
      "507/97:\n",
      "v_df_1 = pd.DataFrame(columns=date_columns)\n",
      "v_df_1\n",
      "507/98:\n",
      "v_df_1 = pd.DataFrame(columns=['location',date_columns])\n",
      "v_df_1\n",
      "507/99:\n",
      "v_df_1 = pd.DataFrame(columns='location',date_columns)\n",
      "v_df_1\n",
      "507/100:\n",
      "v_df_1 = pd.DataFrame(columns=('location',date_columns))\n",
      "v_df_1\n",
      "507/101:\n",
      "v_df_1 = pd.DataFrame(columns=date_columns)\n",
      "v_df_1 = pd.DataFrame(loc=idx, column='location')\n",
      "v_df_1\n",
      "507/102:\n",
      "v_df_1 = pd.DataFrame(columns=date_columns)\n",
      "v_df_1 = pd.DataFrame(loc=0, column='location')\n",
      "v_df_1\n",
      "507/103:\n",
      "v_df_1 = pd.DataFrame(columns=date_columns)\n",
      "v_df_1.insert(loc=0, column='location')\n",
      "v_df_1\n",
      "507/104:\n",
      "v_df_1 = pd.DataFrame(columns=date_columns)\n",
      "v_df_1.insert(loc=0, column='location',value=0)\n",
      "v_df_1\n",
      "507/105:\n",
      "location_column = v_data['location'].unique().sort()\n",
      "location_column\n",
      "507/106:\n",
      "location_column = v_data['location'].unique().sort()\n",
      "location_column\n",
      "507/107:\n",
      "location_column = v_data['location'].unique().sort()\n",
      "print(location_column)\n",
      "507/108:\n",
      "location_column = v_data['location'].unique()\n",
      "print(location_column)\n",
      "507/109:\n",
      "location_column = v_data['location'].unique()\n",
      "location_column.sort()\n",
      "print(location_column)\n",
      "507/110:\n",
      "v_df_1 = pd.DataFrame(columns=date_columns)\n",
      "v_df_1.insert(loc=0, column='location',value=location_column)\n",
      "v_df_1\n",
      "507/111:\n",
      "new_data = v_data.drop(columns=['total_boosters','daily_vaccinations_raw','daily_vaccinations','total_vaccinations_per_hundred',\n",
      "                                'people_vaccinated_per_hundred','people_fully_vaccinated_per_hundred',\n",
      "                                'total_boosters_per_hundred','daily_vaccinations_per_million'], axis=1)\n",
      "new_data\n",
      "507/112:\n",
      "new_data = v_data.drop(columns=['iso_code','total_boosters','daily_vaccinations_raw','daily_vaccinations','total_vaccinations_per_hundred',\n",
      "                                'people_vaccinated_per_hundred','people_fully_vaccinated_per_hundred',\n",
      "                                'total_boosters_per_hundred','daily_vaccinations_per_million'], axis=1)\n",
      "new_data\n",
      "507/113: new_data.isnull().sum()\n",
      "507/114: new_data.isnull()\n",
      "507/115: new_data.fillna(0)\n",
      "507/116: new_data.isnull(0).sum()\n",
      "507/117: new_data.isnull().sum()\n",
      "507/118: new_data.fillna(0)\n",
      "507/119: new_data.isnull().sum()\n",
      "507/120: new_data.fillna(0,inplace=True)\n",
      "507/121: new_data.isnull().sum()\n",
      "507/122:\n",
      "new_data.fillna(0,inplace=True)\n",
      "new_data\n",
      "507/123:\n",
      "v_data_1 = new_data.groupby(['date'])[[\"total_vaccinations\",\"people_vaccinated\",\"people_fully_vaccinated\",\"location\"]].sum().reset_index()\n",
      "v_data_1\n",
      "507/124:\n",
      "v_data_1 = new_data.groupby(['date'])[[\"total_vaccinations\",\"people_vaccinated\",\"people_fully_vaccinated\"]].sum().reset_index()\n",
      "v_data_1\n",
      "507/125: v_data_1[\"total_vaccinations\"].iloc[-1]\n",
      "507/126: v_data_1[\"people_vaccinated\"].iloc[-1]\n",
      "507/127: v_data_1[\"total_vaccinations\"].iloc[-1]\n",
      "507/128: v_data_1[\"people_fully_vaccinated\"].iloc[-1]\n",
      "507/129:\n",
      "date = new_data['date'].unique()\n",
      "date.sort()\n",
      "date[-1]\n",
      "507/130: new_data.loc[new_data['location'] == 'World' and new_data['date']== date[-1]]\n",
      "507/131: new_data.loc[new_data['location'] == 'World']\n",
      "507/132: new_data.loc[new_data['location'] == 'World'][-1]\n",
      "507/133: globale = new_data.loc[new_data['location'] == 'World']\n",
      "507/134:\n",
      "globale = new_data.loc[new_data['location'] == 'World']\n",
      "globale\n",
      "507/135:\n",
      "globale = new_data.loc[new_data['location'] == 'World']\n",
      "globale[-1]\n",
      "507/136:\n",
      "globale = new_data.loc[new_data['location'] == 'World'].reset_index()\n",
      "globale\n",
      "507/137:\n",
      "globale = new_data.loc[new_data['location'] == 'World'].reset_index()\n",
      "globale[-1]\n",
      "507/138:\n",
      "globale = new_data.loc[new_data['location'] == 'World'].reset_index()\n",
      "globale\n",
      "507/139:\n",
      "globale = new_data.loc[new_data['location'] == 'World'].reset_index()\n",
      "globale['date'].iloc[-1]\n",
      "507/140: new_data.loc[(new_data['location'] == 'World' and new_data['date']==date[-1])]\n",
      "507/141: new_data.loc[(new_data['location'] == 'World',new_data['date']==date[-1])]\n",
      "507/142: new_data.loc[new_data['location'] == 'World']\n",
      "507/143: globale_date = new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]\n",
      "507/144:\n",
      "globale_date = new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]\n",
      "globale_data\n",
      "507/145:\n",
      "globale_date = new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]\n",
      "globale_data\n",
      "507/146:\n",
      "global_date = new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]\n",
      "global_data\n",
      "507/147: global_date = new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]\n",
      "507/148: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]\n",
      "507/149: new_data['location'][new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]]\n",
      "507/150: new_data['location'].iloc[new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]]\n",
      "507/151: new_data['location'][new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]]\n",
      "507/152: new_data['location'].[new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]]\n",
      "507/153: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]\n",
      "507/154: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1:0]\n",
      "507/155: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1:1]\n",
      "507/156: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]\n",
      "507/157: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,0]\n",
      "507/158: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]\n",
      "507/159: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,1]\n",
      "507/160: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,3]\n",
      "507/161: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,4]\n",
      "507/162: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,5]\n",
      "507/163: country_name = new_data['location'].unique()\n",
      "507/164: country_name\n",
      "507/165:\n",
      "country_name = new_data['location'].unique()\n",
      "country_dropdown = {}\n",
      "\n",
      "for cntry in country_name:\n",
      "    country_dropdown[\"label\"]=cntry\n",
      "    county_dropdown[\"value\"]=cntry\n",
      "507/166:\n",
      "country_name = new_data['location'].unique()\n",
      "country_dropdown = {}\n",
      "\n",
      "for cntry in country_name:\n",
      "    country_dropdown[\"label\"]=cntry\n",
      "    country_dropdown[\"value\"]=cntry\n",
      "507/167:\n",
      "country_name\n",
      "country_dropdown\n",
      "507/168:\n",
      "country_name = new_data['location'].unique()\n",
      "country_dropdown = {}\n",
      "\n",
      "for cntry in country_name:\n",
      "    country_dropdown[cntry]=cntry\n",
      "    country_dropdown[cntry]=cntry\n",
      "507/169:\n",
      "country_name\n",
      "country_dropdown\n",
      "507/170: date = new_data['date'].unique().sort()[-1]\n",
      "507/171: date = new_data['date'].unique().sort()\n",
      "507/172:\n",
      "date = new_data['date'].unique().sort()\n",
      "date\n",
      "507/173:\n",
      "date = new_data['date'].unique()\n",
      "date\n",
      "507/174:\n",
      "date = new_data['date'].unique()\n",
      "date.sort()\n",
      "507/175:\n",
      "date = new_data['date'].unique()\n",
      "date.sort()\n",
      "date\n",
      "507/176:\n",
      "date = new_data['date'].unique()\n",
      "date.sort()\n",
      "date[-1]\n",
      "507/177:\n",
      "date = new_data['date'].unique()\n",
      "date.sort()\n",
      "type(date[-1])\n",
      "507/178:\n",
      "date = new_data['date'].unique()\n",
      "date.sort()\n",
      "date = pd.to_datetime(date[-1])\n",
      "type(date)\n",
      "507/179:\n",
      "date = new_data['date'].unique()\n",
      "date.sort()\n",
      "date = pd.to_datetime(date[-1])\n",
      "date\n",
      "507/180:\n",
      "date = new_data['date'].unique()\n",
      "date.sort()\n",
      "date = pd.to_datetime(date[-1])\n",
      "str(date[-1].strftime(\"%B %d, %Y\"))\n",
      "507/181:\n",
      "date = new_data['date'].unique()\n",
      "date.sort()\n",
      "date = pd.to_datetime(date[-1])\n",
      "str(date.strftime(\"%B %d, %Y\"))\n",
      "507/182:\n",
      "covid_data_2 = new_data.groupby([\"date\", \"location\"])[[\"total_vaccinations\",\n",
      "                                                       \"people_vaccinated\", \"people_fully_vaccinated\"]].sum().reset_index()\n",
      "covid_data_2\n",
      "508/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "508/2:\n",
      "data1 = pd.read_csv(\"country_wise_latest.csv\")\n",
      "data2 = pd.read_csv(\"covid_19_clean_complete.csv\")\n",
      "data3 = pd.read_csv(\"full_grouped.csv\")\n",
      "data4 = pd.read_csv(\"day_wise.csv\")\n",
      "508/3:\n",
      "v_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv'\n",
      "v_data = pd.read_csv(v_data)\n",
      "v_data\n",
      "508/4:\n",
      "new_data = v_data.drop(columns=['iso_code','total_boosters','daily_vaccinations_raw','daily_vaccinations','total_vaccinations_per_hundred',\n",
      "                                'people_vaccinated_per_hundred','people_fully_vaccinated_per_hundred',\n",
      "                                'total_boosters_per_hundred','daily_vaccinations_per_million'], axis=1)\n",
      "new_data\n",
      "508/5: new_data.isnull().sum()\n",
      "508/6:\n",
      "new_data.fillna(0,inplace=True)\n",
      "new_data\n",
      "508/7: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]\n",
      "508/8: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,3]\n",
      "508/9: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,4]\n",
      "508/10: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,5]\n",
      "508/11:\n",
      "country_name = new_data['location'].unique()\n",
      "country_dropdown = {}\n",
      "\n",
      "for cntry in country_name:\n",
      "    country_dropdown[cntry]=cntry\n",
      "    country_dropdown[cntry]=cntry\n",
      "508/12:\n",
      "country_name\n",
      "country_dropdown\n",
      "508/13:\n",
      "date = new_data['date'].unique()\n",
      "date.sort()\n",
      "date = pd.to_datetime(date[-1])\n",
      "str(date.strftime(\"%B %d, %Y\"))\n",
      "508/14: total_vaccinations  people_vaccinated   people_fully_vaccinated\n",
      "508/15:\n",
      "covid_data_2 = new_data.groupby([\"date\", \"location\"])[[\"total_vaccinations\",\n",
      "                                                       \"people_vaccinated\", \"people_fully_vaccinated\"]].sum().reset_index()\n",
      "covid_data_2\n",
      "508/16:\n",
      "covid_data_2 = new_data.groupby([\"date\", \"location\"])[[\"total_vaccinations\",\n",
      "                                                       \"people_vaccinated\", \"people_fully_vaccinated\"]].sum().reset_index()\n",
      "covid_data_2.head(200)\n",
      "508/17:\n",
      "covid_data_2 = new_data.groupby([\"date\", \"location\"])[[\"total_vaccinations\",\n",
      "                                                       \"people_vaccinated\", \"people_fully_vaccinated\"]].sum().reset_index()\n",
      "covid_data_2.head(188)\n",
      "508/18: value_confirmed = covid_data_2[covid_data_2[\"location\"] == \"World\"][\"total_vaccinations\"].iloc[-1] - covid_data_2[covid_data_2[\"location\"] == \"World\"][\"total_vaccinations\"].iloc[-2]\n",
      "508/19:\n",
      "value_confirmed = covid_data_2[covid_data_2[\"location\"] == \"World\"][\"total_vaccinations\"].iloc[-1] - covid_data_2[covid_data_2[\"location\"] == \"World\"][\"total_vaccinations\"].iloc[-2]\n",
      "value_confirmed\n",
      "508/20: new_data[\"location\"]==\"World\"\n",
      "508/21: new_data[\"location\"].iloc[\"World\"]\n",
      "508/22: new_data.loc[new_data['location'] == 'World']\n",
      "508/23: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]\n",
      "508/24: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-2]\n",
      "508/25: 6133859009.0 - 6117325514.0\n",
      "508/26:\n",
      "value_confirmed = covid_data_2[covid_data_2[\"location\"] == \"World\"][\"total_vaccinations\"].iloc[-1] - covid_data_2[covid_data_2[\"location\"] == \"World\"][\"total_vaccinations\"].iloc[-3]\n",
      "value_confirmed\n",
      "508/27: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-3]\n",
      "508/28: 6133859009.0 - 6101198008.0\n",
      "508/29:\n",
      "value_confirmed = covid_data_2[covid_data_2[\"location\"] == \"World\"][\"people_vaccinated\"].iloc[-1] - covid_data_2[covid_data_2[\"location\"] == \"World\"][\"people_vaccinated\"].iloc[-3]\n",
      "value_confirmed\n",
      "508/30:\n",
      "covid_data_3 = covid_data_2[covid_data_2[\"location\"] ==\n",
      "                             \"World\"][[\"location\", \"date\", \"total_vaccinations\"]].reset_index()\n",
      "covid_data_3\n",
      "508/31:\n",
      "covid_data_3[\"daily_confirmed\"] = covid_data_3[\"total_vaccinations\"] - covid_data_3[\"total_vaccinations\"].shift(1)\n",
      "covid_data_3\n",
      "508/32:\n",
      "covid_data_3 = covid_data_2[covid_data_2[\"location\"] ==\n",
      "                             \"India\"][[\"location\", \"date\", \"total_vaccinations\"]].reset_index()\n",
      "covid_data_3\n",
      "508/33:\n",
      "covid_data_3[\"daily_confirmed\"] = covid_data_3[\"total_vaccinations\"] - covid_data_3[\"total_vaccinations\"].shift(1)\n",
      "covid_data_3\n",
      "508/34: covid_data_3[\"rolling_avg\"] = covid_data_3[\"daily_confirmed\"].rolling(window=7).mean()\n",
      "508/35: covid_data_3\n",
      "508/36: covid_data_3[\"rolling_avg\"] = covid_data_3[\"daily_confirmed\"].rolling(window=2).mean()\n",
      "508/37: covid_data_3\n",
      "508/38: covid_data_3[\"rolling_avg\"] = covid_data_3[\"daily_confirmed\"].rolling(window=7).mean()\n",
      "508/39: covid_data_3\n",
      "508/40:\n",
      "url_confirmed = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n",
      "confirmed = pd.read_csv(url_confirmed)\n",
      "confirmed.head()\n",
      "508/41:\n",
      "url_confirmed = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n",
      "confirmed = pd.read_csv(url_confirmed)\n",
      "confirmed.shape\n",
      "508/42: len(new_data[\"location\"].unique())\n",
      "508/43: len(confirmed[\"Country/Region\"].unique())\n",
      "508/44:\n",
      "list1 = list(confirmed[\"Country/Region\"].unique())\n",
      "list 2 = list(new_data[\"location\"].unique())\n",
      "508/45: list(set(list2) - set(list1)) + list(set(list1) - set(list2))\n",
      "508/46:\n",
      "list1 = list(confirmed[\"Country/Region\"].unique())\n",
      "list2 = list(new_data[\"location\"].unique())\n",
      "508/47: list(set(list2) - set(list1)) + list(set(list1) - set(list2))\n",
      "508/48: len(list(set(list2) - set(list1)) + list(set(list1) - set(list2)))\n",
      "508/49: list(set(list2) - set(list1)) + list(set(list1) - set(list2))\n",
      "508/50:\n",
      "test = new_data[\"location\"]\n",
      "test\n",
      "508/51:\n",
      "test = new_data[\"location\"]\n",
      "test.head()\n",
      "508/52:\n",
      "test = new_data[\"location\"]\n",
      "test\n",
      "508/53:\n",
      "test = new_data[\"location\"].unique()\n",
      "test\n",
      "508/54:\n",
      "test = new_data[\"location\"].unique()\n",
      "test.head()\n",
      "508/55:\n",
      "test = pd.DataFrame(new_data[\"location\"].unique())\n",
      "test.head()\n",
      "508/56:\n",
      "test = pd.DataFrame(new_data[\"location\"].unique(), columns=\"location\")\n",
      "test.head()\n",
      "508/57:\n",
      "test = pd.DataFrame(new_data[\"location\"].unique(), columns:\"location\")\n",
      "test.head()\n",
      "508/58:\n",
      "test = pd.DataFrame(new_data[\"location\"].unique(), columns:[\"location\"])\n",
      "test.head()\n",
      "508/59:\n",
      "test = pd.DataFrame(new_data[\"location\"].unique(), columns=[\"location\"])\n",
      "test.head()\n",
      "508/60:\n",
      "url_confirmed = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n",
      "confirmed = pd.read_csv(url_confirmed)\n",
      "confirmed.head()\n",
      "508/61:\n",
      "test = confirmed.groupby([\"Country/Region\"])[[\"Lat\", \"Long\"]].sum().reset_index()\n",
      "test\n",
      "508/62:\n",
      "test = pd.DataFrame(new_data[\"location\"].unique(), columns=[\"location\"])\n",
      "test\n",
      "508/63:\n",
      "test = confirmed.groupby([\"Country/Region\"])[[\"Lat\", \"Long\"]].reset_index()\n",
      "test\n",
      "508/64:\n",
      "test = confirmed.groupby([\"Country/Region\"])[[\"Lat\", \"Long\"]]\n",
      "test\n",
      "508/65:\n",
      "test = confirmed.groupby([\"Country/Region\"])[[\"Lat\", \"Long\"]].sum().reset_index()\n",
      "test\n",
      "508/66:\n",
      "url_confirmed = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n",
      "confirmed = pd.read_csv(url_confirmed)\n",
      "confirmed\n",
      "508/67: test.to_dict()\n",
      "508/68: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"]\n",
      "508/69: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"][\"lat\"]\n",
      "508/70: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"]\n",
      "508/71: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[1]\n",
      "508/72: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[:1]\n",
      "508/73: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[:2]\n",
      "508/74: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[1:]\n",
      "508/75: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[2:]\n",
      "508/76: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[:]\n",
      "508/77: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[:-1]\n",
      "508/78: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[::1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508/79: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[::2]\n",
      "508/80: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[\"lat\"]\n",
      "508/81: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[]\n",
      "508/82: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"]\n",
      "508/83: confirmed[\"Country/Region\"]==\"India\"\n",
      "508/84: confirmed[\"Country/Region\"]\n",
      "508/85: confirmed[\"Country/Region\"].[\"lat\"]\n",
      "508/86: confirmed[\"Country/Region\"][\"lat\"]\n",
      "508/87: confirmed[\"Country/Region\"][\"Lat\"]\n",
      "508/88: confirmed[\"Country/Region\"].[\"Lat\"]\n",
      "508/89: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[:,3]\n",
      "508/90: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[:]\n",
      "508/91: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[:2]\n",
      "508/92: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[:,2]\n",
      "508/93: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[,2]\n",
      "508/94: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[:,2]\n",
      "508/95:\n",
      "dic = {}\n",
      "\n",
      "for places in confirmed[\"Country/Region\"].unique():\n",
      "    dic[places] = confirmed.loc[confirmed[\"Country/Region\"]==places].iloc[:,2]\n",
      "    \n",
      "dic\n",
      "508/96: confirmed.isnull().sum()\n",
      "508/97: confirmed[\"Country/Region\"].unique()\n",
      "508/98: confirmed[\"Country/Region\"]==\"India\".iloc[:,2]\n",
      "508/99: confirmed.loc[confirmed[\"Country/Region\"]==\"India\"].iloc[:,2]\n",
      "508/100:\n",
      "dic = {}\n",
      "\n",
      "for places in (confirmed[\"Country/Region\"].unique()):\n",
      "    dic[places] = confirmed.loc[confirmed[\"Country/Region\"]==places].iloc[:,2]\n",
      "    \n",
      "dict\n",
      "508/101:\n",
      "dic = {}\n",
      "\n",
      "for places in (confirmed[\"Country/Region\"].unique()):\n",
      "    dic[places] = confirmed.loc[confirmed[\"Country/Region\"]==places].iloc[:,2]\n",
      "    \n",
      "dic\n",
      "508/102: confirmed.loc[confirmed[\"Country/Region\"]==\"Zimbabwe\"].iloc[:,2]\n",
      "508/103: confirmed.loc[confirmed[\"Country/Region\"]==\"Zimbabwe\"].iloc[:,2][1]\n",
      "508/104: confirmed.loc[confirmed[\"Country/Region\"]==\"Zimbabwe\"].iloc[:,2]\n",
      "508/105: type(confirmed.loc[confirmed[\"Country/Region\"]==\"Zimbabwe\"].iloc[:,2])\n",
      "508/106:\n",
      "d = confirmed.loc[confirmed[\"Country/Region\"]==\"Zimbabwe\"].iloc[:,2]\n",
      "d\n",
      "508/107:\n",
      "d = confirmed.loc[confirmed[\"Country/Region\"]==\"Zimbabwe\"].iloc[:,2]\n",
      "d[0]\n",
      "508/108:\n",
      "d = confirmed.loc[confirmed[\"Country/Region\"]==\"Zimbabwe\"].iloc[:,2]\n",
      "d[[0]]\n",
      "508/109: confirmed.loc[confirmed[\"Country/Region\"]==\"Zimbabwe\"].iloc[:,2]\n",
      "508/110: confirmed.iloc[:,2]\n",
      "508/111: confirmed.iloc[1:,2]\n",
      "508/112: confirmed.loc[India,\"Lat\"]\n",
      "508/113: confirmed.loc[\"India\",\"Lat\"]\n",
      "508/114: confirmed.loc[\"Country/Region\",\"Lat\"]\n",
      "508/115: confirmed.loc[[\"Country/Region\"],[\"Lat\"]]\n",
      "508/116: confirmed.iloc[:,2]\n",
      "510/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "510/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "510/3: df.head()\n",
      "510/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "510/5:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "510/6:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "510/7:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "510/8:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "510/9:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "510/10:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "510/11:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "510/12:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "510/13:\n",
      "print(df[\"stroke\"].value_counts())\n",
      "sns.countplot(x=df[\"stroke\"], data=df)\n",
      "510/14: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "510/15: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "510/16: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "510/17: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "510/18: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "510/19: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "510/20: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "510/21: sns.distplot(df[\"age\"], fit=norm)\n",
      "510/22: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "510/23: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "510/24: sns.boxplot(x=df[\"age\"], data=df)\n",
      "510/25: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "510/26: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "510/27:\n",
      "data = df.corr(method='pearson')\n",
      "fig = plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n",
      "510/28: print(\"Missing values in bmi before: \",df[\"bmi\"].isnull().sum())\n",
      "510/29: df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "510/30: print(\"Missing values in bmi after: \",df[\"bmi\"].isnull().sum())\n",
      "510/31:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound: {}, lower bound: {}\".format(upper_bound,lower_bound))\n",
      "510/32:\n",
      "df.drop(df[df['bmi'] > upper_bound].index, inplace = True)\n",
      "df.drop(df[df['bmi'] < lower_bound].index, inplace = True)\n",
      "510/33:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2,figsize=(15,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax = axes[0])\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax = axes[1])\n",
      "510/34:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df.drop(df[df[\"gender\"]==\"Other\"].index, inplace=True)\n",
      "510/35:\n",
      "# For gender column.\n",
      "sex = pd.get_dummies(df[\"gender\"], drop_first=True)\n",
      "df = pd.concat([df,sex],axis=1)\n",
      "510/36:\n",
      "# For Ever_married colummn.\n",
      "married_status = pd.get_dummies(df[\"ever_married\"], drop_first=True)\n",
      "df = pd.concat([df, married_status], axis=1)\n",
      "510/37:\n",
      "# For Residence type column.\n",
      "residence = pd.get_dummies(df[\"Residence_type\"], drop_first=True)\n",
      "df = pd.concat([df, residence], axis=1)\n",
      "510/38:\n",
      "df[\"Work_Type\"] = df[\"work_type\"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})\n",
      "df[\"Smoking_Status\"] = df[\"smoking_status\"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})\n",
      "510/39: df.head()\n",
      "510/40:\n",
      "df.drop([\"id\",\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"],axis=1, inplace=True)\n",
      "df.head()\n",
      "510/41:\n",
      "df.rename(columns={\"Male\":\"Gender\",\"Yes\":\"Ever_Married\",\"Urban\":\"Residence_type\"}, inplace=True)\n",
      "df.head()\n",
      "510/42:\n",
      "from imblearn.over_sampling import SMOTE\n",
      "\n",
      "smote = SMOTE(random_state = 42)\n",
      "X = df.drop(['stroke'],axis=1)\n",
      "y = df['stroke']\n",
      "X,y= smote.fit_resample(X,y)\n",
      "y = pd.DataFrame({'stroke':y})\n",
      "sns.countplot(data = y, x = 'stroke', y= None)\n",
      "plt.show()\n",
      "print(y.value_counts())\n",
      "510/43:\n",
      "df = pd.concat([X,y],axis = 1)\n",
      "df.head()\n",
      "510/44:\n",
      "X = df.drop([\"stroke\"], axis=1)\n",
      "y = df[\"stroke\"]\n",
      "510/45:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "ss = StandardScaler()\n",
      "X = ss.fit_transform(X)\n",
      "510/46: x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
      "510/47:\n",
      "lr = LogisticRegression(solver=\"liblinear\").fit(x_train,y_train)\n",
      "gnb = GaussianNB().fit(x_train,y_train)\n",
      "knnc = KNeighborsClassifier().fit(x_train,y_train)\n",
      "dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)\n",
      "rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)\n",
      "xgbc = XGBClassifier().fit(x_train,y_train)\n",
      "catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)\n",
      "510/48: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,catbc]\n",
      "510/49:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    error = -cross_val_score(model,x_test,y_test,cv=10,scoring=\"neg_mean_squared_error\",verbose=False).mean()\n",
      "    print(name + \": \")\n",
      "    print(\"-\" * 50)\n",
      "    print(\"Accuracy Score: \",accuracy_score(y_test,predict))\n",
      "    print(\"Cross Validation Score: \",CV)\n",
      "    print(\"Error: \",np.sqrt(error))\n",
      "    print(\"R-square value: \",r2_score(y_test,predict))\n",
      "    print(\"Confusion matrix: \")\n",
      "    print(confusion_matrix(y_test,predict))\n",
      "    print(\"-\" * 100)\n",
      "510/50:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Accuracy\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    accuracy = accuracy_score(y_test,predict)\n",
      "    result = pd.DataFrame([[name,accuracy*100]],columns=[\"MODELS\",\"Accuracy\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"Accuracy\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"ACCURACY\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
      "plt.show()\n",
      "510/51:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"CV\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    result = pd.DataFrame([[name,CV*100]],columns=[\"MODELS\",\"CV\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"CV\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"CV\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL CROSS VALIDATION COMPARISON\")\n",
      "plt.show()\n",
      "510/52:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "r_auc = roc_auc_score(y_test,r_prob)\n",
      "510/53:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    auroc_score = roc_auc_score(y_test,predict)\n",
      "    print(name+\" score: \",auroc_score)\n",
      "    print(\"-\"*50)\n",
      "510/54:\n",
      "r_fpr,r_tpr,_= roc_curve(y_test,r_prob)\n",
      "model_dict={}\n",
      "\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    fpr,tpr,_= roc_curve(y_test,predict)\n",
      "    model_dict[name]=[fpr,tpr]\n",
      "510/55:\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(r_fpr,r_tpr,linestyle=\"--\")\n",
      "plt.plot(model_dict[\"LogisticRegression\"][0],model_dict[\"LogisticRegression\"][1],linestyle='dotted', label=\"LogisticRegression\")\n",
      "plt.plot(model_dict[\"GaussianNB\"][0],model_dict[\"GaussianNB\"][1],linestyle='dotted',label=\"GaussianNB\")\n",
      "plt.plot(model_dict[\"KNeighborsClassifier\"][0],model_dict[\"KNeighborsClassifier\"][1],linestyle='dotted', label=\"KNeighborsClassifier\")\n",
      "plt.plot(model_dict[\"DecisionTreeClassifier\"][0],model_dict[\"DecisionTreeClassifier\"][1],linestyle='dotted', label=\"DecisionTreeClassifier\")\n",
      "plt.plot(model_dict[\"RandomForestClassifier\"][0],model_dict[\"RandomForestClassifier\"][1],linestyle='dotted', label=\"RandomForestClassifier\")\n",
      "plt.plot(model_dict[\"XGBClassifier\"][0],model_dict[\"XGBClassifier\"][1],linestyle='dotted', label=\"XGBClassifier\")\n",
      "plt.plot(model_dict[\"CatBoostClassifier\"][0],model_dict[\"CatBoostClassifier\"][1],linestyle='dotted', label=\"CatBoostClassifier\")\n",
      "\n",
      "plt.title(\"ROC plot\")\n",
      "plt.xlabel(\"False positive rate.\")\n",
      "plt.ylabel(\"True positive rate.\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "511/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "import warnings\n",
      "\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from xgboost import XGBClassifier\n",
      "from catboost import CatBoostClassifier\n",
      "\n",
      "\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score\n",
      "\n",
      "from pandas_profiling import ProfileReport\n",
      "\n",
      "from scipy.stats import norm\n",
      "511/2: df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
      "511/3: df.head()\n",
      "511/4:\n",
      "print(\"Shape\")\n",
      "print(df.shape)\n",
      "print(\"-\"*100)\n",
      "print(\"Columns\")\n",
      "columns = df.columns\n",
      "print(columns)\n",
      "print(\"-\"*100)\n",
      "print(\"Data information: \")\n",
      "print(df.info())\n",
      "print(\"-\"*100)\n",
      "print(\"Data description: \")\n",
      "print(df.describe())\n",
      "print(\"-\"*100)\n",
      "print(\"Null values count: \")\n",
      "print(df.isnull().sum())\n",
      "print(\"-\"*100)\n",
      "511/5:\n",
      "print(\"Gender: \",df[\"gender\"].unique())\n",
      "print(\"Hypertension: \",df[\"hypertension\"].unique())\n",
      "print(\"Heart Disease: \",df[\"heart_disease\"].unique())\n",
      "print(\"Ever Married: \",df[\"ever_married\"].unique())\n",
      "print(\"Work Type: \",df[\"work_type\"].unique())\n",
      "print(\"Residence type: \",df[\"Residence_type\"].unique())\n",
      "print(\"Smoking status: \",df[\"smoking_status\"].unique())\n",
      "print(\"Stroke: \",df[\"stroke\"].unique())\n",
      "511/6:\n",
      "print(df[\"gender\"].value_counts())\n",
      "sns.set(style=\"darkgrid\")\n",
      "sns.countplot(x=df[\"gender\"],data=df)\n",
      "511/7:\n",
      "print(df[\"hypertension\"].value_counts())\n",
      "sns.countplot(x=df[\"hypertension\"], data=df)\n",
      "511/8:\n",
      "print(df[\"heart_disease\"].value_counts())\n",
      "sns.countplot(x=df[\"heart_disease\"], data=df)\n",
      "511/9:\n",
      "print(df[\"ever_married\"].value_counts())\n",
      "sns.countplot(x=df[\"ever_married\"], data=df)\n",
      "511/10:\n",
      "print(df[\"work_type\"].value_counts())\n",
      "sns.countplot(x=df[\"work_type\"], data=df)\n",
      "511/11:\n",
      "print(df[\"Residence_type\"].value_counts())\n",
      "sns.countplot(x=df[\"Residence_type\"], data=df)\n",
      "511/12:\n",
      "print(df[\"smoking_status\"].value_counts())\n",
      "sns.countplot(x=df[\"smoking_status\"], data=df)\n",
      "511/13:\n",
      "print(df[\"stroke\"].value_counts())\n",
      "sns.countplot(x=df[\"stroke\"], data=df)\n",
      "511/14: sns.countplot(x=df[\"hypertension\"], hue=df[\"stroke\"], data=df)\n",
      "511/15: sns.countplot(x=df[\"gender\"], hue=df[\"stroke\"], data=df)\n",
      "511/16: sns.countplot(x=df[\"heart_disease\"], hue=df[\"stroke\"], data=df)\n",
      "511/17: sns.countplot(x=df[\"ever_married\"], hue=df[\"stroke\"], data=df)\n",
      "511/18: sns.countplot(x=df[\"work_type\"], hue=df[\"stroke\"], data=df)\n",
      "511/19: sns.countplot(x=df[\"Residence_type\"], hue=df[\"stroke\"], data=df)\n",
      "511/20: sns.countplot(x=df[\"smoking_status\"], hue=df[\"stroke\"], data=df)\n",
      "511/21: sns.distplot(df[\"age\"], fit=norm)\n",
      "511/22: sns.distplot(df[\"bmi\"], fit=norm)\n",
      "511/23: sns.distplot(df[\"avg_glucose_level\"], fit=norm)\n",
      "511/24: sns.boxplot(x=df[\"age\"], data=df)\n",
      "511/25: sns.boxplot(x=df[\"bmi\"], data=df)\n",
      "511/26: sns.boxplot(x=df[\"avg_glucose_level\"], data=df)\n",
      "511/27:\n",
      "data = df.corr(method='pearson')\n",
      "fig = plt.figure(figsize=(15,8))\n",
      "sns.heatmap(data,annot=True,cbar=True,linewidths=1)\n",
      "511/28: print(\"Missing values in bmi before: \",df[\"bmi\"].isnull().sum())\n",
      "511/29: df[\"bmi\"].fillna(value=df[\"bmi\"].mean(), inplace=True)\n",
      "511/30: print(\"Missing values in bmi after: \",df[\"bmi\"].isnull().sum())\n",
      "511/31:\n",
      "q1,q3 = np.percentile(df[\"bmi\"],[25,75])\n",
      "print(q1,q3)\n",
      "iqr = q3-q1\n",
      "upper_bound = q3+(1.5*iqr)\n",
      "lower_bound = q1-(1.5*iqr)\n",
      "print(\"upper bound: {}, lower bound: {}\".format(upper_bound,lower_bound))\n",
      "511/32:\n",
      "df.drop(df[df['bmi'] > upper_bound].index, inplace = True)\n",
      "df.drop(df[df['bmi'] < lower_bound].index, inplace = True)\n",
      "511/33:\n",
      "print(\"After outlier removal\")\n",
      "fig, axes = plt.subplots(1, 2,figsize=(15,5))\n",
      "sns.boxplot(x=df[\"bmi\"], data=df, ax = axes[0])\n",
      "sns.distplot(df[\"bmi\"], fit=norm, ax = axes[1])\n",
      "511/34:\n",
      "# There is only one \"other\" category in the gender. So, we should remove it.\n",
      "df.drop(df[df[\"gender\"]==\"Other\"].index, inplace=True)\n",
      "511/35:\n",
      "# For gender column.\n",
      "sex = pd.get_dummies(df[\"gender\"], drop_first=True)\n",
      "df = pd.concat([df,sex],axis=1)\n",
      "511/36:\n",
      "# For Ever_married colummn.\n",
      "married_status = pd.get_dummies(df[\"ever_married\"], drop_first=True)\n",
      "df = pd.concat([df, married_status], axis=1)\n",
      "511/37:\n",
      "# For Residence type column.\n",
      "residence = pd.get_dummies(df[\"Residence_type\"], drop_first=True)\n",
      "df = pd.concat([df, residence], axis=1)\n",
      "511/38:\n",
      "df[\"Work_Type\"] = df[\"work_type\"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})\n",
      "df[\"Smoking_Status\"] = df[\"smoking_status\"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})\n",
      "511/39: df.head()\n",
      "511/40:\n",
      "df.drop([\"id\",\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"],axis=1, inplace=True)\n",
      "df.head()\n",
      "511/41:\n",
      "df.rename(columns={\"Male\":\"Gender\",\"Yes\":\"Ever_Married\",\"Urban\":\"Residence_type\"}, inplace=True)\n",
      "df.head()\n",
      "511/42:\n",
      "from imblearn.over_sampling import SMOTE\n",
      "\n",
      "smote = SMOTE(random_state = 42)\n",
      "X = df.drop(['stroke'],axis=1)\n",
      "y = df['stroke']\n",
      "X,y= smote.fit_resample(X,y)\n",
      "y = pd.DataFrame({'stroke':y})\n",
      "sns.countplot(data = y, x = 'stroke', y= None)\n",
      "plt.show()\n",
      "print(y.value_counts())\n",
      "511/43:\n",
      "df = pd.concat([X,y],axis = 1)\n",
      "df.head()\n",
      "511/44:\n",
      "X = df.drop([\"stroke\"], axis=1)\n",
      "y = df[\"stroke\"]\n",
      "511/45:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "ss = StandardScaler()\n",
      "X = ss.fit_transform(X)\n",
      "511/46: x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
      "511/47:\n",
      "lr = LogisticRegression(solver=\"liblinear\").fit(x_train,y_train)\n",
      "gnb = GaussianNB().fit(x_train,y_train)\n",
      "knnc = KNeighborsClassifier().fit(x_train,y_train)\n",
      "dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)\n",
      "rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)\n",
      "xgbc = XGBClassifier().fit(x_train,y_train)\n",
      "catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)\n",
      "511/48: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,catbc]\n",
      "511/49:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    error = -cross_val_score(model,x_test,y_test,cv=10,scoring=\"neg_mean_squared_error\",verbose=False).mean()\n",
      "    print(name + \": \")\n",
      "    print(\"-\" * 50)\n",
      "    print(\"Accuracy Score: \",accuracy_score(y_test,predict))\n",
      "    print(\"Cross Validation Score: \",CV)\n",
      "    print(\"Error: \",np.sqrt(error))\n",
      "    print(\"R-square value: \",r2_score(y_test,predict))\n",
      "    print(\"Confusion matrix: \")\n",
      "    print(confusion_matrix(y_test,predict))\n",
      "    print(\"-\" * 100)\n",
      "511/50:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"Accuracy\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict(x_test)\n",
      "    accuracy = accuracy_score(y_test,predict)\n",
      "    result = pd.DataFrame([[name,accuracy*100]],columns=[\"MODELS\",\"Accuracy\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"Accuracy\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"ACCURACY\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
      "plt.show()\n",
      "511/51:\n",
      "df = pd.DataFrame(columns=[\"MODELS\",\"CV\"])\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()\n",
      "    result = pd.DataFrame([[name,CV*100]],columns=[\"MODELS\",\"CV\"])\n",
      "    df = df.append(result)\n",
      "    \n",
      "figure = plt.figure(figsize=(20,8))   \n",
      "sns.barplot(x=\"CV\",y=\"MODELS\",data=df,color=\"k\")\n",
      "plt.xlabel(\"CV\")\n",
      "plt.ylabel(\"MODELS\")\n",
      "plt.xlim(0,100)\n",
      "plt.title(\"MODEL CROSS VALIDATION COMPARISON\")\n",
      "plt.show()\n",
      "511/52:\n",
      "r_prob = [0 for _ in range(len(y_test))]\n",
      "r_auc = roc_auc_score(y_test,r_prob)\n",
      "511/53:\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    auroc_score = roc_auc_score(y_test,predict)\n",
      "    print(name+\" score: \",auroc_score)\n",
      "    print(\"-\"*50)\n",
      "511/54:\n",
      "r_fpr,r_tpr,_= roc_curve(y_test,r_prob)\n",
      "model_dict={}\n",
      "\n",
      "for model in model_names:\n",
      "    name = model.__class__.__name__\n",
      "    predict = model.predict_proba(x_test)[:,1]\n",
      "    fpr,tpr,_= roc_curve(y_test,predict)\n",
      "    model_dict[name]=[fpr,tpr]\n",
      "511/55:\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(r_fpr,r_tpr,linestyle=\"--\")\n",
      "plt.plot(model_dict[\"LogisticRegression\"][0],model_dict[\"LogisticRegression\"][1],linestyle='dotted', label=\"LogisticRegression\")\n",
      "plt.plot(model_dict[\"GaussianNB\"][0],model_dict[\"GaussianNB\"][1],linestyle='dotted',label=\"GaussianNB\")\n",
      "plt.plot(model_dict[\"KNeighborsClassifier\"][0],model_dict[\"KNeighborsClassifier\"][1],linestyle='dotted', label=\"KNeighborsClassifier\")\n",
      "plt.plot(model_dict[\"DecisionTreeClassifier\"][0],model_dict[\"DecisionTreeClassifier\"][1],linestyle='dotted', label=\"DecisionTreeClassifier\")\n",
      "plt.plot(model_dict[\"RandomForestClassifier\"][0],model_dict[\"RandomForestClassifier\"][1],linestyle='dotted', label=\"RandomForestClassifier\")\n",
      "plt.plot(model_dict[\"XGBClassifier\"][0],model_dict[\"XGBClassifier\"][1],linestyle='dotted', label=\"XGBClassifier\")\n",
      "plt.plot(model_dict[\"CatBoostClassifier\"][0],model_dict[\"CatBoostClassifier\"][1],linestyle='dotted', label=\"CatBoostClassifier\")\n",
      "\n",
      "plt.title(\"ROC plot\")\n",
      "plt.xlabel(\"False positive rate.\")\n",
      "plt.ylabel(\"True positive rate.\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "512/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "512/2: dataset = pd.read_csv(\"data.csv\")\n",
      "512/3: dataset.head()\n",
      "512/4: dataset.columns()\n",
      "512/5: dataset.columns\n",
      "512/6:\n",
      "dataset.drop(labels=['PassengerId','WikiId', 'Name_wiki',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class'], axis=1, inplce=True)\n",
      "dataset.head()\n",
      "512/7:\n",
      "dataset.drop(labels=['PassengerId','WikiId', 'Name_wiki',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "513/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "513/2: dataset = pd.read_csv(\"data.csv\")\n",
      "513/3: dataset.head()\n",
      "513/4: dataset.columns\n",
      "513/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin'\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "513/6:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "513/7: Now, before taking care of the Missing data fist we have to look for the missing data i.e. we will check wahat percentage of data we are missing and based on that we will make our dicision.\n",
      "513/8: dataset.isnull().sum()\n",
      "513/9: dataset.isna().sum()\n",
      "513/10: len(dataset[\"Survived\"])\n",
      "513/11: len(dataset[\"Survived\"])/dataset[\"survived\"].isnull().sum()\n",
      "513/12: len(dataset[\"Survived\"])/dataset[\"Survived\"].isnull().sum()\n",
      "513/13: len(dataset[\"Survived\"])/dataset[\"Survived\"].isnull().sum()*100\n",
      "513/14: len(dataset[\"Survived\"])/dataset[\"Survived\"].isnull().sum()\n",
      "513/15: (len(dataset[\"Survived\"])/dataset[\"Survived\"].isnull().sum())*100\n",
      "513/16: len(dataset[\"Survived\"])/dataset[\"Survived\"].isnull().sum()\n",
      "513/17: dataset.shape\n",
      "513/18: len(dataset[\"Age\"])\n",
      "513/19: dataset.describe\n",
      "513/20: dataset.descrption\n",
      "513/21: dataset.info()\n",
      "513/22:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}: {}\".formate(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "513/23:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}: {}\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "513/24:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}: {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "513/25:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:\\t{}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "513/26:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}: \\t{}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "513/27:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:\\t\\t{}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "513/28:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "514/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "514/2: dataset = pd.read_csv(\"data.csv\")\n",
      "514/3: dataset.head()\n",
      "514/4: dataset.columns\n",
      "514/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "514/6: dataset.isnull().sum()\n",
      "514/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "515/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "515/2: dataset = pd.read_csv(\"data.csv\")\n",
      "515/3: dataset.head()\n",
      "515/4: dataset.columns\n",
      "515/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "515/6: dataset.isnull().sum()\n",
      "515/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "515/8:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "dataset[\"Age\"].plot(kind='kde', ax=ax)\n",
      "515/9: import seaborn as sns\n",
      "515/10: sns.distplot(dataset[\"Age\"], fit=norm)\n",
      "515/11:\n",
      "import seaborn as sns\n",
      "from scipy.stats import norm\n",
      "515/12: sns.distplot(dataset[\"Age\"], fit=norm)\n",
      "515/13: sns.distplot(dataset[\"Age\"], fit=norm, color=\"Red\")\n",
      "515/14: temp_dataset1 = dataset\n",
      "515/15:\n",
      "temp_dataset1 = dataset\n",
      "tem_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean())\n",
      "515/16:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean())\n",
      "temp_dataset1.head()\n",
      "515/17:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean())\n",
      "temp_dataset1[\"Age\"].isnull().sum()\n",
      "515/18:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean())\n",
      "temp_dataset1[\"Age\"].isnull().sum()\n",
      "515/19:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(),inplace=True)\n",
      "temp_dataset1[\"Age\"].isnull().sum()\n",
      "515/20: dataset[\"Age\"].isnull().sum()\n",
      "516/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "516/2: dataset = pd.read_csv(\"data.csv\")\n",
      "516/3: dataset.head()\n",
      "516/4: dataset.columns\n",
      "516/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "516/6: dataset.isnull().sum()\n",
      "516/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "516/8:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(),inplace=True)\n",
      "temp_dataset1[\"Age\"].isnull().sum()\n",
      "516/9: dataset[\"Age\"].isnull().sum()\n",
      "516/10: dataset\n",
      "516/11:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1\n",
      "517/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "517/2: dataset = pd.read_csv(\"data.csv\")\n",
      "517/3: dataset.head()\n",
      "517/4: dataset.columns\n",
      "517/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "517/6: dataset.isnull().sum()\n",
      "517/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "517/8:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1\n",
      "517/9: dataset[\"Age\"].isnull().sum()\n",
      "517/10: dataset\n",
      "517/11:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean())\n",
      "temp_dataset1\n",
      "517/12:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean())\n",
      "temp_dataset1\n",
      "517/13: dataset[\"Age\"].isnull().sum()\n",
      "517/14:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mea)\n",
      "temp_dataset1\n",
      "517/15:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean)\n",
      "temp_dataset1\n",
      "517/16:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].mean())\n",
      "517/17:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].mean()\n",
      "517/18:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean())\n",
      "517/19:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean())\n",
      "temp_dataset1\n",
      "517/20:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(), inplace=True)\n",
      "temp_dataset1\n",
      "517/21: dataset[\"Age\"].isnull().sum()\n",
      "517/22: dataset\n",
      "517/23:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].mean()\n",
      "517/24: dataset\n",
      "517/25: dataset.isnull().sum()\n",
      "518/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "518/2: dataset = pd.read_csv(\"data.csv\")\n",
      "518/3: dataset.head()\n",
      "518/4: dataset.columns\n",
      "518/5:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "518/6: dataset.isnull().sum()\n",
      "518/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "518/8:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].mean()\n",
      "518/9:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(), inplace=True)\n",
      "temp_dataset1\n",
      "518/10: dataset[\"Age\"].isnull().sum()\n",
      "518/11: dataset\n",
      "518/12: dataset.isnull().sum()\n",
      "520/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "520/2: dataset = pd.read_csv(\"data.csv\")\n",
      "520/3: dataset.head()\n",
      "520/4: dataset.columns\n",
      "520/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "520/6: dataset.isnull().sum()\n",
      "520/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "520/8:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].mean()\n",
      "520/9:\n",
      "temp_dataset1 = dataset\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(), inplace=True)\n",
      "temp_dataset1\n",
      "520/10: dataset[\"Age\"].isnull().sum()\n",
      "520/11: dataset\n",
      "520/12: dataset.isnull().sum()\n",
      "521/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "521/2: dataset = pd.read_csv(\"data.csv\")\n",
      "521/3: dataset.head()\n",
      "521/4: dataset.columns\n",
      "521/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "521/6: dataset.isnull().sum()\n",
      "521/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "521/8:\n",
      "temp_dataset1 = dataset.copy()\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(), inplace=True)\n",
      "temp_dataset1\n",
      "521/9: dataset[\"Age\"].isnull().sum()\n",
      "521/10: dataset\n",
      "521/11: dataset.isnull().sum()\n",
      "521/12:\n",
      "temp_dataset1 = dataset.copy()\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(), inplace=True)\n",
      "temp_dataset1[\"Age\"].isnull().sum()\n",
      "521/13:\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "temp_dataset2 = dataset.copy()\n",
      "\n",
      "imputer = SimpleImputer(missing_values=np.nan, strategy=mean)\n",
      "imputer.fit(temp_dataset2[\"Age\"])\n",
      "temp_dataset2[\"Age\"] = imputer.transform(temp_dataset2[\"Age\"])\n",
      "521/14:\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "temp_dataset2 = dataset.copy()\n",
      "\n",
      "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "imputer.fit(temp_dataset2[\"Age\"])\n",
      "temp_dataset2[\"Age\"] = imputer.transform(temp_dataset2[\"Age\"])\n",
      "521/15:\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "temp_dataset2 = dataset.copy()\n",
      "\n",
      "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "imputer.fit(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"] = imputer.transform(temp_dataset2[[\"Age\"]])\n",
      "521/16:\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "temp_dataset2 = dataset.copy()\n",
      "\n",
      "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "imputer.fit(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"] = imputer.transform(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2\n",
      "521/17:\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "temp_dataset2 = dataset.copy()\n",
      "\n",
      "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "imputer.fit(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"] = imputer.transform(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"].isnull().sum()\n",
      "521/18: dataset[\"Age\"].mean()\n",
      "521/19: int(dataset[\"Age\"].mean())\n",
      "521/20: dataset[\"Age\"].fillna(int(dataset[\"Age\"].mean()),inplace=True)\n",
      "521/21:\n",
      "dataset[\"Age\"].fillna(int(dataset[\"Age\"].mean()),inplace=True)\n",
      "dataset\n",
      "521/22: dataset.dropna(subset=[\"Survived\",\"Embarked\"])\n",
      "521/23: dataset.dropna(subset=[\"Survived\",\"Embarked\"], inplace=True)\n",
      "521/24:\n",
      "dataset.dropna(subset=[\"Survived\",\"Embarked\"], inplace=True)\n",
      "dataset.isnull().sum()\n",
      "521/25: dataset\n",
      "521/26: temp_dataset3 = dataset.copy()\n",
      "521/27:\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "521/28:\n",
      "\n",
      "##### Using pandas get_dummies ##############\n",
      "\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column = pd.get_dummies(temp_dataset3[\"Embarked\"])\n",
      "temp_column\n",
      "521/29:\n",
      "\n",
      "##### Using pandas get_dummies ##############\n",
      "\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column\n",
      "521/30:\n",
      "\n",
      "##### Using pandas get_dummies ##############\n",
      "\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column = pd.get_dummies(temp_dataset3[\"Embarked\"])\n",
      "temp_column\n",
      "521/31:\n",
      "\n",
      "##### Using pandas get_dummies ##############\n",
      "\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column = pd.get_dummies(temp_dataset3[\"Embarked\"])\n",
      "temp_column\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column],axis=1)\n",
      "temp_dataset3.head()\n",
      "521/32:\n",
      "\n",
      "##### Using pandas get_dummies ##############\n",
      "\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column = pd.get_dummies(temp_dataset3[\"Embarked\"])\n",
      "temp_column\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column],axis=1)\n",
      "temp_dataset3\n",
      "521/33:\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "le = LabelEncoder()\n",
      "temp_dataset4[\"Embarked\"] = le.fit_transform([temp_dataset4[\"Embarked\"]]) \n",
      "temp_dataset4\n",
      "521/34:\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "le = LabelEncoder()\n",
      "temp_dataset4[\"Embarked\"] = le.fit_transform(temp_dataset4[\"Embarked\"]) \n",
      "temp_dataset4\n",
      "521/35:\n",
      "\n",
      "##### Using pandas get_dummies ##############\n",
      "\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column],axis=1)\n",
      "temp_dataset3\n",
      "521/36:\n",
      "\n",
      "##### Using pandas get_dummies ##############\n",
      "\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,[temp_column1, temp_column2],axis=1)\n",
      "temp_dataset3\n",
      "521/37:\n",
      "\n",
      "##### Using pandas get_dummies ##############\n",
      "\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,[temp_column1, temp_column2]],axis=1)\n",
      "temp_dataset3\n",
      "521/38:\n",
      "\n",
      "##### Using pandas get_dummies ##############\n",
      "\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)\n",
      "temp_dataset3\n",
      "521/39: dataset\n",
      "521/40:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm1 = ohe.fit_transform(temp_dataset4[\"Embarked\"])\n",
      "\n",
      "temp_dataset4[\"Embarked\"] = pd.concat([temp_dataset4, temp_clm1],axis=1)\n",
      "temp_dataset4\n",
      "521/41:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm1 = ohe.fit_transform([temp_dataset4[\"Embarked\"]])\n",
      "\n",
      "temp_dataset4[\"Embarked\"] = pd.concat([temp_dataset4, temp_clm1],axis=1)\n",
      "temp_dataset4\n",
      "521/42:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm1 = ohe.fit_transform([temp_dataset4[\"Embarked\"]].reshape(-1,1))\n",
      "\n",
      "temp_dataset4[\"Embarked\"] = pd.concat([temp_dataset4, temp_clm1],axis=1)\n",
      "temp_dataset4\n",
      "521/43:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm1 = ohe.fit_transform(np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1))\n",
      "\n",
      "temp_dataset4[\"Embarked\"] = pd.concat([temp_dataset4, temp_clm1],axis=1)\n",
      "temp_dataset4\n",
      "521/44:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "522/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "522/2: dataset = pd.read_csv(\"data.csv\")\n",
      "522/3: dataset.head()\n",
      "522/4: dataset.columns\n",
      "522/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "522/6: dataset.isnull().sum()\n",
      "522/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "522/8:\n",
      "temp_dataset1 = dataset.copy()\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(), inplace=True)\n",
      "temp_dataset1[\"Age\"].isnull().sum()\n",
      "522/9:\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "temp_dataset2 = dataset.copy()\n",
      "\n",
      "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "imputer.fit(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"] = imputer.transform(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"].isnull().sum()\n",
      "522/10: dataset[\"Age\"].fillna(int(dataset[\"Age\"].mean()),inplace=True)\n",
      "522/11:\n",
      "dataset.dropna(subset=[\"Survived\",\"Embarked\"], inplace=True)\n",
      "dataset.isnull().sum()\n",
      "522/12:\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)\n",
      "temp_dataset3\n",
      "522/13:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "522/14:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_dataset4 = np.array(temp_dataset4).reshape(-1,1)\n",
      "\n",
      "temp_dataset4\n",
      "522/15:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "\n",
      "temp_dataset4\n",
      "522/16:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "\n",
      "print(temp_dataset4)\n",
      "\n",
      "temp_dataset4 = ohe.fit_transform(temp_dataset4)\n",
      "temp_dataset4\n",
      "522/17: temp_dataset4 = dataset.copy()\n",
      "522/18:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"])\n",
      "temp_clm\n",
      "522/19:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"])\n",
      "temp_clm\n",
      "522/20:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"])\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm\n",
      "522/21:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"])\n",
      "#temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm\n",
      "522/22:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "#temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm\n",
      "522/23:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"])\n",
      "#temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm\n",
      "522/24:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "#temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm\n",
      "522/25:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(1,-1)\n",
      "#temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm\n",
      "522/26:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(1,-1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm\n",
      "522/27:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm\n",
      "522/28:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm, columns=[\"Q\",\"S\"])\n",
      "522/29:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm, columns=[\"Q\",\"S\"])\n",
      "\n",
      "temp_clm\n",
      "522/30:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm, columns=[\"Q\",\"S\"])\n",
      "\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4\n",
      "522/31:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm, columns=[\"Q\",\"S\"])\n",
      "\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4\n",
      "522/32:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm, columns=[\"Q\",\"S\"])\n",
      "print(temp_clm.shape)\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4\n",
      "522/33:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm)\n",
      "print(temp_clm.shape)\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4\n",
      "522/34:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "#temp_clm = pd.DataFrame(data=temp_clm)\n",
      "print(temp_clm.shape)\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4\n",
      "522/35:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "#temp_clm = pd.DataFrame(data=temp_clm)\n",
      "print(temp_clm.shape)\n",
      "#temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "#temp_dataset4\n",
      "522/36:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "#temp_clm = pd.DataFrame(data=temp_clm)\n",
      "print(temp_clm)\n",
      "print(temp_clm.shape)\n",
      "#temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "#temp_dataset4\n",
      "522/37:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm)\n",
      "print(temp_clm)\n",
      "print(temp_clm.shape)\n",
      "#temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "#temp_dataset4\n",
      "522/38:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm)\n",
      "print(temp_clm)\n",
      "print(temp_clm.shape)\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4\n",
      "522/39:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm)\n",
      "print(temp_clm)\n",
      "#print(temp_clm.shape)\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4\n",
      "522/40:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "#temp_clm = pd.DataFrame(data=temp_clm)\n",
      "print(temp_clm)\n",
      "#print(temp_clm.shape)\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4\n",
      "522/41:\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "print(type(temp_column1))\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)\n",
      "temp_dataset3\n",
      "522/42:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm)\n",
      "print(type(temp_clm))\n",
      "#print(temp_clm.shape)\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4\n",
      "522/43:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm)\n",
      "print(temp_clm)\n",
      "#print(temp_clm.shape)\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4\n",
      "522/44:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm, columns=[\"Q\",\"S\"])\n",
      "print(temp_clm)\n",
      "#print(temp_clm.shape)\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4\n",
      "522/45:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm, columns=[\"Q\",\"S\"])\n",
      "print(temp_clm)\n",
      "#print(temp_clm.shape)\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4.isnull().sum()\n",
      "522/46:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm, columns=[\"Q\",\"S\"])\n",
      "print(temp_clm)\n",
      "#print(temp_clm.shape)\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4.isnull()\n",
      "522/47:\n",
      "pd.set_option('display.height', 1000)\n",
      "pd.set_option('display.max_rows', 1000)\n",
      "pd.set_option('display.max_columns', 500)\n",
      "pd.set_option('display.width', 1000)\n",
      "522/48:\n",
      "\n",
      "pd.set_option('display.max_rows', 1000)\n",
      "pd.set_option('display.max_columns', 500)\n",
      "pd.set_option('display.width', 1000)\n",
      "522/49:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm, columns=[\"Q\",\"S\"])\n",
      "print(temp_clm)\n",
      "#print(temp_clm.shape)\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4.isnull()\n",
      "522/50:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm, columns=[\"Q\",\"S\"])\n",
      "print(temp_clm)\n",
      "#print(temp_clm.shape)\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4.isnull().head(100)\n",
      "522/51:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm, columns=[\"Q\",\"S\"])\n",
      "print(temp_clm)\n",
      "#print(temp_clm.shape)\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4.head(100)\n",
      "522/52: dataset.head(100)\n",
      "522/53:\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "print(type(temp_column1))\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)\n",
      "temp_dataset3.head(100)\n",
      "522/54:\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "print(type(temp_column1))\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)\n",
      "temp_dataset3\n",
      "522/55:\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)\n",
      "temp_dataset3\n",
      "523/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "523/2: dataset = pd.read_csv(\"data.csv\")\n",
      "523/3: dataset.head()\n",
      "523/4: dataset.columns\n",
      "523/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "523/6: dataset.isnull().sum()\n",
      "523/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "523/8:\n",
      "temp_dataset1 = dataset.copy()\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(), inplace=True)\n",
      "temp_dataset1[\"Age\"].isnull().sum()\n",
      "523/9:\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "temp_dataset2 = dataset.copy()\n",
      "\n",
      "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "imputer.fit(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"] = imputer.transform(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"].isnull().sum()\n",
      "523/10: dataset[\"Age\"].fillna(int(dataset[\"Age\"].mean()),inplace=True)\n",
      "523/11:\n",
      "dataset.dropna(subset=[\"Survived\",\"Embarked\"], inplace=True)\n",
      "dataset.isnull().sum()\n",
      "523/12:\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)\n",
      "temp_dataset3\n",
      "523/13:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "\n",
      "\n",
      "ohe = OneHotEncoder(drop=\"first\", sparse=False)\n",
      "temp_clm = np.array(temp_dataset4[\"Embarked\"]).reshape(-1,1)\n",
      "temp_clm = ohe.fit_transform(temp_clm)\n",
      "temp_clm = pd.DataFrame(data=temp_clm, columns=[\"Q\",\"S\"])\n",
      "print(temp_clm)\n",
      "#print(temp_clm.shape)\n",
      "temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)\n",
      "temp_dataset4.head(100)\n",
      "523/14:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "523/15: temp_dataset4\n",
      "523/16: temp_dataset4 = np.array(temp_dataset4)\n",
      "523/17:\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "temp_dataset4\n",
      "523/18:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/19:\n",
      "\n",
      "\n",
      "pd.set_option('display.max_columns', 500)\n",
      "523/20:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/21:\n",
      "\n",
      "\n",
      "pd.set_option('display.max_columns', 500)\n",
      "pd.set_option('display.width', 1000)\n",
      "523/22:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/23: np.set_printoptions(threshold=sys.maxsize)\n",
      "523/24:\n",
      "import sys\n",
      "np.set_printoptions(threshold=sys.maxsize)\n",
      "523/25:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/26:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])])\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/27:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])])\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/28:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/29:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [\"Sex\",\"Embarked\"])], remainder='passthrough')\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/30:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2])], remainder='passthrough')\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/31:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/32:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,:6])], remainder='passthrough')\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/33:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/34:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), slice(2,6))], remainder='passthrough')\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/35:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2]),('cat', OneHotEncoder(), [6])], remainder='passthrough')\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/36:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2]),('encode', OneHotEncoder(), [6])], remainder='passthrough')\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/37:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "temp_dataset4\n",
      "523/38:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2])], remainder='passthrough')\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [6])], remainder='passthrough')\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "\n",
      "temp_dataset4\n",
      "523/39:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "\n",
      "temp_dataset4\n",
      "524/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "524/2: dataset = pd.read_csv(\"data.csv\")\n",
      "524/3: dataset.head()\n",
      "524/4: dataset.columns\n",
      "524/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "524/6: dataset.isnull().sum()\n",
      "524/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "524/8:\n",
      "temp_dataset1 = dataset.copy()\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(), inplace=True)\n",
      "temp_dataset1[\"Age\"].isnull().sum()\n",
      "524/9:\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "temp_dataset2 = dataset.copy()\n",
      "\n",
      "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "imputer.fit(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"] = imputer.transform(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"].isnull().sum()\n",
      "524/10: dataset[\"Age\"].fillna(int(dataset[\"Age\"].mean()),inplace=True)\n",
      "524/11:\n",
      "dataset.dropna(subset=[\"Survived\",\"Embarked\"], inplace=True)\n",
      "dataset.isnull().sum()\n",
      "524/12:\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)\n",
      "temp_dataset3\n",
      "524/13:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "\n",
      "temp_dataset4\n",
      "524/14:\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "temp_dataset4\n",
      "524/15:\n",
      "\n",
      "\n",
      "pd.set_option('display.max_columns', 500)\n",
      "pd.set_option('display.width', 1000)\n",
      "524/16:\n",
      "import sys\n",
      "np.set_printoptions(threshold=sys.maxsize)\n",
      "524/17:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "\n",
      "temp_dataset4\n",
      "524/18:\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "temp_dataset5 = dataset.copy()\n",
      "\n",
      "le = LabelEncoder()\n",
      "temp_dataset5[\"Sex\"] = le.fit_transform(temp_dataset5[\"Sex\"])\n",
      "\n",
      "temp_dataset5\n",
      "524/19:\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "temp_dataset5 = dataset.copy()\n",
      "\n",
      "le = LabelEncoder()\n",
      "temp_dataset5[\"Sex\"] = le.fit_transform(temp_dataset5[\"Sex\",\"Embarked\"])\n",
      "\n",
      "temp_dataset5\n",
      "524/20:\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "temp_dataset5 = dataset.copy()\n",
      "\n",
      "le = LabelEncoder()\n",
      "temp_dataset5[\"Sex\"] = le.fit_transform(temp_dataset5[[\"Sex\",\"Embarked\"]])\n",
      "\n",
      "temp_dataset5\n",
      "524/21:\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "temp_dataset5 = dataset.copy()\n",
      "\n",
      "le = LabelEncoder()\n",
      "temp_dataset5[\"Sex\"] = le.fit_transform(temp_dataset5[\"Sex\"])\n",
      "temp_dataset5[\"Embarked\"] = le.fit_transform(temp_dataset5[\"Embarked\"])\n",
      "\n",
      "temp_dataset5\n",
      "524/22:\n",
      "from sklearn.preprocessing inport OrdinalEncoder\n",
      "\n",
      "temp_dataset6 = dataset.copy()\n",
      "\n",
      "oe = OrdinalEncoder()\n",
      "\n",
      "Sex = oe.fit_transform(np.array(temp_dataset6[\"Sex\"]).reshape(-1,1))\n",
      "Embarked = oe.fit_transform(np.array(temp_dataset6[\"Embarked\"]).reshape(-1,1))\n",
      "\n",
      "temp_dataset6 = pd.concat([temp_dataset6,Sex,Embarked], axis=1)\n",
      "temp_dataset6\n",
      "524/23:\n",
      "from sklearn.preprocessing import OrdinalEncoder\n",
      "\n",
      "temp_dataset6 = dataset.copy()\n",
      "\n",
      "oe = OrdinalEncoder()\n",
      "\n",
      "Sex = oe.fit_transform(np.array(temp_dataset6[\"Sex\"]).reshape(-1,1))\n",
      "Embarked = oe.fit_transform(np.array(temp_dataset6[\"Embarked\"]).reshape(-1,1))\n",
      "\n",
      "temp_dataset6 = pd.concat([temp_dataset6,Sex,Embarked], axis=1)\n",
      "temp_dataset6\n",
      "524/24:\n",
      "N_Embarked = pd.get_dummies(datasetset[\"Embarked\"], drop_first=True)\n",
      "N_Sex = pd.get_dummies(dataset[\"Sex\"], drop_first=True)\n",
      "\n",
      "dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)\n",
      "temp_dataset3\n",
      "524/25:\n",
      "N_Embarked = pd.get_dummies(dataset[\"Embarked\"], drop_first=True)\n",
      "N_Sex = pd.get_dummies(dataset[\"Sex\"], drop_first=True)\n",
      "\n",
      "dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)\n",
      "temp_dataset3\n",
      "524/26:\n",
      "N_Embarked = pd.get_dummies(dataset[\"Embarked\"], drop_first=True)\n",
      "N_Sex = pd.get_dummies(dataset[\"Sex\"], drop_first=True)\n",
      "\n",
      "dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)\n",
      "dataset\n",
      "524/27:\n",
      "N_Embarked = pd.get_dummies(dataset[\"Embarked\"], drop_first=True)\n",
      "N_Sex = pd.get_dummies(dataset[\"Sex\"], drop_first=True)\n",
      "\n",
      "dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)\n",
      "print(dataset.head())\n",
      "\n",
      "# We, can remove the Sex and Embarked Columns as we have encoded them.\n",
      "524/28:\n",
      "N_Embarked = pd.get_dummies(dataset[\"Embarked\"], drop_first=True)\n",
      "N_Sex = pd.get_dummies(dataset[\"Sex\"], drop_first=True)\n",
      "\n",
      "dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)\n",
      "print(dataset.head())\n",
      "\n",
      "# We, can remove the Sex and Embarked Columns as we have encoded them.\n",
      "525/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "525/2: dataset = pd.read_csv(\"data.csv\")\n",
      "525/3: dataset.head()\n",
      "525/4: dataset.columns\n",
      "525/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "525/6: dataset.isnull().sum()\n",
      "525/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "525/8:\n",
      "temp_dataset1 = dataset.copy()\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(), inplace=True)\n",
      "temp_dataset1[\"Age\"].isnull().sum()\n",
      "525/9:\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "temp_dataset2 = dataset.copy()\n",
      "\n",
      "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "imputer.fit(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"] = imputer.transform(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"].isnull().sum()\n",
      "525/10: dataset[\"Age\"].fillna(int(dataset[\"Age\"].mean()),inplace=True)\n",
      "525/11:\n",
      "dataset.dropna(subset=[\"Survived\",\"Embarked\"], inplace=True)\n",
      "dataset.isnull().sum()\n",
      "525/12:\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)\n",
      "temp_dataset3\n",
      "525/13:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "\n",
      "temp_dataset4\n",
      "525/14:\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "temp_dataset5 = dataset.copy()\n",
      "\n",
      "le = LabelEncoder()\n",
      "temp_dataset5[\"Sex\"] = le.fit_transform(temp_dataset5[\"Sex\"])\n",
      "temp_dataset5[\"Embarked\"] = le.fit_transform(temp_dataset5[\"Embarked\"])\n",
      "\n",
      "temp_dataset5\n",
      "525/15:\n",
      "N_Embarked = pd.get_dummies(dataset[\"Embarked\"], drop_first=True)\n",
      "N_Sex = pd.get_dummies(dataset[\"Sex\"], drop_first=True)\n",
      "\n",
      "dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)\n",
      "print(dataset.head())\n",
      "\n",
      "# We, can remove the Sex and Embarked Columns as we have encoded them.\n",
      "525/16:\n",
      "\n",
      "\n",
      "pd.set_option('display.max_columns', 500)\n",
      "pd.set_option('display.width', 1000)\n",
      "525/17:\n",
      "import sys\n",
      "np.set_printoptions(threshold=sys.maxsize)\n",
      "525/18:\n",
      "N_Embarked = pd.get_dummies(dataset[\"Embarked\"], drop_first=True)\n",
      "N_Sex = pd.get_dummies(dataset[\"Sex\"], drop_first=True)\n",
      "\n",
      "dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)\n",
      "print(dataset.head())\n",
      "\n",
      "# We, can remove the Sex and Embarked Columns as we have encoded them.\n",
      "\n",
      "dataset.drop(columns=[\"Sex\",\"Embarked\"], axis=1)\n",
      "print(dataset.head())\n",
      "526/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "526/2: dataset = pd.read_csv(\"data.csv\")\n",
      "526/3: dataset.head()\n",
      "526/4: dataset.columns\n",
      "526/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "526/6: dataset.isnull().sum()\n",
      "526/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "526/8:\n",
      "temp_dataset1 = dataset.copy()\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(), inplace=True)\n",
      "temp_dataset1[\"Age\"].isnull().sum()\n",
      "526/9:\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "temp_dataset2 = dataset.copy()\n",
      "\n",
      "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "imputer.fit(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"] = imputer.transform(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"].isnull().sum()\n",
      "526/10: dataset[\"Age\"].fillna(int(dataset[\"Age\"].mean()),inplace=True)\n",
      "526/11:\n",
      "dataset.dropna(subset=[\"Survived\",\"Embarked\"], inplace=True)\n",
      "dataset.isnull().sum()\n",
      "526/12:\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)\n",
      "temp_dataset3\n",
      "526/13:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "\n",
      "temp_dataset4\n",
      "526/14:\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "temp_dataset5 = dataset.copy()\n",
      "\n",
      "le = LabelEncoder()\n",
      "temp_dataset5[\"Sex\"] = le.fit_transform(temp_dataset5[\"Sex\"])\n",
      "temp_dataset5[\"Embarked\"] = le.fit_transform(temp_dataset5[\"Embarked\"])\n",
      "\n",
      "temp_dataset5\n",
      "526/15:\n",
      "N_Embarked = pd.get_dummies(dataset[\"Embarked\"], drop_first=True)\n",
      "N_Sex = pd.get_dummies(dataset[\"Sex\"], drop_first=True)\n",
      "\n",
      "dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)\n",
      "print(dataset.head())\n",
      "\n",
      "# We, can remove the Sex and Embarked Columns as we have encoded them.\n",
      "\n",
      "dataset.drop(columns=[\"Sex\",\"Embarked\"], axis=1)\n",
      "print(dataset.head())\n",
      "526/16:\n",
      "\n",
      "\n",
      "pd.set_option('display.max_columns', 500)\n",
      "pd.set_option('display.width', 1000)\n",
      "526/17:\n",
      "import sys\n",
      "np.set_printoptions(threshold=sys.maxsize)\n",
      "527/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "527/2: dataset = pd.read_csv(\"data.csv\")\n",
      "527/3: dataset.head()\n",
      "527/4: dataset.columns\n",
      "527/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "527/6: dataset.isnull().sum()\n",
      "527/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "527/8:\n",
      "temp_dataset1 = dataset.copy()\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(), inplace=True)\n",
      "temp_dataset1[\"Age\"].isnull().sum()\n",
      "527/9:\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "temp_dataset2 = dataset.copy()\n",
      "\n",
      "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "imputer.fit(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"] = imputer.transform(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"].isnull().sum()\n",
      "527/10: dataset[\"Age\"].fillna(int(dataset[\"Age\"].mean()),inplace=True)\n",
      "527/11:\n",
      "dataset.dropna(subset=[\"Survived\",\"Embarked\"], inplace=True)\n",
      "dataset.isnull().sum()\n",
      "527/12:\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)\n",
      "temp_dataset3\n",
      "527/13:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "\n",
      "temp_dataset4\n",
      "527/14:\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "temp_dataset5 = dataset.copy()\n",
      "\n",
      "le = LabelEncoder()\n",
      "temp_dataset5[\"Sex\"] = le.fit_transform(temp_dataset5[\"Sex\"])\n",
      "temp_dataset5[\"Embarked\"] = le.fit_transform(temp_dataset5[\"Embarked\"])\n",
      "\n",
      "temp_dataset5\n",
      "527/15:\n",
      "N_Embarked = pd.get_dummies(dataset[\"Embarked\"], drop_first=True)\n",
      "N_Sex = pd.get_dummies(dataset[\"Sex\"], drop_first=True)\n",
      "\n",
      "dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)\n",
      "print(dataset.head())\n",
      "\n",
      "# We, can remove the Sex and Embarked Columns as we have encoded them.\n",
      "\n",
      "dataset.drop(columns=[\"Sex\",\"Embarked\"], axis=1, inplace=True)\n",
      "print(dataset.head())\n",
      "527/16:\n",
      "\n",
      "\n",
      "pd.set_option('display.max_columns', 500)\n",
      "pd.set_option('display.width', 1000)\n",
      "527/17:\n",
      "import sys\n",
      "np.set_printoptions(threshold=sys.maxsize)\n",
      "527/18: X = dataset.drop(dataset[\"Survived\"],axis=1)\n",
      "527/19: X = dataset.drop(columns=[\"Survived\"],axis=1)\n",
      "527/20:\n",
      "X = dataset.drop(columns=[\"Survived\"],axis=1)\n",
      "X\n",
      "527/21:\n",
      "X = dataset.drop(columns=[\"Survived\"],axis=1)\n",
      "y = dataset[\"Survived\"]\n",
      "\n",
      "y\n",
      "527/22:\n",
      "X = dataset.drop(columns=[\"Survived\"],axis=1)\n",
      "y = dataset[\"Survived\"]\n",
      "527/23: x\n",
      "527/24: X\n",
      "527/25: X.isnull().sum()\n",
      "527/26: x\n",
      "527/27: X\n",
      "527/28: y\n",
      "527/29: y.isnull().sum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527/30: y.isnull().sum()\n",
      "527/31:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "X_train\n",
      "527/32: X_test\n",
      "527/33: y_test\n",
      "527/34: y_train\n",
      "527/35:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train[\"Age\",\"SibSp\"] = sc.fit_transform(X_train[\"Age\",\"SibSp\"])\n",
      "X_test[\"Age\",\"SibSp\"] = sc.transform(X_test[\"Age\",\"SibSp\"])\n",
      "527/36:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train[[\"Age\"],[\"SibSp\"]] = sc.fit_transform(X_train[\"Age\",\"SibSp\"])\n",
      "X_test[\"Age\",\"SibSp\"] = sc.transform(X_test[\"Age\",\"SibSp\"])\n",
      "527/37:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train[[\"Age\",\"SibSp\"]] = sc.fit_transform(X_train[[\"Age\",\"SibSp\"]])\n",
      "X_test[[\"Age\",\"SibSp\"]] = sc.transform(X_test[[\"Age\",\"SibSp\"]])\n",
      "527/38: X_train[:, 1:3]\n",
      "527/39:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "527/40: X_train\n",
      "527/41: X_train[:, 1:3]\n",
      "527/42: X_train[:, 1]\n",
      "527/43: X_train[:, 1:]\n",
      "527/44:\n",
      "np.array(X_train)\n",
      "X_train[:, 1:3]\n",
      "527/45:\n",
      "X_train = np.array(X_train)\n",
      "X_train[:, 1:3]\n",
      "527/46:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "527/47: X_train\n",
      "527/48: X_train[[\"Age\",\"Sibsp\"]]\n",
      "527/49: X_train[[\"Age\",\"SibSp\"]]\n",
      "527/50:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train[[\"Age\",\"SibSp\"]] = sc.fit_transform(X_train[[\"Age\",\"SibSp\"]])\n",
      "X_test[[\"Age\",\"SibSp\"]] = sc.transform(X_test[[\"Age\",\"SibSp\"]])\n",
      "527/51: X_train\n",
      "527/52: X_train[:, 1]\n",
      "527/53: X_train.iloc[:, 1]\n",
      "527/54: X_train.iloc[:, 1:3]\n",
      "527/55:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])\n",
      "X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])\n",
      "527/56: X_train\n",
      "527/57:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train.loc[:, 1:3] = sc.fit_transform(X_train.loc[:, 1:3])\n",
      "X_test.loc[:, 1:3] = sc.transform(X_test.loc[:, 1:3])\n",
      "527/58:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])\n",
      "X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])\n",
      "527/59: X_train.loc[:, 1]\n",
      "527/60: X_train.loc[:,1]\n",
      "527/61: X_train.loc[1,1]\n",
      "528/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "528/2: dataset = pd.read_csv(\"data.csv\")\n",
      "528/3: dataset.head()\n",
      "528/4: dataset.columns\n",
      "528/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "528/6: dataset.isnull().sum()\n",
      "528/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "528/8:\n",
      "temp_dataset1 = dataset.copy()\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(), inplace=True)\n",
      "temp_dataset1[\"Age\"].isnull().sum()\n",
      "528/9:\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "temp_dataset2 = dataset.copy()\n",
      "\n",
      "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "imputer.fit(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"] = imputer.transform(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"].isnull().sum()\n",
      "528/10: dataset[\"Age\"].fillna(int(dataset[\"Age\"].mean()),inplace=True)\n",
      "528/11:\n",
      "dataset.dropna(subset=[\"Survived\",\"Embarked\"], inplace=True)\n",
      "dataset.isnull().sum()\n",
      "528/12:\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)\n",
      "temp_dataset3\n",
      "528/13:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "\n",
      "temp_dataset4\n",
      "528/14:\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "temp_dataset5 = dataset.copy()\n",
      "\n",
      "le = LabelEncoder()\n",
      "temp_dataset5[\"Sex\"] = le.fit_transform(temp_dataset5[\"Sex\"])\n",
      "temp_dataset5[\"Embarked\"] = le.fit_transform(temp_dataset5[\"Embarked\"])\n",
      "\n",
      "temp_dataset5\n",
      "528/15:\n",
      "N_Embarked = pd.get_dummies(dataset[\"Embarked\"], drop_first=True)\n",
      "N_Sex = pd.get_dummies(dataset[\"Sex\"], drop_first=True)\n",
      "\n",
      "dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)\n",
      "print(dataset.head())\n",
      "\n",
      "# We, can remove the Sex and Embarked Columns as we have encoded them.\n",
      "\n",
      "dataset.drop(columns=[\"Sex\",\"Embarked\"], axis=1, inplace=True)\n",
      "print(dataset.head())\n",
      "528/16:\n",
      "X = dataset.drop(columns=[\"Survived\"],axis=1)\n",
      "y = dataset[\"Survived\"]\n",
      "528/17:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "528/18:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])\n",
      "X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])\n",
      "528/19: X_train\n",
      "528/20: X_test\n",
      "528/21:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])\n",
      "X_test.iloc[:, 1:3] = sc.fit_transform(X_test.iloc[:, 1:3])\n",
      "528/22: X_train\n",
      "528/23: X_test\n",
      "528/24:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])\n",
      "X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])\n",
      "528/25: X_train\n",
      "528/26: X_test\n",
      "528/27:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])\n",
      "X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])\n",
      "528/28: X_train\n",
      "528/29: X_test\n",
      "529/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "529/2: dataset = pd.read_csv(\"data.csv\")\n",
      "529/3: dataset.head()\n",
      "529/4: dataset.columns\n",
      "529/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "529/6: dataset.isnull().sum()\n",
      "529/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "529/8:\n",
      "temp_dataset1 = dataset.copy()\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(), inplace=True)\n",
      "temp_dataset1[\"Age\"].isnull().sum()\n",
      "529/9:\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "temp_dataset2 = dataset.copy()\n",
      "\n",
      "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "imputer.fit(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"] = imputer.transform(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"].isnull().sum()\n",
      "529/10: dataset[\"Age\"].fillna(int(dataset[\"Age\"].mean()),inplace=True)\n",
      "529/11:\n",
      "dataset.dropna(subset=[\"Survived\",\"Embarked\"], inplace=True)\n",
      "dataset.isnull().sum()\n",
      "529/12:\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)\n",
      "temp_dataset3\n",
      "529/13:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "\n",
      "temp_dataset4\n",
      "529/14:\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "temp_dataset5 = dataset.copy()\n",
      "\n",
      "le = LabelEncoder()\n",
      "temp_dataset5[\"Sex\"] = le.fit_transform(temp_dataset5[\"Sex\"])\n",
      "temp_dataset5[\"Embarked\"] = le.fit_transform(temp_dataset5[\"Embarked\"])\n",
      "\n",
      "temp_dataset5\n",
      "529/15:\n",
      "N_Embarked = pd.get_dummies(dataset[\"Embarked\"], drop_first=True)\n",
      "N_Sex = pd.get_dummies(dataset[\"Sex\"], drop_first=True)\n",
      "\n",
      "dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)\n",
      "print(dataset.head())\n",
      "\n",
      "# We, can remove the Sex and Embarked Columns as we have encoded them.\n",
      "\n",
      "dataset.drop(columns=[\"Sex\",\"Embarked\"], axis=1, inplace=True)\n",
      "print(dataset.head())\n",
      "529/16:\n",
      "X = dataset.drop(columns=[\"Survived\"],axis=1)\n",
      "y = dataset[\"Survived\"]\n",
      "529/17:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "529/18:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])\n",
      "X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])\n",
      "529/19: X_train\n",
      "529/20: X_test\n",
      "531/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "531/2: dataset = pd.read_csv(\"data.csv\")\n",
      "531/3: dataset.head()\n",
      "531/4: dataset.columns\n",
      "531/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "531/6: dataset.isnull().sum()\n",
      "531/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "531/8: dataset[\"Age\"].fillna(int(dataset[\"Age\"].mean()),inplace=True)\n",
      "531/9:\n",
      "dataset.dropna(subset=[\"Survived\",\"Embarked\"], inplace=True)\n",
      "dataset.isnull().sum()\n",
      "531/10:\n",
      "N_Embarked = pd.get_dummies(dataset[\"Embarked\"], drop_first=True)\n",
      "N_Sex = pd.get_dummies(dataset[\"Sex\"], drop_first=True)\n",
      "\n",
      "dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)\n",
      "print(dataset.head())\n",
      "\n",
      "# We, can remove the Sex and Embarked Columns as we have encoded them.\n",
      "\n",
      "dataset.drop(columns=[\"Sex\",\"Embarked\"], axis=1, inplace=True)\n",
      "print(dataset.head())\n",
      "531/11:\n",
      "X = dataset.drop(columns=[\"Survived\"],axis=1)\n",
      "y = dataset[\"Survived\"]\n",
      "531/12:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "531/13:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])\n",
      "X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])\n",
      "531/14: X_train\n",
      "531/15: X_test\n",
      "531/16: X_train.head()\n",
      "532/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "532/2: dataset = pd.read_csv(\"data.csv\")\n",
      "532/3: dataset.head()\n",
      "532/4: dataset.columns\n",
      "532/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "532/6: dataset.isnull().sum()\n",
      "532/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "532/8: dataset[\"Age\"].fillna(int(dataset[\"Age\"].mean()),inplace=True)\n",
      "532/9:\n",
      "dataset.dropna(subset=[\"Survived\",\"Embarked\"], inplace=True)\n",
      "dataset.isnull().sum()\n",
      "532/10:\n",
      "N_Embarked = pd.get_dummies(dataset[\"Embarked\"], drop_first=True)\n",
      "N_Sex = pd.get_dummies(dataset[\"Sex\"], drop_first=True)\n",
      "\n",
      "dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)\n",
      "print(dataset.head())\n",
      "\n",
      "# We, can remove the Sex and Embarked Columns as we have encoded them.\n",
      "\n",
      "dataset.drop(columns=[\"Sex\",\"Embarked\"], axis=1, inplace=True)\n",
      "print(dataset.head())\n",
      "532/11:\n",
      "X = dataset.drop(columns=[\"Survived\"],axis=1)\n",
      "y = dataset[\"Survived\"]\n",
      "532/12:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "532/13:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])\n",
      "X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])\n",
      "532/14: X_train.head()\n",
      "533/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "533/2: !pip install pandas\n",
      "534/1: !pip install pandas\n",
      "534/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "535/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "535/2: !pip install matplotlib, numpy\n",
      "535/3: !pip install \"matplotlib\",\"numpy\"\n",
      "535/4: !pip install matplotlib\n",
      "536/1: !pip install matplotlib\n",
      "536/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "536/3:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "538/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "538/2: dataset = pd.read_csv(\"data.csv\")\n",
      "538/3: dataset.head()\n",
      "538/4: dataset.columns\n",
      "538/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "538/6: dataset.isnull().sum()\n",
      "538/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "538/8:\n",
      "temp_dataset1 = dataset.copy()\n",
      "temp_dataset1[\"Age\"].fillna(temp_dataset1[\"Age\"].mean(), inplace=True)\n",
      "temp_dataset1[\"Age\"].isnull().sum()\n",
      "538/9:\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "temp_dataset2 = dataset.copy()\n",
      "\n",
      "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "imputer.fit(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"] = imputer.transform(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"].isnull().sum()\n",
      "538/10:\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "temp_dataset2 = dataset.copy()\n",
      "\n",
      "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "imputer.fit(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"] = imputer.transform(temp_dataset2[[\"Age\"]])\n",
      "temp_dataset2[\"Age\"].isnull().sum()\n",
      "538/11: dataset[\"Age\"].fillna(int(dataset[\"Age\"].mean()),inplace=True)\n",
      "538/12:\n",
      "dataset.dropna(subset=[\"Survived\",\"Embarked\"], inplace=True)\n",
      "dataset.isnull().sum()\n",
      "538/13:\n",
      "temp_dataset3 = dataset.copy()\n",
      "temp_dataset3\n",
      "\n",
      "temp_column1 = pd.get_dummies(temp_dataset3[\"Embarked\"], drop_first=True)\n",
      "temp_column2 = pd.get_dummies(temp_dataset3[\"Sex\"], drop_first=True)\n",
      "\n",
      "temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)\n",
      "temp_dataset3\n",
      "538/14:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "\n",
      "temp_dataset4 = dataset.copy()\n",
      "temp_dataset4 = np.array(temp_dataset4)\n",
      "print(temp_dataset4)\n",
      "\n",
      "ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')\n",
      "temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))\n",
      "\n",
      "temp_dataset4\n",
      "538/15:\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "temp_dataset5 = dataset.copy()\n",
      "\n",
      "le = LabelEncoder()\n",
      "temp_dataset5[\"Sex\"] = le.fit_transform(temp_dataset5[\"Sex\"])\n",
      "temp_dataset5[\"Embarked\"] = le.fit_transform(temp_dataset5[\"Embarked\"])\n",
      "\n",
      "temp_dataset5\n",
      "538/16:\n",
      "N_Embarked = pd.get_dummies(dataset[\"Embarked\"], drop_first=True)\n",
      "N_Sex = pd.get_dummies(dataset[\"Sex\"], drop_first=True)\n",
      "\n",
      "dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)\n",
      "print(dataset.head())\n",
      "\n",
      "# We, can remove the Sex and Embarked Columns as we have encoded them.\n",
      "\n",
      "dataset.drop(columns=[\"Sex\",\"Embarked\"], axis=1, inplace=True)\n",
      "print(dataset.head())\n",
      "538/17:\n",
      "X = dataset.drop(columns=[\"Survived\"],axis=1)\n",
      "y = dataset[\"Survived\"]\n",
      "538/18:\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "538/19:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])\n",
      "X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])\n",
      "538/20: X_train\n",
      "538/21: X_test\n",
      "538/22: dataset = pd.read_csv(\"train.csv\")\n",
      "538/23: dataset.head()\n",
      "538/24: dataset.isnull().sum()\n",
      "538/25: dataset['y'].fillna(dataset[\"y\"].mean())\n",
      "538/26: dataset.isnull().sum()\n",
      "538/27: dataset[\"y\"].fillna(dataset[\"y\"].mean())\n",
      "538/28: dataset.isnull().sum()\n",
      "538/29: dataset[\"y\"].fillna(dataset[\"y\"].mean())\n",
      "538/30: dataset[\"y\"].fillna(dataset[\"y\"].mean(), inplace=True)\n",
      "538/31: dataset.isnull().sum()\n",
      "538/32:\n",
      "X = dataset.drop(columns=[\"y\"],axis=1)\n",
      "y = dataset[\"x\"]\n",
      "538/33:\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "538/34: X_train\n",
      "538/35: X_test\n",
      "541/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "541/2: dataset = pd.read_csv(\"data.csv\")\n",
      "541/3: dataset.head()\n",
      "541/4: dataset.columns\n",
      "541/5:\n",
      "dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',\n",
      "       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',\n",
      "       'Class','Fare'], axis=1, inplace=True)\n",
      "dataset.head()\n",
      "541/6: dataset.isnull().sum()\n",
      "541/7:\n",
      "for ele in dataset.columns:\n",
      "    print(\"{}:  {}%\".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))\n",
      "541/8: dataset[\"Age\"].fillna(int(dataset[\"Age\"].mean()),inplace=True)\n",
      "541/9:\n",
      "dataset.dropna(subset=[\"Survived\",\"Embarked\"], inplace=True)\n",
      "dataset.isnull().sum()\n",
      "541/10:\n",
      "N_Embarked = pd.get_dummies(dataset[\"Embarked\"], drop_first=True)\n",
      "N_Sex = pd.get_dummies(dataset[\"Sex\"], drop_first=True)\n",
      "\n",
      "dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)\n",
      "print(dataset.head())\n",
      "\n",
      "# We, can remove the Sex and Embarked Columns as we have encoded them.\n",
      "\n",
      "dataset.drop(columns=[\"Sex\",\"Embarked\"], axis=1, inplace=True)\n",
      "print(dataset.head())\n",
      "541/11:\n",
      "X = dataset.drop(columns=[\"Survived\"],axis=1)\n",
      "y = dataset[\"Survived\"]\n",
      "541/12:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "541/13:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])\n",
      "X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])\n",
      "541/14: X_train.head()\n",
      "538/36: .ipynb_checkpoints\n",
      "538/37: .ipynb_checkpoints/\n",
      "538/38: \".ipynb_checkpoints/\"\"\n",
      "538/39: \".ipynb_checkpoints/\"\n",
      "538/40: %history -g\n",
      "538/41: %history -g -f demo.ipyb\n",
      "542/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "542/2: dataset = pd.read_csv(\"data.csv\")\n",
      "542/3: dataset.head()\n",
      "542/4: dataset.columns\n",
      "543/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "543/2: dataset = pd.read_csv(\"train.csv\")\n",
      "543/3: dataset.head()\n",
      "543/4: dataset.isnull().sum()\n",
      "543/5: dataset[\"Age\"].fillna(int(dataset[\"Age\"].mean()),inplace=True)\n",
      "543/6: dataset[\"y\"].fillna(int(dataset[\"y\"].mean()),inplace=True)\n",
      "543/7:\n",
      "X = dataset.drop(columns=[\"y\"],axis=1)\n",
      "y = dataset[\"x\"]\n",
      "543/8:\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "543/9: X_train\n",
      "543/10: X_test\n",
      "543/11:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "regressor = LinearRegression()\n",
      "regressor.fit(X_train, y_train)\n",
      "543/12: y_predict = regressor.predict(X_test)\n",
      "543/13:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "plt.show()\n",
      "544/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "544/2: dataset = pd.read_csv(\"train.csv\")\n",
      "544/3: dataset.head()\n",
      "544/4: dataset.isnull().sum()\n",
      "544/5: dataset['y'].unique()\n",
      "544/6: dataset[\"y\"].fillna(int(dataset[\"y\"].mean()),inplace=True)\n",
      "544/7:\n",
      "X = dataset.drop(columns=[\"y\"],axis=1)\n",
      "y = dataset[\"x\"]\n",
      "544/8:\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "544/9: X_train\n",
      "544/10: X_test\n",
      "544/11:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "regressor = LinearRegression()\n",
      "regressor.fit(X_train, y_train)\n",
      "544/12: y_predict = regressor.predict(X_test)\n",
      "544/13:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "plt.show()\n",
      "544/14: %history -g\n",
      "544/15: %history -g -f demo.ipyb\n",
      "545/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "545/2: dataset = pd.read_csv(\"train.csv\")\n",
      "545/3: dataset.head()\n",
      "545/4: dataset.isnull().sum()\n",
      "545/5: dataset['y'].unique().max()\n",
      "545/6: dataset[\"y\"].fillna(int(dataset[\"y\"].mean()),inplace=True)\n",
      "545/7:\n",
      "X = dataset.drop(columns=[\"y\"],axis=1)\n",
      "y = dataset[\"x\"]\n",
      "545/8:\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "545/9: X_train\n",
      "545/10: X_test\n",
      "545/11:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "regressor = LinearRegression()\n",
      "regressor.fit(X_train, y_train)\n",
      "545/12: y_predict = regressor.predict(X_test)\n",
      "545/13:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "plt.show()\n",
      "545/14: %history -g\n",
      "545/15: %history -g -f demo.ipyb\n",
      "546/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "546/2: dataset = pd.read_csv(\"train.csv\")\n",
      "546/3: dataset.head()\n",
      "546/4: dataset.isnull().sum()\n",
      "546/5: dataset[\"y\"].fillna(int(dataset[\"y\"].mean()),inplace=True)\n",
      "546/6: dataset['y'].unique().max()\n",
      "546/7:\n",
      "X = dataset.drop(columns=[\"y\"],axis=1)\n",
      "y = dataset[\"x\"]\n",
      "546/8:\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "546/9: X_train\n",
      "546/10: X_test\n",
      "546/11:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "regressor = LinearRegression()\n",
      "regressor.fit(X_train, y_train)\n",
      "546/12: y_predict = regressor.predict(X_test)\n",
      "546/13:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "plt.show()\n",
      "546/14: %history -g\n",
      "546/15: #%history -g -f demo.ipyb\n",
      "546/16:\n",
      "plt.scatter(X_test, y_test, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "plt.show()\n",
      "546/17:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "plt.show()\n",
      "ax = plt.gca()\n",
      "ax.set_ylim(0,200)\n",
      "546/18:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "ax = plt.gca()\n",
      "ax.set_ylim(0,200)\n",
      "plt.show()\n",
      "546/19:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "ax = plt.gca()\n",
      "ax.set_ylim(0,200)\n",
      "ax.set_xlim(0,200)\n",
      "plt.show()\n",
      "546/20:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "ax = plt.gca()\n",
      "ax.set_ylim(0,150)\n",
      "ax.set_xlim(0,150)\n",
      "plt.show()\n",
      "546/21:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "ax = plt.gca()\n",
      "ax.set_ylim(0,110)\n",
      "ax.set_xlim(0,110)\n",
      "plt.show()\n",
      "546/22:\n",
      "plt.scatter(X_test, y_test, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "ax.set_ylim(0,110)\n",
      "ax.set_xlim(0,110)\n",
      "plt.show()\n",
      "546/23:\n",
      "plt.scatter(X_test, y_test, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "ax.set_ylim(0,110)\n",
      "ax.set_xlim(0,110)\n",
      "plt.show()\n",
      "546/24:\n",
      "plt.scatter(X_test, y_test, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "ax = plt.gca()\n",
      "ax.set_ylim(0,110)\n",
      "ax.set_xlim(0,110)\n",
      "plt.show()\n",
      "547/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "547/2: dataset = pd.read_csv(\"train.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547/3: dataset.head()\n",
      "547/4: dataset.isnull().sum()\n",
      "547/5: dataset[\"y\"].fillna(int(dataset[\"y\"].mean()),inplace=True)\n",
      "547/6: dataset['y'].unique().max()\n",
      "547/7:\n",
      "X = dataset.drop(columns=[\"y\"],axis=1)\n",
      "y = dataset[\"x\"]\n",
      "547/8:\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "547/9: X_train\n",
      "547/10: X_test\n",
      "547/11:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "regressor = LinearRegression()\n",
      "regressor.fit(X_train, y_train)\n",
      "547/12: y_predict = regressor.predict(X_test)\n",
      "547/13:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "\n",
      "plt.show()\n",
      "547/14:\n",
      "plt.scatter(X_test, y_test, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "ax = plt.gca()\n",
      "ax.set_ylim(0,110)\n",
      "ax.set_xlim(0,110)\n",
      "plt.show()\n",
      "547/15: %history -g\n",
      "547/16: #%history -g -f demo.ipyb\n",
      "548/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "548/2: dataset = pd.read_csv(\"train.csv\")\n",
      "548/3: dataset.head()\n",
      "548/4: dataset.isnull().sum()\n",
      "548/5: dataset[\"y\"].fillna(int(dataset[\"y\"].mean()),inplace=True)\n",
      "548/6: dataset['y'].unique().max()\n",
      "548/7:\n",
      "X = dataset.drop(columns=[\"y\"],axis=1)\n",
      "y = dataset[\"x\"]\n",
      "548/8:\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "548/9: X_train\n",
      "548/10: X_test\n",
      "548/11:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "regressor = LinearRegression()\n",
      "regressor.fit(X_train, y_train)\n",
      "548/12: y_predict = regressor.predict(X_test)\n",
      "548/13:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "ax = plt.gca()\n",
      "ax.set_ylim(0,110)\n",
      "ax.set_xlim(0,110)\n",
      "plt.show()\n",
      "548/14:\n",
      "plt.scatter(X_test, y_test, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "ax = plt.gca()\n",
      "ax.set_ylim(0,110)\n",
      "ax.set_xlim(0,110)\n",
      "plt.show()\n",
      "548/15: %history -g\n",
      "548/16: #%history -g -f demo.ipyb\n",
      "549/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "549/2: dataset = pd.read_csv(\"train.csv\")\n",
      "549/3: dataset.head()\n",
      "549/4: dataset.isnull().sum()\n",
      "549/5: dataset[\"y\"].fillna(int(dataset[\"y\"].mean()),inplace=True)\n",
      "549/6: dataset['y'].unique().max()\n",
      "549/7:\n",
      "X = dataset.drop(columns=[\"y\"],axis=1)\n",
      "y = dataset[\"x\"]\n",
      "549/8:\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "549/9: X_train\n",
      "549/10: X_test\n",
      "549/11:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "regressor = LinearRegression()\n",
      "regressor.fit(X_train, y_train)\n",
      "549/12: y_predict = regressor.predict(X_test)\n",
      "549/13:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "ax = plt.gca()\n",
      "ax.set_ylim(0,110)\n",
      "ax.set_xlim(0,110)\n",
      "plt.show()\n",
      "549/14:\n",
      "plt.scatter(X_test, y_test, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "ax = plt.gca()\n",
      "ax.set_ylim(0,110)\n",
      "ax.set_xlim(0,110)\n",
      "plt.show()\n",
      "549/15: %history -g\n",
      "549/16: #%history -g -f demo.ipyb\n",
      "549/17:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "ax = plt.gca()\n",
      "ax.set_ylim(0,50)\n",
      "ax.set_xlim(0,50)\n",
      "plt.show()\n",
      "549/18:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "ax = plt.gca()\n",
      "\n",
      "plt.show()\n",
      "549/19:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "549/20:\n",
      "plt.scatter(X_test, y_test, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "\n",
      "plt.show()\n",
      "   1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "   2: dataset = pd.read_csv(\"train.csv\")\n",
      "   3: dataset.head()\n",
      "   4: dataset.isnull().sum()\n",
      "   5: dataset[\"y\"].fillna(int(dataset[\"y\"].mean()),inplace=True)\n",
      "   6: dataset['y'].unique().max()\n",
      "   7:\n",
      "X = dataset.drop(columns=[\"y\"],axis=1)\n",
      "y = dataset[\"x\"]\n",
      "   8:\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "   9: X_train\n",
      "  10: X_test\n",
      "  11:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "regressor = LinearRegression()\n",
      "regressor.fit(X_train, y_train)\n",
      "  12: y_predict = regressor.predict(X_test)\n",
      "  13:\n",
      "plt.scatter(X_train, y_train, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "  14:\n",
      "plt.scatter(X_test, y_test, color = 'red')\n",
      "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
      "plt.title(\"X vs Y\")\n",
      "plt.xlabel(\"X values\")\n",
      "plt.ylabel(\"Y values\")\n",
      "\n",
      "plt.show()\n",
      "  15: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%history -g -f demo.ipyb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
