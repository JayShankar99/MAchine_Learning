 1/1: runfile('C:/Users/Jay/.spyder-py3/temp.py', wdir='C:/Users/Jay/.spyder-py3')
 2/1: runfile('C:/Users/Jay/.spyder-py3/temp.py', wdir='C:/Users/Jay/.spyder-py3')
 2/2: runfile('C:/Users/Jay/.spyder-py3/temp.py', wdir='C:/Users/Jay/.spyder-py3')
 2/3: runfile('C:/Users/Jay/.spyder-py3/temp.py', wdir='C:/Users/Jay/.spyder-py3')
 2/4: runfile('C:/Users/Jay/.spyder-py3/temp.py', wdir='C:/Users/Jay/.spyder-py3')
 2/5: runfile('C:/Users/Jay/.spyder-py3/temp.py', wdir='C:/Users/Jay/.spyder-py3')
 2/6: runfile('C:/Users/Jay/.spyder-py3/trial1.py', wdir='C:/Users/Jay/.spyder-py3')
 2/7: runfile('C:/Users/Jay/.spyder-py3/trial1.py', wdir='C:/Users/Jay/.spyder-py3')
 2/8: runfile('C:/Users/Jay/.spyder-py3/trial1.py', wdir='C:/Users/Jay/.spyder-py3')
 2/9: runfile('C:/Users/Jay/.spyder-py3/trial1.py', wdir='C:/Users/Jay/.spyder-py3')
 3/1: runfile('C:/Users/Jay/.spyder-py3/trial1.py', wdir='C:/Users/Jay/.spyder-py3')
 3/2: runfile('C:/Users/Jay/.spyder-py3/trial1.py', wdir='C:/Users/Jay/.spyder-py3')
 3/3: k = temp.factorial(6)
 3/4: runfile('C:/Users/Jay/.spyder-py3/trial2.py', wdir='C:/Users/Jay/.spyder-py3')
 3/5: runfile('C:/Users/Jay/.spyder-py3/trial2.py', wdir='C:/Users/Jay/.spyder-py3')
 3/6: runfile('C:/Users/Jay/.spyder-py3/trial2.py', wdir='C:/Users/Jay/.spyder-py3')
 3/7: runfile('C:/Users/Jay/.spyder-py3/trial2.py', wdir='C:/Users/Jay/.spyder-py3')
 3/8: clear
 3/9: runfile('C:/Users/Jay/.spyder-py3/all_call.py', wdir='C:/Users/Jay/.spyder-py3')
3/10: clear
3/11: runfile('C:/Users/Jay/.spyder-py3/all_call.py', wdir='C:/Users/Jay/.spyder-py3')
3/12: clear
3/13: runfile('C:/Users/Jay/.spyder-py3/all_call.py', wdir='C:/Users/Jay/.spyder-py3')
3/14: runfile('C:/Users/Jay/.spyder-py3/all_call.py', wdir='C:/Users/Jay/.spyder-py3')
3/15: clear
3/16: runfile('C:/Users/Jay/.spyder-py3/all_call.py', wdir='C:/Users/Jay/.spyder-py3')
3/17: clear
 6/1:
a =20
b= 30
print(a+b)
 7/1: runfile('C:/Users/Jay/.spyder-py3/all_call.py', wdir='C:/Users/Jay/.spyder-py3')
 8/1: runcell(0, 'C:/Users/Jay/demo.py')
 8/2: runfile('C:/Users/Jay/demo.py', wdir='C:/Users/Jay')
 8/3: runfile('C:/Users/Jay/demo.py', wdir='C:/Users/Jay')
 9/1: arr = [1,0,0,1,0,1]
 9/2:
arr = [1,0,0,1,0,1]
j=0
for i in range(len(arr)):
    if arr[i]<1:
        temp = arr[i]
        arr[i] = arr[j]
        arr[j] = temp
        
arr
 9/3:
arr = [1,0,0,1,0,1]
j=0
for i in range(len(arr)):
    if arr[i]<1:
        temp = arr[i]
        arr[i] = arr[j]
        arr[j] = temp
        j += 1
        
arr
 9/4:
arr = [0,1,0,0,1,0,1]
j=0
for i in range(len(arr)):
    if arr[i]<1:
        temp = arr[i]
        arr[i] = arr[j]
        arr[j] = temp
        j += 1
        
arr
 9/5:
arr = [0,1,0,0,1,0,1,0,1,0,0]
j=0
for i in range(len(arr)):
    if arr[i]<1:
        temp = arr[i]
        arr[i] = arr[j]
        arr[j] = temp
        j += 1
        
arr
 9/6:
arr = [0,1,0,0,1,0,1,0,1,0,0]
j=0
for i in range(len(arr)):
    if arr[i]<1:
        temp = arr[i]
        arr[i] = arr[j]
        arr[j] = temp
        j += 1
        
arr
10/1:
import matplotlib.pyplot as plt
x1 = [1,2,3,4,5]
x2 = [1,2,3,4,5]

y1 = [2,4,1,4,-1] 
y2 = [5,-2,-4,7,8]

plt.subplot(2,1,1)

plt.plot(x1, y1)
plt.title("Subplot 1")
plt.xlabel("x1")
plt.ylabel("y1")

plt.subplot(2,1,2)
plt.plot(x2, y2)
plt.title("Subplot 2")
plt.xlabel("x2")
plt.ylabel("y2")

plt.show()
10/2:
import matplotlib.pyplot as plt
x1 = [1,2,3,4,5]
x2 = [1,2,3,4,5]

y1 = [2,4,1,4,-1] 
y2 = [5,-2,-4,7,8]

plt.subplot(2,1,1)

plt.plot(x1, y1)
plt.title("Subplot 1")
plt.xlabel("x1")
plt.ylabel("y1")

plt.subplot(2,1,2)
plt.plot(x2, y2)
plt.title("Subplot 2")
plt.xlabel("x2")
plt.ylabel("y2")

plt.show()
10/3:
import matplotlib.pyplot as plt

x1 = [1,2,3,4,5]
x2 = [1,2,3,4,5]

y1 = [2,4,1,4,-1] 
y2 = [5,-2,-4,7,8]

plt.subplot(1,1,1)

plt.plot(x1, y1)
plt.title("Subplot 1")
plt.xlabel("x1")
plt.ylabel("y1")

plt.subplot(1,2,2)
plt.plot(x2, y2)
plt.title("Subplot 2")
plt.xlabel("x2")
plt.ylabel("y2")

plt.show()
10/4:
import matplotlib.pyplot as plt
x1 = [1,2,3,4,5]
x2 = [1,2,3,4,5]

y1 = [2,4,1,4,-1] 
y2 = [5,-2,-4,7,8]

plt.subplot(1,2,1)

plt.plot(x1, y1)
plt.title("Subplot 1")
plt.xlabel("x1")
plt.ylabel("y1")

plt.subplot(1,2,2)
plt.plot(x2, y2)
plt.title("Subplot 2")
plt.xlabel("x2")
plt.ylabel("y2")

plt.show()
10/5:
import matplotlib.pyplot as plt
x1 = [1,2,3,4,5]
x2 = [1,2,3,4,5]

y1 = [2,4,1,4,-1] 
y2 = [5,-2,-4,7,8]

plt.subplot(1,2,1)

plt.plot(x1, y1)
plt.title("Subplot 1")
plt.xlabel("x1")
plt.ylabel("y1")

plt.subplot(1,2,1)
plt.plot(x2, y2)
plt.title("Subplot 2")
plt.xlabel("x2")
plt.ylabel("y2")

plt.show()
10/6:
import matplotlib.pyplot as plt
x1 = [1,2,3,4,5]
x2 = [1,2,3,4,5]

y1 = [2,4,1,4,-1] 
y2 = [5,-2,-4,7,8]

plt.subplot(1,1,1)

plt.plot(x1, y1)
plt.title("Subplot 1")
plt.xlabel("x1")
plt.ylabel("y1")

plt.subplot(1,2,1)
plt.plot(x2, y2)
plt.title("Subplot 2")
plt.xlabel("x2")
plt.ylabel("y2")

plt.show()
10/7:
import matplotlib.pyplot as plt
x1 = [1,2,3,4,5]
x2 = [1,2,3,4,5]

y1 = [2,4,1,4,-1] 
y2 = [5,-2,-4,7,8]

plt.subplot(1,1,1)

plt.plot(x1, y1)
plt.title("Subplot 1")
plt.xlabel("x1")
plt.ylabel("y1")

plt.subplot(1,2,2)
plt.plot(x2, y2)
plt.title("Subplot 2")
plt.xlabel("x2")
plt.ylabel("y2")

plt.show()
10/8:
import matplotlib.pyplot as plt
x1 = [1,2,3,4,5]
x2 = [1,2,3,4,5]

y1 = [2,4,1,4,-1] 
y2 = [5,-2,-4,7,8]

plt.subplot(1,2,1)

plt.plot(x1, y1)
plt.title("Subplot 1")
plt.xlabel("x1")
plt.ylabel("y1")

plt.subplot(1,2,2)
plt.plot(x2, y2)
plt.title("Subplot 2")
plt.xlabel("x2")
plt.ylabel("y2")

plt.show()
11/1:
for i in range(11):
    if i==0 or i==5 or i==10:
        print("+----+----+")
    else:
        print("|    |    |")
11/2:
for i in range(11):
    if i==0 or i==5 or i==10:
        print("+----+----+")
    else:
        print("|   |  |")
11/3:
for i in range(11):
    if i==0 or i==5 or i==10:
        print("+----+----+")
    else:
        print("|   |  |")
11/4:
for i in range(11):
    if i==0 or i==5 or i==10:
        print("+----+----+")
    else:
        print("|  |  |")
11/5:
for i in range(11):
    if i==0 or i==5 or i==10:
        print("+----+----+")
    else:
        print("|  |   |")
12/1:
import matplotlib.pyplot as plt
import numpy as np

x = np.arange(0,2*np.pi,0.01)

y1 = np.sin(x)
y2 = np.cos(x)


plt.plot(x,y1,label='Sin')
plt.plot(x,y2,label='Cos')
plt.xlabel("X-axis")
plt.ylabel("Y-axis")
plt.grid()
plt.legend()
plt.title("Sin and Cos graph")
plt.show()
12/2:
import matplotlib.pyplot as plt
import numpy as np

x = np.arange(0,2*np.pi,0.01)

y1 = np.sin(x)
y2 = np.cos(x)


plt.plot(x,y1,label='Sin')
plt.plot(x,y2,label='Cos')
plt.xlabel("X-axis")
plt.ylabel("Y-axis")
plt.grid()
plt.legend()
plt.title("Sin and Cos graph")
plt.show()
12/3:
import matplotlib.pyplot as plt
import numpy as np

x = np.arange(0,2*np.pi,0.01)

y1 = np.sin(x)
y2 = np.cos(x)
y3 = np.tan(x)


plt.plot(x,y1,label='Sin')
plt.plot(x,y2,label='Cos')
plt.show(x,y3,label='Tan')
plt.xlabel("X-axis")
plt.ylabel("Y-axis")
plt.grid()
plt.legend()
plt.title("Sin and Cos graph")
plt.show()
12/4:
import matplotlib.pyplot as plt
import numpy as np

x = np.arange(0,2*np.pi,0.01)

y1 = np.sin(x)
y2 = np.cos(x)
y3 = np.tan(x)


plt.plot(x,y1,label='Sin')
plt.plot(x,y2,label='Cos')
plt.plot(x,y3,label='Tan')
plt.xlabel("X-axis")
plt.ylabel("Y-axis")
plt.grid()
plt.legend()
plt.title("Sin and Cos graph")
plt.show()
13/1: runfile('F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)/Reading_CSV_file.py', wdir='F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)')
13/2: runfile('F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)/Reading_CSV_file.py', wdir='F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)')
13/3: runfile('F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)/Reading_CSV_file.py', wdir='F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)')
13/4: runfile('F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)/Reading_CSV_file.py', wdir='F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)')
13/5: runfile('F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)/Reading_CSV_file.py', wdir='F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)')
13/6: runfile('F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)/Reading_CSV_file.py', wdir='F:/Pythn prgramming practice/Machine Learning (ML)/Data_Reading(Panda)')
14/1:
import pandas as pd

pokemon = pd.read_csv("F:\\Pythn prgramming practice\\Machine Learning (ML)\\Data_Reading(Panda)\\pokemon1.csv",
    usecols=["Name"],squeeze=True)

#   squeeze -> basicaly this argument converts the data into a series.

#print(pokemon.head())
print(pokemon.sort_values())

#   sort_values() -> This method sort the values in ass
#print(pokemon.sort_values().head())
14/2:
import pandas as pd

pokemon = pd.read_csv("F:\\Pythn prgramming practice\\Machine Learning (ML)\\Data_Reading(Panda)\\pokemon1.csv",
    usecols=["Name"],squeeze=True)

#   squeeze -> basicaly this argument converts the data into a series.

#print(pokemon.head())
pokemon.sort_values()

#   sort_values() -> This method sort the values in ass
#print(pokemon.sort_values().head())
15/1: x = "$121,121"
15/2:
x = "$121,121"

print(int(x))
15/3:
x = "$121,121"

y = str()

y = x.split("$")
print(y)
15/4:
x = "$121,121"

y = str()

y = x.split("$,")
print(y)
15/5:
x = "$121,121"

y = str()

y = x.split("$")
y = y.split(",")
print(y)
15/6:
x = "$121,121"

y = str()

y = x.split("$")
y = y[1].split(",")
print(y)
15/7:
x = "$121,121"

y = str()

y = x.split("$")
y = ",".join(y[1])
print(y)
15/8:
x = "$121,121"

y = str()

y = x.split("$")
y = ",".join(y)
print(y)
15/9:
x = "$121,121"

y = str()

y = x.split("$")
y = "".join(y[1])
print(y)
15/10:
x = "$121,121"

y = str()

y = x.split("$")
y = ",".join(y[1])
print(y)
15/11:
x = "$121,121"

y = str()

y = x.split("$")
y = "".join(y[1])
print(y)
15/12:
x = "$121,121"

y = str()

y = x.split("$")
print(type(y))
y = "".join(y[1])
print(y)
15/13:
x = "$121,121,545"

y = str()

y = x.split("$")
y = y.split(y[1])
print(type(y))
y = "".join(y[1])
print(y)
15/14:
x = "$121,121,545"

y = str()

y = x.split("$")
y = y[1].split(",")
print(type(y))
y = "".join(y[1])
print(y)
15/15:
x = "$121,121,545"

y = str()

y = x.split("$")
y = y[1].split(",")
print(type(y))
y = "".join(y)
print(y)
15/16:
x = "$121,121,545"

y = str()

y = x.split("$")
y = y[1].split(",")
print(type(y))
y = "".join(y)
print(type(y))
15/17:
x = "$121,121,545"

y = str()

y = x.split("$")
y = y[1].split(",")
print(type(y))
y = "".join(y)
print(type(int(y)))
15/18:
x = "$121,121,545"

y = str()

y = x.split("$")
y = y[1].split(",")
print(type(y))
y = int("".join(y))
print(type(y))
15/19:
x = "$121,121,545"

y = str()

y = x.split("$")
y = y[1].split(",")

y = int("".join(y))
print(type(y))
15/20:
x = "$121,121,545"


y = x.split("$")
y = y[1].split(",")

y = int("".join(y))
print(type(y))
15/21:
x = "$121,121,545"


y = x.split("$")
y = y[1].split(",")

y = int("".join(y))
y = y*2
print(y)
print(type(y))
15/22:
x = "$121,121,545"


y = x.split("$")
y = y[1].split(",")

y = int("".join(y))
print(type(y))
16/1:
days = int(input("Enter number of days"))
year = days//365
days =days%365
month = days//30
week = days//7
days = days%7
print(year," Year",month," Months",week," Week",days," Days")
16/2:
days = int(input("Enter number of days"))
year = days//365
days =days%365
month = days//30
week = days//7
days = days%7
print(year," Year",month," Months",week," Week",days," Days")
16/3: 500//365
16/4: 500%365
16/5: 500%365
16/6: 500%365
16/7:
days = int(input("Enter number of days"))
year = days//365
days = days%365
month = days//30
week = days//7
days = days%7
print(year," Year",month," Months",week," Week",days," Days")
16/8: 500%365
16/9:
days = int(input("Enter number of days"))
year = days//365
days = days%365
month = days//30
week = days//7
days = days%7
print(year," Year",month," Months",week," Week",days," Days")
16/10:
days = int(input("Enter number of days"))
year = days//365
days = days%365
month = days//30
week = days//7
days = days%7
print(year," Year",month," Months",week," Week",days," Days")
16/11: 500%365
16/12: 5%5
16/13: 5/5
16/14: 4/5
16/15: 6/5
16/16:
if 5/5 >5//5:
    true
else:
    false
16/17:
if 5/5 >5//5:
    print("true")
else:
    print("false")
16/18:
if 6/5 >6//5:
    print("true")
else:
    print("false")
16/19:
import math

math.abs(-12)
16/20:
import math

abs(-12)
16/21:
import math

math.floor(9/2)
16/22: import pandas as pd
16/23:
import pandas as pd

pokemon = pd.read_csv("F:\\Pythn prgramming practice\\Machine Learning (ML)\\Data_Reading(Panda)\\pokemon.csv")

pokemon.info
16/24:
import pandas as pd

pokemon = pd.read_csv("F:\\Pythn prgramming practice\\Machine Learning (ML)\\Data_Reading(Panda)\\pokemon.csv")

pokemon.info()
16/25: pokemon.get_dtype_counts()
16/26:
a=[1,2,3]
a.insert(1,5)
a
16/27:
a=[]
a.insert(1,5)
a
16/28: hash(2)
16/29: hash(1,2)
16/30: hash(2./)
16/31: hash(2,/)
16/32:
a=2
b=3
c="**"
acb
16/33:
a=2
b=3
c="**"
a*c*b
16/34:
a=2
b=3
c="**"
a**b
16/35:
l = [[10,2,5],[7,1,0],[9,9,9],[1,23,12],[6,5,9]]
l.sort()
16/36:
l = [[10,2,5],[7,1,0],[9,9,9],[1,23,12],[6,5,9]]
l.sort()
l
16/37:
l = [[10,2,5],[7,1,0],[9,9,9],[1,23,12],[6,5,9]]
l.sort(key = 1)
l
16/38:
l = [[10,2,5],[7,1,0],[9,9,9],[1,23,12],[6,5,9]]
l.sort()
l
16/39: SUBSTR(SQUARE ANS ALWAYS WORK HARD,14,6)
18/1:
import pandas as pd
import matplotlib.pyplot as plt
18/2:
import pandas as pd
import matplotlib.pyplot as plt
18/3:
arr = [[1,2,3],[4,5,6]]
max(arr)
18/4:
arr = [[1,2,3],[4,5,6]]
max(arr[1][])
18/5:
arr = [[1,2,3],[4,5,6]]
max(arr[1])
19/1: data = pd.read_csv("employ.csv")
19/2:
import pandas as pd
import matplotlib.pyplot as plt
19/3: data = pd.read_csv("employ.csv")
19/4:
data = pd.read_csv("employ.csv")
data.head()
19/5:
real_x = data.iloc[:0]values
real_x
19/6:
real_x = data.iloc[:,0]values
real_x
19/7:
real_x = data.iloc[:,0]values
real_x
19/8:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
19/9:
real_x = data.iloc[:,0]values
real_x
19/10:
real_x = data.iloc[:,0].values
real_x
19/11:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
19/12:
real_x = data.iloc[:,0].values
real_y = data.iloc[:,1].values.reshape()
real_y
19/13:
real_x = data.iloc[:,0].values
real_y = data.iloc[:,1].values.reshape
real_y
19/14:
real_x = data.iloc[:,0].values
real_y = data.iloc[:,1].values.reshape
real_y
19/15: training_x,testing_x,training_y,testing_y = train_test_split(real_x, real_y, test_size = 0.3, random_state = 0)
19/16:
real_x = data.iloc[:,0].values
real_y = data.iloc[:,1].values
real_y
19/17: training_x,testing_x,training_y,testing_y = train_test_split(real_x, real_y, test_size = 0.3, random_state = 0)
19/18:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
19/19:
lin = LinearRegression()
lin.fit(training_x,training_y)
19/20:
real_x = data.iloc[:,0].values
real_y = data.iloc[:,1].values
real_x = real_x.reshape(-1,1)
real_y = real_y.reshape(-1,1)
real_y
19/21:
real_x = data.iloc[:,0].values
real_y = data.iloc[:,1].values
real_x = real_x.reshape(-1,1)
real_y = real_y.reshape(1,1)
real_y
19/22:
real_x = data.iloc[:,0].values
real_y = data.iloc[:,1].values
real_x = real_x.reshape(-1,1)
real_y = real_y.reshape(0,1)
real_y
19/23:
real_x = data.iloc[:,0].values
real_y = data.iloc[:,1].values
real_x = real_x.reshape(-1,1)
real_y = real_y.reshape(2,1)
real_y
19/24:
real_x = data.iloc[:,0].values
real_y = data.iloc[:,1].values
real_x = real_x.reshape(-1,1)
real_y = real_y.reshape(-2,1)
real_y
19/25:
real_x = data.iloc[:,0].values
real_y = data.iloc[:,1].values
real_x = real_x.reshape(-1,1)
real_y = real_y.reshape(-3,1)
real_y
19/26:
real_x = data.iloc[:,0].values
real_y = data.iloc[:,1].values
real_x = real_x.reshape(-1,1)
real_y = real_y.reshape(-1,1)
real_y
19/27:
lin = LinearRegression()
lin.fit(training_x,training_y)
19/28:
lin = LinearRegression()
lin.fit(training_x,training_y)
19/29:
real_x = data.iloc[:,0].values
real_y = data.iloc[:,1].values
real_x = real_x.reshape(-1,1)
real_y = real_y.reshape(-1,1)
19/30:
lin = LinearRegression()
lin.fit(training_x,training_y)
19/31: training_x,testing_x,training_y,testing_y = train_test_split(real_x, real_y, test_size = 0.3, random_state = 0)
19/32:
lin = LinearRegression()
lin.fit(training_x,training_y)
19/33: predict_y = lin.predict(testing_x)
19/34: testing_y[3]
19/35: testing_y[3]
19/36: testing_y[0]
19/37: testing_y[1]
19/38:
data = pd.read_csv("employ.csv")
data.head(10)
19/39: testing_y[2]
19/40: testing_y[3]
19/41: testing_y[2]
19/42: testing_y[-1]
19/43: testing_y[-2]
19/44: testing_y[2]
19/45:
training_x,testing_x,training_y,testing_y = train_test_split(real_x, real_y, test_size = 0.3, random_state = 0)
testing_y
19/46: testing_y[2]
19/47:
predict_y = lin.predict(testing_x)
predict_y
19/48: predict_y[2]
19/49: predict_y(39891)
19/50: predict_y[(39891)]
19/51: predict_y[39891]
19/52: predict_y[2]
21/1: ##PRICE PREDICTOR
21/2:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
21/3:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
21/4:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
21/5: arr = np.array([i for i in range(11)])
21/6:
arr = np.array([i for i in range(11)])
arr
21/7:
arr = np.array([i for i in range(11)])
arr1 = np.array([2*i for i in range(11)])
21/8:
arr = np.array([i for i in range(11)])
arr1 = np.array([2*i for i in range(11)])
arr
arr1
21/9: arr
21/10:
a = np.sum(y*x)
a
21/11:
a = np.sum(arr*arr1)
a
21/12: arr
21/13:
a = np.sum(arr*arr1)
n = np.size(arr)
n
21/14:
predict = -0.0586206896552 + 1.45747126437*x
predict
21/15:
predict = -0.0586206896552 + 1.45747126437*arr
predict
21/16:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
22/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
22/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
22/3: data = pd.read_csv("Book1.csv")
22/4:
data = pd.read_csv("Book1.csv")
data.head(5)
22/5: arr = np.array(data.PovPct)
22/6:
arr = np.array(data.PovPct)
arr
22/7: arr1 = np.array(Brth15to17)
22/8: arr1 = np.array(data.Brth15to17)
22/9:
arr1 = np.array(data.Brth15to17)
arr1
22/10:
plt.scatter(arr,arr1,c="m",label = "Scatter plot")
plt.xlabel("PovPct")
plt.ylabel("Brth15to17")
plt.legend()
plt.show()
22/11:
# READING DATA
data = pd.read_csv("Book1.csv")
print(data.shape)
data.head(5)
22/12:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)

# Total numbers of values
n = len(x)

# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
22/13:
# COLLECTING X, Y
x = np.array(data.PovPct)
y = np.array(data.Brth15to17)
22/14:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)

# Total numbers of values
n = len(x)

# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
22/15:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)

# Total numbers of values
n = len(x)

# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
22/16:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)

# Total numbers of values
n = len(x)

# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
    
numerator
22/17:
# COLLECTING X, Y
x = np.array(data.PovPct)
y = np.array(data.Brth15to17)

x
22/18:
# COLLECTING X, Y
x = np.array(data.PovPct)
y = np.array(data.Brth15to17)

print(x)
y
22/19:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)

# Total numbers of values
n = len(x)
print(n)

# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
    
numerator
22/20:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)

# Total numbers of values
n = len(x)
print(n)

# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n-1):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
    
numerator
22/21:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)

# Total numbers of values
n = len(x)
print(n)

# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n-1):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
    
numerator
22/22:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)

# Total numbers of values
n = len(x)
print(n-1)

# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n-1):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
    
numerator
22/23:
# COLLECTING X, Y
x = data["PovPct"].values
y = data["Brth15to17"].values

print(x)
y
22/24:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)

# Total numbers of values
n = len(x)
print(n-1)

# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n-1):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
    
numerator
22/25:
# READING DATA
data = pd.read_csv("Book1.csv")
print(data.shape)
data.head(5)
22/26:
# COLLECTING X, Y
x = data["PovPct"].values
y = data["Brth15to17"].values

print(x)
y
22/27:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)

# Total numbers of values
n = len(x)
print(n-1)

# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n-1):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
    
numerator
22/28:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)

print(mean_x,mean_y)
# Total numbers of values
n = len(x)
print(n-1)

# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n-1):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
    
numerator
22/29:
# COLLECTING X, Y
x = data["PovPct"].values[:-2]
y = data["Brth15to17"].values[:-2]

print(x)
y
22/30:
# COLLECTING X, Y
x = data["PovPct"].values[:-1]
y = data["Brth15to17"].values[:-2]

print(x)
y
22/31:
# COLLECTING X, Y
x = data["PovPct"].values[:-1]
y = data["Brth15to17"].values[:-1]

print(x)
y
22/32:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)

print(mean_x,mean_y)
# Total numbers of values
n = len(x)
print(n-1)

# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n-1):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
    
numerator
22/33:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)


# Total numbers of values
n = len(x)
print(n-1)

# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n-1):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
22/34:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)


# Total numbers of values
n = len(x)


# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
22/35:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)


# Total numbers of values
n = len(x)


# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
    
m = numerator//denominator
c = mean_y - m*mean_x

print(m,c)
22/36:
# Mean of X, Y
mean_x = np.mean(x)
mean_y = np.mean(y)


# Total numbers of values
n = len(x)


# Using the formula to calculate m and c
numerator = 0
denominator = 0

for i in range(n):
    numerator += (x[i]-mean_x)*(y[i]-mean_y)
    denominator += (x[i]-mean_x)**2
    
m = numerator/denominator
c = mean_y - m*mean_x

print(m,c)
22/37:
Y = m*x + c

plt.plot(x,Y,c="g",label = "Regration linr")

plt.scatter(arr,arr1,c="m",label = "Scatter plot")
plt.xlabel("PovPct")
plt.ylabel("Brth15to17")
plt.legend()
plt.show()
22/38:
# Calculating the R-squared value

ss_t = 0       # Total sum of squares(y-mean_y)**2
ss_r = 0       # Total sum of reseduals(y-predicted - mean_y)**2

for i in range(n):
    y_predict = m*x[i] + c
    ss_t += (Y[i] - mean_y)**2
    ss_r += (Y[i] - y_predict)**2
r2 = 1 - (ss_r/ss_t)
print(r2)
22/39:
# Calculating the R-squared value

ss_t = 0       # Total sum of squares(y-mean_y)**2
ss_r = 0       # Total sum of reseduals(y-predicted - mean_y)**2

for i in range(n):
    y_predict = m*x[i] + c
    ss_t += (Y[i] - mean_y)**2
    ss_r += (Y[i] - y_predict)**2
r2 =(ss_r/ss_t)
print(r2)
22/40:
# Calculating the R-squared value

ss_t = 0       # Total sum of squares(y-mean_y)**2
ss_r = 0       # Total sum of reseduals(y-predicted - mean_y)**2

for i in range(n):
    y_predict = m*x[i] + c
    ss_t += (Y[i] - mean_y)**2
    ss_r += (Y[i] - y_predict)**2
print(ss_t,ss_r)
r2 = (ss_r/ss_t)
print(r2)
22/41:
x = x.reshape((m,1))
x
22/42:
#x = x.reshape((m,1))
x
22/43:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
22/44:
#x = x.reshape((m,1))
reg = LinearRegression()
reg = reg.fit(x,y)
Y_predict = reg.predict(x)

mse = mean_squared_error(x,Y_predict)
rsme = np.sqrt(mse)
r2 _score = reg.score(x,y)
r2_score
22/45:
#x = x.reshape((m,1))
reg = LinearRegression()
reg = reg.fit(x,y)
Y_predict = reg.predict(x)

mse = mean_squared_error(x,Y_predict)
rsme = np.sqrt(mse)
r2_score = reg.score(x,y)
r2_score
22/46:
x = x.reshape((-1,1))
reg = LinearRegression()
reg = reg.fit(x,y)
Y_predict = reg.predict(x)

mse = mean_squared_error(x,Y_predict)
rsme = np.sqrt(mse)
r2_score = reg.score(x,y)
r2_score
22/47:
x = x.reshape((1,-1))
reg = LinearRegression()
reg = reg.fit(x,y)
Y_predict = reg.predict(x)

mse = mean_squared_error(x,Y_predict)
rsme = np.sqrt(mse)
r2_score = reg.score(x,y)
r2_score
22/48:
x = x.reshape(1,-1)
reg = LinearRegression()
reg = reg.fit(x,y)
Y_predict = reg.predict(x)

mse = mean_squared_error(x,Y_predict)
rsme = np.sqrt(mse)
r2_score = reg.score(x,y)
r2_score
22/49:
x = x.reshape(-1,-1)
reg = LinearRegression()
reg = reg.fit(x,y)
Y_predict = reg.predict(x)

mse = mean_squared_error(x,Y_predict)
rsme = np.sqrt(mse)
r2_score = reg.score(x,y)
r2_score
22/50:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
Y_predict = reg.predict(x)

mse = mean_squared_error(x,Y_predict)
rsme = np.sqrt(mse)
r2_score = reg.score(x,y)
r2_score
22/51:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
22/52:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
22/53:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
reg.predict(20.1)
22/54:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
reg.predict([20.1])
22/55:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
reg.predict(31.5)
22/56:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
reg.predict[20.1]
22/57:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
reg.predict[[20.1]
22/58:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
reg.predict[[20.1]]
22/59:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
reg.predict([20.1])
22/60:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
reg.predict([[20.1]])
22/61:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
p = reg.predict(data.PovPct)
data["predicted"] = p
data.head(10)
22/62:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
p = reg.predict(data["PovPct"].values[:-1])
data["predicted"] = p
data.head(10)
22/63:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
p = reg.predict(data[["PovPct"]].values[:-1])
data["predicted"] = p
data.head(10)
22/64:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
p = reg.predict(data[["PovPct"]].values)
data["predicted"] = p
data.head(10)
22/65:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
p = reg.predict(data[["PovPct"]].values[:-1])
data["predicted"] = p
data.head(10)
22/66:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
p = reg.predict(data[["PovPct"]].values[:-2])
data["predicted"] = p
data.head(10)
22/67:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
p = reg.predict(data[["PovPct"]].values!=nan)
data["predicted"] = p
data.head(10)
22/68:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
p = reg.predict(data[["PovPct"]].values!="nan")
data["predicted"] = p
data.head(10)
22/69:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
p = reg.predict((data[["PovPct"]].values)!="nan")
data["predicted"] = p
data.head(10)
22/70:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
p = reg.predict(x)
data["predicted"] = p
data.head(10)
22/71:
# COLLECTING X, Y
x = data["PovPct"].values[:-1]
y = data["Brth15to17"].values[:-1]

print(x)
y
22/72:
# READING DATA
data = pd.read_csv("Book1.csv")
print(data.shape)
data.head(5)
22/73:
# COLLECTING X, Y
x = data["PovPct"].values
y = data["Brth15to17"].values

print(x)
y
22/74:
# COLLECTING X, Y
x = data["PovPct"].values
y = data["Brth15to17"].values

print(x)
y
22/75:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
p = reg.predict(x)
data["predicted"] = p
data.head(10)
22/76:
# READING DATA
data = pd.read_csv("Book1.csv")
print(data.shape)
data.head(5)
22/77:
# COLLECTING X, Y
x = data["PovPct"].values
y = data["Brth15to17"].values

print(x)
y
22/78:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
p = reg.predict(x)
data["predicted"] = p
data.head(10)
22/79:
x = x.reshape(-1,1)
reg = LinearRegression()
reg = reg.fit(x,y)
reg.coef_
reg.intercept_
p = reg.predict(x)
data["predicted"] = p
data.head(10)
22/80:
# Calculating the R-squared value

ss_t = 0       # Total sum of squares(y-mean_y)**2
ss_r = 0       # Total sum of reseduals(y-predicted - mean_y)**2

for i in range(n):
    y_predict = m*x[i] + c
    ss_t += (Y[i] - mean_y)**2
    ss_r += (Y[i] - y_predict)**2

r2 = (ss_r/ss_t)
print(r2)
24/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
24/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
24/3:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
24/4:
aar = [1,2,3,4,5,6,7]
aar.reshape(1,-1)
arr
24/5:
aar = [1,2,3,4,5,6,7]
aar.reshape(-1,1)
arr
24/6:
aar = [1,2,3,4,5,6,7]
arr = np.array(aar)
arr
24/7:
aar = [1,2,3,4,5,6,7]
arr = np.array(aar)
arr
arr.reshape(1,-1)
arr
24/8:
aar = [1,2,3,4,5,6,7]
arr = np.array(aar)
arr
arr.reshape(-1,-1)
arr
24/9:
aar = [1,2,3,4,5,6,7]
arr = np.array(aar)
arr
arr.reshape(-1,1)
arr
24/10:
aar = [1,2,3,4,5,6,7]
arr = np.array(aar)
arr
arr.reshape(-1,1)
arr
24/11:
aar = [1,2,3,4,5,6,7]
arr = np.array(aar)
arr
arr.reshape(-1,1)
arr
24/12:
aar = [1,2,3,4,5,6,7]
arr = np.array(aar)
arr
arr.reshape(1,1)
arr
24/13:
aar = [1,2,3,4,5,6,7]
arr = np.array(aar)
arr
arr.reshape(1,-1)
arr
24/14:
aar = [1,2,3,4,5,6,7]
arr = np.array(aar)
arr
arr.reshape(1,-1)
arr
24/15:
data = pd.read_csv("Foodtruck.csv")
data.head(10)
24/16: data.describe()
24/17: data.shape
24/18:
data = pd.read_csv("Foodtruck.csv")
data.head(5)
24/19:
p = data["Population"].values
p
24/20:
p = data["Population"].values
p = p.reshape(1,-1)
p
24/21:
p = data["Population"].values
p = p.reshape(1,-1)
p
24/22:
p = data["Population"].values
p = p.reshape(-1,1)
p
24/23:
p = data["Population"].values.reshape(-1,1)
p
24/24:
x = data["population"].values.reshape(-1,1)
y = data["Profit"].values.reshape(-1,1)
y
24/25:
x = data["Population"].values.reshape(-1,1)
y = data["Profit"].values.reshape(-1,1)
y
24/26:
x = data["Population"].values.reshape(-1,1)
y = data["Profit"].values.reshape(-1,1)
x
24/27:
x = data["Population"].values.reshape(-1,1)
y = data["Profit"].values.reshape(-1,1)
24/28:
plt.scatter(x,y,c="g",marker="o",label = "datapoints")
plt.xlabel("Population")
plt.ylabel("Profit")
plt.legend()
plt.show()
24/29:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.matrics import mean_squared_error
24/30:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
24/31:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
24/32: training_x,testing_x,training_y,testing_y = train_test_split(x, y, test_size = 0.3, random_state = 0)
24/33:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
24/34: training_x,testing_x,training_y,testing_y = train_test_split(x, y, test_size = 0.3, random_state = 0)
24/35:
reg = LinearRegression()
reg.fit(training_x,training_y)
24/36: y_predict = model.predict(testing_x)
24/37: y_predict = reg.predict(testing_x)
24/38: y_predict = reg.predict(testing_x)
24/39:
y_predict = reg.predict(testing_x)
y_predict
24/40: y_predict = reg.predict(testing_x)
24/41: testing_y[3]
24/42: y_predict[3]
24/43:
y_predict = reg.predict(testing_x)
mse = mean_squared_error(testing_y,y_predicted)
mse
24/44:
y_predict = reg.predict(testing_x)
mse = mean_squared_error(testing_y,y_predict)
mse
24/45:
y_predict = reg.predict(testing_x)
mse = mean_squared_error(testing_y,y_predict)
print("Mean Squared error = ",mse)
print("Coefficient = ",reg.coef)
print("Intercept = ",reg.intercept)
24/46:
y_predict = reg.predict(testing_x)
mse = mean_squared_error(testing_y,y_predict)
print("Mean Squared error = ",mse)
print("Coefficient = ",reg.coef_)
print("Intercept = ",reg.intercept_)
24/47:
plt.scatter(testing_x,testing_y,c="g",marker="o",label = "datapoints")
plt.xlabel("Population")
plt.ylabel("Profit")
plt.legend()
plt.show()
24/48:
plt.scatter(testing_x,testing_y,c="g",marker="o",label = "datapoints")
plt.plot(testing_x,y_predict,c="m",label = "regration")
plt.xlabel("Population")
plt.ylabel("Profit")
plt.legend()
plt.show()
24/49:
plt.scatter(testing_x,testing_y,c="g",marker="o",label = "datapoints")
plt.scatter(testing_x,y_predict,c="m",label = "regration")
plt.xlabel("Population")
plt.ylabel("Profit")
plt.legend()
plt.show()
24/50:
plt.scatter(testing_x,testing_y,c="g",marker="o",label = "datapoints")
plt.plot(testing_x,y_predict,c="m",label = "regration")
plt.scatter(testing_x,y_predict,c="r")
plt.xlabel("Population")
plt.ylabel("Profit")
plt.legend()
plt.show()
25/1:
arr[] = {4,0,2,1,3}
for i in range(0,len(arr)):
    arr = arr[arr[i]]
    
arr
25/2:
arr = {4,0,2,1,3}
for i in range(0,len(arr)):
    arr = arr[arr[i]]
    
arr
25/3:
arr = [4,0,2,1,3]

for i in range(0,len(arr)):
    arr = arr[arr[i]]
    
arr
25/4:
arr = [4,0,2,1,3]

for i in range(0,len(arr)):
    arr[i] = arr[arr[i]]
    
arr
25/5:
arr = [4,0,2,1,3]
arr1

for i in range(0,len(arr)):
    arr1.append(arr[arr[i]])
    
arr1
25/6:
arr = [4,0,2,1,3]
arr1=[]

for i in range(0,len(arr)):
    arr1.append(arr[arr[i]])
    
arr1
25/7:
arr = [4,0,2,1,3]
arr1=[]

for i in range(0,len(arr)):
    arr1.append(arr[arr[i]])
    
arr = arr1
arr
25/8:
arr = [4,0,2,1,3]
arr1=[]

for i in range(0,len(arr)):
    arr1.append(arr[arr[i]])
    
arr = arr1
arr
25/9:
arr = [1,0]
arr1=[]

for i in range(0,len(arr)):
    arr1.append(arr[arr[i]])
    
arr = arr1
arr
25/10:
arr = [1,0]
arr1=[]

for i in range(0,len(arr)):
    arr1.append(arr[arr[i]])
    
arr = arr1
arr
25/11: arr = [int(x) for x in input().strip().split()]
25/12: arr
25/13:
arr
arr1=[]

for i in range(0,len(arr)):
    arr1.append(arr[arr[i]])
    
arr = arr1
arr
25/14:
arr
arr1=[]

for i in range(len(arr)):
    arr1.append(arr[arr[i]])
    
arr = arr1
arr
25/15:
arr
arr1=[]

for i in range(len(arr)):
    arr1.append(arr[arr[i]])
    
arr = arr1
arr
25/16:
arr
arr1=[]

for i in range(len(arr)):
    arr1.append(arr[arr[i]])
    
arr = arr1
arr
26/1:
arr = [1,0]
arr1=[]

for i in range(0,len(arr)):
    arr1.append(arr[arr[i]])
    
arr = arr1
arr
26/2: arr = [int(x) for x in input().strip().split()]
26/3:
arr
arr1=[]

for i in range(len(arr)):
    arr1.append(arr[arr[i]])
    
arr = arr1
arr
26/4:
def arrange(arr,n):
    m=[]
    for i in range(n):
        m.append(arr[arr[i]])
    arr=m
t=int(input())
while(t>0):
    n=int(input())
    arr=[int(x) for x in input().strip().split()]
    arrange(arr,n)
    for i in arr:
        print(i,end=" ")
    print()
    t=-1
26/5:
def arrange(arr,n):
    m=[]
    for i in range(n):
        m.append(arr[arr[i]])
    arr=m

t=int(input())
while(t>0):
    n=int(input())
    arr=[int(x) for x in input().strip().split()]
    arrange(arr,n)
    for i in arr:
        print(i,end=" ")
    print()
    t=-1
26/6:
def arrange(arr,n):
    m=[]
    for i in range(n):
        m.append(arr[arr[i]])
    arr=m
    return arr

t=int(input())
while(t>0):
    n=int(input())
    arr=[int(x) for x in input().strip().split()]
    arrange(arr,n)
    for i in arr:
        print(i,end=" ")
    print()
    t=-1
26/7:
def arrange(arr,n):
    m=[]
    for i in range(n):
        m.append(arr[arr[i]])
    arr[:]=m[:]
    return arr

t=int(input())
while(t>0):
    n=int(input())
    arr=[int(x) for x in input().strip().split()]
    arrange(arr,n)
    for i in arr:
        print(i,end=" ")
    print()
    t=-1
26/8:
def arrange(arr,n):
    m=[]
    for i in range(n):
        m.append(arr[arr[i]])
    arr[:]=m[:]
  

t=int(input())
while(t>0):
    n=int(input())
    arr=[int(x) for x in input().strip().split()]
    arrange(arr,n)
    for i in arr:
        print(i,end=" ")
    print()
    t=-1
26/9:
arr = [1,0]
arr1=[]

for i in range(0,len(arr)):
    arr.append(arr[arr[i]])
    

arr
26/10:
arr = [1,2]
arr1 = [2,1]
arr = arr1
arr
27/1:
arr = [1,0]
arr1=[]

for i in range(0,len(arr)):
    arr.append(arr[arr[i]])
    

arr
27/2:
arr = [1,0]
arr1=[]

for i in range(0,len(arr)):
    arr.append(arr[arr[i]])
    

arr[:] = arr[n:]
arr
27/3:
arr = [1,0]
arr1=[]

for i in range(0,len(arr)):
    arr.append(arr[arr[i]])
    

arr[:] = arr[2:]
arr
30/1:
arr = [1,2,3,3]
arr.counts()
30/2:
arr = [1,2,3,3]
arr.count()
30/3:
arr = [1,2,3,3]
count(arr)
30/4:
arr = [1,2,3,3]
arr.count(3)
30/5:
arr = ["1","2","3","3"]
arr.type()
30/6:
arr = ["1","2","3","3"]
type(arr)
30/7:
arr = ["1","2","3","3"]
type(arr[i])
30/8:
arr = ["1","2","3","3"]
type(arr[0])
30/9:
arr = ["1","2","3","3"]
arr = int(arr)
type(arr[0])
30/10:
arr = ["1","2","3","3"]
arr[:] = int(arr{:})
type(arr[0])
30/11:
arr = ["1","2","3","3"]
arr[:] = int(arr[:])
type(arr[0])
30/12:
arr = ["1","2","3","3"]
arr = int(arr[i] for i in range(len(arr)))
type(arr[0])
30/13:
arr = ["1","2","3","3"]
arr = int(arr[i] for i in range(len(arr)))
type(arr[0])
30/14:
arr = ["1","2","3","3"]
arr = int(arr[i] for i in range(len(4)))
type(arr[0])
30/15:
arr = ["1","2","3","3"]
arr = int(i for i in range(len(4)))
type(arr[0])
30/16:
arr = ["1","2","3","3"]
arr = int(i for i in arr)
type(arr[0])
30/17:
arr = ["1","2","3","3"]
arr = (int(i) for i in arr)
type(arr[0])
30/18:
arr = ["1","2","3","3"]
arr[] = (int(i) for i in arr)
type(arr[0])
30/19:
arr = [0]*5
arr
30/20:
arr = ["1","3","2"]
sorted(arr)
arr
30/21:
arr = ["1","3","2"]
sorted(arr)
arr
30/22:
arr = ["1","3","2"]
sorted(arr)
arr
30/23:
arr = ["1","3","2"]
max(arr)
30/24:
i=1,j=0
    
while(j!=10):
    if((i%2==0) or (i%3==0) or (i%5==0) or (i=1)):
        j+=1
        i+=1
i
31/1: import pandas as pd
31/2: data = pd.read_csv("climate.csv")
31/3: data = pd.read_csv("climate.csv")
31/4: data = pd.read_csv("climate.csv")
31/5:
data = pd.read_csv("climate.csv")
data.head()
31/6:
data = pd.read_csv("climate.csv")
data.head()
31/7:
data = pd.read_csv("climate.csv")
data.head()
31/8:
data = pd.read_csv("climate.csv")
data.head()
31/9: import pandas as pd
31/10:
data = pd.read_csv("rainfall in india 1901-2015.csv")
data.head()
31/11:
data = pd.read_csv("rainfall in india 1901-2015.csv")
data.head()
data.info()
31/12:
data = pd.read_csv("rainfall in india 1901-2015.csv")
data.head()
data.describe()
31/13: data.hist(bins=50, figsize=(20,15))
31/14: data.hist(bins=10, figsize=(20,15))
31/15: data.hist(bins=20, figsize=(20,15))
31/16: data.hist(bins=5, figsize=(20,15))
31/17: data.hist(bins=15, figsize=(20,15))
31/18: data.bar()
31/19: data.plot()
34/1: import pandas as pd
34/2: housing = pd.read_csv("data.csv")
34/3: housing.head()
34/4: housing.info()
34/5: housing['CHAS'].value_counts()
34/6: housing.describe()
34/7: %matplotlib inline
34/8:
# # For plotting histogram
# import matplotlib.pyplot as plt
# housing.hist(bins=50, figsize=(20, 15))
34/9:
# For learning purpose
import numpy as np
def split_train_test(data, test_ratio):
    np.random.seed(42)
    shuffled = np.random.permutation(len(data))
    print(shuffled)
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled[:test_set_size]
    train_indices = shuffled[test_set_size:] 
    return data.iloc[train_indices], data.iloc[test_indices]
34/10: # train_set, test_set = split_train_test(housing, 0.2)
34/11: # print(f"Rows in train set: {len(train_set)}\nRows in test set: {len(test_set)}\n")
34/12:
from sklearn.model_selection import train_test_split
train_set, test_set  = train_test_split(housing, test_size=0.2, random_state=42)
print(f"Rows in train set: {len(train_set)}\nRows in test set: {len(test_set)}\n")
34/13:
from sklearn.model_selection import StratifiedShuffleSplit
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing['CHAS']):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]
34/14: strat_test_set['CHAS'].value_counts()
34/15: strat_train_set['CHAS'].value_counts()
34/16: # 95/7
34/17: # 376/28
34/18: housing = strat_train_set.copy()
34/19:
corr_matrix = housing.corr()
corr_matrix['MEDV'].sort_values(ascending=False)
34/20:
# from pandas.plotting import scatter_matrix
# attributes = ["MEDV", "RM", "ZN", "LSTAT"]
# scatter_matrix(housing[attributes], figsize = (12,8))
34/21: housing.plot(kind="scatter", x="RM", y="MEDV", alpha=0.8)
34/22: housing["TAXRM"] = housing['TAX']/housing['RM']
34/23: housing.head()
34/24:
corr_matrix = housing.corr()
corr_matrix['MEDV'].sort_values(ascending=False)
34/25: housing.plot(kind="scatter", x="TAXRM", y="MEDV", alpha=0.8)
34/26:
housing = strat_train_set.drop("MEDV", axis=1)
housing_labels = strat_train_set["MEDV"].copy()
34/27:
# To take care of missing attributes, you have three options:
#     1. Get rid of the missing data points
#     2. Get rid of the whole attribute
#     3. Set the value to some value(0, mean or median)
34/28:
a = housing.dropna(subset=["RM"]) #Option 1
a.shape
# Note that the original housing dataframe will remain unchanged
34/29:
housing.drop("RM", axis=1).shape # Option 2
# Note that there is no RM column and also note that the original housing dataframe will remain unchanged
34/30: median = housing["RM"].median() # Compute median for Option 3
34/31:
housing["RM"].fillna(median) # Option 3
# Note that the original housing dataframe will remain unchanged
34/32: housing.shape
34/33: housing.describe() # before we started filling missing attributes
34/34:
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="median")
imputer.fit(housing)
34/35: imputer.statistics_
34/36: X = imputer.transform(housing)
34/37: housing_tr = pd.DataFrame(X, columns=housing.columns)
34/38: housing_tr.describe()
34/39:
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
my_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="median")),
    #     ..... add as many as you want in your pipeline
    ('std_scaler', StandardScaler()),
])
34/40: housing_num_tr = my_pipeline.fit_transform(housing)
34/41: housing_num_tr.shape
34/42:
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
# model = LinearRegression()
# model = DecisionTreeRegressor()
model = RandomForestRegressor()
model.fit(housing_num_tr, housing_labels)
34/43: some_data = housing.iloc[:5]
34/44: some_labels = housing_labels.iloc[:5]
34/45: prepared_data = my_pipeline.transform(some_data)
34/46: model.predict(prepared_data)
34/47: list(some_labels)
34/48:
from sklearn.metrics import mean_squared_error
housing_predictions = model.predict(housing_num_tr)
mse = mean_squared_error(housing_labels, housing_predictions)
rmse = np.sqrt(mse)
34/49: rmse
34/50:
# 1 2 3 4 5 6 7 8 9 10
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, housing_num_tr, housing_labels, scoring="neg_mean_squared_error", cv=10)
rmse_scores = np.sqrt(-scores)
34/51: rmse_scores
34/52:
def print_scores(scores):
    print("Scores:", scores)
    print("Mean: ", scores.mean())
    print("Standard deviation: ", scores.std())
34/53: print_scores(rmse_scores)
34/54:
from joblib import dump, load
dump(model, 'Dragon.joblib')
34/55:
X_test = strat_test_set.drop("MEDV", axis=1)
Y_test = strat_test_set["MEDV"].copy()
X_test_prepared = my_pipeline.transform(X_test)
final_predictions = model.predict(X_test_prepared)
final_mse = mean_squared_error(Y_test, final_predictions)
final_rmse = np.sqrt(final_mse)
# print(final_predictions, list(Y_test))
34/56: final_rmse
34/57: prepared_data[0]
34/58:
from joblib import dump, load
import numpy as np
model = load('Dragon.joblib') 
features = np.array([[-5.43942006, 4.12628155, -1.6165014, -0.67288841, -1.42262747,
       -11.44443979304, -49.31238772,  7.61111401, -26.0016879 , -0.5778192 ,
       -0.97491834,  0.41164221, -66.86091034]])
model.predict(features)
36/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
36/2: data = pd.read_csv("Bihar.csv")
36/3:
data = pd.read_csv("Bihar.csv")
data.describe()
36/4: data.hist(bins=10, figsize=(20,15))
36/5: data.hist(bins=10, figsize=(1,15))
36/6: data.hist(bins=10, figsize=(10,15))
36/7: data.hist(bins=10, figsize=(100,15))
36/8: data.hist(bins=10, figsize=(50,15))
36/9: data.hist(bins=10, figsize=(20,15))
36/10: data.hist(bins=5, figsize=(20,15))
36/11: data.hist(xlabel=data["Year"],bins=5, figsize=(20,15))
36/12: data.hist(xlabel=data.["Year"],bins=5, figsize=(20,15))
36/13: data.hist(xlabelsize=data.["Year"],bins=5, figsize=(20,15))
36/14: data.hist(xlabelsize=data."Year",bins=5, figsize=(20,15))
36/15: data.hist(xlabelsize=data["YEAR"],bins=5, figsize=(20,15))
36/16: plt.plot(data)
36/17: plt.plot(data.["YEAR"])
36/18: plt.plot(data."YEAR")
36/19: plt.plot(data["YEAR"])
36/20: plt.plot(data["YEAR"],data["JAN"])
36/21: plt.plot(data["YEAR"],data["JAN","FEB"])
36/22: plt.plot(data["YEAR"],data["JAN"],data["FEB"])
36/23: plt.plot(data["YEAR"],data["JAN"])
36/24: data.labels()
36/25: data.label()
36/26: data.attributes()
36/27:

plt.plot(data["YEAR"],data[["JAN"],["FEB"]])
36/28:

plt.plot(data["YEAR"],data[["JAN","FEB"]])
36/29:

plt.plot(data["YEAR"],data[["JAN","FEB","MAR"]])
36/30: data.info()
36/31:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
36/32: train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)
36/33:
train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)
train_set
36/34:
train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)
len(train_set)
36/35:
train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)
print(len(train_set), len(test_set))
36/36:
train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)
print(len(train_set), len(test_set))
36/37:
corelation = data.corr()
corelation["YEAR"].sort_values(ascending=False)
36/38:
corelation = data.corr()
corelation["YEAR"].sort_values(ascending=False)
36/39:
attributes = ["YEAR", "ANNUAL", "Jan-Feb", "Mar-May", "Jun-Sep", "Oct-Dec"]
scatter_matrix(data[attributes], figsize = (12,8))
38/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
38/2:
data = pd.read_csv("Bihar.csv")
data.describe()
38/3:

plt.plot(data["YEAR"],data[["JAN","FEB","MAR"]])
38/4: data.info()
38/5:
train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)
print(len(train_set), len(test_set))
38/6:
corelation = data.corr()
corelation["YEAR"].sort_values(ascending=False)
38/7:
attributes = ["YEAR", "ANNUAL", "Jan-Feb", "Mar-May", "Jun-Sep", "Oct-Dec"]
scatter_matrix(data[attributes], figsize = (12,8))
38/8:
#attributes = ["YEAR", "ANNUAL", "Jan-Feb", "Mar-May", "Jun-Sep", "Oct-Dec"]
#scatter_matrix(data[attributes], figsize = (12,8))
38/9: data.plot(kind="scatter", x="YEAR", y="ANNUAL", alpha=0.8)
38/10: data.plot(kind="scatter", x="YEAR", y="June-Sep", alpha=0.8)
38/11: data.plot(kind="scatter", x="YEAR", y="Jun-Sep", alpha=0.8)
38/12: data.plot(kind="scatter", x="YEAR", y="Jun-Sep", alpha=0.1)
38/13: data.plot(kind="scatter", x="YEAR", y="Jun-Sep", alpha=0.8)
38/14:
attributes = ["YEAR", "ANNUAL", "Jan-Feb", "Mar-May", "Jun-Sep", "Oct-Dec"]
scatter_matrix(data[attributes], figsize = (12,8))
39/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
# model = LinearRegression()
# model = DecisionTreeRegressor()
model = RandomForestRegressor()
model.fit(housing_num_tr, housing_labels)
40/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
# model = LinearRegression()
# model = DecisionTreeRegressor()
40/2:
data = pd.read_csv("Bihar.csv")
data.describe()
40/3:

plt.plot(data["YEAR"],data[["JAN","FEB","MAR"]])
40/4: data.info()
40/5:
train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)
print(len(train_set), len(test_set))
40/6:
corelation = data.corr()
corelation["YEAR"].sort_values(ascending=False)
40/7:
#attributes = ["YEAR", "ANNUAL", "Jan-Feb", "Mar-May", "Jun-Sep", "Oct-Dec"]
#scatter_matrix(data[attributes], figsize = (12,8))
40/8: data.plot(kind="scatter", x="YEAR", y="Jun-Sep", alpha=0.8)
40/9:
data = pd.read_csv("Bihar.csv")
data.describe()
data_labels
40/10:
data = pd.read_csv("Bihar.csv")
data.describe()
data_labels()
41/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
# model = LinearRegression()
# model = DecisionTreeRegressor()
41/2:
data = pd.read_csv("Bihar.csv")
data.describe()
41/3:

plt.plot(data["YEAR"],data[["JAN","FEB","MAR"]])
41/4: data.info()
41/5:
train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 42)
print(len(train_set), len(test_set))
41/6:
corelation = data.corr()
corelation["YEAR"].sort_values(ascending=False)
41/7:
#attributes = ["YEAR", "ANNUAL", "Jan-Feb", "Mar-May", "Jun-Sep", "Oct-Dec"]
#scatter_matrix(data[attributes], figsize = (12,8))
41/8: data.plot(kind="scatter", x="YEAR", y="Jun-Sep", alpha=0.8)
41/9:

plt.plot(data["YEAR"],data[["JAN","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]])
41/10:

plt.plot(data["YEAR"],data[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]])
41/11:
corelation = data.corr()
corelation["ANNUAL"].sort_values(ascending=False)
41/12:
data = pd.read_csv("Bihar.csv")
data.head(10)
42/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
42/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
data.head()
data.describe()
42/3: data.hist(bins=15, figsize=(20,15))
42/4: data.plot(data.["YEAR"])
42/5:
data = pd.read_csv("rainfall in india 1901-2015.csv")
data.head()
data.info()
43/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
43/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
43/3:
data = data.fillna(np.median(data))
data.describe()
43/4:
data = data.fillna(np.mean(data))
data.describe()
43/5:
print(data.isnull().sum())
data = data.fillna(np.mean(data))
data.describe()
44/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
44/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
44/3:
print(data.isnull().sum())
data = data.fillna(np.mean(data))
data.describe()
44/4: #data.hist(bins=15, figsize=(20,15))
44/5: data.plot(data.["YEAR"])
45/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
45/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
45/3:
print(data.isnull().sum())
data = data.fillna(np.mean(data))
data.describe()
45/4: #data.hist(bins=15, figsize=(20,15))
45/5: data.plot(data.["YEAR"])
45/6:
print(data.isnull().sum())
data = data.fillna(np.median(data))
data.describe()
46/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
46/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
46/3:
print(data.isnull().sum())
data = data.fillna(np.median(data))
data.describe()
47/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
47/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
47/3:
print(data.isnull().sum())
median = data["JAN"].median()
data["JAN"].fillna(median)
print(data.isnull().sum)
47/4: #data.hist(bins=15, figsize=(20,15))
47/5: data.plot(data.["YEAR"])
48/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
48/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
48/3:
print(data.isnull().sum())
median = data["JAN"].median()
data["JAN"].fillna(median)
print(data.isnull().sum())
48/4: #data.hist(bins=15, figsize=(20,15))
48/5: data.plot(data.["YEAR"])
49/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
49/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
49/3:
print(data.isnull().sum())
median = data["JAN"].median()
print(median)
data["JAN"].fillna(median)
print(data.isnull().sum())
49/4: #data.hist(bins=15, figsize=(20,15))
49/5: data.plot(data.["YEAR"])
50/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
50/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
50/3:
print(data.isnull().sum())
median = data["JAN"].median()
print(median)
data = data["JAN"].fillna(median)
print(data.isnull().sum())
50/4: #data.hist(bins=15, figsize=(20,15))
50/5: data.plot(data.["YEAR"])
51/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
51/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
51/3:
print(data.isnull().sum())
impuret = SimpleImputer(strategy = "median")
imputer.fit(data)

print(data.isnull().sum())
52/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
52/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
52/3:
print(data.isnull().sum())
imputer = SimpleImputer(strategy = "median")
imputer.fit(data)

print(data.isnull().sum())
52/4:
print(data.isnull().sum())
imputer = SimpleImputer(strategy = "median")data[1:]
imputer.fit(data)

print(data.isnull().sum())
52/5:
print(data.isnull().sum())
imputer = SimpleImputer(strategy = "median").data[1:]
imputer.fit(data)

print(data.isnull().sum())
52/6:
print(data.isnull().sum())
imputer = SimpleImputer(strategy = "median")data[1:]
imputer.fit(data)

print(data.isnull().sum())
52/7:
print(data.isnull().sum())
imputer = SimpleImputer(strategy = "median")
imputer.fit(data)

print(data.isnull().sum())
52/8:
print(data.isnull().sum())
imputer = SimpleImputer(strategy = "mean")
imputer.fit(data)

print(data.isnull().sum())
52/9:
print(data.isnull().sum())
imputer = SimpleImputer(strategy = "mean",axis=1)
imputer.fit(data)

print(data.isnull().sum())
52/10:
print(data.isnull().sum())
imputer = SimpleImputer(strategy = "mean",axis=1)
imputer.fit(data["JAN"])

print(data.isnull().sum())
52/11:
print(data.isnull().sum())
imputer = SimpleImputer(strategy = "mean")
imputer.fit(data["JAN"])

print(data.isnull().sum())
52/12:
print(data.isnull().sum())
imputer = SimpleImputer(strategy = "mean")
imputer.fit(data[["JAN"]])

print(data.isnull().sum())
52/13:
print(data.isnull().sum())
imputer = SimpleImputer(strategy = "mean")
imputer.fit(data[["JAN"]])

print(data.isnull().sum())
52/14:
print(data.isnull().sum())
imputer = SimpleImputer(strategy = "mean")
imputer.fit(data)

print(data.isnull().sum())
52/15:
print(data.isnull().sum())
data = data.fillna.mean(data)
print(data.isnull().sum())
52/16:
print(data.isnull().sum())
data = data.fillna(np.mean(data))
print(data.isnull().sum())
52/17: data = data.fillna(np.mean(data))
52/18: #data.plot(data.["YEAR"])
52/19: y = data["YEAR"]
52/20:
y = data["YEAR"]
x = data[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
tain_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)
print(train_x,train_y,test_x,test_y)
52/21:
y = data["YEAR"]
x = data[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)
print(train_x,train_y,test_x,test_y)
52/22:
y = data["YEAR"]
x = data[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)
print(len(train_x),len(train_y),len(test_x),len(test_y))
52/23:
model = LinearRegression()
model.fit(train_x,train_y)
52/24:
some_data = data.iloc[:5]
some_data
52/25:
some_data = data.iloc[:5]
some_labels = train_y.iloc[:5]
some_labels
52/26:
y = data["ANNUAL"]
x = data[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)
#print(len(train_x),len(train_y),len(test_x),len(test_y))
52/27:
model = LinearRegression()
model.fit(train_x,train_y)
52/28:
some_data = data.iloc[:5]
some_labels = train_y.iloc[:5]
some_labels
52/29: predicted_data = model.predict(some_data)
52/30:
some_data = train_x.iloc[:5]
some_labels = train_y.iloc[:5]
52/31: predicted_data = model.predict(some_data)
52/32:
predicted_data = model.predict(some_data)
predicted_data
52/33: list(some_labels)
52/34:
predicted_anual = model.predict(train_x)
mse = mean_squared_error(train_y, predicted_anual)
rmse = np.sqrt(mse)
print(mse,rmse)
53/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
53/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
53/3: data = data.fillna(np.mean(data))
53/4: #data.hist(bins=15, figsize=(20,15))
53/5: #data.plot(data.["YEAR"])
53/6:
y = data["ANNUAL"]
x = data[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)
#print(len(train_x),len(train_y),len(test_x),len(test_y))
53/7:
model = LinearRegression()
model.fit(train_x,train_y)
53/8:
some_data = train_x.iloc[:5]
some_labels = train_y.iloc[:5]
53/9:
predicted_data = model.predict(some_data)
predicted_data
53/10: list(some_labels)
53/11:
predicted_anual = model.predict(train_x)
mse = mean_squared_error(train_y, predicted_anual)
rmse = np.sqrt(mse)
print(mse,rmse)
54/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
54/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
54/3: data = data.fillna(np.mean(data))
54/4: #data.hist(bins=15, figsize=(20,15))
54/5: #data.plot(data.["YEAR"])
54/6:
y = data["ANNUAL"]
x = data[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)
#print(len(train_x),len(train_y),len(test_x),len(test_y))
54/7:
model = LinearRegression()
model.fit(train_x,train_y)
54/8:
predicted_anual = model.predict(train_x)
mse = mean_squared_error(train_y, predicted_anual)
rmse = np.sqrt(mse)
print(mse,rmse)
54/9:
predicted_anual = model.predict(test_x)
mse = mean_squared_error(test_y, predicted_anual)
rmse = np.sqrt(mse)
print(mse,rmse)
mae = mean_absolute_error(test_y,predict_anual)
print(mae)
54/10:
predicted_anual = model.predict(test_x)
mse = mean_squared_error(test_y, predicted_anual)
rmse = np.sqrt(mse)
print(mse,rmse)
mae = mean_absolute_error(test_y,predicted_anual)
print(mae)
54/11:
predicted_anual = model.predict(test_x)
mse = mean_squared_error(test_y, predicted_anual)
rmse = np.sqrt(mse)
print(mse,rmse)
mae = mean_absolute_error(test_y,predicted_anual)
print(mae)
r2 = r2_score(test_y,predicted_anual)
print(r2)
55/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
55/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
55/3: data = data.fillna(np.mean(data))
55/4: #data.hist(bins=15, figsize=(20,15))
55/5: #data.plot(data.["YEAR"])
55/6:
y = data["ANNUAL"]
x = data[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)
#print(len(train_x),len(train_y),len(test_x),len(test_y))
55/7:
model = LinearRegression()
model.fit(train_x,train_y)
55/8:
predicted_anual = model.predict(test_x)
mse = mean_squared_error(test_y, predicted_anual)
rmse = np.sqrt(mse)
print(mse,rmse)
mae = mean_absolute_error(test_y,predicted_anual)
print(mae)
r2 = r2_score(test_y,predicted_anual)
print(r2)
56/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
56/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
56/3: data = data.fillna(np.mean(data))
56/4: #data.hist(bins=15, figsize=(20,15))
56/5: #data.plot(data.["YEAR"])
56/6:
y = data["ANNUAL"]
x = data[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)
#print(len(train_x),len(train_y),len(test_x),len(test_y))
56/7:
model = DecisionTreeRegressor()
model.fit(train_x,train_y)
56/8:
predicted_anual = model.predict(test_x)
mse = mean_squared_error(test_y, predicted_anual)
rmse = np.sqrt(mse)
print(mse,rmse)
mae = mean_absolute_error(test_y,predicted_anual)
print(mae)
r2 = r2_score(test_y,predicted_anual)
print(r2)
56/9:
# 1843.6557087291785 42.9378121092491
# 10.520464918764377
# 0.9978930199054238
57/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
57/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
57/3: data = data.fillna(np.mean(data))
57/4: #data.hist(bins=15, figsize=(20,15))
57/5: #data.plot(data.["YEAR"])
57/6:
y = data["ANNUAL"]
x = data[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)
#print(len(train_x),len(train_y),len(test_x),len(test_y))
57/7:
model = RandomForestRegressor()
model.fit(train_x,train_y)
57/8:
predicted_anual = model.predict(test_x)
mse = mean_squared_error(test_y, predicted_anual)
rmse = np.sqrt(mse)
print(mse,rmse)
mae = mean_absolute_error(test_y,predicted_anual)
print(mae)
r2 = r2_score(test_y,predicted_anual)
print(r2)
57/9:
# 1843.6557087291785 42.9378121092491
# 10.520464918764377
# 0.9978930199054238

# 8353.710373708016 91.39863441927355
# 56.11648325302063
# 0.9904531516432695
57/10: plt.scatter(predicted_anual,test_y)
57/11: plt.plot(predicted_anual,test_y)
58/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
58/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
58/3: data = data.fillna(np.mean(data))
58/4: #data.hist(bins=15, figsize=(20,15))
58/5: #data.plot(data.["YEAR"])
58/6:
y = data["ANNUAL"]
x = data[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)
#print(len(train_x),len(train_y),len(test_x),len(test_y))
58/7:
model = LinearRegression()
model.fit(train_x,train_y)
58/8:
predicted_anual = model.predict(test_x)
mse = mean_squared_error(test_y, predicted_anual)
rmse = np.sqrt(mse)
print(mse,rmse)
mae = mean_absolute_error(test_y,predicted_anual)
print(mae)
r2 = r2_score(test_y,predicted_anual)
print(r2)
58/9:
# 1843.6557087291785 42.9378121092491
# 10.520464918764377
# 0.9978930199054238

# 8353.710373708016 91.39863441927355
# 56.11648325302063
# 0.9904531516432695

# 4668.890150852565 68.32927740619364
# 34.12003601668758
# 0.9946642648271947
58/10: plt.scatter(predicted_anual,test_y)
58/11: plt.plot(predicted_anual,test_y)
59/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
59/2:
data = pd.read_csv("rainfall in india 1901-2015.csv")
print("\nDiscription\n")
print(data.describe())
data.info()
59/3: data = data.fillna(np.mean(data))
59/4: #data.hist(bins=15, figsize=(20,15))
59/5: #data.plot(data.["YEAR"])
59/6:
y = data["ANNUAL"]
x = data[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.2, random_state = 42)
#print(len(train_x),len(train_y),len(test_x),len(test_y))
59/7:
model = DecisionTreeRegressor()
model.fit(train_x,train_y)
59/8:
predicted_anual = model.predict(test_x)
mse = mean_squared_error(test_y, predicted_anual)
rmse = np.sqrt(mse)
print(mse,rmse)
mae = mean_absolute_error(test_y,predicted_anual)
print(mae)
r2 = r2_score(test_y,predicted_anual)
print(r2)
59/9:
# 1843.6557087291785 42.9378121092491
# 10.520464918764377
# 0.9978930199054238

# 8353.710373708016 91.39863441927355
# 56.11648325302063
# 0.9904531516432695

# 4668.890150852565 68.32927740619364
# 34.12003601668758
# 0.9946642648271947
59/10: plt.scatter(predicted_anual,test_y)
59/11: plt.plot(predicted_anual,test_y)
41/13:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
60/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
60/2:
data = pd.read_csv("Bihar-2006-2015.csv")
data.head()
60/3:
data = pd.read_csv("practice.csv")
data.head()
60/4:
data = pd.read_csv("practice.csv")
data.head()
60/5:
x = data["IQ1"]
x
60/6:
x = data["IQ1"].values()
x
60/7:
x = data["iQ1"].values()
x
60/8:
x = data["IQ1"].values()
x
60/9:
x = data.["IQ1"]
x
60/10:
x = data["IQ1"]
x
60/11:
x = data["IQ1"].value
x
60/12:
x = data["IQ1"].values
x
60/13:
x = data["IQ1"].values()
x
60/14:
x = data["IQ1"].value()
x
61/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
61/2:
data = pd.read_csv("practice.csv")
data.head()
61/3:
x = data["IQ1"].value()
x
62/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
62/2:
data = pd.read_csv("practice.csv")
data.head()
62/3:
x = data["IQ1"].value()
x
62/4:
x = data["Q1"].value()
x
62/5:
x = data["Q1"]
x
62/6:
x = data["Q1"]
x
62/7:
x = data["Q1"]
x
62/8:
x = data["Q1"]
x
62/9:
x = data["Q1"]
x
62/10:
x = data["Q1"]
x
62/11:
x = data["Q1"]
x
62/12:
x = data["Q1"]
x
62/13:
x = data["Q1"]
x
62/14: x = data["STUDENT ID"]
62/15:
x = data["STUDENT ID"]
x
62/16: x = data["Q1"]
62/17: x = data["q1"]
62/18: x = data["Q 1"]
62/19: x = data["Q 2"]
62/20: x = data["Q3"]
62/21: x = data["IQ1 "]
62/22: x = data["IQ 1"]
62/23: x = data["IQ 1"]
63/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
63/2:
data = pd.read_csv("practice.csv")
data.head()
63/3: x = data["IQ 1"]
63/4: x = data["Q 1"]
63/5: x = data["Q 1"]
64/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
64/2:
data = pd.read_csv("practice.csv")
data.head()
64/3: x = data["Q 1"]
64/4: x = data["Q 2"]
64/5:
x = data["Q 2"]
x
64/6:
x = data["Q 2"]
y = data["Q 3"]
64/7:
x = data["Q 2"]
y = data["Q 4"]
65/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
65/2:
data = pd.read_csv("practice1.csv")
data.head()
66/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
66/2:
data = pd.read_csv("practice1.csv")
data.head()
66/3:
x = data["Q 2"]
y = data["Q 4"]
66/4:
x = data["Q 2"]
y = data["Q 5"]
67/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
67/2:
data = pd.read_csv("practice1.csv")
data.head()
67/3:
x = data["iq1"]
y = data["iq2"]
67/4:
x = data.iloc["iq1"]
y = data["iq2"]
67/5:
x = data.iloc[1]
y = data["iq2"]
67/6:
x = data.iloc[1]
x
67/7:
x = data.iloc[:1]
x
68/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
68/2:
data = pd.read_csv("practice.csv")
data.head()
68/3:
x = data["iq1"]
y = data["iq2"]
68/4:
x = data.iloc[:,[1]]
x
68/5:
x = data.iloc[:,[1]]
y = data.iloc[:,[2]]
print(x,y)
68/6:
x = data.iloc[:,[1]]
y = data.iloc[:,[2]]
print(x)
y
68/7:
x = data.iloc[:,[1]]
y = data.iloc[:,[2]]
68/8:
x = data.iloc[:,[1]]
y = data.iloc[:,[2]]
68/9:
x = data.iloc[:,[1]]
y = data.iloc[:,[2]]
68/10:
x = data.iloc[:,[1]]
y = data.iloc[:,[2]]
68/11: train_x,test_x,train_y,test_y =
68/12: plt.plot(x,y)
68/13: plt.scatter(x,y)
68/14: plt.scatter(iq1,iq2)
68/15:
iq1 = data.iloc[:,[1]]
iq2 = data.iloc[:,[2]]
68/16: plt.scatter(iq1,iq2)
68/17: plt.scatter(iq1,iq2,linewidth=200)
68/18: plt.scatter(iq1,iq2,linewidth=20)
68/19: plt.scatter(iq1,iq2,linewidth=2)
68/20: plt.scatter(iq1,iq2,figsize(200,200))
68/21: plt.scatter(iq1,iq2)
68/22:
model = LinearRegression()
model.fit(iq1,iq2)
68/23:
train_x,test_x,train_y,test_y = (iq1,iq2,test_size=0.2,random_state=42)

model = LinearRegression()
model.fit(train_x,train_y)
68/24:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
68/25:
train_x,test_x,train_y,test_y = train_test_split(iq1,iq2,test_size=0.2,random_state=42)

model = LinearRegression()
model.fit(train_x,train_y)
68/26: predict_y = model.predict(test_x)
68/27:
predict_y = model.predict(test_x)
predict_y
68/28:
some_x = test_x.iloc[:10]
some_y = test_yiloc[:10]

predict_y = model.predict(some_x)
predict_y
68/29:
some_x = test_x.iloc[:10]
some_y = test_y.iloc[:10]

predict_y = model.predict(some_x)
predict_y
68/30:
some_x = test_x.iloc[:10]
some_y = test_y.iloc[:10]

predict_y = model.predict(some_x)
predict_y
68/31: some_y
68/32:
plt.scatter(iq1,iq2,color="m")
plt.plot(iq1,iq2,color="b")
68/33:
plt.scatter(iq1,iq2,color="m")
plt.plot(iq1,predict_y,color="g")
68/34:
plt.scatter(iq1,iq2,color="m")
plt.plot(y_test,predict_y,color="g")
68/35:
plt.scatter(iq1,iq2,color="m")
plt.plot(x_test,predict_y,color="g")
68/36:
plt.scatter(iq1,iq2,color="m")
plt.scatter(x_test,predict_y,color="g")
68/37:
plt.scatter(iq1,iq2,color="m")
plt.scatter(test_x,predict_y,color="g")
68/38:
#plt.scatter(test_x,iq2,color="m")
plt.scatter(test_y,predict_y,color="g")
68/39:
#plt.scatter(test_x,iq2,color="m")
plt.scatter(test_y,predict_y,color="g")
68/40:
#plt.scatter(test_x,iq2,color="m")
plt.scatter(test_x,predict_y,color="g")
68/41:
model = LinearRegression()
x = x.fit(x,y)
y_predict = model.predict(x)
69/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
69/2:
data = pd.read_csv("practice.csv")
data.head()
69/3:
x = data.iloc[:,[1]]
y = data.iloc[:,[2]]
69/4:
model = LinearRegression()
x = x.fit(x,y)
y_predict = model.predict(x)
69/5:
model = LinearRegression()
x = model.fit(x,y)
y_predict = model.predict(x)
69/6:
x = data.iloc[:,[1]].reshape(-1,1)
y = data.iloc[:,[2]].reshape(-1,1)
69/7:
x = data.iloc[:,[1]]
x = x.reshape(-1,1)
y = data.iloc[:,[2]]
y = y.reshape(-1,1)
69/8:
x = data.iloc[:,[1]]

y = data.iloc[:,[2]]
x
69/9:
x = data.iloc[:,[1]]

y = data.iloc[:,[2]]
x = np.reshape(x)
69/10:
x = data.iloc[:,[1]]

y = data.iloc[:,[2]]
x = x.reshape(1,-1)
69/11:
x = data.iloc[:,[1]]

y = data.iloc[:,[2]]
x = x.reshape(-1,-1)
70/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
70/2:
data = pd.read_csv("practice.csv")
data.head()
70/3:
x = data.iloc[:,[1]]

y = data.iloc[:,[2]]
x = x.reshape(-1,-1)
70/4:
x = data.iloc[[:,[1]]]

y = data.iloc[:,[2]]
x = x.reshape(-1,-1)
70/5:
x = data.iloc[:,[1]]

y = data.iloc[:,[2]]
x = x.reshape(-1,-1)
70/6:
model = LinearRegression()
x = model.fit([x],[y])
y_predict = model.predict(x)
70/7:
model = LinearRegression()
x = model.fit([x],y)
y_predict = model.predict(x)
70/8:
x = np.array(data.iloc[:,[1]])

y = data.iloc[:,[2]]
70/9:
model = LinearRegression()
x = model.fit(x,y)
y_predict = model.predict(x)
70/10:
x = np.array(data.iloc[:,[1]])

y = np.array(data.iloc[:,[2]])
x
70/11:
x = np.array(data.iloc[:,[1]])

y = np.array(data.iloc[:,[2]])
70/12:
model = LinearRegression()
x = model.fit(x,y)
y_predict = model.predict(x)
70/13:
x = np.array(data.iloc[:,[1]])

y = np.array(data.iloc[:,[2]])
x.reshape(-1,1)
70/14:
x = np.array(data.iloc[:,[1]])

y = np.array(data.iloc[:,[2]])
x = x.reshape(-1,1)
y = y.reshape(-1,1)
70/15:
model = LinearRegression()
x = model.fit(x,y)
y_predict = model.predict(x)
70/16:
model = LinearRegression()
x = model.fit(x,y)
y_predict = model.predict(x)
70/17:
model = LinearRegression()
x = model.fit(x,y)
y_predict = model.predict(x)
70/18:
model = LinearRegression()
model.fit(x,y)
y_predict = model.predict(x)
70/19:
model = LinearRegression()
model.fit(x,y)
y_predict = model.predict(x)
70/20:
x = np.array(data.iloc[:,[1]])

y = np.array(data.iloc[:,[2]])
x = x.reshape(-1,1)
y = y.reshape(-1,1)
70/21:
model = LinearRegression()
model.fit(x,y)
y_predict = model.predict(x)
70/22:
model = LinearRegression()
model.fit(x,y)
y_predict = model.predict(x)
70/23:
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
70/24: train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)
70/25:
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)
len(test_x)
70/26:
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)
len(test_x)
len(train_x)
70/27:
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)
model1 = LinearRegression()
model1.fit(trainx_train_y)
70/28:
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)
model1 = LinearRegression()
model1.fit(train_x,train_y)
70/29:
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)
model1 = LinearRegression()
model1.fit(train_x,train_y)
predict_y = model1.predict(text_x)
plt.scatter(test_x,test_y)
plt.plot(test_x,predict_y,color="m")
70/30:
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)
model1 = LinearRegression()
model1.fit(train_x,train_y)
predict_y = model1.predict(test_x)
plt.scatter(test_x,test_y)
plt.plot(test_x,predict_y,color="m")
70/31:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
70/32:
print("Mean squared error", mean_squared_error(test_y,predict_y))
print("\nMean absolute error", mean_absolute_error(test_y,predict_y))
print("\nR square score", r2_score(test_y,predict_y))
70/33:
print("Mean squared error", mean_squared_error(test_y,predict_y))
print("\nMean absolute error", mean_absolute_error(test_y,predict_y))
print("\nR square score", r2_score(test_y,predict_y))
70/34:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
71/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
71/2:
data = pd.read_csv("practice.csv")
data.head()
71/3:
x = np.array(data.iloc[:,[1]])

y = np.array(data.iloc[:,[2]])
x = x.reshape(-1,1)
y = y.reshape(-1,1)
71/4:
model = LinearRegression()
model.fit(x,y)
y_predict = model.predict(x)
71/5:
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
71/6:
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)
model1 = DecisionTreeRegressor()
model1.fit(train_x,train_y)
predict_y = model1.predict(test_x)
plt.scatter(test_x,test_y)
plt.plot(test_x,predict_y,color="m")
71/7:
print("Mean squared error", mean_squared_error(test_y,predict_y))
print("\nMean absolute error", mean_absolute_error(test_y,predict_y))
print("\nR square score", r2_score(test_y,predict_y))
72/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
72/2:
data = pd.read_csv("practice.csv")
data.head()
72/3:
x = np.array(data.iloc[:,[1]])

y = np.array(data.iloc[:,[2]])
x = x.reshape(-1,1)
y = y.reshape(-1,1)
72/4:
model = LinearRegression()
model.fit(x,y)
y_predict = model.predict(x)
72/5:
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
72/6:
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)
model1 = RandomForestRegressor()
model1.fit(train_x,train_y)
predict_y = model1.predict(test_x)
plt.scatter(test_x,test_y)
plt.plot(test_x,predict_y,color="m")
72/7:
print("Mean squared error", mean_squared_error(test_y,predict_y))
print("\nMean absolute error", mean_absolute_error(test_y,predict_y))
print("\nR square score", r2_score(test_y,predict_y))
73/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
73/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
73/3:
data = pd.read_csv("practice.csv")
data.head()
73/4:
data = pd.read_csv("practice.csv")
data.head()
73/5:
x_axis = data[["Medu","Fedu"]]
y_axis = data["iq5"]
73/6:
x_axis = data[["Medu","Fedu"]]
y_axis = data[:,[5]]
73/7:
x_axis = data[["Medu","Fedu"]]
y_axis = data.iloc[:,[5]]
73/8:
x_axis = data[["Medu","Fedu"]]
y_axis = data.iloc[:,[5]]
x_axis
73/9:
x_axis = data[["Medu","Fedu"]]
y_axis = data.iloc[:,[5]]
73/10:
x_axis = data[["Medu","Fedu"]]
y_axis = data.iloc[:,[5]]
73/11: train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)
73/12:
train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)
model2 = LinearRegression()
model2.fit(train_x,train_y)
predict_y = model2.predict(test_x)
predict_y
73/13:
train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)
model2 = LinearRegression()
model2.fit(train_x,train_y)
predict_y = model2.predict(test_x)
some_y = predict[:5]
some_y
73/14:
train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)
model2 = LinearRegression()
model2.fit(train_x,train_y)
predict_y = model2.predict(test_x)
some_y = predict_y[:5]
some_y
73/15:
train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)
model2 = LinearRegression()
model2.fit(train_x,train_y)
predict_y = model2.predict(test_x)
some_y = predict_y[:5]
some_y
73/16: test_y[:5]
73/17:
train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)
model2 = LinearRegression()
model2.fit(train_x,train_y)
predict_y = model2.predict(test_x)
predict_y[:5]
73/18: test_y[:5]
73/19:
train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)
model2 = LinearRegression()
model2.fit(train_x,train_y)
predict_y = model2.predict(test_x)
73/20: print("MSE",mean_absolute_error(test_y,predict_y))
73/21:
print("MSE",mean_absolute_error(test_y,predict_y))
print("R squared score",r2_score(test_y,predict_y))
74/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
74/2:
data = pd.read_csv("practice.csv")
data.head()
74/3:
x_axis = data[["Medu","Fedu"]]
y_axis = data.iloc[:,[5]]
74/4:
train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)
model2 = DecisionTreeRegressor()
model2.fit(train_x,train_y)
predict_y = model2.predict(test_x)
74/5:
print("MSE",mean_absolute_error(test_y,predict_y))
print("R squared score",r2_score(test_y,predict_y))
75/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
75/2:
data = pd.read_csv("practice.csv")
data.head()
75/3:
x_axis = data[["Medu","Fedu"]]
y_axis = data.iloc[:,[5]]
75/4:
train_x,test_x,train_y,test_y = train_test_split(x_axis, y_axis, test_size = 0.2,random_state = 42)
model2 = RandomForestRegressor()
model2.fit(train_x,train_y)
predict_y = model2.predict(test_x)
75/5:
print("MSE",mean_absolute_error(test_y,predict_y))
print("R squared score",r2_score(test_y,predict_y))
76/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
76/2:
data = pd.read_csv("practice.csv")
data.head()
76/3:
x = np.array(data.iloc[:,[1]])

y = np.array(data.iloc[:,[2]])
x = x.reshape(-1,1)
y = y.reshape(-1,1)
76/4:
model = LinearRegression()
model.fit(x,y)
y_predict = model.predict(x)
76/5:
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/6:
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)
model1 = LinearRegression()
model1.fit(train_x,train_y)
predict_y = model1.predict(test_x)
plt.scatter(test_x,test_y)
plt.plot(test_x,predict_y,color="m")
76/7:
print("Mean squared error", mean_squared_error(test_y,predict_y))
print("\nMean absolute error", mean_absolute_error(test_y,predict_y))
print("\nR square score", r2_score(test_y,predict_y))
76/8:
plt.figuresize(10,20)
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/9:
plt.figsize(10,20)
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/10:
plt.figsize(10,20)
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/11:
plt.figsize(10,20)
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/12:
plt.figsize(10,20)
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/13:
plt.figsize(10,20)
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/14:
plt.figsize(10,20)
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/15:
plt.figsize(10,20)
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/16:
plt.figsize(10,20)
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/17:
plt.figsize(10,20)
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/18:
plt.figsize(10,20)
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/19:
plt.figsize(10,20)
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/20:
plt.figure(figsize=(10,20))
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/21:
plt.figure(figsize=(20,10))
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/22:
plt.figure(figsize=(10,10))
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/23:
plt.figure(figsize=(5,10))
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/24:
plt.scatter(x,y)
plt.plot(x,y_predict,color="m")
76/25:
x = np.array(data.iloc[:,[1]])

y = np.array(data.iloc[:,[2]])
x = x.reshape(-1,1)
y = y.reshape(-1,1)

plt.sccatter(x,y)
76/26:
x = np.array(data.iloc[:,[1]])

y = np.array(data.iloc[:,[2]])
x = x.reshape(-1,1)
y = y.reshape(-1,1)

plt.scatter(x,y)
76/27:
x = np.array(data.iloc[:,[1]])

y = np.array(data.iloc[:,[2]])
x = x.reshape(-1,1)
y = y.reshape(-1,1)

plt.plot(data)
76/28:
x = np.array(data.iloc[:,[1]])

y = np.array(data.iloc[:,[2]])
x = x.reshape(-1,1)
y = y.reshape(-1,1)

plt.plot(data.iloc[:,[1,2,3,4,5]])
76/29:
x = np.array(data.iloc[:,[1]])

y = np.array(data.iloc[:,[2]])
x = x.reshape(-1,1)
y = y.reshape(-1,1)

plt.scatter(data.iloc[:,[1,2,3,4,5]])
76/30:
x = np.array(data.iloc[:,[1]])

y = np.array(data.iloc[:,[2]])
x = x.reshape(-1,1)
y = y.reshape(-1,1)

plt.scatter(data.iloc[:,[7,8]],data.iloc[:,[1,2,3,4,5]])
76/31:
x = np.array(data.iloc[:,[1]])

y = np.array(data.iloc[:,[2]])
plt.scatter(x,y)
x = x.reshape(-1,1)
y = y.reshape(-1,1)
76/32:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2)
plt.xlabel("IQ1")
plt.ylabel("IQ2")
76/33:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
76/34:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
76/35:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3,label = IQ3)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
plt.lagend()
76/36:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
plt.lagend()
76/37:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
plt.legend()
76/38:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
plt.legend()
76/39:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)


iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis
iq4 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ4")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ4", fontsize = 20)




plt.legend()
76/40:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.subplot()
plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)


iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis
iq4 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ4")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ4", fontsize = 20)




plt.legend()
76/41:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)


plt.subplot(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)


iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis
iq4 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ4")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ4", fontsize = 20)




plt.legend()
76/42:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.subplot(1,2)
plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)


iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis
iq4 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ4")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ4", fontsize = 20)




plt.legend()
76/43:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.subplot(1,2,1)
plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)


iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis
iq4 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ4")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ4", fontsize = 20)




plt.legend()
76/44:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.subplot(1,2,2)
plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)


iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis
iq4 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ4")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ4", fontsize = 20)




plt.legend()
76/45:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.subplot(2,1,2)
plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)


iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis
iq4 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ4")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ4", fontsize = 20)




plt.legend()
76/46:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.subplot(3,1,3)
plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)


iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis
iq4 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ4")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ4", fontsize = 20)




plt.legend()
76/47:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)


plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)
plt.subplot(3,1,3)
plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
76/48:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)


plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
76/49:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)


plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.subplot(1,1,2)
plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
76/50:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)


plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.subplot(2,1,2)
plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
76/51:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)


plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.subplot(3,1,2)
plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
76/52:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)


plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.subplot(2,2,2)
plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
76/53:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)


plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.subplot(,,2)
plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
76/54:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)


plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)


iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.subplot(grid[0,1])
plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
76/55:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)


plt.scatter(iq1,iq2, label="IQ2")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
76/56:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)


plt.scatter(iq1,iq2)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
76/57:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x
iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.subplot(grid[0,1])
plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
76/58:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x
iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
76/59:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x
iq4 = np.array(data.iloc[:,[4]])      # Taking iq3 as y-axis
iq4 = iq4.reshape(-1,1)

plt.scatter(iq1,iq3)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ4", fontsize = 20)
76/60:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2)
plt.plot(iq1,iq2)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
76/61:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

#plt.scatter(iq1,iq2)
plt.plot(iq1,iq2)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
76/62:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
76/63:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
plt.grid()
76/64:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2,s=100)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
plt.grid()
76/65:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2,s=2)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
plt.grid()
76/66:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2,s=10)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
plt.grid()
76/67:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2,s=50)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
plt.grid()
76/68:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2,s=40)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
plt.grid()
76/69:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2,s=30)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
plt.grid()
76/70:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2,s=30,alpha=10)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
plt.grid()
76/71:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2,s=30,alpha=1)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
plt.grid()
76/72:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2,s=30,alpha=.5)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
plt.grid()
76/73:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)

plt.scatter(iq1,iq2,s=30,alpha=0.5)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
plt.grid()
76/74:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x
iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ3")
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
plt.grid()
76/75:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x
iq4 = np.array(data.iloc[:,[4]])      # Taking iq3 as y-axis
iq4 = iq4.reshape(-1,1)

plt.scatter(iq1,iq3)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ4", fontsize = 20)
plt.grid()
76/76:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq1,iq3)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
76/77:
iq2 = np.array(data.iloc[:,[1]])      # Taking iq2 as x
iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3)
plt.xlabel("IQ1", fontsize = 20)                                   # IQ2 x IQ3
plt.ylabel("IQ3", fontsize = 20)
plt.grid()
76/78:
iq2 = np.array(data.iloc[:,[1]])      # Taking iq2 as x
iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq2,iq3)
plt.xlabel("IQ1", fontsize = 20)                                   # IQ2 x IQ3
plt.ylabel("IQ3", fontsize = 20)
plt.grid()
76/79:
iq2 = np.array(data.iloc[:,[1]])      # Taking iq2 as x
iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq2,iq3)
plt.xlabel("IQ1", fontsize = 20)                                   # IQ2 x IQ3
plt.ylabel("IQ3", fontsize = 20)
plt.grid()
76/80:
iq2 = np.array(data.iloc[:,[1]])      # Taking iq2 as x
iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
#iq3 = iq3.reshape(-1,1)

plt.scatter(iq2,iq3)
plt.xlabel("IQ1", fontsize = 20)                                   # IQ2 x IQ3
plt.ylabel("IQ3", fontsize = 20)
plt.grid()
76/81:
iq2 = np.array(data.iloc[:,[1]])      # Taking iq2 as x
iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
#iq3 = iq3.reshape(-1,1)

plt.scatter(iq2,iq3)
plt.xlabel("IQ2", fontsize = 20)                                   # IQ2 x IQ3
plt.ylabel("IQ3", fontsize = 20)
plt.grid()
76/82:
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x
iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
#iq3 = iq3.reshape(-1,1)

plt.scatter(iq2,iq3)
plt.xlabel("IQ2", fontsize = 20)                                   # IQ2 x IQ3
plt.ylabel("IQ3", fontsize = 20)
plt.grid()
76/83:
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x
iq4 = np.array(data.iloc[:,[4]])      # Taking iq3 as y-axis
iq4 = iq4.reshape(-1,1)

plt.scatter(iq2,iq4)
plt.xlabel("IQ2", fontsize = 20)                                   # IQ2 x IQ3
plt.ylabel("IQ4", fontsize = 20)
plt.grid()
76/84:
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq2,iq5)
plt.xlabel("IQ2", fontsize = 20)                                   # IQ2 x IQ5
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
76/85:
iq3 = np.array(data.iloc[:,[2]])      # Taking iq3 as x
iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis
iq4 = iq4.reshape(-1,1)

plt.scatter(iq3,iq4)
plt.xlabel("IQ3", fontsize = 20)                                   # IQ2 x IQ5
plt.ylabel("IQ4", fontsize = 20)
plt.grid()
76/86:
iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x
iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis
iq4 = iq4.reshape(-1,1)

plt.scatter(iq3,iq4)
plt.xlabel("IQ3", fontsize = 20)                                   # IQ3 x IQ4
plt.ylabel("IQ4", fontsize = 20)
plt.grid()
76/87:
iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq3,iq5)
plt.xlabel("IQ3", fontsize = 20)                                   # IQ3 x IQ5
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
76/88:
iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq4,iq5)
plt.xlabel("IQ4", fontsize = 20)                                   # IQ4 x IQ5
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
77/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
77/2:
data = pd.read_csv("practice.csv")
data.head()
77/3:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)                                              # IQ1 x IQ2

plt.scatter(iq1,iq2,s=30,alpha=0.5)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
plt.grid()
77/4:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x
iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3, label = "IQ3")                                   # IQ1 x IQ3
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
plt.grid()
77/5:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x
iq4 = np.array(data.iloc[:,[4]])      # Taking iq3 as y-axis
iq4 = iq4.reshape(-1,1)

plt.scatter(iq1,iq3)
plt.xlabel("IQ1", fontsize = 20)                                    # IQ1 x IQ4
plt.ylabel("IQ4", fontsize = 20)
plt.grid()
77/6:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq1,iq3)
plt.xlabel("IQ1", fontsize = 20)                                   # IQ1 x IQ5
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
77/7:
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x
iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq2,iq3)
plt.xlabel("IQ2", fontsize = 20)                                   # IQ2 x IQ3
plt.ylabel("IQ3", fontsize = 20)
plt.grid()
77/8:
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x
iq4 = np.array(data.iloc[:,[4]])      # Taking iq as y-axis
iq4 = iq4.reshape(-1,1)

plt.scatter(iq2,iq4)
plt.xlabel("IQ2", fontsize = 20)                                   # IQ2 x IQ4
plt.ylabel("IQ4", fontsize = 20)
plt.grid()
77/9:
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq2,iq5)
plt.xlabel("IQ2", fontsize = 20)                                   # IQ2 x IQ5
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
77/10:
iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x
iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis
iq4 = iq4.reshape(-1,1)

plt.scatter(iq3,iq4)
plt.xlabel("IQ3", fontsize = 20)                                   # IQ3 x IQ4
plt.ylabel("IQ4", fontsize = 20)
plt.grid()
77/11:
iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq3,iq5)
plt.xlabel("IQ3", fontsize = 20)                                   # IQ3 x IQ5
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
77/12:
iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq4,iq5)
plt.xlabel("IQ4", fontsize = 20)                                   # IQ4 x IQ5
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
77/13:
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=42)
model1 = LinearRegression()
model1.fit(train_x,train_y)
predict_y = model1.predict(test_x)
plt.scatter(test_x,test_y)
plt.plot(test_x,predict_y,color="m")
77/14:

plt.scatter(iq1,iq2,label = "IQ2")
plt.scatter(iq1,iq3, label = "IQ3")
plt.scatter(iq1,iq4,label = "IQ4") 
plt.scatter(iq1,iq5,label = "IQ5")
plt.grid()
77/15:

plt.scatter(iq1,iq2,label = "IQ2")
plt.scatter(iq1,iq3, label = "IQ3")
plt.scatter(iq1,iq4,label = "IQ4") 
plt.scatter(iq1,iq5,label = "IQ5")
plt.grid()
plt.legend()
77/16:

plt.scatter(iq1,iq2,label = "IQ2",alpha = 0.6)
plt.scatter(iq1,iq3, label = "IQ3",alpha = 0.6)
plt.scatter(iq1,iq4,label = "IQ4",alpha = 0.6) 
plt.scatter(iq1,iq5,label = "IQ5",alpha = 0.6)
plt.grid()
plt.legend()
77/17:

plt.scatter(iq1,iq2,label = "IQ2",alpha = 0.6)
plt.scatter(iq1,iq3, label = "IQ3",alpha = 0.6)
plt.scatter(iq1,iq4,label = "IQ4",alpha = 0.6) 
plt.scatter(iq1,iq5,label = "IQ5",alpha = 0.6)
plt.xlabel("IQ1",fontsize = 20)
plt.grid()
plt.legend()
77/18:
plt.scatter(iq2,iq3, label = "IQ3",alpha = 0.6)
plt.scatter(iq2,iq4,label = "IQ4",alpha = 0.6) 
plt.scatter(iq2,iq5,label = "IQ5",alpha = 0.6)
plt.xlabel("IQ2",fontsize=20)
plt.grid()
plt.legend()
77/19:
plt.scatter(iq3,iq4,label = "IQ4",alpha = 0.6) 
plt.scatter(iq3,iq5,label = "IQ5",alpha = 0.6)
plt.xlabel("IQ3",fontsize=20)
plt.grid()
plt.legend()
78/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
78/2:
data = pd.read_csv("practice.csv")
data.head()
78/3:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)                                              # IQ1 x IQ2

plt.scatter(iq1,iq2,s=30)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
plt.grid()
78/4:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x
iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3)                                   # IQ1 x IQ3
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
plt.grid()
78/5:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x
iq4 = np.array(data.iloc[:,[4]])      # Taking iq3 as y-axis
iq4 = iq4.reshape(-1,1)

plt.scatter(iq1,iq4)
plt.xlabel("IQ1", fontsize = 20)                                    # IQ1 x IQ4
plt.ylabel("IQ4", fontsize = 20)
plt.grid()
78/6:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq1,iq5)
plt.xlabel("IQ1", fontsize = 20)                                   # IQ1 x IQ5
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
78/7:

plt.scatter(iq1,iq2,label = "IQ2",alpha = 0.6)
plt.scatter(iq1,iq3, label = "IQ3",alpha = 0.6)
plt.scatter(iq1,iq4,label = "IQ4",alpha = 0.6) 
plt.scatter(iq1,iq5,label = "IQ5",alpha = 0.6)
plt.xlabel("IQ1",fontsize = 20)
plt.grid()
plt.legend()
78/8:
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x
iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq2,iq3)
plt.xlabel("IQ2", fontsize = 20)                                   # IQ2 x IQ3
plt.ylabel("IQ3", fontsize = 20)
plt.grid()
78/9:
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x
iq4 = np.array(data.iloc[:,[4]])      # Taking iq as y-axis
iq4 = iq4.reshape(-1,1)

plt.scatter(iq2,iq4)
plt.xlabel("IQ2", fontsize = 20)                                   # IQ2 x IQ4
plt.ylabel("IQ4", fontsize = 20)
plt.grid()
78/10:
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq2,iq5)
plt.xlabel("IQ2", fontsize = 20)                                   # IQ2 x IQ5
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
78/11:
plt.scatter(iq2,iq3, label = "IQ3",alpha = 0.6)
plt.scatter(iq2,iq4,label = "IQ4",alpha = 0.6) 
plt.scatter(iq2,iq5,label = "IQ5",alpha = 0.6)
plt.xlabel("IQ2",fontsize=20)
plt.grid()
plt.legend()
78/12:
iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x
iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis
iq4 = iq4.reshape(-1,1)

plt.scatter(iq3,iq4)
plt.xlabel("IQ3", fontsize = 20)                                   # IQ3 x IQ4
plt.ylabel("IQ4", fontsize = 20)
plt.grid()
78/13:
iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq3,iq5)
plt.xlabel("IQ3", fontsize = 20)                                   # IQ3 x IQ5
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
78/14:
plt.scatter(iq3,iq4,label = "IQ4",alpha = 0.6) 
plt.scatter(iq3,iq5,label = "IQ5",alpha = 0.6)
plt.xlabel("IQ3",fontsize=20)
plt.grid()
plt.legend()
78/15:
iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq4,iq5)
plt.xlabel("IQ4", fontsize = 20)                                   # IQ4 x IQ5
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
78/16:
train_x,test_x,train_y,test_y = train_test_split(iq1,iq2,test_size=0.2,random_state=42)
model1 = LinearRegression()
model1.fit(train_x,train_y)
predict_y = model1.predict(test_x)
plt.scatter(test_x,test_y)
plt.plot(test_x,predict_y,color="m")
78/17:
print("Mean squared error(mse)", mean_squared_error(test_y,predict_y))
print("\nMean absolute error(mae)", mean_absolute_error(test_y,predict_y))
print("\nR square score(r2_score)", r2_score(test_y,predict_y))
78/18:
plt.scatter(iq2,iq3, label = "IQ3",alpha = 0.6,color="m")
plt.scatter(iq2,iq4,label = "IQ4",alpha = 0.6) 
plt.scatter(iq2,iq5,label = "IQ5",alpha = 0.6)
plt.xlabel("IQ2",fontsize=20)
plt.grid()
plt.legend()
78/19:
plt.scatter(iq2,iq3, label = "IQ3",alpha = 0.6)
plt.scatter(iq2,iq4,label = "IQ4",alpha = 0.6) 
plt.scatter(iq2,iq5,label = "IQ5",alpha = 0.6)
plt.xlabel("IQ2",fontsize=20)
plt.grid()
plt.legend()
82/1:
a = [1,2,3,4,4,5]
print(set(a))
82/2:
a = [1,2,3,4,4,5]
print(set(a))
82/3:
a = [1,2,3,4,4,5]
print(set(a))
82/4:
a = [1,2,3,4,4,5]
print(set(a))
82/5:
a = [1,2,3,4,4,5]
print(set(a))
82/6:
a = [1,2,3,4,4,5]
print(set(a))
82/7:
a = [1,2,3,3,4,4,5]
print(set(a))
82/8:
a = [1,2,3,3,4,4,5]
print(set(a))
82/9:
a = [1,1,2,3,3,4,4,5]
print(set(a))
82/10:
a = [1,1,2,3,3,4,4,5]
print(set(a))
82/11:
a = [1,1,2,3,3,4,4,5]
print(set(a))
82/12:
a = [1,1,2,3,3,4,4,5]
print(set(a))
82/13:
a = [1,1,2,3,3,4,4,5]
print(set(a))
82/14:
a = [1,1,2,3,3,4,4,5]
print(set(a))
82/15:
a = [1,1,2,3,3,4,4,5]
print(set(a))
83/1:
a = [1,1,2,3,3,4,4,5]
print(set(a))
83/2: a = [1,1,2,3,3,4,4,5]
83/3:
a = [1,1,2,3,3,4,4,5]
print(set(a))
83/4:
a = [1,1,2,3,3,4,4,5]
b = []
for i in a:
    if i not in b:
        b.append(i)
        
b
83/5:
a = [1,1,2,4,4,3,3,5]
set(a)
b = []
for i in a:
    if i not in b:
        b.append(i)
        
b
83/6:
a = [1,1,2,4,4,3,3,5]
print(set(a))
b = []
for i in a:
    if i not in b:
        b.append(i)
        
b
84/1:
data = pd.read_csv("Bihar-2006-2015.csv")
data.head()
85/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
85/2:
data = pd.read_csv("Bihar-2006-2015.csv")
data.head()
85/3:
data = pd.read_csv("Bihar-2006-2015.csv")
data.head(10)
85/4:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
85/5:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
data1.head(10)
85/6:
x = data1["YEAR"]
y = data1["ANNUAL"]
plt.scatter(x,y)
85/7:
x = data1["YEAR"],data2["YEAR"]
y = data1["ANNUAL"]
plt.scatter(x,y)
85/8:
x = data1["YEAR"],data2["YEAR"]
y = data1["ANNUAL"],data2["ANNUAL"]
plt.scatter(x,y)
85/9: x = data[["Jan-Feb","Mar-May","Jun-Sep",Oct-Dec]]
85/10: x = data[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
85/11:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]
85/12:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]
plt.plot(x,y)
85/13:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]
plt.hist(bins=20)
85/14:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]
plt.hist(y,bins=20)
85/15:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]
plt.hist(x,y,bins=20)
85/16:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]
plt.hist(x,bins=20)
85/17:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]
plt.hist(x,bins=5)
85/18:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(x,y)
85/19:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]
x
#plt.scatter(x,y)
85/20:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(x[0],y)
85/21:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y)
85/22:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y)
plt.scatter(data1["Mar-May"],y)
85/23:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.plot(data1["Jan-Feb"],y)
plt.scatter(data1["Mar-May"],y)
85/24:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y)
plt.scatter(data1["Mar-May"],y)
85/25:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y)
plt.scatter(data1["Mar-May"],y)
plt.scatter(data1["Jun-Sep"],y)
plt.scatter(data1["Oct-Dec"],y)
85/26:
x = data1[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
85/27: x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)
85/28:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
85/29:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
85/30:
y_predict = model.predict(x_test)
y_predict
85/31: y_test
86/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
86/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
data1.head(10)
86/3:
x = data1[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
86/4:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
86/5:
y_predict = model.predict(x_test)
y_predict
86/6: y_test
87/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
87/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
data1.head(10)
87/3:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
87/4:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
87/5:
y_predict = model.predict(x_test)
y_predict
87/6: model.predict([["2006","0.1","94.3","936.3","22.1"]])
87/7: model.predict(["2006","0.1","94.3","936.3","22.1"])
87/8:
test = data1.iloc[0,[0,2,3,4,5]]
test
#model.predict(["2006","0.1","94.3","936.3","22.1"])
87/9:
test = data1.iloc[0,[0,2,3,4,5]]
test = test.reshape(-1,1)
test
#model.predict(["2006","0.1","94.3","936.3","22.1"])
87/10:
test = list(data1.iloc[0,[0,2,3,4,5]])
test = test.reshape(-1,1)
test
#model.predict(["2006","0.1","94.3","936.3","22.1"])
87/11:
test = list(data1.iloc[0,[0,2,3,4,5]])
#test = test.reshape(-1,1)
test
#model.predict(["2006","0.1","94.3","936.3","22.1"])
87/12:
test = list(data1.iloc[0,[0,2,3,4,5]])
#test = test.reshape(-1,1)
test
model.predict(test)
87/13:
test = list(data1.iloc[0,[0,2,3,4,5]])
#test = test.reshape(-1,1)
test
model.predict([test])
88/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
88/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
data1.head(10)
88/3:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
88/4:
x = data2["YEAR"]
y = data2["Jan-Feb"]
88/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
89/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
89/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
data1.head(10)
89/3:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
89/4:
x = data2["YEAR"].reshape(-1,1)
y = data2["Jan-Feb"].reshape(-1,1)
89/5:
x = data2["YEAR"]
y = data2["Jan-Feb"]
x
89/6:
x = data2["YEAR"]
y = data2["Jan-Feb"]
x = x.reshape(-1,1)
89/7:
x = list(data2["YEAR"])
y = data2["Jan-Feb"]
x = x.reshape(-1,1)
89/8:
x = np.array(data2["YEAR"])
y = data2["Jan-Feb"]
x = x.reshape(-1,1)
89/9:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["Jan-Feb"]).reshape(-1,1)
89/10:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["Jan-Feb"]).reshape(-1,1)
90/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
90/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
data1.head(10)
90/3:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
90/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["Jan-Feb"]).reshape(-1,1)
90/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
90/6:
y_predict = model.predict(x_test)
y_predict
90/7:
test = list(data1.iloc[0,[0,2,3,4,5]])
#test = test.reshape(-1,1)
test
model.predict([test])
90/8:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(10))
90/9: y_test
90/10:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = DecisionTreeRegressor()
model.fit(x_train,y_train)
90/11:
y_predict = model.predict(x_test)
y_predict
90/12: y_test
90/13:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = RandomForestRegressor()
model.fit(x_train,y_train)
90/14:
y_predict = model.predict(x_test)
y_predict
90/15: y_test
90/16:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
90/17:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
91/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
91/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(10))
91/3:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
91/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["Jan-Feb"]).reshape(-1,1)
scaler = StandardScaler()
x = x.fit_transform(x)
y = y.fit_transform(y)
92/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
92/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(10))
92/3:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
92/4:
x = data2["YEAR"]
y = data2["Jan-Feb"]
scaler = StandardScaler()
x = x.fit_transform(x)
y = y.fit_transform(y)
92/5:
x = data2.iloc[:,[0]]
y = data2["Jan-Feb"]
scaler = StandardScaler()
x = x.fit_transform(x)
y = y.fit_transform(y)
92/6:
x = data2.iloc[:,[0]]
print(x)
y = data2["Jan-Feb"]
scaler = StandardScaler()
x = x.fit_transform(x)
y = y.fit_transform(y)
92/7:
x = data2.iloc[:,[0]].values
print(x)
y = data2["Jan-Feb"]
scaler = StandardScaler()
x = x.fit_transform(x)
y = y.fit_transform(y)
92/8:
x = data2.iloc[:,0].values
print(x)
y = data2["Jan-Feb"]
scaler = StandardScaler()
x = x.fit_transform(x)
y = y.fit_transform(y)
92/9:
x = data2.iloc[:,0].values
print(x)
y = data2.iloc[:,2].values
print(y)
scaler = StandardScaler()
x = x.fit_transform(x)
y = y.fit_transform(y)
92/10:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["Jan-Feb"]).reshape(-1,1)
scaler = StandardScaler()
x = scaler.fit_transform(x)
y = scaler.fit_transform(y)
93/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
93/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(10))
93/3:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
93/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["Jan-Feb"]).reshape(-1,1)
scaler = StandardScaler()
x = scaler.fit_transform(x)
y = scaler.fit_transform(y)
93/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = RandomForestRegressor()
model.fit(x_train,y_train)
93/6:
y_predict = model.predict(x_test)
y_predict
93/7: y_test
94/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
94/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(10))
94/3:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
94/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["Jan-Feb"]).reshape(-1,1)
scaler = StandardScaler()
x = scaler.fit_transform(x)
y = scaler.fit_transform(y)
94/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
94/6:
y_predict = model.predict(x_test)
y_predict
94/7: y_test
94/8: r2_score(y_test,y_predict)
95/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
95/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(10))
95/3:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
95/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["ANNUAL"]).reshape(-1,1)
scaler = StandardScaler()
x = scaler.fit_transform(x)
y = scaler.fit_transform(y)
95/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
95/6:
y_predict = model.predict(x_test)
y_predict
95/7: y_test
95/8: r2_score(y_test,y_predict)
96/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
96/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(10))
96/3:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
96/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["ANNUAL"]).reshape(-1,1)
#scaler = StandardScaler()
#x = scaler.fit_transform(x)
#y = scaler.fit_transform(y)
96/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
96/6:
y_predict = model.predict(x_test)
y_predict
96/7: y_test
96/8: r2_score(y_test,y_predict)
96/9: model.predict([[2006]])
96/10: model.predict([[2007]])
96/11: model.predict([[2006]])
98/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
98/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(10))
98/3:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
98/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["ANNUAL"]).reshape(-1,1)
#scaler = StandardScaler()
#x = scaler.fit_transform(x)
#y = scaler.fit_transform(y)
98/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
98/6:
y_predict = model.predict(x_test)
y_predict
98/7: model.predict([[2006]])
98/8: r2_score(y_test,y_predict)
98/9:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
98/10:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(10))
98/11:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
98/12:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["ANNUAL"]).reshape(-1,1)
#scaler = StandardScaler()
#x = scaler.fit_transform(x)
#y = scaler.fit_transform(y)
98/13:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
98/14:
y_predict = model.predict(x_test)
y_predict
98/15: model.predict([[2006]])
98/16: r2_score(y_test,y_predict)
99/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
99/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(10))
99/3:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
100/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
100/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(11))
100/3:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
101/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
101/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(11))
101/3:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
102/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
102/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(11))
102/3:
x = data2[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data2["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
103/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
103/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(11))
103/3:
x = data1[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
103/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["ANNUAL"]).reshape(-1,1)
#scaler = StandardScaler()
#x = scaler.fit_transform(x)
#y = scaler.fit_transform(y)
103/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
103/6:
y_predict = model.predict(x_test)
y_predict
103/7: model.predict([[2006]])
103/8: r2_score(y_test,y_predict)
103/9: y_test
103/10: model.predict([[2007]])
104/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
104/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(11))
104/3:
x = data1[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
104/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["ANNUAL"]).reshape(-1,1)
#scaler = StandardScaler()
#x = scaler.fit_transform(x)
#y = scaler.fit_transform(y)
104/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
104/6:
y_predict = model.predict(x_test)
y_predict
104/7: model.predict([[2007]])
104/8: r2_score(y_test,y_predict)
105/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
105/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(11))
105/3:
x = data1[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
105/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["ANNUAL"]).reshape(-1,1)
#scaler = StandardScaler()
#x = scaler.fit_transform(x)
#y = scaler.fit_transform(y)
105/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
105/6:
y_predict = model.predict(x_test)
y_predict
105/7: model.predict([[2007]])
105/8: r2_score(y_test,y_predict)
106/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
106/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(11))
106/3:
x = data1[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
106/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["ANNUAL"]).reshape(-1,1)
#scaler = StandardScaler()
#x = scaler.fit_transform(x)
#y = scaler.fit_transform(y)
106/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = RandomForestRegressor()
model.fit(x_train,y_train)
106/6: y_predict = model.predict(x_test)
106/7: model.predict([[2007]])
106/8: r2_score(y_test,y_predict)
107/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
107/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(11))
107/3:
x = data1[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
107/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["ANNUAL"]).reshape(-1,1)
#scaler = StandardScaler()
#x = scaler.fit_transform(x)
#y = scaler.fit_transform(y)
107/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = DecisionTreeRegressor()
model.fit(x_train,y_train)
107/6: y_predict = model.predict(x_test)
107/7: model.predict([[2007]])
107/8: r2_score(y_test,y_predict)
108/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
108/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(11))
108/3:
x = data1[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
108/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["ANNUAL"]).reshape(-1,1)
#scaler = StandardScaler()
#x = scaler.fit_transform(x)
#y = scaler.fit_transform(y)
108/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
108/6: y_predict = model.predict(x_test)
108/7: model.predict([[2007]])
108/8: r2_score(y_test,y_predict)
109/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
109/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(11))
109/3:
x = data1[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
109/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["Jan-Feb"]).reshape(-1,1)
#scaler = StandardScaler()
#x = scaler.fit_transform(x)
#y = scaler.fit_transform(y)
109/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
109/6: y_predict = model.predict(x_test)
109/7: model.predict([[2007]])
109/8: r2_score(y_test,y_predict)
110/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
110/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(11))
110/3:
x = data1[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
110/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["Jun-Sep"]).reshape(-1,1)
#scaler = StandardScaler()
#x = scaler.fit_transform(x)
#y = scaler.fit_transform(y)
110/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
110/6: y_predict = model.predict(x_test)
110/7: model.predict([[2007]])
110/8: r2_score(y_test,y_predict)
110/9: mean_squared_error(y_test,y_predict)
110/10: mean_absolute_error(y_test,y_predict)
111/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
111/2:
data1 = pd.read_csv("Bihar-2006-2015.csv")
data2 = pd.read_csv("Bihar-1996-2005.csv")
print(data1.head(10))
print(data2.head(11))
111/3:
x = data1[["YEAR","Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y = data1["ANNUAL"]

plt.scatter(data1["Jan-Feb"],y,label="Jan-Feb")
plt.scatter(data1["Mar-May"],y,label="Mar-May")
plt.scatter(data1["Jun-Sep"],y,label="Jun-Sep")
plt.scatter(data1["Oct-Dec"],y,label="Oct-Dec")
plt.grid()
plt.legend()
111/4:
x = np.array(data2["YEAR"]).reshape(-1,1)
y = np.array(data2["Oct-Dec"]).reshape(-1,1)
#scaler = StandardScaler()
#x = scaler.fit_transform(x)
#y = scaler.fit_transform(y)
111/5:
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)
model = LinearRegression()
model.fit(x_train,y_train)
111/6: y_predict = model.predict(x_test)
111/7: model.predict([[2007]])
111/8: mean_absolute_error(y_test,y_predict)
114/1:
arr = [32,1,0,34,0,23]
arr1 = [0]*len(arr)
j = 0

for i in arr:
    if i!=arr1[j]:
        arr1[j] = i
        j++
        
arr1
114/2:
arr = [32,1,0,34,0,23]
arr1 = [0]*len(arr)
j = 0

for i in arr:
    if i!=arr1[j]:
        arr1[j] = i
        j=j+1
        
arr1
114/3:
game_board = [[0,0,0],[0,0,0],[0,0,0]]
print(game_board)
114/4:
game_board = [[0,0,0],[0,0,0],[0,0,0]]

for i in range(0,3):
    for j in range(0,3):
        print(game_board[i][j])
    print()
114/5:
game_board = [[0,0,0],[0,0,0],[0,0,0]]

for i in range(0,3):
    for j in range(0,3):
        print(game_board[i][j],end = " ")
    print()
114/6:
import random


def random_number_generator():
    random.randint(1,100)
114/7:
import random


random.randint(1,100)
114/8:
import random


random.randint(1,100)
114/9:
import random


random.randint(1,100)
114/10:
import random


random.randint(1,100)
114/11:
import random


random.randint(1,100)
114/12:
import random


random.randint(1,100)
114/13:
import random


random.randint(1,100)
114/14:
import random


random.randint(1,100)
114/15:
import random


random.randint(1,100)
114/16:
import random


random.randint(1,100)
114/17:
import random


random.randint(1,100)
114/18:
import random


random.randint(1,100)
114/19:
import random


random.randint(1,100)
114/20:
import random


random.randint(1,100)
114/21:
import random


random.randint(1,100)
114/22:
import random


random.randint(1,100)
114/23:
import random


random.randint(1,100)
114/24:
import random


random.randint(1,100)
114/25:
import random


def random_number_generator():
    return random.randint(1,100)




def print_board(game_board):
    for i in range(0,3):
        for j in range(0,3):
            print(game_board[i][j],end = " ")
        print()


def game():
    game_board = [[0,0,0],[0,0,0],[0,0,0]]
    i = 0
    while(i!=3):
        j = 0
        while(j!=3):
            value = random_number_generator()
            if(value not in game_board):
                game_board[i][j] = value
                j+=1
                
    print_board()
115/1:
import random


def random_number_generator():
    return random.randint(1,100)




def print_board(game_board):
    for i in range(0,3):
        for j in range(0,3):
            print(game_board[i][j],end = " ")
        print()


def game():
    game_board = [[0,0,0],[0,0,0],[0,0,0]]
    i = 0
    while(i!=3):
        j = 0
        while(j!=3):
            value = random_number_generator()
            if(value not in game_board):
                game_board[i][j] = value
                j+=1
                
    print_board()
115/2:
import random


def random_number_generator():
    return random.randint(1,100)




def print_board(game_board):
    for i in range(0,3):
        for j in range(0,3):
            print(game_board[i][j],end = " ")
        print()


def game():
    game_board = [[0,0,0],[0,0,0],[0,0,0]]
    i = 0
    while(i!=3):
        j = 0
        while(j!=3):
            value = random_number_generator()
            if(value not in game_board):
                game_board[i][j] = value
                j+=1
                
    print_board()

game()
115/3:
import random


def random_number_generator():
    return random.randint(1,100)




def print_board(game_board):
    for i in range(0,3):
        for j in range(0,3):
            print(game_board[i][j],end = " ")
        print()


def game():
    game_board = [[0,0,0],[0,0,0],[0,0,0]]
    i = 0
    while(i!=3):
        j = 0
        while(j!=3):
            value = random_number_generator()
            if(value not in game_board):
                game_board[i][j] = value
                j+=1
        i+=1
                
    print_board()

game()
115/4:
import random


def random_number_generator():
    return random.randint(1,100)




def print_board(game_board):
    for i in range(0,3):
        for j in range(0,3):
            print(game_board[i][j],end = " ")
        print()


def game():
    game_board = [[0,0,0],[0,0,0],[0,0,0]]
    i = 0
    while(i!=3):
        j = 0
        while(j!=3):
            value = random_number_generator()
            if(value not in game_board):
                game_board[i][j] = value
                j+=1
        i+=1
                
    print_board()

game()
115/5:
import random


def random_number_generator():
    return random.randint(1,100)




def print_board(game_board):
    for i in range(0,3):
        for j in range(0,3):
            print(game_board[i][j],end = " ")
        print()


def game():
    game_board = [[0,0,0],[0,0,0],[0,0,0]]
    i = 0
    while(i!=3):
        j = 0
        while(j!=3):
            value = random_number_generator()
            if(value not in game_board):
                game_board[i][j] = value
                j+=1
        i+=1
                
    print_board(game_board)

game()
115/6:
import random


def random_number_generator():
    return random.randint(1,100)




def print_board(game_board):
    for i in range(0,3):
        for j in range(0,3):
            print(game_board[i][j],end = " ")
        print()


def game():
    game_board = [[0,0,0],[0,0,0],[0,0,0]]
    i = 0
    while(i!=3):
        j = 0
        while(j!=3):
            value = random_number_generator()
            if(value not in game_board):
                game_board[i][j] = value
                j+=1
                
        i+=1
                
    print_board(game_board)

game()
115/7:
import random


def random_number_generator():
    return random.randint(1,100)




def print_board(game_board):
    for i in range(0,3):
        for j in range(0,3):
            print(game_board[i][j],end = " ")
        print()


def game():
    game_board = [[0,0,0],[0,0,0],[0,0,0]]
    i = 0
    while(i!=3):
        j = 0
        while(j!=3):
            value = random_number_generator()
            if(value not in game_board):
                game_board[i][j] = value
                j+=1
                
        i+=1
                
    print_board(game_board)

game()
115/8:
import random

score = 0

def random_number_generator():
    return random.randint(1,100)




def print_board(game_board):
    for i in range(0,3):
        for j in range(0,3):
            print(game_board[i][j],end = " ")
        print()

        
def win_lose(game_board):
    user_input = int(input("Gusse the max value on the board: "))
    if user_input == max(game_board):
        return 2
    
    return -1        

def game():
    game_board = [[0,0,0],[0,0,0],[0,0,0]]
    i = 0
    while(i!=3):
        j = 0
        while(j!=3):
            value = random_number_generator()
            if(value not in game_board):
                game_board[i][j] = value
                j+=1          
        i+=1
    print_board(game_board)
    temp_score = win_lose(game_board)
    if temp_score == -1:
        print("Game over\n")
        print("Your score = ",score)
    else:
        score = score + temp_score
        game() 
    
game()
115/9:
import random


def random_number_generator():
    return random.randint(1,100)




def print_board(game_board):
    for i in range(0,3):
        for j in range(0,3):
            print(game_board[i][j],end = " ")
        print()

        
def win_lose(game_board):
    user_input = int(input("Gusse the max value on the board: "))
    if user_input == max(game_board):
        return 2
    
    return -1        

def game(score):
    game_board = [[0,0,0],[0,0,0],[0,0,0]]
    i = 0
    while(i!=3):
        j = 0
        while(j!=3):
            value = random_number_generator()
            if(value not in game_board):
                game_board[i][j] = value
                j+=1          
        i+=1
    print_board(game_board)
    temp_score = win_lose(game_board)
    if temp_score == -1:
        print("Game over\n")
        print("Your score = ",score)
    else:
        score = score + temp_score
        game(score) 

if __name__ == "__main__"
    score = 0
    game(score)
115/10:
import random


def random_number_generator():
    return random.randint(1,100)




def print_board(game_board):
    for i in range(0,3):
        for j in range(0,3):
            print(game_board[i][j],end = " ")
        print()

        
def win_lose(game_board):
    user_input = int(input("Gusse the max value on the board: "))
    if user_input == max(game_board):
        return 2
    
    return -1        

def game(score):
    game_board = [[0,0,0],[0,0,0],[0,0,0]]
    i = 0
    while(i!=3):
        j = 0
        while(j!=3):
            value = random_number_generator()
            if(value not in game_board):
                game_board[i][j] = value
                j+=1          
        i+=1
    print_board(game_board)
    temp_score = win_lose(game_board)
    if temp_score == -1:
        print("Game over\n")
        print("Your score = ",score)
    else:
        score = score + temp_score
        game(score) 

if __name__ == "__main__":
    score = 0
    game(score)
115/11:
import random


def random_number_generator():
    return random.randint(1,100)




def print_board(game_board):
    for i in range(0,3):
        for j in range(0,3):
            print(game_board[i][j],end = " ")
        print()

        
def win_lose(game_board):
    user_input = int(input("Gusse the max value on the board: "))
    if user_input == max(game_board):
        return 2
    
    return -1        

def game(score):
    game_board = [[0,0,0],[0,0,0],[0,0,0]]
    i = 0
    while(i!=3):
        j = 0
        while(j!=3):
            value = random_number_generator()
            if(value not in game_board):
                game_board[i][j] = value
                j+=1          
        i+=1
    print_board(game_board)
    temp_score = win_lose(game_board)
    if temp_score == -1:
        print("Game over\n")
        print("Your score = ",score)
    else:
        score = score + temp_score
        game(score) 

if __name__ == "__main__":
    score = 0
    game(score)
115/12:
import random


def random_number_generator():
    return random.randint(1,100)




def print_board(game_board):
    for i in range(0,3):
        for j in range(0,3):
            print(game_board[i][j],end = " ")
        print()

        
def win_lose(game_board):
    user_input = int(input("Gusse the max value on the board: "))
    if user_input == max(game_board):
        return 2
    
    return -1        

def game(score):
    game_board = [[0,0,0],[0,0,0],[0,0,0]]
    i = 0
    while(i!=3):
        j = 0
        while(j!=3):
            value = random_number_generator()
            if(value not in game_board):
                game_board[i][j] = value
                j+=1          
        i+=1
    print_board(game_board)
    temp_score = win_lose(game_board)
    print(temp_score)
    if (temp_score == -1):
        print("Game over\n")
        print("Your score = ",score)
    else:
        score = score + temp_score
        game(score) 

if __name__ == "__main__":
    score = 0
    game(score)
115/13:
a = [[1,2,3],[4,5,6],[7,8,9]]
max(a)
116/1:
import random


def random_number_generator():
    return random.randint(1,100)




def print_board(game_board):
    for i in range(0,3):
        for j in range(0,3):
            print(game_board[i][j],end = " ")
        print()

        
def win_lose(game_board):
    user_input = int(input("Gusse the max value on the board: "))
    if ((user_input == max(game_board[0])) or (user_input == max(game_board[1])) or (user_input == max(game_board[2]))) :
        return 2
    
    return -1        

def game(score):
    game_board = [[0,0,0],[0,0,0],[0,0,0]]
    i = 0
    while(i!=3):
        j = 0
        while(j!=3):
            value = random_number_generator()
            if(value not in game_board):
                game_board[i][j] = value
                j+=1          
        i+=1
    print_board(game_board)
    temp_score = win_lose(game_board)
    print(temp_score)
    if (temp_score == -1):
        print("Game over\n")
        print("Your score = ",score)
    else:
        score = score + temp_score
        game(score) 

if __name__ == "__main__":
    score = 0
    game(score)
116/2:
a = [[1,2,3],[4,5,6],[7,8,9]]
max(a)
117/1:
import pandas as pd
import matplotlib as plt
import numpy as np
117/2: bihar1 = pd.read_csv("Bihar1.csv")
117/3:
bihar1 = pd.read_csv("Bihar1.csv")
bihar2 = pd.read_csv("Bihar2.csv")
jharkhand1 = pd.read_csv("Jharkhand1.csv")
jharkhand2 = pd.read_csv("Jharkhand2.csv")
117/4:
bihar1 = pd.read_csv("Bihar1.csv")
bihar2 = pd.read_csv("Bihar2.csv")
jharkhand1 = pd.read_csv("Jharkhand1.csv")
jharkhand2 = pd.read_csv("Jharkhand2.csv")
117/5: plt.scatter(bihar)
117/6:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
117/7: plt.scatter(bihar)
117/8: plt.scatter(bihar)
117/9:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
117/10: plt.scatter(bihar1)
117/11: plt.plot(bihar1)
117/12: bihar1.head(2)
117/13:
bihar1_x = bihar1.iloc[:,[0]].values()
bihar1_y = bihar1.iloc[:,[1:]].values()
plt.scatter(bihar)
117/14:
bihar1_x = bihar1.iloc[:,[0]].values()
bihar1_y = bihar1.iloc[:,[1:]].values()
plt.scatter(bihar1_x,bihar_y)
117/15:
bihar1_x = bihar1.iloc[:,[0]].values()
bihar1_y = bihar1.iloc[:,[1:]].values()
plt.scatter(bihar1_x,bihar_y)
117/16:
bihar1_x = bihar1.iloc[:,[0]].values()
bihar1_y = bihar1.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12,13]].values()
plt.scatter(bihar1_x,bihar_y)
117/17:
bihar1_x = bihar1.iloc[:,[0]].values()
bihar1_y = bihar1.iloc[:,[1]].values()
plt.scatter(bihar1_x,bihar_y)
117/18:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[1]]
plt.scatter(bihar1_x,bihar_y)
117/19:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[1]]
plt.scatter(bihar1_x,bihar1_y)
117/20:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[1]]
plt.scatter(bihar1_x,bihar1_y)

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[1]]
plt.scatter(bihar1_x,bihar1_y)
117/21:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[1]]
plt.scatter(bihar1_x,bihar1_y)
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[1]]
plt.scatter(bihar1_x,bihar1_y)
117/22:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[1]]
plt.scatter(bihar1_x,bihar1_y)
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[1]]
plt.scatter(jharkhand1_x,jharkhand1_y)
117/23:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[1]]
plt.scatter(bihar1_x,bihar1_y)
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[1]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.scatter(bihar1_x,bihar1_y)
plt.show()
117/24:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[1]]
plt.scatter(bihar1_x,bihar1_y)
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[1]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.show()
117/25:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[1]]
plt.scatter(bihar1_x,bihar1_y)
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[1]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.show()
plt.grid()
117/26:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[1]]
plt.scatter(bihar1_x,bihar1_y)
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[1]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.show()
plt.grid()
117/27:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[1]]
plt.scatter(bihar1_x,bihar1_y)
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[1]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
118/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
118/2:
bihar1 = pd.read_csv("Bihar1.csv")
bihar2 = pd.read_csv("Bihar2.csv")
jharkhand1 = pd.read_csv("Jharkhand1.csv")
jharkhand2 = pd.read_csv("Jharkhand2.csv")
118/3: bihar1.head(2)
118/4:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[1]]
plt.scatter(bihar1_x,bihar1_y)
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[1]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
118/5:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[2]]
plt.scatter(bihar1_x,bihar1_y)
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[2]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
119/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
119/2:
bihar1 = pd.read_csv("Bihar1.csv")
bihar2 = pd.read_csv("Bihar2.csv")
jharkhand1 = pd.read_csv("Jharkhand1.csv")
jharkhand2 = pd.read_csv("Jharkhand2.csv")
119/3: bihar1.head(2)
119/4:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[14]]
plt.scatter(bihar1_x,bihar1_y)
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[14]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
119/5:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[15]]
plt.scatter(bihar1_x,bihar1_y)
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[15]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
120/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
120/2:
bihar1 = pd.read_csv("Bihar1.csv")
bihar2 = pd.read_csv("Bihar2.csv")
jharkhand1 = pd.read_csv("Jharkhand1.csv")
jharkhand2 = pd.read_csv("Jharkhand2.csv")
120/3: bihar1.head(2)
120/4:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[14]]
plt.scatter(bihar1_x,bihar1_y)
plt.title("BIHAR")
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[14]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
120/5:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[15]]
plt.scatter(bihar1_x,bihar1_y)
plt.title("BIHAR")
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[15]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
120/6:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[16]]
plt.scatter(bihar1_x,bihar1_y)
plt.title("BIHAR")
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[16]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
120/7:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[17]]
plt.scatter(bihar1_x,bihar1_y)
plt.title("BIHAR")
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[17]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
121/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
121/2:
bihar1 = pd.read_csv("Bihar1.csv")
bihar2 = pd.read_csv("Bihar2.csv")
jharkhand1 = pd.read_csv("Jharkhand1.csv")
jharkhand2 = pd.read_csv("Jharkhand2.csv")
121/3: bihar1.head(2)
121/4:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[14]]
plt.scatter(bihar1_x,bihar1_y)
plt.title("BIHAR")
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[14]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
121/5:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[15]]
plt.scatter(bihar1_x,bihar1_y)
plt.title("BIHAR")
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[15]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
121/6:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[16]]
plt.scatter(bihar1_x,bihar1_y)
plt.title("BIHAR")
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[16]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
121/7:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[17]]
plt.scatter(bihar1_x,bihar1_y)
plt.title("BIHAR")
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[17]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
121/8:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[13]]
plt.scatter(bihar1_x,bihar1_y)
plt.title("BIHAR")
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[13]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
123/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
123/2:
bihar1 = pd.read_csv("Bihar1.csv")

jharkhand1 = pd.read_csv("Jharkhand1.csv")
123/3: bihar1.head(2)
123/4:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[14]]
plt.scatter(bihar1_x,bihar1_y)
plt.title("BIHAR")
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[14]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
123/5:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[15]]
plt.scatter(bihar1_x,bihar1_y)
plt.title("BIHAR")
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[15]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
123/6:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[16]]
plt.scatter(bihar1_x,bihar1_y)
plt.title("BIHAR")
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[16]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
123/7:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[17]]
plt.scatter(bihar1_x,bihar1_y)
plt.title("BIHAR")
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[17]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
123/8:
bihar1_x = bihar1.iloc[:,[0]]
bihar1_y = bihar1.iloc[:,[13]]
plt.scatter(bihar1_x,bihar1_y)
plt.title("BIHAR")
plt.show()

jharkhand1_x = jharkhand1.iloc[:,[0]]
jharkhand1_y = jharkhand1.iloc[:,[13]]
plt.scatter(jharkhand1_x,jharkhand1_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1901-1958)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1901-1958)" )
plt.legend()
plt.grid()
plt.show()
122/1: bihar2.head(2)
122/2:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
122/3:

bihar2 = pd.read_csv("Bihar2.csv")

jharkhand2 = pd.read_csv("Jharkhand2.csv")
122/4: bihar2.head(2)
122/5:
bihar2_x = bihar2.iloc[:,[0]]
bihar2_y = bihar2.iloc[:,[14]]
plt.scatter(bihar2_x,bihar2_y)
plt.title("BIHAR")
plt.show()

jharkhand2_x = jharkhand2.iloc[:,[0]]
jharkhand2_y = jharkhand2.iloc[:,[14]]
plt.scatter(jharkhand2_x,jharkhand2_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar1_x,bihar1_y,label = "Bihar(1959-2015)")
plt.plot(jharkhand1_x,jharkhand1_y,label = "Jharkhand(1959-2015)" )
plt.legend()
plt.grid()
plt.show()
122/6:
bihar2_x = bihar2.iloc[:,[0]]
bihar2_y = bihar2.iloc[:,[14]]
plt.scatter(bihar2_x,bihar2_y)
plt.title("BIHAR")
plt.show()

jharkhand2_x = jharkhand2.iloc[:,[0]]
jharkhand2_y = jharkhand2.iloc[:,[14]]
plt.scatter(jharkhand2_x,jharkhand2_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar2_x,bihar2_y,label = "Bihar(1959-2015)")
plt.plot(jharkhand2_x,jharkhand2_y,label = "Jharkhand(1959-2015)" )
plt.legend()
plt.grid()
plt.show()
124/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
124/2:

bihar2 = pd.read_csv("Bihar2.csv")

jharkhand2 = pd.read_csv("Jharkhand2.csv")
124/3: bihar2.head(2)
124/4:
bihar2_x = bihar2.iloc[:,[0]]
bihar2_y = bihar2.iloc[:,[14]]
plt.scatter(bihar2_x,bihar2_y)
plt.title("BIHAR")
plt.show()

jharkhand2_x = jharkhand2.iloc[:,[0]]
jharkhand2_y = jharkhand2.iloc[:,[14]]
plt.scatter(jharkhand2_x,jharkhand2_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar2_x,bihar2_y,label = "Bihar(1959-2015)")
plt.plot(jharkhand2_x,jharkhand2_y,label = "Jharkhand(1959-2015)" )
plt.legend()
plt.grid()
plt.show()
124/5:
bihar2_x = bihar2.iloc[:,[0]]
bihar2_y = bihar2.iloc[:,[15]]
plt.scatter(bihar2_x,bihar2_y)
plt.title("BIHAR")
plt.show()

jharkhand2_x = jharkhand2.iloc[:,[0]]
jharkhand2_y = jharkhand2.iloc[:,[15]]
plt.scatter(jharkhand2_x,jharkhand2_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar2_x,bihar2_y,label = "Bihar(1959-2015)")
plt.plot(jharkhand2_x,jharkhand2_y,label = "Jharkhand(1959-2015)" )
plt.legend()
plt.grid()
plt.show()
124/6:
bihar2_x = bihar2.iloc[:,[0]]
bihar2_y = bihar2.iloc[:,[16]]
plt.scatter(bihar2_x,bihar2_y)
plt.title("BIHAR")
plt.show()

jharkhand2_x = jharkhand2.iloc[:,[0]]
jharkhand2_y = jharkhand2.iloc[:,[16]]
plt.scatter(jharkhand2_x,jharkhand2_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar2_x,bihar2_y,label = "Bihar(1959-2015)")
plt.plot(jharkhand2_x,jharkhand2_y,label = "Jharkhand(1959-2015)" )
plt.legend()
plt.grid()
plt.show()
124/7:
bihar2_x = bihar2.iloc[:,[0]]
bihar2_y = bihar2.iloc[:,[17]]
plt.scatter(bihar2_x,bihar2_y)
plt.title("BIHAR")
plt.show()

jharkhand2_x = jharkhand2.iloc[:,[0]]
jharkhand2_y = jharkhand2.iloc[:,[17]]
plt.scatter(jharkhand2_x,jharkhand2_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar2_x,bihar2_y,label = "Bihar(1959-2015)")
plt.plot(jharkhand2_x,jharkhand2_y,label = "Jharkhand(1959-2015)" )
plt.legend()
plt.grid()
plt.show()
124/8:
bihar2_x = bihar2.iloc[:,[0]]
bihar2_y = bihar2.iloc[:,[13]]
plt.scatter(bihar2_x,bihar2_y)
plt.title("BIHAR")
plt.show()

jharkhand2_x = jharkhand2.iloc[:,[0]]
jharkhand2_y = jharkhand2.iloc[:,[13]]
plt.scatter(jharkhand2_x,jharkhand2_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar2_x,bihar2_y,label = "Bihar(1959-2015)")
plt.plot(jharkhand2_x,jharkhand2_y,label = "Jharkhand(1959-2015)" )
plt.legend()
plt.grid()
plt.show()
124/9: bihar2.tail(2)
125/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
125/2:
bihar3 = pd.read_csv("Bihar3.csv")


jharkhand3 = pd.read_csv("Jharkhand3.csv")
125/3:
bihar3 = pd.read_csv("Bihar3(2016-2018).csv")


jharkhand3 = pd.read_csv("Jharkhand3(2016-2018).csv")
125/4: bihar3.head()
125/5:
bihar3_x = bihar3.iloc[:,[0]]
bihar3_y = bihar3.iloc[:,[14]]
plt.scatter(bihar3_x,bihar3_y)
plt.title("BIHAR")
plt.show()

jharkhand3_x = jharkhand3.iloc[:,[0]]
jharkhand3_y = jharkhand3.iloc[:,[14]]
plt.scatter(jharkhand3_x,jharkhand3_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar3_x,bihar3_y,label = "Bihar(2016-2018)")
plt.plot(jharkhand3_x,jharkhand3_y,label = "Jharkhand(2016-2018)" )
plt.legend()
plt.grid()
plt.show()
125/6:
print(bihar3.head())
jharkhand3
125/7:
bihar3_x = bihar3.iloc[:,[0]]
bihar3_y = bihar3.iloc[:,[15]]
plt.scatter(bihar3_x,bihar3_y)
plt.title("BIHAR")
plt.show()

jharkhand3_x = jharkhand3.iloc[:,[0]]
jharkhand3_y = jharkhand3.iloc[:,[15]]
plt.scatter(jharkhand3_x,jharkhand3_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar3_x,bihar3_y,label = "Bihar(2016-2018)")
plt.plot(jharkhand3_x,jharkhand3_y,label = "Jharkhand(2016-2018)" )
plt.legend()
plt.grid()
plt.show()
126/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
126/2:
bihar3 = pd.read_csv("Bihar3(2016-2018).csv")


jharkhand3 = pd.read_csv("Jharkhand3(2016-2018).csv")
126/3:
print(bihar3.head())
jharkhand3
126/4:
bihar3_x = bihar3.iloc[:,[0]]
bihar3_y = bihar3.iloc[:,[14]]
plt.scatter(bihar3_x,bihar3_y)
plt.title("BIHAR")
plt.show()

jharkhand3_x = jharkhand3.iloc[:,[0]]
jharkhand3_y = jharkhand3.iloc[:,[14]]
plt.scatter(jharkhand3_x,jharkhand3_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar3_x,bihar3_y,label = "Bihar(2016-2018)")
plt.plot(jharkhand3_x,jharkhand3_y,label = "Jharkhand(2016-2018)" )
plt.legend()
plt.grid()
plt.show()
126/5:
bihar3_x = bihar3.iloc[:,[0]]
bihar3_y = bihar3.iloc[:,[15]]
plt.scatter(bihar3_x,bihar3_y)
plt.title("BIHAR")
plt.show()

jharkhand3_x = jharkhand3.iloc[:,[0]]
jharkhand3_y = jharkhand3.iloc[:,[15]]
plt.scatter(jharkhand3_x,jharkhand3_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar3_x,bihar3_y,label = "Bihar(2016-2018)")
plt.plot(jharkhand3_x,jharkhand3_y,label = "Jharkhand(2016-2018)" )
plt.legend()
plt.grid()
plt.show()
126/6:
bihar3_x = bihar3.iloc[:,[0]]
bihar3_y = bihar3.iloc[:,[16]]
plt.scatter(bihar3_x,bihar3_y)
plt.title("BIHAR")
plt.show()

jharkhand3_x = jharkhand3.iloc[:,[0]]
jharkhand3_y = jharkhand3.iloc[:,[16]]
plt.scatter(jharkhand3_x,jharkhand3_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar3_x,bihar3_y,label = "Bihar(2016-2018)")
plt.plot(jharkhand3_x,jharkhand3_y,label = "Jharkhand(2016-2018)" )
plt.legend()
plt.grid()
plt.show()
126/7:
bihar3_x = bihar3.iloc[:,[0]]
bihar3_y = bihar3.iloc[:,[17]]
plt.scatter(bihar3_x,bihar3_y)
plt.title("BIHAR")
plt.show()

jharkhand3_x = jharkhand3.iloc[:,[0]]
jharkhand3_y = jharkhand3.iloc[:,[17]]
plt.scatter(jharkhand3_x,jharkhand3_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar3_x,bihar3_y,label = "Bihar(2016-2018)")
plt.plot(jharkhand3_x,jharkhand3_y,label = "Jharkhand(2016-2018)" )
plt.legend()
plt.grid()
plt.show()
126/8:
bihar3_x = bihar3.iloc[:,[0]]
bihar3_y = bihar3.iloc[:,[13]]
plt.scatter(bihar3_x,bihar3_y)
plt.title("BIHAR")
plt.show()

jharkhand3_x = jharkhand3.iloc[:,[0]]
jharkhand3_y = jharkhand3.iloc[:,[13]]
plt.scatter(jharkhand3_x,jharkhand3_y)
plt.title("JHARKHAND")
plt.show()

plt.plot(bihar3_x,bihar3_y,label = "Bihar(2016-2018)")
plt.plot(jharkhand3_x,jharkhand3_y,label = "Jharkhand(2016-2018)" )
plt.legend()
plt.grid()
plt.show()
128/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
128/2:
data = pd.read_csv("practice.csv")
data.head()
128/3:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x-axis
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as y-axis

iq1 = iq1.reshape(-1,1)
iq2 = iq2.reshape(-1,1)                                              # IQ1 x IQ2

plt.scatter(iq1,iq2,s=30)
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ2", fontsize = 20)
plt.grid()
128/4:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x
iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq1,iq3)                                   # IQ1 x IQ3
plt.xlabel("IQ1", fontsize = 20)
plt.ylabel("IQ3", fontsize = 20)
plt.grid()
128/5:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x
iq4 = np.array(data.iloc[:,[4]])      # Taking iq3 as y-axis
iq4 = iq4.reshape(-1,1)

plt.scatter(iq1,iq4)
plt.xlabel("IQ1", fontsize = 20)                                    # IQ1 x IQ4
plt.ylabel("IQ4", fontsize = 20)
plt.grid()
128/6:
iq1 = np.array(data.iloc[:,[1]])      # Taking iq1 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq1,iq5)
plt.xlabel("IQ1", fontsize = 20)                                   # IQ1 x IQ5
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
128/7:

plt.scatter(iq1,iq2,label = "IQ2",alpha = 0.6)
plt.scatter(iq1,iq3, label = "IQ3",alpha = 0.6)
plt.scatter(iq1,iq4,label = "IQ4",alpha = 0.6) 
plt.scatter(iq1,iq5,label = "IQ5",alpha = 0.6)
plt.xlabel("IQ1",fontsize = 20)
plt.grid()
plt.legend()
128/8:
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x
iq3 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
iq3 = iq3.reshape(-1,1)

plt.scatter(iq2,iq3)
plt.xlabel("IQ2", fontsize = 20)                                   # IQ2 x IQ3
plt.ylabel("IQ3", fontsize = 20)
plt.grid()
128/9:
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x
iq4 = np.array(data.iloc[:,[4]])      # Taking iq as y-axis
iq4 = iq4.reshape(-1,1)

plt.scatter(iq2,iq4)
plt.xlabel("IQ2", fontsize = 20)                                   # IQ2 x IQ4
plt.ylabel("IQ4", fontsize = 20)
plt.grid()
128/10:
iq2 = np.array(data.iloc[:,[2]])      # Taking iq2 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq3 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq2,iq5)
plt.xlabel("IQ2", fontsize = 20)                                   # IQ2 x IQ5
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
128/11:
plt.scatter(iq2,iq3, label = "IQ3",alpha = 0.6)
plt.scatter(iq2,iq4,label = "IQ4",alpha = 0.6) 
plt.scatter(iq2,iq5,label = "IQ5",alpha = 0.6)
plt.xlabel("IQ2",fontsize=20)
plt.grid()
plt.legend()
128/12:
iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x
iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as y-axis
iq4 = iq4.reshape(-1,1)

plt.scatter(iq3,iq4)
plt.xlabel("IQ3", fontsize = 20)                                   # IQ3 x IQ4
plt.ylabel("IQ4", fontsize = 20)
plt.grid()
128/13:
iq3 = np.array(data.iloc[:,[3]])      # Taking iq3 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq3,iq5)
plt.xlabel("IQ3", fontsize = 20)                                   # IQ3 x IQ5
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
128/14:
plt.scatter(iq3,iq4,label = "IQ4",alpha = 0.6) 
plt.scatter(iq3,iq5,label = "IQ5",alpha = 0.6)
plt.xlabel("IQ3",fontsize=20)
plt.grid()
plt.legend()
128/15:
iq4 = np.array(data.iloc[:,[4]])      # Taking iq4 as x
iq5 = np.array(data.iloc[:,[5]])      # Taking iq5 as y-axis
iq5 = iq5.reshape(-1,1)

plt.scatter(iq4,iq5)
plt.xlabel("IQ4", fontsize = 20)                                   # IQ4 x IQ5
plt.ylabel("IQ5", fontsize = 20)
plt.grid()
128/16:
train_x,test_x,train_y,test_y = train_test_split(iq1,iq2,test_size=0.2,random_state=42)
model1 = LinearRegression()
model1.fit(train_x,train_y)
predict_y = model1.predict(test_x)
plt.scatter(test_x,test_y)
plt.plot(test_x,predict_y,color="m")
128/17:
print("Mean squared error(mse)", mean_squared_error(test_y,predict_y))
print("\nMean absolute error(mae)", mean_absolute_error(test_y,predict_y))
print("\nR square score(r2_score)", r2_score(test_y,predict_y))
129/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
129/2:
bihar = pd.read_csv("Bihar_entier.csv")
jharkhand  = pd.read_csv("Jharkhand_entier.csv")
129/3: bihar.head(2)
129/4:
x_axis = bihar.iloc[:,[0]]
bihar_y = bihar.iloc[:,[13]]
jharkhand_y = jharkhand.iloc[:,[13]]
129/5:
plt.scatter(x_axis,bihar_y,label="Bihar")
plt.scatter(x_axis,jharkhand_y,label="Jharkhand")
129/6:
plt.scatter(x_axis,bihar_y,label="Bihar")
plt.scatter(x_axis,jharkhand_y,label="Jharkhand")
plt.legend()
129/7:
plt.scatter(x_axis,bihar_y,label="Bihar")
plt.scatter(x_axis,jharkhand_y,label="Jharkhand")
plt.legend()
129/8:
plt.plot(x_axis,bihar_y,label="Bihar")
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.legend()
129/9: bihar.tail(10)
130/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
130/2:
bihar = pd.read_csv("Bihar_entier.csv")
jharkhand  = pd.read_csv("Jharkhand_entier.csv")
130/3: bihar.tail(10)
130/4:
x_axis = bihar.iloc[:,[0]]
bihar_y = bihar.iloc[:,[13]]
jharkhand_y = jharkhand.iloc[:,[13]]
130/5:
plt.scatter(x_axis,bihar_y,label="Bihar")
plt.scatter(x_axis,jharkhand_y,label="Jharkhand")
plt.legend()
130/6:
plt.plot(x_axis,bihar_y,label="Bihar")
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.legend()
130/7:
plt.plot(x_axis,bihar_y,label="Bihar")
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("Year(1901-2018)")
plt.ylabel("Rainfall")
plt.legend()
130/8:
plt.plot(x_axis,bihar_y,label="Bihar")
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("Year(1901-2018)",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
130/9:
x_axis = bihar.iloc[[115:],[0]]
bihar_y = bihar.iloc[:,[13]]
jharkhand_y = jharkhand.iloc[[115]:],[13]]
130/10:
plt.scatter(x_axis,bihar_y,label="Bihar")
plt.scatter(x_axis,jharkhand_y,label="Jharkhand")
plt.legend()
130/11:
plt.scatter(x_axis,bihar_y,label="Bihar")
plt.scatter(x_axis,jharkhand_y,label="Jharkhand")
plt.legend()
130/12:
x_axis = bihar.iloc[[115:],[0]]
bihar_y = bihar.iloc[:,[13]]
jharkhand_y = jharkhand.iloc[[115:],[13]]
130/13:
x_axis = bihar.iloc[[115:],[0]]
bihar_y = bihar.iloc[:,[13]]
jharkhand_y = jharkhand.iloc[115:,[13]]
130/14:
x_axis = bihar.iloc[:,[0]]
bihar_y = bihar.iloc[:,[13]]
jharkhand_y = jharkhand.iloc[:,[13]]
131/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
131/2:
bihar = pd.read_csv("Bihar3.csv")
jharkhand  = pd.read_csv("Jharkhand3.csv")
131/3:
bihar = pd.read_csv("Bihar3(2016-2018).csv")
jharkhand  = pd.read_csv("Jharkhand3(2016-2018).csv")
132/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
132/2:
bihar = pd.read_csv("Bihar3(2016-2018).csv")
jharkhand  = pd.read_csv("Jharkhand3(2016-2018).csv")
132/3: bihar.tail(10)
132/4:
x_axis = bihar.iloc[:,[0]]
bihar_y = bihar.iloc[:,[13]]
jharkhand_y = jharkhand.iloc[:,[13]]
132/5:
plt.scatter(x_axis,bihar_y,label="Bihar")
plt.scatter(x_axis,jharkhand_y,label="Jharkhand")
plt.legend()
132/6:
plt.plot(x_axis,bihar_y,label="Bihar")
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("Year(1901-2018)",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
132/7: jharkhand.tail(10)
132/8:
x_axis = bihar.iloc[:,[0]]
bihar_y = bihar.iloc[:,[6]]
jharkhand_y = jharkhand.iloc[:,[6]]
132/9:
plt.scatter(x_axis,bihar_y,label="Bihar")
plt.scatter(x_axis,jharkhand_y,label="Jharkhand")
plt.legend()
132/10:
plt.plot(x_axis,bihar_y,label="Bihar")
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("Year(1901-2018)",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
133/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
133/2:
bihar = pd.read_csv("Bihar_entier.csv")
jharkhand  = pd.read_csv("Jharkhand_entier.csv")
133/3: jharkhand.tail(10)
133/4:
x_axis = bihar.iloc[:,[0]]
bihar_y = bihar.iloc[:,[6]]
jharkhand_y = jharkhand.iloc[:,[6]]
133/5:
plt.scatter(x_axis,bihar_y,label="Bihar")
plt.scatter(x_axis,jharkhand_y,label="Jharkhand")
plt.legend()
133/6:
plt.plot(x_axis,bihar_y,label="Bihar")
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("Year(1901-2018)",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
133/7:
x_axis = bihar.iloc[:,[0]]
bihar_y = bihar.iloc[:,[7]]
jharkhand_y = jharkhand.iloc[:,[7]]
133/8:
plt.scatter(x_axis,bihar_y,label="Bihar")
plt.scatter(x_axis,jharkhand_y,label="Jharkhand")
plt.legend()
133/9:
plt.plot(x_axis,bihar_y,label="Bihar")
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("Year(1901-2018)",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
133/10:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("July",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
133/11:
#plt.plot(x_axis,bihar_y,label="Bihar")
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("July",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
134/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
134/2:
bihar = pd.read_csv("Bihar_entier.csv")
jharkhand  = pd.read_csv("Jharkhand_entier.csv")
134/3: jharkhand.tail(10)
134/4:
x_axis = bihar.iloc[:,[0]]
bihar_y = bihar.iloc[:,[6]]
jharkhand_y = jharkhand.iloc[:,[6]]
134/5:
#plt.scatter(x_axis,bihar_y,label="Bihar")
#plt.scatter(x_axis,jharkhand_y,label="Jharkhand")
#plt.legend()
134/6:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("June",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
134/7:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("June",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
avg(bihar["Jun"])
134/8:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("June",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
bihar["Jun"].mean()
134/9:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("June",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
bihar["JUN"].mean()
134/10:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("June",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
bihar["JUN"].avg()
134/11:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("June",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
bihar["JUN"].average()
134/12:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("June",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
134/13:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("June",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
bihar["JUN"].mean()
134/14:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("June",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
bihar["JUL"].mean()
134/15:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("June",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
#bihar["JUL"].mean()
134/16:
#plt.plot(x_axis,bihar_y,label="Bihar")
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("June",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
#bihar["JUL"].mean()
134/17:
x_axis = bihar.iloc[:,[0]]
bihar_y = bihar.iloc[:,[7]]
jharkhand_y = jharkhand.iloc[:,[7]]
134/18:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("June",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
bihar["JUL"].mean()
134/19:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("July",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
bihar["JUL"].mean()
134/20:
#plt.plot(x_axis,bihar_y,label="Bihar")
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("July",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
bihar["JUL"].mean()
134/21:
#plt.plot(x_axis,bihar_y,label="Bihar")
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("July",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
jharkhand["JUL"].mean()
134/22:
x_axis = bihar.iloc[:,[0]]
bihar_y = bihar.iloc[:,[8]]
jharkhand_y = jharkhand.iloc[:,[8]]
134/23:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("July",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
bihar["AUG"].mean()
jharkhand["JUL"].mean()
134/24:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("July",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
bihar["AUG"].mean()
#jharkhand["AUG"].mean()
134/25:
#plt.plot(x_axis,bihar_y,label="Bihar")
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("July",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
#bihar["AUG"].mean()
jharkhand["AUG"].mean()
134/26:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("September",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
bihar["SEP"].mean()
#jharkhand["AUG"].mean()
134/27:
#plt.plot(x_axis,bihar_y,label="Bihar")
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("September",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
#bihar["SEP"].mean()
jharkhand["SEP"].mean()
134/28:
x_axis = bihar.iloc[:,[0]]
bihar_y = bihar.iloc[:,[8]]
jharkhand_y = jharkhand.iloc[:,[8]]
134/29:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("August",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
bihar["SEP"].mean()
#jharkhand["SEP"].mean()
134/30:
plt.plot(x_axis,bihar_y,label="Bihar")
#plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("August",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
bihar["AUG"].mean()
#jharkhand["SEP"].mean()
134/31:
#plt.plot(x_axis,bihar_y,label="Bihar")
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("August",fontsize = 20)
plt.ylabel("Rainfall",fontsize = 20)
plt.legend()
#bihar["AUG"].mean()
jharkhand["SEP"].mean()
134/32:
x_axis = bihar.iloc[:,[0]]
bihar_y = bihar.iloc[:,[6]]
jharkhand_y = jharkhand.iloc[:,[6]]
134/33:
print(bihar["JUN"].mean())
jharkhand["JUN"].mean()
134/34:
print(bihar["JUN"].mean())
jharkhand["AUG"].mean()
134/35: plt.plt(x_axis,jharkhand_y,label="Jharkhand")
134/36: plt.pl0t(x_axis,jharkhand_y,label="Jharkhand")
134/37: plt.plot(x_axis,jharkhand_y,label="Jharkhand")
134/38:
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("Rainfall")
plt.ylabel("August")
plt.legend()
134/39:
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("Rainfall",fontsize = 20)
plt.ylabel("August",fontsize = 20)
plt.legend()
135/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
135/2:
bihar = pd.read_csv("Bihar_entier.csv")
jharkhand  = pd.read_csv("Jharkhand_entier.csv")
135/3: jharkhand.tail(10)
135/4:
x_axis = bihar.iloc[:,[0]]
bihar_y = bihar.iloc[:,[6]]
jharkhand_y = jharkhand.iloc[:,[6]]
135/5:
plt.plot(x_axis,jharkhand_y,label="Jharkhand")
plt.xlabel("Rainfall",fontsize = 20)
plt.ylabel("August",fontsize = 20)
plt.legend()
135/6:
print(bihar["JUN"].mean())
jharkhand["AUG"].mean()
135/7: jharkhand.tail(2)
135/8:
plt.plot(bihar["YEAR"],bihar["JUN"],label = "Bihar")
plt.xlabel("JUNE",fontsize=20)
plt.ylabel("Rainfall",fontsize=20)
plt.legend()
135/9:
plt.plot(bihar["YEAR"],bihar["JUL"],label = "Bihar")
plt.xlabel("JULY",fontsize=20)
plt.ylabel("Rainfall",fontsize=20)
plt.legend()
135/10:
plt.plot(bihar["YEAR"],bihar["AUG"],label = "Bihar")
plt.xlabel("August",fontsize=20)
plt.ylabel("Rainfall",fontsize=20)
plt.legend()
136/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
136/2:
bihar = pd.read_csv("Bihar_entier.csv")
jharkhand  = pd.read_csv("Jharkhand_entier.csv")
136/3: jharkhand.tail(2)
136/4:
plt.plot(bihar["YEAR"],bihar["JUN"],label = "Bihar")
plt.xlabel("JUNE",fontsize=20)
plt.ylabel("Rainfall",fontsize=20)
plt.legend()
136/5:
plt.plot(bihar["YEAR"],bihar["JUL"],label = "Bihar")
plt.xlabel("JULY",fontsize=20)
plt.ylabel("Rainfall",fontsize=20)
plt.legend()
136/6:
plt.plot(bihar["YEAR"],bihar["AUG"],label = "Bihar")
plt.xlabel("AUGUST",fontsize=20)
plt.ylabel("Rainfall",fontsize=20)
plt.legend()
136/7:
plt.plot(bihar["YEAR"],bihar["SEP"],label = "Bihar")
plt.xlabel("SEPTEMBER",fontsize=20)
plt.ylabel("Rainfall",fontsize=20)
plt.legend()
137/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
137/2:
#bihar = pd.read_csv("Bihar_entier.csv")
bihar  = pd.read_csv("Jharkhand_entier.csv")
137/3: jharkhand.tail(2)
138/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
138/2:
#bihar = pd.read_csv("Bihar_entier.csv")
bihar  = pd.read_csv("Jharkhand_entier.csv")
138/3: bihar.tail(2)
138/4:
plt.plot(bihar["YEAR"],bihar["JUN"],label = "Jharkhand")
plt.xlabel("JUNE",fontsize=20)
plt.ylabel("Rainfall",fontsize=20)
plt.legend()
138/5:
plt.plot(bihar["YEAR"],bihar["JUL"],label = "Jharkhand")
plt.xlabel("JULY",fontsize=20)
plt.ylabel("Rainfall",fontsize=20)
plt.legend()
138/6:
plt.plot(bihar["YEAR"],bihar["AUG"],label = "Jharkhand")
plt.xlabel("AUGUST",fontsize=20)
plt.ylabel("Rainfall",fontsize=20)
plt.legend()
138/7:
plt.plot(bihar["YEAR"],bihar["SEP"],label = "Jharkhand")
plt.xlabel("SEPTEMBER",fontsize=20)
plt.ylabel("Rainfall",fontsize=20)
plt.legend()
139/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
139/2: bihar = pd.read_csv("Bihar_entier.csv")
139/3: bihar.head(2)
139/4: x_data = bihar[["JAN"],["FEB"]]
139/5: x_data = bihar[["JAN","FEB"]]
139/6:
x_data = bihar[["JAN","FEB"]]
x_data
139/7:
x_data = bihar[["JAN","FEB","MAR","APR","MAY","JUN"]]
y_data = bihar["YEAR"]
139/8: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
139/9:
model = LinearRegression()
model.fit(x_train,y_train)
139/10: y_predict = model.predict(x_test)
139/11: y_predict[0]
139/12: y_predict[0]
139/13: x_test[0]
139/14: x_test[[0]]
139/15: x_test
139/16: y_predict
139/17: bihar.head(56)
139/18: bihar.head(60)
140/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
140/2: bihar = pd.read_csv("Bihar_entier.csv")
140/3: bihar.head(60)
140/4:
x_data = bihar[["JAN","FEB","MAR","APR","MAY","JUN"]]
y_data = bihar["JUL"]
140/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
140/6:
model = LinearRegression()
model.fit(x_train,y_train)
140/7: y_predict = model.predict(x_test)
140/8: y_predict
140/9: x_test
141/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
141/2: bihar = pd.read_csv("Bihar_entier.csv")
141/3: bihar.head(2)
141/4:
x_data = bihar[["JAN","FEB","MAR","APR","MAY","JUN"]]
y_data = bihar["JUL"]
141/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
141/6:
model = LinearRegression()
model.fit(x_train,y_train)
141/7: y_predict = model.predict(x_test)
141/8: y_predict
142/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
142/2: bihar = pd.read_csv("Bihar_entier.csv")
142/3: bihar.head(2)
142/4:
x_data = bihar[["JAN","FEB","MAR","APR","MAY","JUN"]]
y_data = bihar["JUL"]
142/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
142/6:
model = LinearRegression()
model.fit(x_train,y_train)
142/7: y_predict = model.predict(x_test)
142/8: y_predict
142/9:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
142/10: y_test
142/11: list(y_test)
143/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
143/2: bihar = pd.read_csv("Bihar_entier.csv")
143/3: bihar.head(2)
143/4:
x_data = bihar[["JAN","FEB","MAR","APR","MAY","JUN"]]
y_data = bihar["JUL"]
143/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
143/6:
model = LinearRegression()
model.fit(x_train,y_train)
143/7: y_predict = model.predict(x_test)
143/8: y_predict
143/9:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
144/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
144/2: bihar = pd.read_csv("Bihar_entier.csv")
144/3: bihar.head(2)
144/4:
x_data = bihar[["JAN","FEB","MAR","APR","MAY","JUN"]]
y_data = bihar["JUL"]
144/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
144/6:
model = DecisionTreeRegressor()
model.fit(x_train,y_train)
144/7: y_predict = model.predict(x_test)
144/8: y_predict
144/9:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
145/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
145/2: bihar = pd.read_csv("Bihar_entier.csv")
145/3: bihar.head(2)
145/4:
x_data = bihar[["JAN","FEB","MAR","APR","MAY","JUN"]]
y_data = bihar["JUL"]
145/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
145/6:
model = RandomForestRegressor()
model.fit(x_train,y_train)
145/7: y_predict = model.predict(x_test)
145/8: y_predict
145/9:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
145/10: y_test
146/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
146/2: bihar = pd.read_csv("Bihar_entier.csv")
146/3: bihar.head(2)
146/4:
x_data = bihar[["JAN","FEB","MAR","APR","MAY","JUN"]]
y_data = bihar["JUL"]
146/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
146/6:
model = DecisionTreeRegressor()
model.fit(x_train,y_train)
146/7: y_predict = model.predict(x_test)
146/8: y_predict
146/9: y_test
146/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
147/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
147/2: bihar = pd.read_csv("Bihar_entier.csv")
147/3: bihar.head(2)
147/4:
x_data = bihar["JUN"]
y_data = bihar["JUL"]
147/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
147/6:
model = DecisionTreeRegressor()
model.fit(x_train,y_train)
147/7:
x_data = bihar["JUN"].reshape(-1,1)
y_data = bihar["JUL"].reshape(-1,1)
147/8:
x_data = bihar["JUN"]
x_data = x_data.reshape(-1,1)
y_data = bihar["JUL"].reshape(-1,1)
147/9:
x_data = bihar[["JUN"]]

y_data = bihar[["JUL"]]
148/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
148/2: bihar = pd.read_csv("Bihar_entier.csv")
148/3: bihar.head(2)
148/4:
x_data = bihar[["JUN"]]

y_data = bihar[["JUL"]]
148/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
148/6:
model = DecisionTreeRegressor()
model.fit(x_train,y_train)
148/7: y_predict = model.predict(x_test)
148/8: y_predict
148/9: y_test
148/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
149/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
149/2: bihar = pd.read_csv("Bihar_entier.csv")
149/3: bihar.head(2)
149/4:
x_data = bihar[["JUN"]]

y_data = bihar[["JUL"]]
149/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
149/6:
model = LinearRegression()
model.fit(x_train,y_train)
149/7: y_predict = model.predict(x_test)
149/8: y_predict
149/9: y_test
149/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
149/11:
x_data = bihar[["JAN","FEB","MAR","APR","JUN"]]
y_data = bihar[["JUL"]]
150/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
150/2: bihar = pd.read_csv("Bihar_entier.csv")
150/3: bihar.head(2)
150/4:
x_data = bihar[["JAN","FEB","MAR","APR","JUN"]]
y_data = bihar[["JUL"]]
150/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
150/6:
model = LinearRegression()
model.fit(x_train,y_train)
150/7: y_predict = model.predict(x_test)
150/8: y_predict
150/9: y_test
150/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
151/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
151/2: bihar = pd.read_csv("Bihar_entier.csv")
151/3: bihar.head(2)
151/4:
x_data = bihar[["JAN","FEB","MAR","APR","JUN"]]
y_data = bihar[["JUL"]]
151/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
151/6:
model = DecisionTreeRegressor()
model.fit(x_train,y_train)
151/7: y_predict = model.predict(x_test)
151/8: y_predict
151/9: y_test
151/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
152/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
152/2: bihar = pd.read_csv("Bihar_entier.csv")
152/3: bihar.head(2)
152/4:
x_data = bihar[["JAN","FEB","MAR","APR","JUN"]]
y_data = bihar[["JUL"]]
152/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
152/6:
model = DecisionTreeRegressor()
model.fit(x_train,y_train)
152/7: y_predict = model.predict(x_test)
152/8: y_predict
152/9: y_test
152/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
153/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
153/2: bihar = pd.read_csv("Bihar_entier.csv")
153/3: bihar.head(2)
153/4:
x_data = bihar[["JAN","FEB","MAR","APR","JUN"]]
y_data = bihar[["JUL"]]
153/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
153/6:
model = SVR()
model.fit(x_train,y_train)
153/7: y_predict = model.predict(x_test)
153/8: y_predict
153/9: y_test
153/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
154/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
154/2: bihar = pd.read_csv("Bihar_entier.csv")
154/3: bihar.head(2)
154/4:
x_data = bihar[["Jan-Feb","Mar-May"]]
y_data = bihar[["Jun-Sep"]]
154/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
154/6:
model = SVR()
model.fit(x_train,y_train)
154/7: y_predict = model.predict(x_test)
154/8: y_predict
154/9: y_test
154/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
155/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
155/2: bihar = pd.read_csv("Bihar_entier.csv")
155/3: bihar.head(2)
155/4:
x_data = bihar[["Jan-Feb","Mar-May"]]
y_data = bihar[["Jun-Sep"]]
155/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
155/6:
model = LinearRegression()
model.fit(x_train,y_train)
155/7: y_predict = model.predict(x_test)
155/8: y_predict
155/9: y_test
155/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
156/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
156/2: bihar = pd.read_csv("Bihar_entier.csv")
156/3: bihar.head(2)
156/4:
x_data = bihar[["Jan-Feb","Mar-May"]]
y_data = bihar[["Jun-Sep"]]
156/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
156/6:
model = DecisionTreeRegressor()
model.fit(x_train,y_train)
156/7: y_predict = model.predict(x_test)
156/8: y_predict
156/9: y_test
156/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
157/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
157/2: bihar = pd.read_csv("Bihar_entier.csv")
157/3: bihar.head(2)
157/4:
x_data = bihar[["Jan-Feb","Mar-May"]]
y_data = bihar[["Jun-Sep"]]
157/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
157/6:
model = RandomForestRegressor()
model.fit(x_train,y_train)
157/7: y_predict = model.predict(x_test)
157/8: y_predict
157/9: y_test
157/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
158/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
158/2: bihar = pd.read_csv("Bihar_entier.csv")
158/3: bihar.head(2)
158/4:
x_data = bihar[["Jan-Feb","Mar-May"]]
y_data = bihar[["Jun-Sep"]]
158/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
158/6:
model = SVR()
model.fit(x_train,y_train)
158/7: y_predict = model.predict(x_test)
158/8: y_predict
158/9: y_test
158/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
159/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
159/2: bihar = pd.read_csv("Bihar_entier.csv")
159/3: bihar.head(2)
159/4:
x_data = bihar[["Mar-May"]]
y_data = bihar[["Jun-Sep"]]
159/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
159/6:
model = SVR()
model.fit(x_train,y_train)
159/7: y_predict = model.predict(x_test)
159/8: y_predict
159/9: y_test
159/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
160/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
160/2: bihar = pd.read_csv("Bihar_entier.csv")
160/3: bihar.head(2)
160/4:
x_data = bihar[["Mar-May"]]
y_data = bihar[["Jun-Sep"]]
160/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
160/6:
model = LinearRegression()
model.fit(x_train,y_train)
160/7: y_predict = model.predict(x_test)
160/8: y_predict
160/9: y_test
160/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
161/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
161/2: bihar = pd.read_csv("Bihar_entier.csv")
161/3: bihar.head(2)
161/4:
x_data = bihar[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y_data = bihar[["ANNUAL"]]
161/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
161/6:
model = LinearRegression()
model.fit(x_train,y_train)
161/7: y_predict = model.predict(x_test)
161/8: y_predict
161/9: y_test
161/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
162/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,root
163/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
163/2: bihar = pd.read_csv("Bihar_entier.csv")
163/3: bihar.head(2)
163/4:
x_data = bihar[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y_data = bihar[["ANNUAL"]]
163/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
163/6:
model = DecisionTreeRegressor()
model.fit(x_train,y_train)
163/7: y_predict = model.predict(x_test)
163/8: y_predict
163/9: y_test
163/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
print("\nRoot mean square error",)
164/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
164/2: bihar = pd.read_csv("Bihar_entier.csv")
164/3: bihar.head(2)
164/4:
x_data = bihar[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y_data = bihar[["ANNUAL"]]
164/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
164/6:
model = RandomForestRegressor()
model.fit(x_train,y_train)
164/7: y_predict = model.predict(x_test)
164/8: y_predict
164/9: y_test
164/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
print("\nRoot mean square error",)
165/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
165/2: bihar = pd.read_csv("Bihar_entier.csv")
165/3: bihar.head(2)
165/4:
x_data = bihar[["Jan-Feb","Mar-May","Jun-Sep","Oct-Dec"]]
y_data = bihar[["ANNUAL"]]
165/5: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
165/6:
model = SVR()
model.fit(x_train,y_train)
165/7: y_predict = model.predict(x_test)
165/8: y_predict
165/9: y_test
165/10:
print("Mean squared error(mse)", mean_squared_error(y_test,y_predict))
print("\nMean absolute error(mae)", mean_absolute_error(y_test,y_predict))
print("\nR square score(r2_score)", r2_score(y_test,y_predict))
print("\nRoot mean square error",)
165/11:
plt.scatter(x_test,y_test)
plt.plot(x_test,y_predict,color="m")
166/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
166/2:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
166/3: suv_data = pd.read_csv()
166/4: SUV_data = pd.read_csv("suv_data.csv")
166/5: SUV_data.head(5)
166/6:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
166/7:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
167/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
167/2: SUV_data = pd.read_csv("suv_data.csv")
167/3: SUV_data.head(5)
168/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
168/2: SUV_data = pd.read_csv("suv_data.csv")
168/3: SUV_data.head(5)
168/4: SUV_data.info()
168/5:
SUV_data.isnull()                                  # to check if any null values is present or not it will return a bollen
                                                   #    if null then True else False
168/6:
#print(SUV_data.isnull())                                  # to check if any null values is present or not it will return a bollen
                                                   #    if null then True else False

SUV_data.isnull().sum()
168/7: sns.countplot.plt(x="Purchased",SUV_data)
168/8: sns.countplot.plt(x="Purchased",data = SUV_data)
168/9: sns.countplot(x="Purchased",data = SUV_data)
168/10: sns.countplot(x="purchased", hue = "Gender", data = SUV_data)
168/11: sns.countplot(x="Purchased", hue = "Gender", data = SUV_data)
168/12: sns.countplot(x="Purchased", hue = "Age", data = SUV_data)
168/13: SUV_data."Age".plt.hist()
168/14: SUV_data["Age"].plt.hist()
168/15: SUV_data."Age".plot.hist()
168/16: SUV_data["Age"].plot.hist()
168/17: gender = pd.get_dummies(SUV_data["Gender"])
168/18:
gender = pd.get_dummies(SUV_data["Gender"])
gender
168/19:
gender = pd.get_dummies(SUV_data["Gender"],drop_first=True)
gender
168/20:
SUV_data = pd.concat(["gender"],axis=1)
SUV_data
168/21:
SUV_data = pd.concat([gender],axis=1)
SUV_data
168/22:
SUV_data = pd.concat([gender],axis=1)
SUV_data.head(2)
168/23:
SUV_data = pd.concat([SUV_data,gender],axis=1)
SUV_data.head(2)
168/24: SUV_data.head(5)
169/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
169/2: SUV_data = pd.read_csv("suv_data.csv")
169/3: SUV_data.head(5)
169/4: SUV_data.info()
169/5:
#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen
                                    #    if null then True else False

SUV_data.isnull().sum()            # return the sum of each column
169/6: sns.countplot(x="Purchased",data = SUV_data)
169/7: sns.countplot(x="Purchased", hue = "Gender", data = SUV_data)    # hue = particular column name
169/8: SUV_data["Age"].plot.hist()
169/9:
gender = pd.get_dummies(SUV_data["Gender"],drop_first=True)
gender
169/10:
SUV_data = pd.concat([SUV_data,gender],axis=1)
SUV_data.head(2)
169/11:
SUV_data.drop(["User ID","Gender"],axis=1,inplace=True)
SUV_data
169/12:
x = SUV_data.drop(["Purchased"],axis=1)
y = SUV_data["Purchased"]
169/13: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
169/14:
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
model = LogisticRegression()
model.fit(x_tarin,y_test)
169/15:
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
model = LogisticRegression()
model.fit(x_train,y_test)
169/16:
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
model = LogisticRegression()
model.fit(x_train,y_train)
169/17: predicted = model.predict(y_train)
169/18: predicted = model.predict([y_train])
169/19: predicted = model.predict(x_test)
169/20:
predicted = model.predict(x_test)
predicted
169/21: y_test
169/22: list(y_test)
169/23:
z = list(y_test)
z
170/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
170/2: SUV_data = pd.read_csv("suv_data.csv")
170/3: SUV_data.head(5)
170/4: SUV_data.info()
170/5:
#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen
                                    #    if null then True else False

SUV_data.isnull().sum()            # return the sum of each column
170/6: sns.countplot(x="Purchased",data = SUV_data)
170/7: sns.countplot(x="Purchased", hue = "Gender", data = SUV_data)    # hue = particular column name
170/8: SUV_data["Age"].plot.hist()
170/9:
gender = pd.get_dummies(SUV_data["Gender"],drop_first=True)
gender
170/10:
SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code
SUV_data.head(2)
170/11:
SUV_data.drop(["User ID","Gender"],axis=1,inplace=True)
SUV_data
170/12:
x = SUV_data.([["Age","EstimatedSalary","Male"]])
y = SUV_data["Purchased"]
171/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
171/2: SUV_data = pd.read_csv("suv_data.csv")
171/3: SUV_data.head(5)
171/4: SUV_data.info()
171/5:
#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen
                                    #    if null then True else False

SUV_data.isnull().sum()            # return the sum of each column
171/6: sns.countplot(x="Purchased",data = SUV_data)
171/7: sns.countplot(x="Purchased", hue = "Gender", data = SUV_data)    # hue = particular column name
171/8: SUV_data["Age"].plot.hist()
171/9:
gender = pd.get_dummies(SUV_data["Gender"],drop_first=True)
gender
171/10:
SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code
SUV_data.head(2)
171/11:
SUV_data.drop(["User ID","Gender"],axis=1,inplace=True)
SUV_data
171/12:
x = SUV_data.["Age","EstimatedSalary","Male"]
y = SUV_data["Purchased"]
172/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
172/2: SUV_data = pd.read_csv("suv_data.csv")
172/3: SUV_data.head(5)
172/4: SUV_data.info()
172/5:
#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen
                                    #    if null then True else False

SUV_data.isnull().sum()            # return the sum of each column
172/6: sns.countplot(x="Purchased",data = SUV_data)
172/7: sns.countplot(x="Purchased", hue = "Gender", data = SUV_data)    # hue = particular column name
172/8: SUV_data["Age"].plot.hist()
172/9:
gender = pd.get_dummies(SUV_data["Gender"],drop_first=True)
gender
172/10:
SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code
SUV_data.head(2)
172/11:
SUV_data.drop(["User ID","Gender"],axis=1,inplace=True)
SUV_data
172/12:
x = SUV_data["Age","EstimatedSalary","Male"]
y = SUV_data["Purchased"]
172/13:
x = SUV_data[["Age","EstimatedSalary","Male"]]
y = SUV_data["Purchased"]
173/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
173/2: SUV_data = pd.read_csv("suv_data.csv")
173/3: SUV_data.head(5)
173/4: SUV_data.info()
173/5:
#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen
                                    #    if null then True else False

SUV_data.isnull().sum()            # return the sum of each column
173/6: sns.countplot(x="Purchased",data = SUV_data)
173/7: sns.countplot(x="Purchased", hue = "Gender", data = SUV_data)    # hue = particular column name
173/8: SUV_data["Age"].plot.hist()
173/9:
gender = pd.get_dummies(SUV_data["Gender"],drop_first=True)
gender
173/10:
SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code
SUV_data.head(2)
173/11:
SUV_data.drop(["User ID","Gender"],axis=1,inplace=True)
SUV_data
173/12:
x = SUV_data[["Age","EstimatedSalary","Male"]]
y = SUV_data["Purchased"]
173/13:
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
model = LogisticRegression()
model.fit(x_train,y_train)
173/14:
predicted = model.predict(x_test)
predicted
174/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
174/2: SUV_data = pd.read_csv("suv_data.csv")
174/3: SUV_data.head(5)
174/4: SUV_data.info()
174/5:
#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen
                                    #    if null then True else False

SUV_data.isnull().sum()            # return the sum of each column
174/6: sns.countplot(x="Purchased",data = SUV_data)
174/7: sns.countplot(x="Purchased", hue = "Gender", data = SUV_data)    # hue = particular column name
174/8: SUV_data["Age"].plot.hist()
174/9:
gender = pd.get_dummies(SUV_data["Gender"],drop_first=True)
gender
174/10:
SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code
SUV_data.head(2)
174/11:
SUV_data.drop(["User ID","Gender"],axis=1,inplace=True)
SUV_data
174/12:
x = SUV_data[["Age","EstimatedSalary"]]
y = SUV_data["Purchased"]
174/13:
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
model = LogisticRegression()
model.fit(x_train,y_train)
174/14:
predicted = model.predict(x_test)
predicted
174/15:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
174/16: accuracy_score(y_test,predicted)
175/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
175/2: SUV_data = pd.read_csv("suv_data.csv")
175/3: SUV_data.head(5)
175/4: SUV_data.info()
175/5:
#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen
                                    #    if null then True else False

SUV_data.isnull().sum()            # return the sum of each column
175/6: sns.countplot(x="Purchased",data = SUV_data)
175/7: sns.countplot(x="Purchased", hue = "Gender", data = SUV_data)    # hue = particular column name
175/8: SUV_data["Age"].plot.hist()
175/9:
gender = pd.get_dummies(SUV_data["Gender"],drop_first=True)
gender
175/10:
SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code
SUV_data.head(2)
175/11:
SUV_data.drop(["User ID","Gender"],axis=1,inplace=True)
SUV_data
175/12:
x = SUV_data[["Age","EstimatedSalary"]]
y = SUV_data["Purchased"]
175/13:
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
model = LogisticRegression()
model.fit(x_train,y_train)
175/14:
predicted = model.predict(x_test)
predicted
175/15: accuracy_score(y_test,predicted)
176/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
176/2: SUV_data = pd.read_csv("suv_data.csv")
176/3: SUV_data.head(5)
176/4: SUV_data.info()
176/5:
#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen
                                    #    if null then True else False

SUV_data.isnull().sum()            # return the sum of each column
176/6: sns.countplot(x="Purchased",data = SUV_data)
176/7: sns.countplot(x="Purchased", hue = "Gender", data = SUV_data)    # hue = particular column name
176/8: SUV_data["Age"].plot.hist()
176/9:
gender = pd.get_dummies(SUV_data["Gender"],drop_first=True)
gender
176/10:
SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code
SUV_data.head(2)
176/11:
SUV_data.drop(["User ID","Gender"],axis=1,inplace=True)
SUV_data
176/12:
x = SUV_data[["Age","EstimatedSalary","Male"]]
y = SUV_data["Purchased"]
176/13:
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
model = LogisticRegression()
model.fit(x_train,y_train)
176/14:
predicted = model.predict(x_test)
predicted
176/15: accuracy_score(y_test,predicted)
176/16:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
176/17:
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
print(x_train)
model = LogisticRegression()
model.fit(x_train,y_train)
176/18:
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.fit_transform(x_test)
pritn(x_train)
176/19:
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.fit_transform(x_test)
print(x_train)
176/20:
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.fit_transform(x_test)
x_train.head(10)
176/21:
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.fit_transform(x_test)
x_train
177/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
177/2: SUV_data = pd.read_csv("suv_data.csv")
177/3: SUV_data.head(5)
177/4: SUV_data.info()
177/5:
#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen
                                    #    if null then True else False

SUV_data.isnull().sum()            # return the sum of each column
177/6: sns.countplot(x="Purchased",data = SUV_data)
177/7: sns.countplot(x="Purchased", hue = "Gender", data = SUV_data)    # hue = particular column name
177/8: SUV_data["Age"].plot.hist()
177/9:
gender = pd.get_dummies(SUV_data["Gender"],drop_first=True)
gender
177/10:
SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code
SUV_data.head(2)
177/11:
SUV_data.drop(["User ID","Gender"],axis=1,inplace=True)
SUV_data
177/12:
x = SUV_data[["Age","EstimatedSalary","Male"]]
y = SUV_data["Purchased"]
177/13: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
177/14:
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.fit_transform(x_test)
177/15:
model = LogisticRegression()
model.fit(x_train,y_train)
177/16:
#    Age  EstimatedSalary  Male
#3     27            57000     0
#18    46            28000     1
#202   39           134000     0
#250   44            39000     0
#274   57            26000     0
..   ...              ...   ...
#71    24            27000     0
#106   26            35000     0
#270   43           133000     0
#348   39            77000     1
#102   32            86000     0
178/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
178/2: SUV_data = pd.read_csv("suv_data.csv")
178/3: SUV_data.head(5)
178/4: SUV_data.info()
178/5:
#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen
                                    #    if null then True else False

SUV_data.isnull().sum()            # return the sum of each column
178/6: sns.countplot(x="Purchased",data = SUV_data)
178/7: sns.countplot(x="Purchased", hue = "Gender", data = SUV_data)    # hue = particular column name
178/8: SUV_data["Age"].plot.hist()
178/9:
gender = pd.get_dummies(SUV_data["Gender"],drop_first=True)
gender
178/10:
SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code
SUV_data.head(2)
178/11:
SUV_data.drop(["User ID","Gender"],axis=1,inplace=True)
SUV_data
178/12:
x = SUV_data[["Age","EstimatedSalary","Male"]]
y = SUV_data["Purchased"]
178/13: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
178/14:
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.fit_transform(x_test)
178/15:
model = LogisticRegression()
model.fit(x_train,y_train)
178/16:
#    Age  EstimatedSalary  Male
#3     27            57000     0
#18    46            28000     1
#202   39           134000     0
#250   44            39000     0
#274   57            26000     0
#..   ...              ...   ...
#71    24            27000     0
#106   26            35000     0
#270   43           133000     0
#348   39            77000     1
#102   32            86000     0
178/17:
predicted = model.predict(x_test)
predicted
178/18: accuracy_score(y_test,predicted)
179/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
179/2: SUV_data = pd.read_csv("suv_data.csv")
179/3: SUV_data.head(5)
179/4: SUV_data.info()
179/5:
#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen
                                    #    if null then True else False

SUV_data.isnull().sum()            # return the sum of each column
179/6: sns.countplot(x="Purchased",data = SUV_data)
179/7: sns.countplot(x="Purchased", hue = "Gender", data = SUV_data)    # hue = particular column name
179/8: SUV_data["Age"].plot.hist()
179/9:
gender = pd.get_dummies(SUV_data["Gender"],drop_first=True)
gender
179/10:
SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code
SUV_data.head(2)
179/11:
SUV_data.drop(["User ID","Gender"],axis=1,inplace=True)
SUV_data
179/12:
x = SUV_data[["Age","EstimatedSalary","Male"]]
y = SUV_data["Purchased"]
179/13: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
179/14:
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.fit_transform(x_test)
179/15:
model = LogisticRegression()
model.fit(x_train,y_train)
179/16:
#    Age  EstimatedSalary  Male
#3     27            57000     0
#18    46            28000     1
#202   39           134000     0
#250   44            39000     0
#274   57            26000     0
#..   ...              ...   ...
#71    24            27000     0
#106   26            35000     0
#270   43           133000     0
#348   39            77000     1
#102   32            86000     0
179/17:
predicted = model.predict(x_test)
predicted
179/18: accuracy_score(y_test,predicted)
179/19:
x_data = [27,57000,0]
x_data = sc.fit_transform(x_data)
179/20:
x_data = [[27,57000,0]]
x_data = sc.fit_transform(x_data)
179/21:
x_data = [[27,57000,0]]
x_data = sc.fit_transform(x_data)
model.predict(x_data)
179/22:
x_data = [[46,41000,0]]
x_data = sc.fit_transform(x_data)
model.predict(x_data)
179/23:
x_data = [[46,41000,0]]
x_data = sc.fit_transform(x_data)
model.predict(x_data)
179/24:
x_data = [[51,23000,1]]
x_data = sc.fit_transform(x_data)
model.predict(x_data)
179/25: y_test
179/26: list(y_test)
180/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
180/2: SUV_data = pd.read_csv("suv_data.csv")
180/3: SUV_data.head(5)
180/4: SUV_data.info()
180/5:
#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen
                                    #    if null then True else False

SUV_data.isnull().sum()            # return the sum of each column
180/6: sns.countplot(x="Purchased",data = SUV_data)
180/7: sns.countplot(x="Purchased", hue = "Gender", data = SUV_data)    # hue = particular column name
180/8: SUV_data["Age"].plot.hist()
180/9:
gender = pd.get_dummies(SUV_data["Gender"],drop_first=True)
gender
180/10:
SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code
SUV_data.head(2)
180/11:
SUV_data.drop(["User ID","Gender"],axis=1,inplace=True)
SUV_data
180/12:
x = SUV_data[["Age","EstimatedSalary","Male"]]
y = SUV_data["Purchased"]
180/13: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
180/14:
sc = StandardScaler()                    # we use Standered scaller to scall down the values to eqal limits because some
x_train = sc.fit_transform(x_train)      # columns are having very high values as compare to the other column which are not
x_test = sc.fit_transform(x_test)        # even close to those values(high) which will affecet out prediction.
180/15:
model = LogisticRegression()
model.fit(x_train,y_train)
180/16:
predicted = model.predict(x_test)
#predicted
180/17: accuracy_score(y_test,predicted)
181/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
181/2: SUV_data = pd.read_csv("suv_data.csv")
181/3: SUV_data.head(5)
181/4: SUV_data.info()
181/5:
#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen
                                    #    if null then True else False

SUV_data.isnull().sum()            # return the sum of each column
181/6: sns.countplot(x="Purchased",data = SUV_data)
181/7: sns.countplot(x="Purchased", hue = "Gender", data = SUV_data)    # hue = particular column name
181/8: SUV_data["Age"].plot.hist()
181/9:
gender = pd.get_dummies(SUV_data["Gender"],drop_first=True)
gender
181/10:
SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code
SUV_data.head(2)
181/11:
SUV_data.drop(["User ID","Gender"],axis=1,inplace=True)
SUV_data
181/12:
x = SUV_data[["Age","EstimatedSalary","Male"]]
y = SUV_data["Purchased"]
181/13: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
181/14:
sc = StandardScaler()                    # we use Standered scaller to scall down the values to eqal limits because some
x_train = sc.fit_transform(x_train)      # columns are having very high values as compare to the other column which are not
x_test = sc.fit_transform(x_test)        # even close to those values(high) which will affecet out prediction.
181/15:
model = LogisticRegression()
model.fit(x_train,y_train)
181/16:
predicted = model.predict(x_test)
#predicted
181/17: accuracy_score(y_test,predicted)
181/18: r2_score(y_test,predicted)
182/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
182/2: Breast = pd.read_csv("Breast_cancer.csv")
182/3: Breast.info()
182/4:
Breast.info()
Breast.head(4)
182/5:
#Breast.info()
Breast.head(4)
183/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
183/2: Breast = pd.read_csv("Breast_cancer.csv")
183/3:
#Breast.info()
Breast.head(4)
183/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
183/5: Breast["class"] = Class_le.fit_transform(Breast["Class:"])
183/6:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast.head(4)
183/7:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast.head(4)
183/8:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast.head(4)
183/9:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
183/10:
Breast = Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat: "],
                     axis=1,inplace=True)
Breast.head(3)
183/11:
Breast = Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(3)
183/12:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(3)
184/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
184/2: Breast = pd.read_csv("Breast_cancer.csv")
184/3:
#Breast.info()
Breast.head(4)
184/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
184/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
184/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(3)
184/7:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
184/8:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
184/9:
x_data = Breast.drop(Breast["Irradiat"])
y_data = Breast["Irradiat"]


#x_train, x_test, y_train, y_test = train_test_split()
184/10:
x_data = Breast.drop(Breast["Irradiat"])
y_data = Breast["Irradiat"]

x_data
#x_train, x_test, y_train, y_test = train_test_split()
184/11:
x_data = Breast.drop(["Irradiat"])
y_data = Breast["Irradiat"]

x_data
#x_train, x_test, y_train, y_test = train_test_split()
184/12:
x_data = Breast.drop("Irradiat")
y_data = Breast["Irradiat"]

x_data
#x_train, x_test, y_train, y_test = train_test_split()
185/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
186/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
186/2: Breast = pd.read_csv("Breast_cancer.csv")
186/3:
#Breast.info()
Breast.head(4)
186/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
186/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
186/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(3)
186/7:
x_data = Breast.drop("Irradiat")
y_data = Breast["Irradiat"]

x_data
#x_train, x_test, y_train, y_test = train_test_split()
186/8:
x_data = Breast.drop(["Irradiat"])
y_data = Breast["Irradiat"]

x_data
#x_train, x_test, y_train, y_test = train_test_split()
187/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
187/2: Breast = pd.read_csv("Breast_cancer.csv")
187/3:
#Breast.info()
Breast.head(4)
187/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
187/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
187/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(3)
187/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_data
#x_train, x_test, y_train, y_test = train_test_split()
187/8:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)
187/9: predicted = model.predicr(x_test)
187/10:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)
187/11: predicted = model.predicr(x_test)
187/12: predicted = model.predict(x_test)
187/13: predicted = model.predict(x_test)
187/14:
predicted = model.predict(x_test)
predict
187/15:
predicted = model.predict(x_test)
predicted
187/16:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
187/17: accuracy_score(y_test,predicted)
187/18: y_test
187/19: list(y_test)
187/20: y_test
188/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
188/2: Breast = pd.read_csv("Breast_cancer.csv")
188/3:
#Breast.info()
Breast.head(4)
188/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
188/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
188/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
188/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)
188/8:
predicted = model.predict(x_test)
predicted
188/9: y_test
188/10: accuracy_score(y_test,predicted)
189/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
189/2: Breast = pd.read_csv("Breast_cancer.csv")
189/3:
#Breast.info()
Breast.head(4)
189/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
#deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
189/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
#Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
189/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
189/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)
189/8:
predicted = model.predict(x_test)
predicted
189/9: y_test
189/10: accuracy_score(y_test,predicted)
190/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
190/2: Breast = pd.read_csv("Breast_cancer.csv")
190/3:
#Breast.info()
Breast.head(4)
190/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
190/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
190/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
190/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)
190/8:
predicted = model.predict(x_test)
predicted
190/9: y_test
190/10: accuracy_score(y_test,predicted)
191/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
191/2: Breast = pd.read_csv("Breast_cancer.csv")
191/3:
#Breast.info()
Breast.head(4)
191/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
191/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
191/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
191/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)
191/8:
predicted = model.predict(x_test)
predicted
191/9: y_test
191/10: accuracy_score(y_test,predicted)
192/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
192/2: Breast = pd.read_csv("Breast_cancer.csv")
192/3:
#Breast.info()
Breast.head(4)
192/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
192/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
192/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
192/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)
192/8:
predicted = model.predict(x_test)
predicted
192/9: y_test
192/10: accuracy_score(y_test,predicted)
193/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
193/2: Breast = pd.read_csv("Breast_cancer.csv")
193/3:
#Breast.info()
Breast.head(4)
193/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
193/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
193/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
193/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
193/8:
predicted = model.predict(x_test)
predicted
193/9: y_test
193/10: accuracy_score(y_test,predicted)
194/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
194/2: Breast = pd.read_csv("Breast_cancer.csv")
194/3:
#Breast.info()
Breast.head(4)
194/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
194/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
194/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
194/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
194/8:
predicted = model.predict(x_test)
predicted
194/9: y_test
194/10: accuracy_score(y_test,predicted)
195/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
195/2: Breast = pd.read_csv("Breast_cancer.csv")
195/3:
#Breast.info()
Breast.head(4)
195/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
195/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
195/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
195/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
195/8:
predicted = model.predict(x_test)
predicted
195/9: y_test
195/10: accuracy_score(y_test,predicted)
196/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
196/2: Breast = pd.read_csv("Breast_cancer.csv")
196/3:
#Breast.info()
Breast.head(4)
196/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
196/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
196/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
196/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
196/8:
predicted = model.predict(x_test)
predicted
196/9: y_test
196/10: accuracy_score(y_test,predicted)
197/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
197/2: Breast = pd.read_csv("Breast_cancer.csv")
197/3:
#Breast.info()
Breast.head(4)
197/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
197/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
197/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
197/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(x_train, y_train)

x_train
197/8:
predicted = model.predict(x_test)
predicted
197/9: y_test
197/10: accuracy_score(y_test,predicted)
198/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
198/2: Breast = pd.read_csv("Breast_cancer.csv")
198/3:
#Breast.info()
Breast.head(4)
198/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
198/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
198/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
198/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(x_train, y_train)

x_train
198/8:
predicted = model.predict(x_test)
predicted
198/9: y_test
198/10: accuracy_score(y_test,predicted)
198/11:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
199/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
199/2: Breast = pd.read_csv("Breast_cancer.csv")
199/3:
#Breast.info()
Breast.head(4)
199/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
199/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
199/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
199/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = RandomForestRegressor()
model.fit(x_train, y_train)

x_train
199/8:
predicted = model.predict(x_test)
predicted
199/9: y_test
199/10: accuracy_score(y_test,predicted)
200/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
200/2: Breast = pd.read_csv("Breast_cancer.csv")
200/3:
#Breast.info()
Breast.head(4)
200/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
200/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
200/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
200/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(x_train, y_train)

x_train
200/8:
predicted = model.predict(x_test)
predicted
200/9: y_test
200/10: accuracy_score(y_test,predicted)
201/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
201/2: Breast = pd.read_csv("Breast_cancer.csv")
201/3:
#Breast.info()
Breast.head(4)
201/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
201/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
201/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
201/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(x_train, y_train)

x_train
201/8:
predicted = model.predict(x_test)
predicted
201/9: #y_test
201/10: accuracy_score(y_test,predicted)
202/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
202/2: Breast = pd.read_csv("Breast_cancer.csv")
202/3:
#Breast.info()
Breast.head(4)
202/4:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
202/5:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
202/6:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
202/7:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
202/8:
predicted = model.predict(x_test)
predicted
202/9: #y_test
202/10: accuracy_score(y_test,predicted)
202/11:
#Breast.info()
Breast.tail(4)
202/12: Breast = pd.read_csv("Breast_cancer.csv")
202/13:
#Breast.info()
Breast.tail(4)
203/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
203/2: Breast = pd.read_csv("Breast_cancer.csv")
203/3:
#Breast.info()
Breast.tail(4)
203/4:
# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can 
# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels
# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event
# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event
# and 1 as recurrence-event or some other values.
203/5:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
203/6:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
203/7:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
203/8:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
203/9:
predicted = model.predict(x_test)
predicted
203/10: #y_test
203/11: accuracy_score(y_test,predicted)
204/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
204/2: Breast = pd.read_csv("Breast_cancer.csv")
204/3:
#Breast.info()
Breast.tail(4)
204/4:
# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can 
# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels
# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event
# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event
# and 1 as recurrence-event or some other values.
204/5:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
#deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
204/6:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
#Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
204/7:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
204/8:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
204/9:
predicted = model.predict(x_test)
predicted
204/10: #y_test
204/11: accuracy_score(y_test,predicted)
205/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
205/2: Breast = pd.read_csv("Breast_cancer.csv")
205/3:
#Breast.info()
Breast.tail(4)
205/4:
# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can 
# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels
# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event
# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event
# and 1 as recurrence-event or some other values.
205/5:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
#deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
205/6:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
#Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
205/7:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
205/8:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
205/9:
predicted = model.predict(x_test)
predicted
205/10: #y_test
205/11: accuracy_score(y_test,predicted)
206/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
206/2: Breast = pd.read_csv("Breast_cancer.csv")
206/3:
#Breast.info()
Breast.tail(4)
206/4:
# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can 
# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels
# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event
# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event
# and 1 as recurrence-event or some other values.
206/5:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
#deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
206/6:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
#Breast["Deg-malig"] = deg_malig_le.fit_transform(Breast["deg-malig:"])
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
206/7:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
206/8:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
206/9:
predicted = model.predict(x_test)
predicted
206/10: #y_test
206/11: accuracy_score(y_test,predicted)
207/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
207/2: Breast = pd.read_csv("Breast_cancer.csv")
207/3:
#Breast.info()
Breast.tail(4)
207/4:
# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can 
# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels
# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event
# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event
# and 1 as recurrence-event or some other values.
207/5:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
#deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
207/6:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = Breast["deg-malig:"]
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
207/7:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
207/8:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
207/9:
predicted = model.predict(x_test)
predicted
207/10: #y_test
207/11: accuracy_score(y_test,predicted)
208/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
208/2: Breast = pd.read_csv("Breast_cancer.csv")
208/3:
#Breast.info()
Breast.tail(4)
208/4:
# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can 
# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels
# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event
# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event
# and 1 as recurrence-event or some other values.
208/5:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
#deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
208/6:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = Breast["deg-malig:"]
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
208/7:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
208/8:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
208/9:
predicted = model.predict(x_test)
predicted
208/10: #y_test
208/11: accuracy_score(y_test,predicted)
208/12:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = Breast["deg-malig:"]
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast
208/13:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = Breast["deg-malig:"]
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.head(4)
208/14:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = Breast["deg-malig:"]
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast
209/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
209/2: Breast = pd.read_csv("Breast_cancer.csv")
209/3:
#Breast.info()
Breast.tail(4)
209/4:
# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can 
# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels
# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event
# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event
# and 1 as recurrence-event or some other values.
209/5:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
#deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
209/6:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = Breast["deg-malig:"]
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast
209/7:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
209/8:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
209/9:
predicted = model.predict(x_test)
predicted
209/10: #y_test
209/11: accuracy_score(y_test,predicted)
210/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
210/2:
Breast = pd.read_csv("Breast_cancer.csv")
Test_data = pd.read_csv("test_data.csv")
210/3:
#Breast.info()
Breast.tail(4)
210/4:
# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can 
# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels
# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event
# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event
# and 1 as recurrence-event or some other values.
210/5:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
#deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
210/6:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = Breast["deg-malig:"]
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.to_csv("New_data.csv",index=False)
210/7:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
210/8:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
210/9:
predicted = model.predict(x_test)
predicted
210/10: #y_test
210/11: accuracy_score(y_test,predicted)
210/12: Test_data.head(4)
211/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
211/2:
Breast = pd.read_csv("Breast_cancer.csv")
Test_data = pd.read_csv("test_data.csv")
211/3:
#Breast.info()
Breast.tail(4)
211/4:
# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can 
# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels
# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event
# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event
# and 1 as recurrence-event or some other values.
211/5:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
#deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
211/6:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = Breast["deg-malig:"]
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
Breast.to_csv("New_data.csv",index=False)
212/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
212/2:
Breast = pd.read_csv("Breast_cancer.csv")
Test_data = pd.read_csv("test_data.csv")
212/3:
#Breast.info()
Breast.tail(4)
212/4:
# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can 
# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels
# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event
# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event
# and 1 as recurrence-event or some other values.
212/5:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
#deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
212/6:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = Breast["deg-malig:"]
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
#Breast.to_csv("New_data.csv",index=False)
212/7:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
212/8:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
212/9:
predicted = model.predict(x_test)
predicted
212/10: Test_data.head(4)
212/11: accuracy_score(y_test,predicted)
212/12: Test_data_x = Test_data.drop(["irradit"],axis=1)
212/13: Test_data_x = Test_data.drop(["irradit:"],axis=1)
213/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
213/2:
Breast = pd.read_csv("Breast_cancer.csv")
Test_data = pd.read_csv("test_data.csv")
213/3:
#Breast.info()
Breast.tail(4)
213/4:
# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can 
# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels
# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event
# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event
# and 1 as recurrence-event or some other values.
213/5:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
#deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
213/6:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = Breast["deg-malig:"]
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
#Breast.to_csv("New_data.csv",index=False)
213/7:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
213/8:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
213/9:
predicted = model.predict(x_test)
predicted
213/10: Test_data.head(4)
213/11: Test_data_x = Test_data.drop(["irradit:"],axis=1)
213/12: Test_data_x = Test_data.drop(["irradiat:"],axis=1)
213/13:
Test_data_x = Test_data.drop(["irradiat:"],axis=1)
Test_data["Predicted"] = model.predict(Test_data_x)
214/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
214/2:
Breast = pd.read_csv("Breast_cancer.csv")
Test_data = pd.read_csv("test_data.csv")
214/3:
#Breast.info()
Breast.tail(4)
214/4:
# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can 
# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels
# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event
# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event
# and 1 as recurrence-event or some other values.
214/5:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
#deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
214/6:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = Breast["deg-malig:"]
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
#Breast.to_csv("New_data.csv",index=False)
214/7:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
214/8:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
214/9:
predicted = model.predict(x_test)
predicted
214/10: Test_data.head(4)
214/11:
Test_data_x = Test_data.drop(["irradiat:"],axis=1)
Test_data["Predicted"] = model.predict(Test_data_x)
214/12: Test_data.head(25)
214/13: accuracy_score(y_test,predicted)
215/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
215/2:
Breast = pd.read_csv("Breast_cancer.csv")
Test_data = pd.read_csv("test_data.csv")
215/3:
#Breast.info()
Breast.tail(4)
215/4:
# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can 
# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels
# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event
# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event
# and 1 as recurrence-event or some other values.
215/5:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
#deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
215/6:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = Breast["deg-malig:"]
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
#Breast.to_csv("New_data.csv",index=False)
215/7:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
215/8:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
215/9:
predicted = model.predict(x_test)
predicted
215/10: Test_data.head(4)
215/11:
Test_data_x = Test_data.drop(["irradiat:"],axis=1)
Test_data["Predicted"] = model.predict(Test_data_x)
215/12: Test_data.head(31)
215/13: accuracy_score(y_test,predicted)
215/14: accuracy_score(Test_data["irradiat:"],Test_data["Predicted"])
215/15:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
215/16:
rmodel = RandomForestClassifier(n_estimators=10)
rmodel.fit(x_train, y_train)
215/17:
rmodel = RandomForestClassifier(n_estimators=10)
rmodel.fit(x_train, y_train)
215/18: rmodelprediction = rmodel.predict(x_test)
215/19: rmodelprediction = rmodel.predict(x_test)
215/20: accuracy_score(y_test, rmodelprediction)
216/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
216/2:
Breast = pd.read_csv("Breast_cancer.csv")
Test_data = pd.read_csv("test_data.csv")
216/3:
#Breast.info()
Breast.tail(4)
216/4:
# We are doing a label encoder to convert the string value to integer value as we all know Machine Learning algorithams can 
# understand only integer value. so what basically a label encoder does is sperate the similar types of strings or labels
# and give them a some specific numbers. For example in the above data set we have class: having values as no-recurrence-event
# and recurrence-event so when we do a label encoder for it what does it does it it give values such as 0 to no-recurrence-event
# and 1 as recurrence-event or some other values.
216/5:
Class_le = LabelEncoder()
age_le = LabelEncoder()
Meno_le = LabelEncoder()
Tumor_le = LabelEncoder()
inv_nodes_le = LabelEncoder()
node_caps_le = LabelEncoder()
#deg_malig_le = LabelEncoder()
breast_le = LabelEncoder()
breast_quad_le = LabelEncoder()
irradiat_le = LabelEncoder()
216/6:
Breast["class"] = Class_le.fit_transform(Breast["Class:"])
Breast["Age"] = age_le.fit_transform(Breast["age:"])
Breast["Menopause"] = Meno_le.fit_transform(Breast["menopause:"])
Breast["Tumor-size"] = Tumor_le.fit_transform(Breast["tumor-size:"])
Breast["Inv-nodes"] = inv_nodes_le.fit_transform(Breast["inv-nodes:"])
Breast["Node-caps"] = node_caps_le.fit_transform(Breast["node-caps:"])
Breast["Deg-malig"] = Breast["deg-malig:"]
Breast["Breast"] = breast_le.fit_transform(Breast["breast:"])
Breast["Breast-quad"] = breast_quad_le.fit_transform(Breast["breast-quad:"])
Breast["Irradiat"] = irradiat_le.fit_transform(Breast["irradiat:"])
#Breast.to_csv("New_data.csv",index=False)
216/7:
Breast.drop(["Class:","age:","menopause:","tumor-size:","inv-nodes:","node-caps:","deg-malig:","breast:","breast-quad:","irradiat:"],
                     axis=1,inplace=True)
Breast.head(10)
216/8:
x_data = Breast.drop(["Irradiat"],axis=1)
y_data = Breast["Irradiat"]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

x_train
216/9:
predicted = model.predict(x_test)
predicted
216/10: Test_data.head(4)
216/11:
Test_data_x = Test_data.drop(["irradiat:"],axis=1)
Test_data["Predicted"] = model.predict(Test_data_x)
216/12: Test_data.head(31)
216/13: accuracy_score(Test_data["irradiat:"],Test_data["Predicted"])
216/14: accuracy_score(y_test,predicted)
216/15:
rmodel = RandomForestClassifier(n_estimators=20)
rmodel.fit(x_train, y_train)
216/16: rmodelprediction = rmodel.predict(x_test)
216/17: accuracy_score(y_test, rmodelprediction)
217/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
217/2: car_data = pd.read_csv("CAR.csv")
217/3: car_data = pd.read_csv("CAR.csv")
217/4: car_head()
217/5: car.head()
217/6: car.head()
217/7: car_data.head()
217/8: car_data.info()
217/9: sns.countplot(x="Class Values",hue="buying",data = car_data)
219/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
219/2: SUV_data = pd.read_csv("suv_data.csv")
219/3: SUV_data.head(5)
219/4: SUV_data.info()
219/5:
#print(SUV_data.isnull())            # to check if any null values is present or not it will return a bollen
                                    #    if null then True else False

SUV_data.isnull().sum()            # return the sum of each column
219/6: sns.countplot(x="Purchased",data = SUV_data)
219/7: sns.countplot(x="Purchased", hue = "Gender", data = SUV_data)    # hue = particular column name
219/8: SUV_data["Age"].plot.hist()
219/9:
gender = pd.get_dummies(SUV_data["Gender"],drop_first=True)
gender
219/10:
SUV_data = pd.concat([SUV_data,gender],axis=1)        # always remember to pass the data here it is SUV_data. then run the code
SUV_data.head(2)
219/11:
SUV_data.drop(["User ID","Gender"],axis=1,inplace=True)
SUV_data
219/12:
x = SUV_data[["Age","EstimatedSalary","Male"]]
y = SUV_data["Purchased"]
219/13: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)
219/14:
sc = StandardScaler()                    # we use Standered scaller to scall down the values to eqal limits because some
x_train = sc.fit_transform(x_train)      # columns are having very high values as compare to the other column which are not
x_test = sc.fit_transform(x_test)        # even close to those values(high) which will affecet out prediction.
219/15:
model = LogisticRegression()
model.fit(x_train,y_train)
219/16:
predicted = model.predict(x_test)
#predicted
219/17: accuracy_score(y_test,predicted)
220/1: a = list("43Ah*ck0rr0nk")
220/2:
a = list("43Ah*ck0rr0nk")
a
220/3:
a = list("43Ah*ck0rr0nk")
a.count('0')
220/4:
a = list("43Ah*ck0rr0nk")
a.count('0')

for i in range(-1,-len(a),-1):
    print(a[i])
220/5:
a = list("43Ah*ck0rr0nk")
a.count('0')

for i in range(-1,-len(a)-1,-1):
    print(a[i])
220/6:
a = list("43Ah*ck0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
220/7:
a = list("43Ah*ck0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
a
220/8: ord(a)
220/9: ord('a')
220/10: ord('A')
220/11: ord('Z')
220/12:
a = list("43Ah*ck0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
a

for i in range(len(a)):
    if (ord(a[i])>64 and ord(a[i])<91):
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        del(a[i+2])
        i = i+2
a
220/13:
a = list("43Ah*ck0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
a

for i in range(len(a)):
    if (ord(a[i])>64 and ord(a[i])<91):
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        i = i+2
a
220/14:
a = list("43Ah*ck0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
a
i=0
while(i<len(a)):
    if (ord(a[i])>64 and ord(a[i])<91):
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        i = i+2
    i+=1
a
220/15:
a = list("43Ah*ck0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
a
i=0
while(i<len(a)):
    if (ord(a[i])>64 and ord(a[i])<91):
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        del(a[i+2])
        i = i+2
    i+=1
a
220/16:
a = list("43Ah*ck0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
a
i=0
while(i<len(a)):
    if (ord(a[i])>64 and ord(a[i])<91):
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        del(a[i+2])
        i = i+2
    i+=1
*a
220/17:
a = list("43Ah*ck0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
a
i=0
while(i<len(a)):
    if (ord(a[i])>64 and ord(a[i])<91):
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        del(a[i+2])
        i = i+2
    i+=1
print(*a)
220/18:
a = list("43Ah*ck0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
a
i=0
while(i<len(a)):
    if (ord(a[i])>64 and ord(a[i])<91):
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        del(a[i+2])
        i = i+2
    i+=1
print(''.join(a))
220/19:
a = list("43Ah*kC*0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
i=0
while(i<len(a)):
    if (ord(a[i])>64 and ord(a[i])<91):
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        del(a[i+2])
        i = i+2
    i+=1
print(''.join(a))
220/20:
a = list("43Ah*kC*0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
i=0
while(i<len(a)):
    if (ord(a[i])>64 and ord(a[i])<91):
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        del(a[i+2])
        i = i+1
    i+=1
print(''.join(a))
220/21:
a = list("43Ah*kC*0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
i=0
while(i<len(a)):
    if (ord(a[i])>64 and ord(a[i])<91):
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        del(a[i+2])
        i = i+2
    i+=1
print(''.join(a))
220/22:
a = list("43Ah*Kc*0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
i=0
while(i<len(a)):
    if (ord(a[i])>64 and ord(a[i])<91):
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        del(a[i+2])
        i = i+2
    i+=1
print(''.join(a))
220/23:
a = list("43Ah*Kc*0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
i=0
while(i<len(a)):
    if (ord(a[i])>64 and ord(a[i])<91):
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        del(a[i+2])
        i = i+2-1
    i+=1
print(''.join(a))
220/24:
a = list("43Ah*Kc*0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
i=0
while(i<len(a)):
    if (ord(a[i])>64 and ord(a[i])<91):
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        del(a[i+2])
        i = i+1
    i+=1
print(''.join(a))
220/25:
a = list("43Ah*ck0rr0nk")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
i=0
while(i<len(a)):
    if (ord(a[i])>64 and ord(a[i])<91):
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        del(a[i+2])
        i = i+1
    i+=1
print(''.join(a))
220/26:
a = list("43Ah*ck0rr0Kn*")
zero = a.count('0')

for i in range(-1,-len(a)-1+zero,-1):
    if a[i] == '0':
        a[i] = a[0]
        del(a[0])
        
i=0
while(i<len(a)):
    if (ord(a[i])>64 and ord(a[i])<91):
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        del(a[i+2])
        i = i+1
    i+=1
print(''.join(a))
220/27:
a = list("hAck3rr4nK")

i=0
while(i<len(a)):
    if (a[i]>0 and a[i]<=10):
        a.insert(0,a[i])
        a[i] = 0
    i+=1
    
a
220/28:
a = list("hAck3rr4nK")

i=0
while(i<len(a)):
    if (int(a[i])>0 and int(a[i])<=10):
        a.insert(0,a[i])
        a[i] = 0
    i+=1
    
a
220/29:
a = list("hAck3rr4nK")

i=0
while(i<len(a)):
    if a[i] is not isaplha():
        a.insert(0,a[i])
        a[i] = 0
    i+=1
    
a
220/30:
a = list("hAck3rr4nK")

i=0
while(i<len(a)):
    if a[i].isalpha()==False:
        a.insert(0,a[i])
        a[i] = 0
    i+=1
    
a
220/31:
a = list("hAck3rr4nK")

i=0
while(i<len(a)):
    if (a[i].isalpha())==False:
        a.insert(0,a[i])
        a[i] = 0
    i+=1
    
a
220/32:
a = list("hAck3rr4nK")

i=0
while(i<len(a)):
    if a[i].isnumeric()==True:
        a.insert(0,a[i])
        a[i] = 0
    i+=1
    
a
220/33:
a = list("hAck3rr4nK")

i=0
while(i<len(a)):
    if a[i].isnumeric()==True:
        a.insert(0,a[i])
        a[i] = '0'
    i+=1
    
a
220/34:
a = list("hAck3rr4nK")
print(a)
i=0
while(i<len(a)):
    if a[i].isnumeric()==True:
        a.insert(0,a[i])
        a[i] = '0'
    i+=1
    
a
220/35:
a = list("hAck3rr4nK")
print(a)
i=0
while(i<len(a)):
    if a[i].isnumeric()==True:
        print(a[i])
    i+=1
    
a
220/36:
a = list("hAck3rr4nK")
print(a)
i=0
while(i<len(a)):
    if a[i].isnumeric()==Frue:
        print(a[i])
    i+=1
    
a
220/37:
a = list("hAck3rr4nK")
print(a)
i=0
while(i<len(a)):
    if a[i].isnumeric()==True:
        print(a[i])
    i+=1
    
a
220/38:
a = list("hAck3rr4nK")
print(a)
i=0
while(i<len(a)):
    if a[i].isnumeric()==True:
        print(a[i])
        a.insert(0,a[i])
    i+=1
    
a
220/39:
a = list("hAck3rr4nK")

a
220/40:
a = list("hAck3rr4nK")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i] = 0
        i+=1
    i+=1
    
a
220/41:
a = list("hAck3rr4nK")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = 0
        i+=1
    i+=1
    
a
220/42:
a = list("hAck3rr4nK")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = 0
        i+=1
    i+=1
    
print(''.join(a))
220/43:
a = list("hAck3rr4nK")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = 0
        i+=1
    i+=1
    
print(''.join(a))
220/44:
a = list("hAck3rr4nK")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
    
print(''.join(a))
220/45: ord('a')
220/46: ord('z')
220/47:
a = list("hAck3rr4nK")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
    
while(i<len(a)):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print("*")
        
    
print(''.join(a))
220/48:
a = list("hAck3rr4nK")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
    
while(i<len(a)):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print("*")
        
    
print(''.join(a))
220/49:
a = list("hAck3rr4nK")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
    
while(i<len(a)):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print("*")
        
    
print(''.join(a))
220/50:
a = list("hAck3rr4nK")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
    
while(i<len(a)):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print("*")
    i+=1
        
    
print(''.join(a))
220/51:
a = list("hAck3rr4nK")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
i=0    
while(i<len(a)):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print("*")
    i+=1
        
    
print(''.join(a))
220/52:
a = list("hAck3rr4nK")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
i=0    
while(i<len(a)):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print(a[i])
    i+=1
        
    
print(''.join(a))
220/53:
a = list("hAck3rr4nk")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
i=0    
while(i<len(a)):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print(a[i])
    i+=1
        
    
print(''.join(a))
220/54:
a = list("hAck3rr4nk")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
i=0    
while(i<len(a)-1):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print(a[i])
    i+=1
        
    
print(''.join(a))
220/55:
a = list("hAck3rr4nk")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
i=0    
while(i<len(a)-1):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print(a[i])
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        
    i+=1
        
    
print(''.join(a))
220/56:
a = list("hAck3rr4nk")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
i=0    
while(i<len(a)-1):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print(a[i])
        temp = a[i]
        a[i] = a[i+1]
        a[i] = temp
        
    i+=1
        
    
print(''.join(a))
220/57:
a = list("hAck3rr4nk")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
i=0    
while(i<len(a)-1):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print(a[i])
        temp = a[i]
        a[i] = a[i+1]
        a[i] = temp
        insert(i+2,"*")
    i+=1
        
    
print(''.join(a))
220/58:
a = list("hAck3rr4nk")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
i=0    
while(i<len(a)-1):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print(a[i])
        temp = a[i]
        a[i] = a[i+1]
        a[i] = temp
        a.insert(i+2,"*")
    i+=1
        
    
print(''.join(a))
220/59:
a = list("hAck3rr4nk")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
i=0    
while(i<len(a)-1):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print(a[i])
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        a.insert(i+2,"*")
    i+=1
        
    
print(''.join(a))
220/60:
a = list("hAck3rr4nK")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
i=0    
while(i<len(a)-1):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print(a[i])
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        a.insert(i+2,"*")
    i+=1
        
    
print(''.join(a))
220/61:
a = list("hAck3Rr4nk")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
i=0    
while(i<len(a)-1):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print(a[i])
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        a.insert(i+2,"*")
    i+=1
        
    
print(''.join(a))
220/62:
a = list("hAck3Rr4nk")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
i=0    
while(i<len(a)-1):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print(a[i])
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        a.insert(i+2,"*")
    i+=1
        
    
print(''.join(a))
220/63:
a = list("hAck3rR4nk")

a
i=0
while(i<len(a)):
    if(a[i].isnumeric()==True):
        a.insert(0,a[i])
        a[i+1] = '0'
        i+=1
    i+=1
i=0    
while(i<len(a)-1):
    if ((ord(a[i])>=97 and ord(a[i])<122) and (ord(a[i+1])>64 and ord(a[i+1])<91)) :
        print(a[i])
        temp = a[i]
        a[i] = a[i+1]
        a[i+1] = temp
        a.insert(i+2,"*")
    i+=1
        
    
print(''.join(a))
221/1:
arr = [0]*5
arr
221/2:
arr[:] = [1]*arr[:]
arr
221/3:
arr[:] = [1]*5
arr
221/4:
def cout(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            while(i<n):
                arr[i]=1
                i+=2
    return arr.count(0)

if __name__ == "__main__":
    n = 5
    print(count(n))
221/5:
def count(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            while(i<n):
                arr[i]=1
                i+=2
    return arr.count(0)

if __name__ == "__main__":
    n = 5
    print(count(n))
221/6:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            while(i<n):
                arr[i]=1
                i+=2
    return arr.count(0)

if __name__ == "__main__":
    n = 5
    print(count1(n))
221/7:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            while(i<n):
                arr[i]=1
                i+=2
    return arr.count(0)

if __name__ == "__main__":
    n = 5
    print(count1(n))
221/8:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            while(i<n):
                arr[i]=1
                i+=2
        arr
    return arr.count(0)

if __name__ == "__main__":
    n = 5
    print(count1(n))
221/9:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            while(i<n):
                arr[i]=1
                i+=2
        print(arr)
    return arr.count(0)

if __name__ == "__main__":
    n = 5
    print(count1(n))
221/10:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            while(i<n):
                arr[i]=0
                i+=2
        print(arr)
    return arr.count(0)

if __name__ == "__main__":
    n = 5
    print(count1(n))
221/11:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            while(i<n):
                if arr[i] == 1:
                    arr[i]=0
                else:
                    arr[i]=1
                
                i+=2
        print(arr)
    return arr.count(0)

if __name__ == "__main__":
    n = 5
    print(count1(n))
221/12:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            while(i<n):
                if arr[i] == 1:
                    arr[i] = 0
                else:
                    arr[i] = 1
                
                i+=2
        print(arr)
    return arr.count(0)

if __name__ == "__main__":
    n = 5
    print(count1(n))
221/13:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            while(i<n):
                if arr[i] == 1:
                    arr[i] = 0
                else:
                    arr[i] = 1
                
                i+=2
        print(arr)
    return arr.count(0)

if __name__ == "__main__":
    n = 4
    print(count1(n))
221/14:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            while(i<n):
                if arr[i] == 1:
                    arr[i] = 0
                else:
                    arr[i] = 1
                
                i+=2
     
    return arr.count(0)

if __name__ == "__main__":
    n = 372
    print(count1(n))
221/15:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            while(i<n):
                if arr[i] == 1:
                    arr[i] = 0
                else:
                    arr[i] = 1
                
                i+=2
     
    return arr.count(0)

if __name__ == "__main__":
    n = 100
    print(count1(n))
221/16:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            while(i<n):
                if arr[i] == 1:
                    arr[i] = 0
                else:
                    arr[i] = 1
                
                i**=2
     
    return arr.count(0)

if __name__ == "__main__":
    n = 100
    print(count1(n))
221/17:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            while(i<n):
                if arr[i] == 1:
                    arr[i] = 0
                else:
                    arr[i] = 1
                
                i = i**2
     
    return arr.count(0)

if __name__ == "__main__":
    n = 100
    print(count1(n))
221/18:
def count1(n):
    arr = [0]*n
    for i in range(n):
        k = 1
        j = k*i
        while(j<n):
            if arr[j] == 0:
                arr[j] = 1
            else:
                arr[j] = 0
            k+=1
            j = k*i
            
     
    return arr.count(0)

if __name__ == "__main__":
    n = 100
    print(count1(n))
221/19:
def count1(n):
    arr = [0]*n
    for i in range(n):
        k = 1
        j = k*i
        while(j<n):
            if arr[j] == 0:
                arr[j] = 1
            else:
                arr[j] = 0
            k+=1
            j = k*i
            
     
    return arr.count(0)

if __name__ == "__main__":
    n = 5
    print(count1(n))
221/20:
def count1(n):
    arr = [0]*n
    for i in range(n):
        k = 1
        j = k*i
        while(j<n):
            if arr[j] == 0:
                arr[j] = 1
            else:
                arr[j] = 0
            k+=1
            j = k*i
            print(j)
            
     
    return arr.count(0)

if __name__ == "__main__":
    n = 5
    print(count1(n))
221/21:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            k = 1
            j = k*i
            while(j<n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                print(j)
            
     
    return arr.count(0)

if __name__ == "__main__":
    n = 5
    print(count1(n))
221/22:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            k = 1
            j = k*i
            while(j<n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
            
     
    return arr.count(0)

if __name__ == "__main__":
    n = 100
    print(count1(n))
221/23:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            k = 1
            j = k*i
            while(j<n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
            
     
    return arr.count(0)

if __name__ == "__main__":
    n = 372
    print(count1(n))
221/24:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            k = 1
            j = k*i
            while(j<n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
            
     
    return arr.count(0)

if __name__ == "__main__":
    n = 100
    print(count1(n))
221/25:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            k = 1
            j = k*i
            while(j<n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
            
     
    return arr.count(0)

if __name__ == "__main__":
    n = 10
    print(count1(n))
221/26:
def count1(n):
    arr = [0]*n
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            k = 1
            j = k*i
            while(j<=n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
            
     
    return arr.count(0)

if __name__ == "__main__":
    n = 10
    print(count1(n))
221/27:
def count1(n):
    arr = [0]*n+1
    arr[0] = 1
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            k = 1
            j = k*i
            while(j<=n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
            
     
    return arr.count(0)

if __name__ == "__main__":
    n = 10
    print(count1(n))
221/28:
def count1(n):
    arr = [0]*(n+1)
    arr[0] = 1
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            k = 1
            j = k*i
            while(j<=n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
            
     
    return arr.count(0)

if __name__ == "__main__":
    n = 10
    print(count1(n))
221/29:
def count1(n):
    arr = [0]*n
    print(arr)
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            k = 1
            j = k*i
            while(j<n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
        print(arr)
                 
    return arr.count(0)

if __name__ == "__main__":
    n = 10
    print(count1(n))
221/30:
def count1(n):
    arr = [0]*(n+1)
    print(arr)
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            k = 1
            j = k*i
            while(j<n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
        print(arr)
                 
    return arr.count(0)

if __name__ == "__main__":
    n = 10
    print(count1(n))
221/31:
def count1(n):
    arr = [0]*(n+1)
    print(arr)
    for i in range(n):
        if i==0:
            arr[:] = [1]*(n+1)
        else:
            k = 1
            j = k*i
            while(j<n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
        print(arr)
                 
    return arr.count(0)

if __name__ == "__main__":
    n = 10
    print(count1(n))
221/32:
def count1(n):
    arr = [0]*(n+1)
    arr[0]=1
    print(arr)
    for i in range(1:n):
        else:
            k = 1
            j = k*i
            while(j<n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
        print(arr)
                 
    return arr.count(0)

if __name__ == "__main__":
    n = 10
    print(count1(n))
221/33:
def count1(n):
    arr = [0]*(n+1)
    arr[0]=1
    print(arr)
    for i in range(1,n):
        else:
            k = 1
            j = k*i
            while(j<n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
        print(arr)
                 
    return arr.count(0)

if __name__ == "__main__":
    n = 10
    print(count1(n))
221/34:
def count1(n):
    arr = [0]*(n+1)
    arr[0]=1
    print(arr)
    for i in range(1,n):
        
            k = 1
            j = k*i
            while(j<n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
        print(arr)
                 
    return arr.count(0)

if __name__ == "__main__":
    n = 10
    print(count1(n))
221/36:
def count1(n):
    arr = [0]*(n+1)
    arr[0]=1
    print(arr)
    for i in range(1,n):
        
        k = 1
        j = k*i
        while(j<n):
            if arr[j] == 0:
                arr[j] = 1
            else:
                arr[j] = 0
            k+=1
            j = k*i
            #print(j)
        print(arr)
                 
    return arr.count(0)

if __name__ == "__main__":
    n = 10
    print(count1(n))
221/37:
def count1(n):
    arr = [0]*(n+1)
    arr[0]=1
    print(arr)
    for i in range(1,n):
        
        k = 1
        j = k*i
        while(j<(n+1)):
            if arr[j] == 0:
                arr[j] = 1
            else:
                arr[j] = 0
            k+=1
            j = k*i
            #print(j)
        print(arr)
                 
    return arr.count(0)

if __name__ == "__main__":
    n = 10
    print(count1(n))
221/38:
def count1(n):
    arr = [0]*(n+1)
    arr[0]=1
    print(arr)
    for i in range(1,n):
        
        k = 1
        j = k*i
        while(j<(n+1)):
            if arr[j] == 0:
                arr[j] = 1
            else:
                arr[j] = 0
            k+=1
            j = k*i
            #print(j)
        print(arr)
                 
    return arr.count(0)

if __name__ == "__main__":
    n = 100
    print(count1(n))
221/39:
def count1(n):
    arr = [0]*(n+1)
    arr[0]=1
    print(arr)
    for i in range(1,n): 
        k = 1
        j = k*i
        while(j<(n+1)):
            if arr[j] == 0:
                arr[j] = 1
            else:
                arr[j] = 0
            k+=1
            j = k*i
            #print(j)
        print(arr)
                 
    return arr.count(0)

if __name__ == "__main__":
    n = 10
    print(count1(n))
221/40:
def count1(n):
    arr = [0]*n
    print(arr)
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            k = 1
            j = k*i
            while(j<n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
        print(arr)
                 
    return arr.count(0)

if __name__ == "__main__":
    n = 10
    print(count1(n))
221/41:
def count1(n):
    arr = [0]*n
    print(arr)
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            k = 1
            j = k*i
            while(j<n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
        print(arr)
                 
    return arr.count(0)

if __name__ == "__main__":
    n = 100
    print(count1(n))
221/42:
def count1(n):
    arr = [0]*n
    print(arr)
    for i in range(n):
        if i==0:
            arr[:] = [1]*n
        else:
            k = 1
            j = k*i
            while(j<n):
                if arr[j] == 0:
                    arr[j] = 1
                else:
                    arr[j] = 0
                k+=1
                j = k*i
                #print(j)
        #print(arr)
                 
    return arr.count(0)

if __name__ == "__main__":
    n = 200
    print(count1(n))
221/43:
def count1(n):
    arr = [0]*n
    
                 
    return arr.count(0)

if __name__ == "__main__":
    n = 5
    print(count1(n))
221/44:
def count1(n):
    arr = [0]*n
    print(arr)
                 
    return arr.count(0)

if __name__ == "__main__":
    n = 5
    print(count1(n))
222/1: from collections import defaultdict
222/2:
from collections import defaultdict

class Graph:
    def __init__(self):
        self.graph = defaultdict(list)
        
    def add_points(self,u,v):
        self.graph[u].append(v)
        
    def view(self):
        print(graph)
        
g = Graph()
g.add_points(0,1)
g.add_points(0,2)
g.add_points(1,2)
g.view()
222/3:
from collections import defaultdict

class Graph:
    def __init__(self):
        self.graph = defaultdict(list)
        
    def add_points(self,u,v):
        self.graph[u].append(v)
        
    def view(self):
        print(self.graph)
        
g = Graph()
g.add_points(0,1)
g.add_points(0,2)
g.add_points(1,2)
g.view()
222/4:
from collections import defaultdict

class Graph:
    def __init__(self):
        self.graph = defaultdict(list)
        
    def add_points(self,u,v):
        self.graph[u].append(v)
        
    def view(self):
        print(self.graph,len(self.graph))
        
g = Graph()
g.add_points(0,1)
g.add_points(0,2)
g.add_points(1,2)
g.view()
222/5:
from collections import defaultdict

class Graph:
    def __init__(self):
        self.graph = defaultdict(list)
        
    def add_points(self,u,v):
        self.graph[u].append(v)
        
    def view(self):
        print(self.graph,len(self.graph))
        print(self.graph[2])
        
g = Graph()
g.add_points(0,1)
g.add_points(0,2)
g.add_points(1,2)
g.view()
222/6:
from collections import defaultdict

class Graph:
    def __init__(self):
        self.graph = defaultdict(list)
        
    def add_points(self,u,v):
        self.graph[u].append(v)
        
    def view(self):
        print(self.graph,len(self.graph))
        print(self.graph[1])
        
g = Graph()
g.add_points(0,1)
g.add_points(0,2)
g.add_points(1,2)
g.view()
222/7:
from collections import defaultdict

class Graph:
    def __init__(self):
        self.graph = defaultdict(list)
        
    def add_points(self,u,v):
        self.graph[u].append(v)
        
    def view(self):
        print(self.graph,len(self.graph))
        print(self.graph[0])
        
g = Graph()
g.add_points(0,1)
g.add_points(0,2)
g.add_points(1,2)
g.view()
223/1:
import pandas as pd
import numpy as np
223/2:
data = pd.read_csv("heart1.csv")
data
224/1:
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
228/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
228/2: data = pd.read_csv("Breast_cancer_dataset.csv")
228/3: data.head()
228/4: data.describe()
228/5: data.lebels()
228/6: data.labels()
228/7: data.label()
228/8: data.columns()
228/9: data.columns
228/10: data.info()
228/11: data.isnull().sum()
228/12: data.drop(["id","Unname: 32"], axis=1, inplace = True)
228/13: data.drop([["id"],["Unname: 32"]], axis=1, inplace = True)
228/14: data.drop([["id"],["Unnamed: 32"]], axis=1, inplace = True)
228/15: data.drop(["id","Unnamed: 32"], axis=1, inplace = True)
228/16: data.head()
228/17: data.unique()
228/18: data[:].unique()
228/19: data["diagnosis"].unique()
228/20:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
228/21:
print(data["diagnosis"].unique())
label_encoder = preprocessing.LabelEncoder()
data["diagnosis"] = label_encoder.fit_transform(data["diagnosis"])
print(data["diagnosis"].unique())
228/22: data.head()
228/23: data.corr()
228/24:
print(data.corr())
sns.heatmap(data)
228/25: sns.heatmap(data)
228/26: sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
228/27:
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
230/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
230/2: data = pd.read_csv("Breast_cancer_dataset.csv")
230/3: data.head()
230/4: data.describe()
230/5: data.info()
230/6: data.isnull().sum()
230/7: data.drop(["id","Unnamed: 32"], axis=1, inplace = True)
230/8: data.head()
230/9:
print(data["diagnosis"].unique())
label_encoder = preprocessing.LabelEncoder()
data["diagnosis"] = label_encoder.fit_transform(data["diagnosis"])
print(data["diagnosis"].unique())
230/10:
data.head()      # M = 1
                 # B = 0
230/11: print(data.corr())
230/12:
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
230/13:
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(data.corr(), annot=True, linewidths=5, fmt= '.1f',ax=ax)
230/14:
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
230/15: sns.pairplot(data,hue = "diagnosis")
230/16: sns.pairplot(data, hue = "diagnosis", var = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
230/17: sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
231/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
231/2: data = pd.read_csv("Breast_cancer_dataset.csv")
231/3: data.head()
231/4: data.describe()
231/5: data.info()
231/6: data.isnull().sum()
231/7: data.drop(["id","Unnamed: 32"], axis=1, inplace = True)
231/8: data.head()
231/9: sns.countplot(data["diagnosis"])
231/10:
print(data["diagnosis"].unique())
label_encoder = preprocessing.LabelEncoder()
data["diagnosis"] = label_encoder.fit_transform(data["diagnosis"])
print(data["diagnosis"].unique())
231/11: sns.countplot(data["diagnosis"])
231/12:
data.head()      # M = 1
                 # B = 0
231/13: print(data.corr())
231/14:
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
231/15: #sns.pairplot(data,hue = "diagnosis")
231/16: sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
232/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
232/2: data = pd.read_csv("Breast_cancer_dataset.csv")
232/3: data.head()
232/4: data.describe()
232/5: data.info()
232/6: data.isnull().sum()
232/7: data.drop(["id","Unnamed: 32"], axis=1, inplace = True)
232/8: data.head()
232/9: sns.countplot(data["diagnosis"])
232/10: sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
232/11:
print(data["diagnosis"].unique())
label_encoder = preprocessing.LabelEncoder()
data["diagnosis"] = label_encoder.fit_transform(data["diagnosis"])
print(data["diagnosis"].unique())
232/12: sns.countplot(data["diagnosis"])
232/13:
data.head()      # M = 1
                 # B = 0
232/14: print(data.corr())
232/15:
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
232/16: #sns.pairplot(data,hue = "diagnosis")
232/17: sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
232/18: sns.countplot(data["radius_mean"])
232/19:
plt.figure(figsize=(20,0))
sns.countplot(data["radius_mean"])
232/20:
plt.figure(figsize=(20,8))
sns.countplot(data["radius_mean"])
232/21:
plt.figure(figsize=(20,10))
sns.countplot(data["radius_mean"])
232/22:
plt.figure(figsize=(20,8))
sns.countplot(data["radius_mean"])
233/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
233/2: data = pd.read_csv("Breast_cancer_dataset.csv")
233/3: data.head()
233/4: data.describe()
233/5: data.info()
233/6: data.isnull().sum()
233/7: data.drop(["id","Unnamed: 32"], axis=1, inplace = True)
233/8: data.head()
233/9: #sns.countplot(data["diagnosis"])
233/10: #sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
233/11:
print(data["diagnosis"].unique())
label_encoder = preprocessing.LabelEncoder()
data["diagnosis"] = label_encoder.fit_transform(data["diagnosis"])
print(data["diagnosis"].unique())
233/12: sns.countplot(data["diagnosis"])
233/13:
data.head()      # M = 1
                 # B = 0
233/14: print(data.corr())
233/15:
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
233/16: #sns.pairplot(data,hue = "diagnosis")
233/17:
sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
# 1 = M, 0 = B
233/18:
plt.figure(figsize=(20,8))
sns.countplot(data["radius_mean"])
233/19: sns.heatmap(data)
233/20:
plt.figure(figsize=(20,5))
sns.heatmap(data)
233/21:
plt.figure(figsize=(15,8))
sns.heatmap(data)
233/22: data.index
233/23: data.columns
233/24: data_x_labels = data.drop(data["diagnosis"], axis=1)
233/25: data_x_labels = data.drop(("diagnosis"), axis=1)
233/26:
data_x_labels = data.drop(("diagnosis"), axis=1)
data_x_labels.head()
233/27:
data_x_labels = data.drop(("diagnosis"), axis=1)
#data_x_labels.head()

data_y_label = data["diagnosis"]
233/28:
data_x_labels = data.drop(("diagnosis"), axis=1)
#data_x_labels.head()

data_y_label = data["diagnosis"]
data_y_label.head()
233/29:
data_x_labels = data.drop(("diagnosis"), axis=1)
#data_x_labels.head()

data_y_label = data["diagnosis"]
233/30: x_train,x_test,y_train,y_test = train_test_split(data_x_labels, data_y_label, test_size = 0.2, random_state = 42)
233/31: x_train.head()
233/32: x_train,x_test,y_train,y_test = train_test_split(data_x_labels, data_y_label, test_size = 0.2, random_state = 42)
233/33: x_train.head()
233/34:
print(x_train.head())
print(y_train.head())
233/35:
# print(x_train.head())
# print(y_train.head())
233/36:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.preprocessing immport StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
233/37:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
233/38:
stnd_scl = StandardScaler()
x_train = stnd_scl.fit_transform(x_train)
x_test = stnd_scl.fit_transform(x_test)
233/39:
print(x_train.head())
# print(y_train.head())
233/40:
stnd_scl = StandardScaler()
x_train = stnd_scl.fit_transform(x_train)
x_test = stnd_scl.fit_transform(x_test)
x_train
234/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
234/2: data = pd.read_csv("Breast_cancer_dataset.csv")
234/3: data.head()
234/4: data.columns
234/5: data.describe()
234/6: data.info()
234/7: data.isnull().sum() # looking for null values
234/8: data.drop(["id","Unnamed: 32"], axis=1, inplace = True) # droping the unwanted columns
234/9: data.head()
234/10: #sns.countplot(data["diagnosis"])
234/11: #sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
234/12:
print(data["diagnosis"].unique())
label_encoder = preprocessing.LabelEncoder()
data["diagnosis"] = label_encoder.fit_transform(data["diagnosis"])
print(data["diagnosis"].unique())
234/13: sns.countplot(data["diagnosis"])
234/14:
data.head()      # M = 1
                 # B = 0
234/15:
plt.figure(figsize=(15,8))              #HEAT MAP
sns.heatmap(data)
234/16: #sns.pairplot(data,hue = "diagnosis")
234/17:
plt.figure(figsize=(20,8))           
sns.countplot(data["radius_mean"])
234/18: data.corr()
234/19:
f,ax = plt.subplots(figsize=(18, 18))    #CORRELATION HEAT MAP
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
234/20:
sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
# 1 = M, 0 = B
234/21:
data_x_labels = data.drop(("diagnosis"), axis=1)
#data_x_labels.head()

data_y_label = data["diagnosis"]
234/22: x_train,x_test,y_train,y_test = train_test_split(data_x_labels, data_y_label, test_size = 0.2, random_state = 42)
234/23:
print(x_train.head())
# print(y_train.head())
234/24:
stnd_scl = StandardScaler()
x_train = stnd_scl.fit_transform(x_train)
x_test = stnd_scl.fit_transform(x_test)
x_train
234/25: model_1 = LogisticRegression()
235/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
235/2: data = pd.read_csv("Breast_cancer_dataset.csv")
235/3: data.head()
235/4: data.columns
235/5: data.describe()
235/6: data.info()
235/7: data.isnull().sum() # looking for null values
235/8: data.drop(["id","Unnamed: 32"], axis=1, inplace = True) # droping the unwanted columns
235/9: data.head()
235/10: #sns.countplot(data["diagnosis"])
235/11: #sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
235/12:
print(data["diagnosis"].unique())
label_encoder = preprocessing.LabelEncoder()
data["diagnosis"] = label_encoder.fit_transform(data["diagnosis"])
print(data["diagnosis"].unique())
235/13: sns.countplot(data["diagnosis"])
235/14:
data.head()      # M = 1
                 # B = 0
235/15:
plt.figure(figsize=(15,8))              #HEAT MAP
sns.heatmap(data)
235/16: #sns.pairplot(data,hue = "diagnosis")
235/17:
plt.figure(figsize=(20,8))           
sns.countplot(data["radius_mean"])
235/18: data.corr()
235/19:
f,ax = plt.subplots(figsize=(18, 18))    #CORRELATION HEAT MAP
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
235/20:
sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
# 1 = M, 0 = B
235/21:
data_x_labels = data.drop(("diagnosis"), axis=1)
#data_x_labels.head()

data_y_label = data["diagnosis"]
235/22:
stnd_scl = StandardScaler()
data_x_labels = stnd_scl.fit_transform(data_x_labels)
#x_test = stnd_scl.fit_transform(x_test)
#x_train
#-1.44075296, -0.43531947, -1.36208497, ...,  0.9320124 ,
 #        2.09724217,  1.88645014]
235/23: x_train,x_test,y_train,y_test = train_test_split(data_x_labels, data_y_label, test_size = 0.2, random_state = 42)
235/24:
print(x_train)
# print(y_train.head())
235/25: model_1 = LogisticRegression()
236/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
236/2: data = pd.read_csv("Breast_cancer_dataset.csv")
236/3: data.head()
236/4: data.columns
236/5: data.describe()
236/6: data.info()
236/7: data.isnull().sum() # looking for null values
236/8: data.drop(["id","Unnamed: 32"], axis=1, inplace = True) # droping the unwanted columns
236/9: data.head()
236/10: #sns.countplot(data["diagnosis"])
236/11: #sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
236/12:
print(data["diagnosis"].unique())
label_encoder = preprocessing.LabelEncoder()
data["diagnosis"] = label_encoder.fit_transform(data["diagnosis"])
print(data["diagnosis"].unique())
236/13: sns.countplot(data["diagnosis"])
236/14:
data.head()      # M = 1
                 # B = 0
236/15:
plt.figure(figsize=(15,8))              #HEAT MAP
sns.heatmap(data)
236/16: #sns.pairplot(data,hue = "diagnosis")
236/17:
plt.figure(figsize=(20,8))           
sns.countplot(data["radius_mean"])
236/18: data.corr()
236/19:
f,ax = plt.subplots(figsize=(18, 18))    #CORRELATION HEAT MAP
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
236/20:
sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
# 1 = M, 0 = B
236/21:
data_x_labels = data.drop(("diagnosis"), axis=1)
#data_x_labels.head()

data_y_label = data["diagnosis"]
236/22:
stnd_scl = StandardScaler()
data_x_labels = stnd_scl.fit_transform(data_x_labels)
#x_test = stnd_scl.fit_transform(x_test)
#x_train
#-1.44075296, -0.43531947, -1.36208497, ...,  0.9320124 ,
 #        2.09724217,  1.88645014]
    #[-1.44798723 -0.45602336 -1.36665103 ...  0.91959172  2.14719008
   #1.85943247]
236/23: x_train,x_test,y_train,y_test = train_test_split(data_x_labels, data_y_label, test_size = 0.2, random_state = 42)
236/24:
print(x_train)
# print(y_train.head())
236/25: model_1 = LogisticRegression()
236/26:
#print(x_train)
# print(y_train.head())
236/27:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.matrics import accuuracy_score
236/28:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuuracy_score
236/29:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score
236/30:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix
236/31:
model_1 = LogisticRegression()
model_1.fit(x_train,y_train)
model_1_y_prediction = model_1.predict(x_test)
print("Accurecy of Logistic Regrassion: ",accuracy_score(y_test, model_1_y_prediction))
236/32:
model_1 = LogisticRegression()
model_1.fit(x_train,y_train)
model_1_y_prediction = model_1.predict(x_test)
print("Accuracy of Logistic Regrassion: ",accuracy_score(y_test, model_1_y_prediction))
236/33:
model_2 = DecisionTreeClassifier()
model_2.fit(x_train,y_train)
model_2_y_prediction = model_2.predict(x_test)
print("Accuracy of Logistic Regrassion: ",accuracy_score(y_test, model_2_y_prediction))
236/34:
model_2 = DecisionTreeClassifier()
model_2.fit(x_train,y_train)
model_2_y_prediction = model_2.predict(x_test)
print("Accuracy of Logistic Regrassion: ",accuracy_score(y_test, model_2_y_prediction))
236/35:
model_2 = DecisionTreeClassifier()
model_2.fit(x_train,y_train)
model_2_y_prediction = model_2.predict(x_test)
print("Accuracy of Logistic Regrassion: ",accuracy_score(y_test, model_2_y_prediction))
236/36:
model_2 = DecisionTreeClassifier()
model_2.fit(x_train,y_train)
model_2_y_prediction = model_2.predict(x_test)
print("Accuracy of Logistic Regrassion: ",accuracy_score(y_test, model_2_y_prediction))
236/37:
model_2 = DecisionTreeClassifier(random_state = 45)
model_2.fit(x_train,y_train)
model_2_y_prediction = model_2.predict(x_test)
print("Accuracy of Logistic Regrassion: ",accuracy_score(y_test, model_2_y_prediction))
236/38:
model_2 = DecisionTreeClassifier(random_state = 45)
model_2.fit(x_train,y_train)
model_2_y_prediction = model_2.predict(x_test)
print("Accuracy of Logistic Regrassion: ",accuracy_score(y_test, model_2_y_prediction))
236/39:
model_2 = DecisionTreeClassifier(random_state = 45)
model_2.fit(x_train,y_train)
model_2_y_prediction = model_2.predict(x_test)
print("Accuracy of Logistic Regrassion: ",accuracy_score(y_test, model_2_y_prediction))
236/40:
model_1 = LogisticRegression()
model_1.fit(x_train,y_train)
model_1_y_prediction = model_1.predict(x_test)
print("Accuracy of Logistic Regrassion: ",accuracy_score(y_test, model_1_y_prediction))
236/41:
model_1 = LogisticRegression()
model_1.fit(x_train,y_train)
model_1_y_prediction = model_1.predict(x_test)
print("Accuracy of Logistic Regrassion: ",accuracy_score(y_test, model_1_y_prediction))
236/42:
model_1 = LogisticRegression()
model_1.fit(x_train,y_train)
model_1_y_prediction = model_1.predict(x_test)
print("Accuracy of Logistic Regrassion: ",accuracy_score(y_test, model_1_y_prediction))
236/43:
model_3 = RandomForestClassifier(n_estimators=100, random_state=45)
model_3.fit(x_train, y_train)
model_3_y_prediction = model_3.predict(x_test)
print("Accuracy of Random Forest Classifier: ",accuracy_score(y_test, model_1_y_prediction))
236/44:
confu_matrix = confusion_matrix(y_test, model_3_y_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
236/45:
confu_matrix = confusion_matrix(y_test, model_3_y_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=False)
plt.show()
236/46:
confu_matrix = confusion_matrix(y_test, model_3_y_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
236/47:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix
236/48:
cross_val = cross_val_score(estimator=model_3, x=x_train, y=y_train)
printf("Cross validation accuracy of Random Forest Classifier: ",cross_val)
printf("Cross validation mean accuracy of Random Forest Classifier: ", cross_val.mean())
236/49:
cross_val = cross_val_score(estimator=model_3, X=x_train, y=y_train)
printf("Cross validation accuracy of Random Forest Classifier: ",cross_val)
printf("Cross validation mean accuracy of Random Forest Classifier: ", cross_val.mean())
236/50:
cross_val = cross_val_score(estimator=model_3, X=x_train, y=y_train)
print("Cross validation accuracy of Random Forest Classifier: ",cross_val)
print("Cross validation mean accuracy of Random Forest Classifier: ", cross_val.mean())
239/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix
239/2: data = pd.read_csv("Breast_cancer_dataset.csv")
239/3: data.head()
239/4: data.columns
239/5: data.describe()
239/6: data.info()
239/7: data.isnull().sum() # looking for null values
239/8: data.drop(["id","Unnamed: 32"], axis=1, inplace = True) # droping the unwanted columns
239/9: data.head()
239/10: #sns.countplot(data["diagnosis"])
239/11: #sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
239/12:
print(data["diagnosis"].unique())
label_encoder = preprocessing.LabelEncoder()
data["diagnosis"] = label_encoder.fit_transform(data["diagnosis"])
print(data["diagnosis"].unique())
239/13: sns.countplot(data["diagnosis"])
239/14:
data.head()      # M = 1 (It means the the tumor is cancerous or having breast cancer.)
                 # B = 0 ( It means the tumor is normal and not a cancerous type.)
239/15:
plt.figure(figsize=(15,8))              #HEAT MAP
sns.heatmap(data)
239/16: #sns.pairplot(data,hue = "diagnosis")
239/17:
plt.figure(figsize=(20,8))           
sns.countplot(data["radius_mean"])
239/18: data.corr()
239/19:
f,ax = plt.subplots(figsize=(18, 18))    #CORRELATION HEAT MAP
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
239/20:
sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
# 1 = M, 0 = B
240/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix
240/2: data = pd.read_csv("Breast_cancer_dataset.csv")
240/3: data.head()
240/4: data.columns
240/5: data.describe()
240/6: data.info()
240/7: data.isnull().sum() # looking for null values
240/8: data.drop(["id","Unnamed: 32"], axis=1, inplace = True) # droping the unwanted columns
240/9: data.head()
240/10: #sns.countplot(data["diagnosis"])
240/11: #sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
240/12:
print(data["diagnosis"].unique())
label_encoder = preprocessing.LabelEncoder()
data["diagnosis"] = label_encoder.fit_transform(data["diagnosis"])
print(data["diagnosis"].unique())
240/13: sns.countplot(data["diagnosis"])
240/14:
data.head()      # M = 1 (It means the the tumor is cancerous or having breast cancer.)
                 # B = 0 ( It means the tumor is normal and not a cancerous type.)
240/15:
plt.figure(figsize=(15,8))              #HEAT MAP
sns.heatmap(data)
240/16: #sns.pairplot(data,hue = "diagnosis")
240/17:
plt.figure(figsize=(20,8))           
sns.countplot(data["radius_mean"])
240/18: data.corr()
240/19:
f,ax = plt.subplots(figsize=(18, 18))    #CORRELATION HEAT MAP
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
240/20:
sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
# 1 = M, 0 = B
240/21:
data_x_labels = data.drop(("diagnosis"), axis=1)
#data_x_labels.head()

data_y_label = data["diagnosis"]
240/22:
stnd_scl = StandardScaler()
data_x_labels = stnd_scl.fit_transform(data_x_labels)
240/23: x_train,x_test,y_train,y_test = train_test_split(data_x_labels, data_y_label, test_size = 0.2, random_state = 42)
240/24:
#print(x_train)
# print(y_train.head())
240/25:
model_1 = LogisticRegression()
model_1.fit(x_train,y_train)
model_1_y_prediction = model_1.predict(x_test)
print("Accuracy of Logistic Regrassion: ",accuracy_score(y_test, model_1_y_prediction))
240/26:
model_2 = DecisionTreeClassifier(random_state = 45)
model_2.fit(x_train,y_train)
model_2_y_prediction = model_2.predict(x_test)
print("Accuracy of Decission Tree Classifier: ",accuracy_score(y_test, model_2_y_prediction))
240/27:
model_3 = RandomForestClassifier(n_estimators=100, random_state=45)
model_3.fit(x_train, y_train)
model_3_y_prediction = model_3.predict(x_test)
print("Accuracy of Random Forest Classifier: ",accuracy_score(y_test, model_1_y_prediction))
240/28:
confu_matrix = confusion_matrix(y_test, model_3_y_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
240/29:
cross_val = cross_val_score(estimator=model_3, X=x_train, y=y_train)
print("Cross validation accuracy of Random Forest Classifier: ",cross_val)
print("Cross validation mean accuracy of Random Forest Classifier: ", cross_val.mean())
242/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
242/2:
data = pd.read_csv("diabetes.csv")
data.head()
242/3: data.isnull().sum()
242/4: data.info()
242/5: data.describe()
242/6: data.corr()
242/7:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
242/8:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True)
242/9:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
242/10: sns.heatmap(data)
242/11:
plt.figure(figsize=(20,8))
sns.heatmap(data)
242/12:
plt.figure(figsize=(15,8))
sns.heatmap(data)
242/13: sns.boxenplot(data)
242/14: plt.boxenplot(data)
242/15: plt.boxplot(data)
242/16: plt.boxplot(data["outcome"])
242/17: plt.boxplot(data("outcome"))
242/18: plt.boxplot(data("Outcome"))
242/19: plt.boxplot(data["Outcome"])
242/20: data_2 = data.drop(["Outcome"],axise=1)
242/21: data_2 = data.drop(["Outcome"],axis=1)
242/22:
data_2 = data.drop(["Outcome"],axis=1)
data_2
242/23:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
242/24:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
ax = sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
ax.tick_params(labelrotation=90)
242/25:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
242/26: data["Outcome"].countplot()
242/27: sns.countplot(data["Outcome"])
242/28:
print(data["Outcome"].count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
242/29:
print(data["Outcome"].count(0))
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
242/30:
print(data["Outcome"].count("0"))
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
242/31:
print(data["Outcome"].count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
242/32:
print(data["Outcome"].count().unique())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
242/33:
print(data["Outcome"].count().unique
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
242/34:
print(data["Outcome"].count().unique)
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
242/35:
print(data["Outcome"].unique.count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
242/36:
print(data["Outcome"].unique().count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
242/37:
print(data["Outcome"].unique().count)
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
242/38:
print(data["Outcome"].unique())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
242/39:
print(count(data["Outcome"].unique())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
242/40:
print(count(data["Outcome"].unique()))
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
242/41:
print(data.groupby(["Oucome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
242/42:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
242/43: sns.pairplot(data)
242/44: sns.pairplot(data, hue="Outcome")
242/45: data.columns
242/46: data = pd.read_csv("diabetes.csv")
242/47: data.head()
242/48:
print(data.columns)
data.shape
242/49:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
242/50:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(data.drop("Outcome"))
242/51:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(data.drop[("Outcome")])
242/52:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(data.drop["Outcome"])
242/53:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(data.drop(("Outcome"),axis=1))
242/54:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(data.drop(("Outcome"),axis=1))
std_scl_x
242/55:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(data.drop(("Outcome"),axis=1))
#std_scl_x
242/56:
x_data = data.drop(("Outcome"), axis=1)
x_data
242/57:
x_data = data.drop(("Outcome"), axis=1)
x_data.head()
242/58:
x_data = data.drop(("Outcome"), axis=1)
y_data = data["Outcome"]
y_data
242/59:
x_data = data.drop(("Outcome"), axis=1)
y_data = data["Outcome"]
242/60:
# Splitting the non scaled data.
x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)
242/61:
# Splitting the non scaled data.
x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)
x_train
242/62:
# Splitting the non scaled data.
x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)
x_train
242/63:
# Featured scaled data.
std_scl_x_train,std_scl_x_test,y_train,y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)
std_scl_x_train
242/64:
# Featured scaled data.
std_scl_x_train,std_scl_x_test,y_train,y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)
std_scl_x_train
y_train
242/65:
# Splitting the non scaled data.
x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)
x_train
y_train
242/66:
# Splitting the non scaled data.
x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

# Featured scaled data.
std_scl_x_train,std_scl_x_test,y_train,y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)
242/67:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import accuracy_score, confusion_matrix
242/68:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_reg_predict1 = log_reg.predict(y_test)
print("Accuracy of Non scaled data in logistic regration: ",accuracy_score(log_reg_predict1,y_test))
242/69:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_reg_predict1 = log_reg.predict(y_test)
print("Accuracy of Non scaled data in logistic regration: ",accuracy_score(y_test,log_reg_predict1))
242/70:
# Splitting the non scaled data.
x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

# Featured scaled data.
std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)
242/71:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_reg_predict1 = log_reg.predict(y_test)
print("Accuracy of Non scaled data in logistic regration: ",accuracy_score(y_test,log_reg_predict1))
244/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import accuracy_score, confusion_matrix
244/2: data = pd.read_csv("diabetes.csv")
244/3: data.head()
244/4:
print(data.columns)
data.shape
244/5: data.isnull().sum()
244/6: data.info()
244/7: data.describe()
244/8:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
244/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
244/10: sns.pairplot(data, hue="Outcome")
244/11: data.corr()
244/12:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
244/13:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
244/14:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(data.drop(("Outcome"),axis=1))
#std_scl_x
244/15:
x_data = data.drop(("Outcome"), axis=1)
y_data = data["Outcome"]
244/16:
# Splitting the non scaled data.
x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

# Featured scaled data.
std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)
244/17:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_reg_predict1 = log_reg.predict(y_test)
print("Accuracy of Non scaled data in logistic regration: ",accuracy_score(y_test,log_reg_predict1))
244/18:
x_data = data.drop(("Outcome"), axis=1)
y_data = data["Outcome"]
y_data
244/19:
x_data = data.drop(("Outcome"), axis=1)
y_data = data["Outcome"]reshape(-1,1)
y_data
244/20:
x_data = data.drop(("Outcome"), axis=1)
y_data = data["Outcome"].reshape(-1,1)
y_data
244/21:
x_data = data.drop(("Outcome"), axis=1)
y_data = data["Outcome"]
y_data.reshape(-1,1)
244/22:
x_data = data.drop(("Outcome"), axis=1)
y_data = data[["Outcome"]
244/23:
x_data = data.drop(("Outcome"), axis=1)
y_data = data[["Outcome"]]
244/24:
x_data = data.drop(("Outcome"), axis=1)
y_data = data[["Outcome"]].reshape(-1,1)
244/25:
x_data = data.drop(("Outcome"), axis=1)
y_data = data[["Outcome"]]
y_data.reshape(-1,1)
244/26:
x_data = data.drop(("Outcome"), axis=1)
y_data = data[["Outcome"]]
y_data
244/27:
x_data = data.drop(("Outcome"), axis=1)
y_data = np.array(data["Outcome"])
y_data
244/28:
x_data = data.drop(("Outcome"), axis=1)
y_data = np.array(data["Outcome"]).reshape(-1,1)
244/29:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_reg_predict1 = log_reg.predict(y_test)
print("Accuracy of Non scaled data in logistic regration: ",accuracy_score(y_test,log_reg_predict1))
244/30:
# Splitting the non scaled data.
x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)
x_train = np.array(x_train).reshape(-1,1)

# Featured scaled data.
std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)
244/31:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_reg_predict1 = log_reg.predict(y_test)
print("Accuracy of Non scaled data in logistic regration: ",accuracy_score(y_test,log_reg_predict1))
245/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import accuracy_score, confusion_matrix
245/2: data = pd.read_csv("diabetes.csv")
245/3: data.head()
245/4:
print(data.columns)
data.shape
245/5: data.isnull().sum()
245/6: data.info()
245/7: data.describe()
245/8:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
245/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
245/10: sns.pairplot(data, hue="Outcome")
245/11: data.corr()
245/12:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
245/13:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
245/14:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(data.drop(("Outcome"),axis=1))
#std_scl_x
245/15:
x_data = data.drop(("Outcome"), axis=1)
y_data = data["Outcome"]
#y_data = np.array(data["Outcome"]).reshape(-1,1)
245/16:
# Splitting the non scaled data.
x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)
#x_train = np.array(x_train).reshape(-1,1)

# Featured scaled data.
#std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)
245/17:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_reg_predict1 = log_reg.predict(y_test)
print("Accuracy of Non scaled data in logistic regration: ",accuracy_score(y_test,log_reg_predict1))
245/18:
x_data = data.drop(("Outcome"), axis=1)
#y_data = data["Outcome"]
y_data = np.array(data["Outcome"]).reshape(-1,1)
245/19:
# Splitting the non scaled data.
x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)
#x_train = np.array(x_train).reshape(-1,1)

# Featured scaled data.
#std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)
245/20:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_reg_predict1 = log_reg.predict(y_test)
print("Accuracy of Non scaled data in logistic regration: ",accuracy_score(y_test,log_reg_predict1))
245/21:
x_data = data.drop(("Outcome"), axis=1)
x_data = np.array(x_data)
print(x_data)
#y_data = data["Outcome"]
y_data = np.array(data["Outcome"]).reshape(-1,1)
245/22:
x_data = data.drop(("Outcome"), axis=1)
print(x_data)
x_data = np.array(x_data)
print(x_data)
#y_data = data["Outcome"]
y_data = np.array(data["Outcome"]).reshape(-1,1)
245/23:
x_data = data.drop(("Outcome"), axis=1)
print(x_data)
x_data = np.array(x_data).reshape(-1,1)
print(x_data)
#y_data = data["Outcome"]
y_data = np.array(data["Outcome"]).reshape(-1,1)
245/24:
# Splitting the non scaled data.
x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)
#x_train = np.array(x_train).reshape(-1,1)

# Featured scaled data.
#std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)
245/25:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(data.drop(("Outcome"),axis=1))
std_scl_x
245/26:
x_data = data.drop(("Outcome"), axis=1)

x_data = np.array(x_data).reshape(-1,1)

#y_data = data["Outcome"]
y_data = np.array(data["Outcome"]).reshape(-1,1)
245/27:
x_data = data.drop(("Outcome"), axis=1)

x_data = np.array(x_data)

#y_data = data["Outcome"]
y_data = np.array(data["Outcome"]).reshape(-1,1)
245/28:
# Splitting the non scaled data.
x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)
#x_train = np.array(x_train).reshape(-1,1)

# Featured scaled data.
#std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)
245/29:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_reg_predict1 = log_reg.predict(y_test)
print("Accuracy of Non scaled data in logistic regration: ",accuracy_score(y_test,log_reg_predict1))
245/30:
# Splitting the non scaled data.
x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)
#x_train = np.array(x_train).reshape(-1,1)
y_train
# Featured scaled data.
#std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)
245/31:
# Splitting the non scaled data.
x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)
#x_train = np.array(x_train).reshape(-1,1)
y_test
# Featured scaled data.
#std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)
243/1:
stnd_scl = StandardScaler()
data_x_labels = stnd_scl.fit_transform(data_x_labels)
246/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix
246/2: data = pd.read_csv("Breast_cancer_dataset.csv")
245/32:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(data.drop(("Outcome"),axis=1))
#std_scl_x
245/33:
x_data = data.drop(("Outcome"), axis=1)

#x_data = np.array(x_data)

y_data = data["Outcome"]
#y_data = np.array(data["Outcome"]).reshape(-1,1)
245/34:
# Splitting the non scaled data.
#x_train,x_test,y_train,y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)
#x_train = np.array(x_train).reshape(-1,1)

# Featured scaled data.
std_scl_x_train,std_scl_x_test,std_scl_y_train,std_scl_y_test = train_test_split(std_scl_x, y_data, test_size=0.2, random_state=42)
245/35:
log_reg = LogisticRegression()
log_reg.fit(std_scl_x_train,std_scl_y_trainy_)
log_reg_predict1 = log_reg.predict(std_scl_y_test)
print("Accuracy of Non scaled data in logistic regration: ",accuracy_score(std_scl_y_test,log_reg_predict1))
245/36:
log_reg = LogisticRegression()
log_reg.fit(std_scl_x_train,std_scl_y_train)
log_reg_predict1 = log_reg.predict(std_scl_y_test)
print("Accuracy of Non scaled data in logistic regration: ",accuracy_score(std_scl_y_test,log_reg_predict1))
247/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import accuracy_score, confusion_matrix
247/2: data = pd.read_csv("diabetes.csv")
247/3: data.head()
247/4:
print(data.columns)
data.shape
247/5: data.isnull().sum()
247/6: data.info()
247/7: data.describe()
247/8:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
247/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
247/10: sns.pairplot(data, hue="Outcome")
247/11: data.corr()
247/12:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
247/13:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
247/14:
#std_scl = StandardScaler()
#std_scl_x = std_scl.fit_transform(data.drop(("Outcome"),axis=1))
#std_scl_x
247/15: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
247/16:
x_data = data.drop(("Outcome"),axis=1)
y_data = data["outcome"]
247/17:
x_data = data.drop(("Outcome"),axis=1)
y_data = data["Outcome"]
247/18: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
247/19:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_reg_predict = log_reg.predict(y_test)
printf(accuracy_score(y_test,log_reg_predict))
247/20:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
247/21:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(y_test)
247/22:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict([y_test])
247/23:
x_data = np.array(data.drop(("Outcome"),axis=1))
y_data = np.array(data["Outcome"])
247/24: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
247/25: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
247/26:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(y_test)
247/27:
x_data = np.array(data.drop(("Outcome"),axis=1)).reshape(-1,1)
y_data = np.array(data["Outcome"]).reshape(-1,1)
247/28: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
247/29:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(y_test)
247/30:
x_data = np.array(data.drop(("Outcome"),axis=1)).reshape(-1,1)
y_data = (data["Outcome"])
247/31: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
247/32:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(y_test)
247/33:
x_data = data.drop(("Outcome"),axis=1)
y_data = data["Outcome"]
248/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import accuracy_score, confusion_matrix
248/2: data = pd.read_csv("diabetes.csv")
248/3: data.head()
248/4:
print(data.columns)
data.shape
248/5: data.isnull().sum()
248/6: data.info()
248/7: data.describe()
248/8:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
248/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
248/10: sns.pairplot(data, hue="Outcome")
248/11: data.corr()
248/12:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
248/13:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
248/14:
#std_scl = StandardScaler()
#std_scl_x = std_scl.fit_transform(data.drop(("Outcome"),axis=1))
#std_scl_x
248/15:
x_data = data.drop(("Outcome"),axis=1)
y_data = data["Outcome"]
248/16: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
248/17:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(y_test)
248/18:
x_data = data.drop(("Outcome"),axis=1)
y_data = data["Outcome"]
x_data
248/19:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
248/20:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_test
248/21:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
y_test
248/22:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_test
248/23:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
y_test = np.array(y_test)
y_test
248/24:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(y_test)
248/25:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
y_test = np.array(y_test).reshape(-1,1)
y_test
248/26:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(y_test)
248/27:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
y_test = np.array(y_test).reshape(-1,1)
x_test
248/28:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
y_test = np.array(y_test).reshape(-1,1)
x_test = np.array(x_test).reshape(-1,1)
248/29:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
y_test = np.array(y_test).reshape(-1,1)
x_test = np.array(x_test).reshape(-1,1)
x_test
248/30:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(y_test)
248/31:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
y_test = np.array(y_test).reshape(-1,1)
x_train = np.array(x_train).reshape(-1,1)
x_train
248/32:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(y_test)
248/33:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
y_test = np.array(y_test).reshape(-1,1)
x_train = np.array(x_train)
x_train
248/34:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(y_test)
248/35:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
y_test = np.array(y_test).reshape(-1,1)

x_train
248/36:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(y_test)
249/1:
x_data = data.iloc[:,:8].values
y_data = data["Outcome"]
250/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import accuracy_score, confusion_matrix
250/2: data = pd.read_csv("diabetes.csv")
250/3: data.head()
250/4:
print(data.columns)
data.shape
250/5: data.isnull().sum()
250/6: data.info()
250/7: data.describe()
250/8:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
250/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
250/10: sns.pairplot(data, hue="Outcome")
250/11: data.corr()
250/12:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
250/13:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
250/14:
#std_scl = StandardScaler()
#std_scl_x = std_scl.fit_transform(data.drop(("Outcome"),axis=1))
#std_scl_x
250/15:
x_data = data.iloc[:,:8].values
y_data = data["Outcome"]
250/16:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
y_test = np.array(y_test).reshape(-1,1)
x_train = np.array(x_train)
x_train
250/17:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(y_test)
250/18:
x_data = data.iloc[:,:8].values
y_data = data["Outcome"]
x_data
250/19:
x_data = data.iloc[:,:8].values
y_data = data["Outcome"]
y_data
250/20:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
y_data
250/21: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
250/22:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(y_test)
250/23:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train.reshape(-1,1))
predict = log_reg.predict(y_test)
250/24:
log_reg = LogisticRegression()
log_reg.fit(x_train.reshape(-1,1),y_train.reshape(-1,1))
predict = log_reg.predict(y_test)
250/25: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
250/26:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
x_data
250/27:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
x_data.head()
250/28: x_train
250/29: y_train
250/30: x_test
250/31: log = LogisticRegression()
250/32:
log = LogisticRegression()
log.fit(x_train,y_train)
250/33:
log = LogisticRegression()
log.fit(x_train,y_train)
predict = log.predict(x_test)
250/34:
log = LogisticRegression()
log.fit(x_train,y_train)
predict = log.predict(x_test)
print(accuracy_score(y_test,predict))
250/35:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
x_data
250/36:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(x_test)
print(accuracy_score(y_test,predict))
250/37:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(x_test)
print(accuracy_score(y_test,predict))
250/38:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(data.drop(("Outcome"),axis=1))
#std_scl_x
250/39: std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
250/40:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(x_test)
print(accuracy_score(y_test,predict))
250/41:
log_reg1 = LogisticRegression()
log_reg1.fit(std_x_train,y_train)
predict = log_reg1.predict(std_x_test)
print(accuracy_score(y_test,predict))
250/42:
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(x_test)
print("Accuracy of Non standarise data in Logistic regrasion:",accuracy_score(y_test,predict))
250/43:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.metrics import accuracy_score, confusion_matrix
250/44:
imput = SimpleImputer(missing_values=0, strategy="mean", axis = 1)
imput.fit(data["Insulin"], inplace=True)
250/45:
imput = SimpleImputer(missing_values=0, strategy="mean")
imput.fit(data["Insulin"], inplace=True)
250/46:
imput = SimpleImputer(missing_values=0, strategy="mean")
imput.fit(data["Insulin"])
250/47:
imput = SimpleImputer(missing_values=0, strategy="mean", axis=0)
imput.fit(data["Insulin"])
250/48:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import Imputer

from sklearn.metrics import accuracy_score, confusion_matrix
250/49:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, Imputer

from sklearn.metrics import accuracy_score, confusion_matrix
250/50:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.metrics import accuracy_score, confusion_matrix
250/51:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
x_data
251/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.metrics import accuracy_score, confusion_matrix
251/2: data = pd.read_csv("diabetes.csv")
251/3: data.head()
251/4:
print(data.columns)
data.shape
251/5: data.isnull().sum()
251/6: data.info()
251/7: data.describe()
251/8:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
251/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
251/10: sns.pairplot(data, hue="Outcome")
251/11: data.corr()
251/12:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
251/13:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
251/14:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(data.drop(("Outcome"),axis=1))
#std_scl_x
251/15:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
251/16:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
x_data
251/17: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
251/18: std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
251/19:
# Non standarise data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(x_test)
print("Accuracy of Non standarise data in Logistic regrassion:",accuracy_score(y_test,predict))
251/20:
# standarise data
log_reg1 = LogisticRegression()
log_reg1.fit(std_x_train,y_train)
predict = log_reg1.predict(std_x_test)
print(accuracy_score(y_test,predict))
251/21:
d_tree = DecisionTreeClassifier()
d_tree.fit(x_train,y_train)
d_predict = d_tree.predict(x_test)
print("Accuracy of Non standarise data in Deccission Tree Classifier:",accuracy_score(y_test,predict))
251/22:
d_tree = DecisionTreeClassifier()
d_tree.fit(x_train,y_train)
d_predict = d_tree.predict(x_test)
print("Accuracy of Non standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
251/23:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict = d_tree.predict(x_test)
print("Accuracy of Non standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
251/24:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(std_x_train,y_train)
d_predict = d_tree.predict(y_test)
print("Accuracy of Non standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
251/25:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(std_x_train,y_train)
d_predict = d_tree.predict(x_test)
print("Accuracy of Non standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
251/26:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(std_x_train,y_train)
d_predict = d_tree.predict(x_test)
print("Accuracy of Non standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
251/27:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
std_scl_x = impute.fit_transform(std_scl_x)
x_data
251/28: std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
251/29: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
251/30: std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
251/31:
# Non standarise data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(x_test)
print("Accuracy of Non standarise data in Logistic regrassion:",accuracy_score(y_test,predict))
251/32:
# standarise data
log_reg1 = LogisticRegression()
log_reg1.fit(std_x_train,y_train)
predict = log_reg1.predict(std_x_test)
print(accuracy_score(y_test,predict))
251/33:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict = d_tree.predict(x_test)
print("Accuracy of Non standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
251/34:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(std_x_train,y_train)
d_predict = d_tree.predict(x_test)
print("Accuracy of Non standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
252/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.metrics import accuracy_score, confusion_matrix
252/2: data = pd.read_csv("diabetes.csv")
252/3: data.head()
252/4:
print(data.columns)
data.shape
252/5: data.isnull().sum()
252/6: data.info()
252/7: data.describe()
252/8:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
252/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
252/10: sns.pairplot(data, hue="Outcome")
252/11: data.corr()
252/12:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
252/13:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
252/14:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(data.drop(("Outcome"),axis=1))
#std_scl_x
252/15:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
252/16:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
std_scl_x = impute.fit_transform(std_scl_x)
x_data
252/17: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
252/18: std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
252/19:
# Non standarise data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(x_test)
print("Accuracy of Non standarise data in Logistic regrassion:",accuracy_score(y_test,predict))
252/20:
# standarise data
log_reg1 = LogisticRegression()
log_reg1.fit(std_x_train,y_train)
predict = log_reg1.predict(std_x_test)
print(accuracy_score(y_test,predict))
252/21:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict = d_tree.predict(x_test)
print("Accuracy of Non standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
252/22:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(std_x_train,y_train)
d_predict = d_tree.predict(x_test)
print("Accuracy of Non standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
252/23:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(std_x_train,y_train)
d_predict = d_tree.predict(std_x_test)
print("Accuracy of Non standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
252/24:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(std_x_train,y_train)
d_predict = d_tree.predict(std_x_test)
print("Accuracy of standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
252/25:
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict = r_forest.predict(x_test)
print("Accuracy of Non standarise data in Random Forest Classifier:",accuracy_score(y_test,r_predict))
252/26:
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(std_scl_x,y_train)
r_predict = r_forest.predict(std_x_test)
print("Accuracy of standarise data in Random Forest Classifier:",accuracy_score(y_test,r_predict))
252/27:
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(std_x_train,y_train)
r_predict = r_forest.predict(std_x_test)
print("Accuracy of standarise data in Random Forest Classifier:",accuracy_score(y_test,r_predict))
253/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.metrics import accuracy_score, confusion_matrix
253/2: data = pd.read_csv("diabetes.csv")
253/3: data.head()
253/4:
print(data.columns)
data.shape
253/5: data.isnull().sum()
253/6: data.info()
253/7: data.describe()
253/8:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
253/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
253/10: sns.pairplot(data, hue="Outcome")
253/11: data.corr()
253/12:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
253/13:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
253/14:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(data.drop(("Outcome"),axis=1))
#std_scl_x
253/15:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
253/16:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
std_scl_x = impute.fit_transform(std_scl_x)
x_data
253/17: x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
253/18: std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
253/19:
# Non standarise data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(x_test)
print("Accuracy of Non standarise data in Logistic regrassion:",accuracy_score(y_test,predict))
253/20:
# standarise data
log_reg1 = LogisticRegression()
log_reg1.fit(std_x_train,y_train)
predict = log_reg1.predict(std_x_test)
print(accuracy_score(y_test,predict))
253/21:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict = d_tree.predict(x_test)
print("Accuracy of Non standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
253/22:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(std_x_train,y_train)
d_predict = d_tree.predict(std_x_test)
print("Accuracy of standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
253/23:
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict = r_forest.predict(x_test)
print("Accuracy of Non standarise data in Random Forest Classifier:",accuracy_score(y_test,r_predict))
253/24:
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(std_x_train,y_train)
r_predict = r_forest.predict(std_x_test)
print("Accuracy of standarise data in Random Forest Classifier:",accuracy_score(y_test,r_predict))
254/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.metrics import accuracy_score, confusion_matrix
254/2: data = pd.read_csv("diabetes.csv")
254/3: data.head()
254/4:
print(data.columns)
data.shape
254/5: data.isnull().sum()
254/6: data.info()
254/7: data.describe()
254/8:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
254/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
254/10: #sns.pairplot(data, hue="Outcome")
254/11: data.corr()
254/12:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
254/13:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
254/14:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(data.drop(("Outcome"),axis=1))
#std_scl_x
254/15:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
254/16:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
std_scl_x = impute.fit_transform(std_scl_x)
x_data
254/17:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
254/18:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
254/19:
# Non standarise data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(x_test)
print("Accuracy of Non standarise data in Logistic regrassion:",accuracy_score(y_test,predict))
254/20:
# standarise data
log_reg1 = LogisticRegression()
log_reg1.fit(std_x_train,y_train)
predict = log_reg1.predict(std_x_test)
print(accuracy_score(y_test,predict))
254/21:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict = d_tree.predict(x_test)
print("Accuracy of Non standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
254/22:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(std_x_train,y_train)
d_predict = d_tree.predict(std_x_test)
print("Accuracy of standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
254/23:
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict = r_forest.predict(x_test)
print("Accuracy of Non standarise data in Random Forest Classifier:",accuracy_score(y_test,r_predict))
254/24:
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(std_x_train,y_train)
r_predict = r_forest.predict(std_x_test)
print("Accuracy of standarise data in Random Forest Classifier:",accuracy_score(y_test,r_predict))
255/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.metrics import accuracy_score, confusion_matrix
255/2: data = pd.read_csv("diabetes.csv")
255/3: data.head()
255/4:
print(data.columns)
data.shape
255/5: data.isnull().sum()
255/6: data.info()
255/7: data.describe()
255/8:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
255/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
255/10: #sns.pairplot(data, hue="Outcome")
255/11: data.corr()
255/12:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
255/13:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
255/14:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
255/15:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
x_data
255/16:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
255/17:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
255/18:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
255/19:
# Non standarise data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
predict = log_reg.predict(x_test)
print("Accuracy of Non standarise data in Logistic regrassion:",accuracy_score(y_test,predict))
255/20:
# standarise data
log_reg1 = LogisticRegression()
log_reg1.fit(std_x_train,y_train)
predict = log_reg1.predict(std_x_test)
print(accuracy_score(y_test,predict))
255/21:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict = d_tree.predict(x_test)
print("Accuracy of Non standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
255/22:
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(std_x_train,y_train)
d_predict = d_tree.predict(std_x_test)
print("Accuracy of standarise data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict))
255/23:
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict = r_forest.predict(x_test)
print("Accuracy of Non standarise data in Random Forest Classifier:",accuracy_score(y_test,r_predict))
255/24:
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(std_x_train,y_train)
r_predict = r_forest.predict(std_x_test)
print("Accuracy of standarise data in Random Forest Classifier:",accuracy_score(y_test,r_predict))
257/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.metrics import accuracy_score, confusion_matrix
257/2: data = pd.read_csv("diabetes.csv")
257/3: data.head()
257/4:
print(data.columns)
data.shape
257/5: data.isnull().sum()
257/6: data.info()
257/7: data.describe()
257/8:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
257/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
257/10: #sns.pairplot(data, hue="Outcome")
257/11: data.corr()
257/12:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
257/13:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
257/14:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
257/15:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
x_data
257/16:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
257/17:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
257/18:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
257/19:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
257/20:
# standardize data
log_reg1 = LogisticRegression()
log_reg1.fit(std_x_train,y_train)
log_predict2 = log_reg1.predict(std_x_test)
print(accuracy_score(y_test,log_predict2))
257/21:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
257/22:
#  standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(std_x_train,y_train)
d_predict2 = d_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict2))
257/23:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
257/24:
# standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(std_x_train,y_train)
r_predict2 = r_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict2))
257/25:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
257/26:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.metrics import accuracy_score, confusion_matrix
258/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.metrics import accuracy_score, confusion_matrix
258/2: data = pd.read_csv("diabetes.csv")
258/3: data.head()
258/4:
print(data.columns)
data.shape
258/5: data.isnull().sum()
258/6: data.info()
258/7: data.describe()
258/8:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
258/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
258/10: #sns.pairplot(data, hue="Outcome")
258/11: data.corr()
258/12:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
258/13:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
258/14:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
258/15:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
x_data
258/16:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
258/17:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
258/18:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
258/19:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
258/20:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = log_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
259/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.metrics import accuracy_score, confusion_matrix
259/2: data = pd.read_csv("diabetes.csv")
259/3: data.head()
259/4:
print(data.columns)
data.shape
259/5: data.isnull().sum()
259/6: data.info()
259/7: data.describe()
259/8:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
259/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
259/10: #sns.pairplot(data, hue="Outcome")
259/11: data.corr()
259/12:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
259/13:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
259/14:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
259/15:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
x_data
259/16:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
259/17:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
259/18:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
259/19:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
259/20:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
259/21:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
259/22:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
259/23:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
259/24:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
259/25:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
259/26:
cross_val = cross_val_score(estimator=model_3, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
259/27:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
260/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.metrics import accuracy_score, confusion_matrix
260/2: data = pd.read_csv("diabetes.csv")
260/3: data.head()
260/4:
print(data.columns)
data.shape
260/5: data.isnull().sum()
260/6: data.info()
260/7: data.describe()
260/8:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
260/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
260/10: sns.pairplot(data, hue="Outcome")
260/11: data.corr()
260/12:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
260/13:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
260/14:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
260/15:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
x_data
260/16:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
260/17:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
260/18:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
260/19:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
260/20:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
260/21:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
260/22:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
260/23:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
260/24:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
260/25:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
260/26:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
261/1:
import pandas as pd
import numpy as np
import mtplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
261/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
261/3:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
261/4: data.info()
263/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
263/2:
data = pd.read_csv("heart1.csv")
data
264/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
264/2:
data = pd.read_csv("heart_disease.csv")
data
264/3: data.info()
264/4: data.isnull().sum()
265/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
265/2:
data = pd.read_csv("heart_disease.csv")
data
265/3: data.info()
265/4: data.isnull().sum()
265/5: data.columns
265/6: data.define
265/7: data.define()
265/8: data.describe
265/9: data.info()
265/10: data.describe()
265/11: sns.pairplot(data["target"])
265/12: sns.countplot(data["target"])
265/13: data.corr()
265/14:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())
265/15:
plt.figure(figsize=(15,8))
sns.heatmap(data)
265/16: sns.pairplot(data, hue="target")
265/17:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
265/18:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
265/19:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
267/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
267/2:
data = pd.read_csv("heart_disease.csv")
data
267/3: data.info()
267/4: data.describe()
267/5: data.columns
267/6: data.info()
267/7: data.isnull().sum()
267/8: data.corr()
267/9:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
267/10:
plt.figure(figsize=(15,8))
sns.heatmap(data)
267/11: sns.pairplot(data, hue="target")
267/12:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
267/13:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
267/14: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope","smoothness_mean"])
267/15: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
267/16:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
267/17:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_y = data["target"]
267/18:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_y = data["target"]
print(all_x)
print(all_y)
267/19:
# Selecting all the features

all_x = data.drop(("target"), axis=1)

# Selecting only importent features

imp_x = data["cp","restecg","thalach","slope"]


y = data["target"]

print(all_x)
print(imp)
print(all_y)
267/20:
# Selecting all the features

all_x = data.drop(("target"), axis=1)

# Selecting only importent features

imp_x = data["cp","restecg","thalach","slope"]


y = data["target"]

print(all_x)
print(imp_x)
print(all_y)
267/21:
# Selecting all the features

all_x = data.drop(("target"), axis=1)

# Selecting only importent features

imp_x = data[["cp"],["restecg"],["thalach"],["slope"]]


y = data["target"]

print(all_x)
print(imp_x)
print(all_y)
267/22:
# Selecting all the features

all_x = data.drop(("target"), axis=1)

# Selecting only importent features

imp_x = data("cp","restecg","thalach","slope")


y = data["target"]

print(all_x)
print(imp_x)
print(all_y)
267/23:
# Selecting all the features

all_x = data.drop(("target"), axis=1)

# Selecting only importent features

imp_x = data["cp","restecg","thalach","slope"]


y = data["target"]

print(all_x)
print(imp_x)
print(all_y)
267/24:
# Selecting all the features

all_x = data.drop(("target"), axis=1)

# Selecting only importent features

imp_x = data[("cp","restecg","thalach","slope")]


y = data["target"]

print(all_x)
print(imp_x)
print(all_y)
267/25:
# Selecting all the features

all_x = data.drop(("target"), axis=1)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]


y = data["target"]

print(all_x)
print(imp_x)
print(all_y)
267/26:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)
267/27: all_x_train,all_test_x,all_y_train,all_y_test = train_test_split(all_x,y)
267/28: all_x_train,all_test_x,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
267/29:
all_x_train,all_test_x,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)

all_x_train.head()
267/30:
all_x_train,all_test_x,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)

all_x_train
267/31:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)

all_x_train
267/32: imp_x_train,imp_x_test,imp
267/33:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
267/34:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
all_y_train
267/35:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
imp_y_train
267/36:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
all_y_train
267/37:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
imp_y_train
267/38:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix
267/39:
# Training and testing on all selected features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train)
lg_model_prediction = lg_model.predict(all_y_test)
print("Accuracy: ", accuracy_score(all_y_test,lg_model_prediction))
267/40:
# Training and testing on all selected features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_y_test)
print("Accuracy: ", accuracy_score(all_y_test,lg_model_prediction))
267/41:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]


y = data["target"]

print(all_x)
print(imp_x)
print(all_y)
267/42:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)
267/43:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
all_y_train
267/44:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
imp_y_train
267/45:
# Training and testing on all selected features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_y_test)
print("Accuracy: ", accuracy_score(all_y_test,lg_model_prediction))
267/46:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x).reshape(-1,1)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x).reshape(-1,1)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(all_y)
267/47:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)
267/48:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
all_y_train
267/49:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
imp_y_train
267/50:
# Training and testing on all selected features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_y_test)
print("Accuracy: ", accuracy_score(all_y_test,lg_model_prediction))
267/51:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x).reshape(-1,1)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x).reshape(-1,1)


y = data["target"]

print(all_x)
print(imp_x)
print(all_y)
267/52:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)
267/53:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
all_y_train
267/54:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]

print(all_x)
print(imp_x)
print(all_y)
267/55:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)
267/56:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
all_y_train
267/57:
# Training and testing on all selected features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_y_test)
print("Accuracy: ", accuracy_score(all_y_test,lg_model_prediction))
267/58:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(all_y)
267/59:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)
267/60:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
all_y_train
267/61:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
imp_y_train
267/62:
# Training and testing on all selected features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_y_test)
print("Accuracy: ", accuracy_score(all_y_test,lg_model_prediction))
267/63:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x).reshape(-1,1)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(all_y)
267/64:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)
267/65:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
all_y_train
267/66:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(all_y)
267/67:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)
267/68:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
all_y_train
267/69:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
imp_y_train
267/70:
# Training and testing on all selected features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_y_test)
print("Accuracy: ", accuracy_score(all_y_test,lg_model_prediction))
270/1:
arr = [[1,1,1,1,1],
       [0,0,0,0,0]]
270/2: arr
270/3: arr.shape()
270/4: arr.shape
270/5:
arr = np.array([[1,1,1,1,1],
       [0,0,0,0,0]])
270/6:
import numpy ans np
arr = np.array([[1,1,1,1,1],
       [0,0,0,0,0]])
270/7:
import numpy as np
arr = np.array([[1,1,1,1,1],
       [0,0,0,0,0]])
270/8: arr
270/9:
arr
arr.shape
270/10:
arr
arr.reshape(1-,1)
270/11:
arr
arr.reshape(-1,1)
270/12:
arr
arr.reshape(1,-1)
267/71:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
#all_x = np.array(all_x)

# Selecting only importent features

#imp_x = data[["cp","restecg","thalach","slope"]]
#imp_x = np.array(imp_x)


y = data["target"]
#y = np.array(y).reshape(-1,1)

print(all_x)
#print(imp_x)
#print(all_y)
267/72:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

#imp_x = data[["cp","restecg","thalach","slope"]]
#imp_x = np.array(imp_x)


y = data["target"]
#y = np.array(y).reshape(-1,1)

print(all_x)
#print(imp_x)
#print(all_y)
267/73:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

#imp_x = data[["cp","restecg","thalach","slope"]]
#imp_x = np.array(imp_x)


y = data["target"]
#y = np.array(y).reshape(-1,1)

print(all_x)
#print(imp_x)
print(all_y)
267/74:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)
267/75:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
267/76:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
all_y_train
267/77:
# Training and testing on all selected features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_y_test)
print("Accuracy: ", accuracy_score(all_y_test,lg_model_prediction))
267/78:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

#imp_x = data[["cp","restecg","thalach","slope"]]
#imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y)

print(all_x)
#print(imp_x)
print(all_y)
267/79:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
267/80:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
all_y_train
267/81:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
imp_y_train
267/82:
# Training and testing on all selected features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_y_test)
print("Accuracy: ", accuracy_score(all_y_test,lg_model_prediction))
267/83:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

#imp_x = data[["cp","restecg","thalach","slope"]]
#imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
#print(imp_x)
print(all_y)
267/84:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
267/85:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
all_y_train
267/86:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
imp_y_train
267/87:
# Training and testing on all selected features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_y_test)
print("Accuracy: ", accuracy_score(all_y_test,lg_model_prediction))
267/88:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

#imp_x = data[["cp","restecg","thalach","slope"]]
#imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
#print(imp_x)
print(y)
267/89:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
267/90:
# Training and testing on all selected features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy: ", accuracy_score(all_y_test,lg_model_prediction))
267/91:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warning

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix
267/92:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix
267/93: warnings.filewarnings("ignore")
267/94:
data = pd.read_csv("heart_disease.csv")
data
267/95: data.info()
267/96: data.describe()
267/97: data.columns
267/98: data.info()
267/99: data.isnull().sum()
267/100: data.corr()
267/101:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
267/102:
plt.figure(figsize=(15,8))
sns.heatmap(data)
267/103: sns.pairplot(data, hue="target")
267/104:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
267/105:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
267/106: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
267/107:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

#imp_x = data[["cp","restecg","thalach","slope"]]
#imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
#print(imp_x)
print(y)
267/108:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
267/109:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
267/110:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
imp_y_train
267/111:
# Training and testing on all selected features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy: ", accuracy_score(all_y_test,lg_model_prediction))
271/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix
271/2: warnings.filterwarnings("ignore")
271/3:
data = pd.read_csv("heart_disease.csv")
data
271/4: data.info()
271/5: data.describe()
271/6: data.columns
271/7: data.info()
271/8: data.isnull().sum()
271/9: data.corr()
271/10:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
271/11:
plt.figure(figsize=(15,8))
sns.heatmap(data)
271/12: sns.pairplot(data, hue="target")
271/13:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
271/14:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
271/15: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
271/16:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

#imp_x = data[["cp","restecg","thalach","slope"]]
#imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
#print(imp_x)
print(y)
271/17:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
271/18:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(y)
271/19:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
271/20:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
271/21:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
imp_y_train
271/22:
# Training and testing on all selected features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy: ", accuracy_score(all_y_test,lg_model_prediction))
271/23:
# Training and testing on all selected features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction))
271/24:
# Training and testing on all features.

lg_model1 = LogisticRegression()
lg_model1.fit(imp_x_train,imp_y_train)
lg_model1_prediction = lg_model1.predict(imp_x_test)
print("Accuracy on Selected features: ", accuracy_score(imp_y_test,lg_model1_prediction))
271/25: dt_model = DecisionTreeClassifier(random_state = 42)
271/26:
dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction))
271/27:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test,dt_model1_prediction))
271/28:
# Training and testing on all features.

rf_model = RandomForestClassifier()
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on selected features: ", accuracy_score(all_y_test, rf_model_prediction))
271/29:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on selected features: ", accuracy_score(all_y_test, rf_model_prediction))
271/30:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test,dt_model1_prediction))
271/31:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction))
271/32:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction))
271/33:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=10, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction))
271/34:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=10, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction))
271/35:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction))
271/36:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction))
272/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix
272/2: warnings.filterwarnings("ignore")
272/3:
data = pd.read_csv("heart_disease.csv")
data
272/4: data.info()
272/5: data.describe()
272/6: data.columns
272/7: data.info()
272/8: data.isnull().sum()
272/9: data.corr()
272/10:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
272/11:
plt.figure(figsize=(15,8))
sns.heatmap(data)
272/12: sns.pairplot(data, hue="target")
272/13:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
272/14:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
272/15: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
272/16:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x).reshape(-1,1)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(y)
272/17:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
272/18:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
272/19:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
imp_y_train
272/20:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x).reshape(-1,1)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print("###################")
print(imp_x)
print(y)
272/21:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print("###################")
print(imp_x)
print(y)
273/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix
273/2: warnings.filterwarnings("ignore")
273/3:
data = pd.read_csv("heart_disease.csv")
data
273/4: data.info()
273/5: data.describe()
273/6: data.columns
273/7: data.info()
273/8: data.isnull().sum()
273/9: data.corr()
273/10:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
273/11:
plt.figure(figsize=(15,8))
sns.heatmap(data)
273/12: sns.pairplot(data, hue="target")
273/13:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
273/14:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
273/15: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
273/16:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(y)
273/17:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
273/18:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
273/19:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
imp_y_train
273/20:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction))
273/21:
# Training and testing on selected features.

lg_model1 = LogisticRegression()
lg_model1.fit(imp_x_train,imp_y_train)
lg_model1_prediction = lg_model1.predict(imp_x_test)
print("Accuracy on Selected features: ", accuracy_score(imp_y_test,lg_model1_prediction))
273/22:
# Training and testing on all features.

dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction))
273/23:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test,dt_model1_prediction))
273/24:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction))
273/25:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction))
273/26:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import accuracy_score, confusion_matrix
273/27:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction))
273/28:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction)*100)
273/29:
# Training and testing on selected features.

nb_model1 = GaussianNB()
nb_model1.fit(imp_x_train,imp_y_train)
nb_model1_prediction = nb_model1.predict(imp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test, nb_model1_prediction)*100)
273/30:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, confusion_matrix
273/31:
# Training and testing on all features.

svm_model = SVC()
svm_model.fit(all_x_train,all_y_train)
svm_model_prediction = svm_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, svm_model_prediction)*100)
273/32:
# Training and testing on selected features.

svm_model1 = SVC()
svm_model1.fit(imp_x_train,imp_y_train)
svm_model1_prediction = svm_model1.predict(immp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test, svm_model1_prediction)*100)
273/33:
# Training and testing on selected features.

svm_model1 = SVC()
svm_model1.fit(imp_x_train,imp_y_train)
svm_model1_prediction = svm_model1.predict(imp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test, svm_model1_prediction)*100)
274/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, confusion_matrix
274/2: warnings.filterwarnings("ignore")
274/3:
data = pd.read_csv("heart_disease.csv")
data
274/4: data.info()
274/5: data.describe()
274/6: data.columns
274/7: data.info()
274/8: data.isnull().sum()
274/9: data.corr()
274/10:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
274/11:
plt.figure(figsize=(15,8))
sns.heatmap(data)
274/12: sns.pairplot(data, hue="target")
274/13:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
274/14:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
274/15: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
274/16:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(y)
274/17:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
274/18:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
274/19:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
imp_y_train
274/20:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction)*100)
274/21:
# Training and testing on selected features.

lg_model1 = LogisticRegression()
lg_model1.fit(imp_x_train,imp_y_train)
lg_model1_prediction = lg_model1.predict(imp_x_test)
print("Accuracy on Selected features: ", accuracy_score(imp_y_test,lg_model1_prediction)*100)
274/22:
# Training and testing on all features.

dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction)*100)
274/23:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test,dt_model1_prediction)*100)
274/24:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction)*100)
274/25:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction)*100)
274/26:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction)*100)
274/27:
# Training and testing on selected features.

nb_model1 = GaussianNB()
nb_model1.fit(imp_x_train,imp_y_train)
nb_model1_prediction = nb_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, nb_model1_prediction)*100)
274/28:
# Training and testing on all features.

svm_model = SVC()
svm_model.fit(all_x_train,all_y_train)
svm_model_prediction = svm_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, svm_model_prediction)*100)
274/29:
# Training and testing on selected features.

svm_model1 = SVC()
svm_model1.fit(imp_x_train,imp_y_train)
svm_model1_prediction = svm_model1.predict(imp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test, svm_model1_prediction)*100)
279/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, confusion_matrix
279/2: warnings.filterwarnings("ignore")
279/3:
data = pd.read_csv("heart_disease.csv")
data
279/4: data.info()
279/5: data.describe()
279/6: data.columns
279/7: data.info()
279/8: data.isnull().sum()
279/9: data.corr()
279/10:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
279/11:
plt.figure(figsize=(15,8))
sns.heatmap(data)
279/12: sns.pairplot(data, hue="target")
279/13:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
279/14:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
279/15: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
279/16:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(y)
279/17:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
279/18:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
279/19:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
imp_y_train
279/20:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction)*100)
279/21:
# Training and testing on selected features.

lg_model1 = LogisticRegression()
lg_model1.fit(imp_x_train,imp_y_train)
lg_model1_prediction = lg_model1.predict(imp_x_test)
print("Accuracy on Selected features: ", accuracy_score(imp_y_test,lg_model1_prediction)*100)
279/22:
confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
279/23:
# Training and testing on all features.

dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction)*100)
279/24:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test,dt_model1_prediction)*100)
279/25:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction)*100)
279/26:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction)*100)
279/27:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction)*100)
279/28:
# Training and testing on selected features.

nb_model1 = GaussianNB()
nb_model1.fit(imp_x_train,imp_y_train)
nb_model1_prediction = nb_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, nb_model1_prediction)*100)
279/29:
# Training and testing on all features.

svm_model = SVC()
svm_model.fit(all_x_train,all_y_train)
svm_model_prediction = svm_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, svm_model_prediction)*100)
279/30:
# Training and testing on selected features.

svm_model1 = SVC()
svm_model1.fit(imp_x_train,imp_y_train)
svm_model1_prediction = svm_model1.predict(imp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test, svm_model1_prediction)*100)
279/31:
confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
279/32:
confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
279/33:
confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
279/34:
confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
279/35:
confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)
print(confu_matrix)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
279/36:
confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)
print(confu_matrix)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')
plt.show()
279/37:
confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')
plt.show()
279/38:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
#imp_y_train
279/39:
confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')
plt.show()
279/40: data.head()
280/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, confusion_matrix
280/2: warnings.filterwarnings("ignore")
280/3:
data = pd.read_csv("heart_disease.csv")
data
280/4: data.head()
280/5: data.info()
280/6: data.describe()
280/7: data.columns
280/8: data.info()
280/9: data.isnull().sum()
280/10: data.corr()
280/11:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
280/12:
plt.figure(figsize=(15,8))
sns.heatmap(data)
280/13: sns.pairplot(data, hue="target")
280/14:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
280/15:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
280/16: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
280/17:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(y)
280/18:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
280/19:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
280/20:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
#imp_y_train
280/21:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction)*100)
280/22:
# Training and testing on selected features.

lg_model1 = LogisticRegression()
lg_model1.fit(imp_x_train,imp_y_train)
lg_model1_prediction = lg_model1.predict(imp_x_test)
print("Accuracy on Selected features: ", accuracy_score(imp_y_test,lg_model1_prediction)*100)
280/23:
confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
280/24:
# Training and testing on all features.

dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction)*100)
280/25:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test,dt_model1_prediction)*100)
280/26:
confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')
plt.show()
280/27:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction)*100)
280/28:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction)*100)
280/29:
confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')
plt.show()
280/30:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction)*100)
280/31:
# Training and testing on selected features.

nb_model1 = GaussianNB()
nb_model1.fit(imp_x_train,imp_y_train)
nb_model1_prediction = nb_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, nb_model1_prediction)*100)
280/32:
confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
280/33:
# Training and testing on all features.

svm_model = SVC()
svm_model.fit(all_x_train,all_y_train)
svm_model_prediction = svm_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, svm_model_prediction)*100)
280/34:
# Training and testing on selected features.

svm_model1 = SVC()
svm_model1.fit(imp_x_train,imp_y_train)
svm_model1_prediction = svm_model1.predict(imp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test, svm_model1_prediction)*100)
280/35:
confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
280/36:
std_scl = StandardScaler()

data = std_scl.fit(data)
280/37:
std_scl = StandardScaler()

data = std_scl.fit(data)
data
281/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, confusion_matrix
281/2: warnings.filterwarnings("ignore")
281/3:
data = pd.read_csv("heart_disease.csv")
data
281/4: data.head()
281/5: data.info()
281/6: data.describe()
281/7: data.columns
281/8: data.info()
281/9: data.isnull().sum()
281/10: data.corr()
281/11:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
281/12:
plt.figure(figsize=(15,8))
sns.heatmap(data)
281/13: #sns.pairplot(data, hue="target")
281/14:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
281/15:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
281/16:
std_scl = StandardScaler()

data = std_scl.fit(data)
data
281/17: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
282/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, confusion_matrix
282/2: warnings.filterwarnings("ignore")
282/3:
data = pd.read_csv("heart_disease.csv")
data
282/4: data.head()
282/5: data.info()
282/6: data.describe()
282/7: data.columns
282/8: data.info()
282/9: data.isnull().sum()
282/10: data.corr()
282/11:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
282/12:
plt.figure(figsize=(15,8))
sns.heatmap(data)
282/13: #sns.pairplot(data, hue="target")
282/14:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
282/15:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
282/16: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
282/17:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(y)
282/18:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
282/19:
plt.figure(figsize=(15,8))
sns.heatmap(all_x)
282/20:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
282/21:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
#imp_y_train
282/22:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction)*100)
282/23:
# Training and testing on selected features.

lg_model1 = LogisticRegression()
lg_model1.fit(imp_x_train,imp_y_train)
lg_model1_prediction = lg_model1.predict(imp_x_test)
print("Accuracy on Selected features: ", accuracy_score(imp_y_test,lg_model1_prediction)*100)
282/24:
confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
282/25:
# Training and testing on all features.

dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction)*100)
282/26:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test,dt_model1_prediction)*100)
282/27:
confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')
plt.show()
282/28:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction)*100)
282/29:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction)*100)
282/30:
confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')
plt.show()
282/31:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction)*100)
282/32:
# Training and testing on selected features.

nb_model1 = GaussianNB()
nb_model1.fit(imp_x_train,imp_y_train)
nb_model1_prediction = nb_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, nb_model1_prediction)*100)
282/33:
confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
282/34:
# Training and testing on all features.

svm_model = SVC()
svm_model.fit(all_x_train,all_y_train)
svm_model_prediction = svm_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, svm_model_prediction)*100)
282/35:
# Training and testing on selected features.

svm_model1 = SVC()
svm_model1.fit(imp_x_train,imp_y_train)
svm_model1_prediction = svm_model1.predict(imp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test, svm_model1_prediction)*100)
282/36:
confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
282/37:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
282/38:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction)*100)
print("R-square on all features: ", r2_score(all_y_test,lg_model_prediction)*100)
282/39:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction)*100)
print("R-square on all features: ", r2_score(all_y_test,lg_model_prediction))
282/40:
# Training and testing on selected features.

lg_model1 = LogisticRegression()
lg_model1.fit(imp_x_train,imp_y_train)
lg_model1_prediction = lg_model1.predict(imp_x_test)
print("Accuracy on Selected features: ", accuracy_score(imp_y_test,lg_model1_prediction)*100)
print("Accuracy on Selected features: ", r2_score(imp_y_test,lg_model1_prediction))
282/41:
# Training and testing on all features.

dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction)*100)
print("Accuracy on all features: ", r2_score(all_y_test,dt_model_prediction)*100)
282/42:
# Training and testing on all features.

dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction)*100)
print("Accuracy on all features: ", r2_score(all_y_test,dt_model_prediction))
282/43:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test,dt_model1_prediction)*100)
print("Accuracy on selected features: ", r2_score(imp_y_test,dt_model1_prediction))
285/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
285/2: warnings.filterwarnings("ignore")
285/3:
data = pd.read_csv("heart_disease.csv")
data
285/4: data.head()
285/5: data.info()
285/6: data.describe()
285/7: data.columns
285/8: data.info()
285/9: data.isnull().sum()
285/10: data.corr()
285/11:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
285/12:
plt.figure(figsize=(15,8))
sns.heatmap(data)
285/13: #sns.pairplot(data, hue="target")
285/14:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
285/15:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
285/16: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
285/17:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(y)
285/18:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
285/19:
plt.figure(figsize=(15,8))
sns.heatmap(all_x)
285/20:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
285/21:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
#imp_y_train
285/22:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction)*100)
print("R-square on all features: ", r2_score(all_y_test,lg_model_prediction))
285/23:
# Training and testing on selected features.

lg_model1 = LogisticRegression()
lg_model1.fit(imp_x_train,imp_y_train)
lg_model1_prediction = lg_model1.predict(imp_x_test)
print("Accuracy on Selected features: ", accuracy_score(imp_y_test,lg_model1_prediction)*100)
print("Accuracy on Selected features: ", r2_score(imp_y_test,lg_model1_prediction))
285/24:
confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
285/25:
# Training and testing on all features.

dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction)*100)
print("Accuracy on all features: ", r2_score(all_y_test,dt_model_prediction))
285/26:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test,dt_model1_prediction)*100)
print("Accuracy on selected features: ", r2_score(imp_y_test,dt_model1_prediction))
285/27:
confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')
plt.show()
285/28:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction)*100)
285/29:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction)*100)
285/30:
confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')
plt.show()
285/31:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction)*100)
285/32:
# Training and testing on selected features.

nb_model1 = GaussianNB()
nb_model1.fit(imp_x_train,imp_y_train)
nb_model1_prediction = nb_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, nb_model1_prediction)*100)
285/33:
confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
285/34:
# Training and testing on all features.

svm_model = SVC()
svm_model.fit(all_x_train,all_y_train)
svm_model_prediction = svm_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, svm_model_prediction)*100)
285/35:
# Training and testing on selected features.

svm_model1 = SVC()
svm_model1.fit(imp_x_train,imp_y_train)
svm_model1_prediction = svm_model1.predict(imp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test, svm_model1_prediction)*100)
285/36:
confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
285/37:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction)*100)
285/38:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction)*100)
285/39:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction)*100)
286/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
286/2: warnings.filterwarnings("ignore")
286/3:
data = pd.read_csv("heart_disease.csv")
data
286/4: data.head()
286/5: data.info()
286/6: data.describe()
286/7: data.columns
286/8: data.info()
286/9: data.isnull().sum()
286/10: data.corr()
286/11:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
286/12:
plt.figure(figsize=(15,8))
sns.heatmap(data)
286/13: #sns.pairplot(data, hue="target")
286/14:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
286/15:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
286/16: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
286/17:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(y)
286/18:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
286/19:
plt.figure(figsize=(15,8))
sns.heatmap(all_x)
286/20:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
286/21:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
#imp_y_train
286/22:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction)*100)
print("R-square on all features: ", r2_score(all_y_test,lg_model_prediction))
286/23:
# Training and testing on selected features.

lg_model1 = LogisticRegression()
lg_model1.fit(imp_x_train,imp_y_train)
lg_model1_prediction = lg_model1.predict(imp_x_test)
print("Accuracy on Selected features: ", accuracy_score(imp_y_test,lg_model1_prediction)*100)
print("Accuracy on Selected features: ", r2_score(imp_y_test,lg_model1_prediction))
286/24:
confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
286/25:
# Training and testing on all features.

dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction)*100)
print("Accuracy on all features: ", r2_score(all_y_test,dt_model_prediction))
286/26:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test,dt_model1_prediction)*100)
print("Accuracy on selected features: ", r2_score(imp_y_test,dt_model1_prediction))
286/27:
confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')
plt.show()
286/28:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction)*100)
286/29:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction)*100)
286/30:
confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')
plt.show()
286/31:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction)*100)
286/32:
# Training and testing on selected features.

nb_model1 = GaussianNB()
nb_model1.fit(imp_x_train,imp_y_train)
nb_model1_prediction = nb_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, nb_model1_prediction)*100)
286/33:
confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
286/34:
# Training and testing on all features.

svm_model = SVC()
svm_model.fit(all_x_train,all_y_train)
svm_model_prediction = svm_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, svm_model_prediction)*100)
286/35:
# Training and testing on selected features.

svm_model1 = SVC()
svm_model1.fit(imp_x_train,imp_y_train)
svm_model1_prediction = svm_model1.predict(imp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test, svm_model1_prediction)*100)
286/36:
confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
287/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
287/2: warnings.filterwarnings("ignore")
287/3:
data = pd.read_csv("heart_disease.csv")
data
287/4: data.head()
287/5: data.info()
287/6: data.describe()
287/7: data.columns
287/8: data.info()
287/9: data.isnull().sum()
287/10: data.corr()
287/11:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
287/12:
plt.figure(figsize=(15,8))
sns.heatmap(data)
287/13: #sns.pairplot(data, hue="target")
287/14:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
287/15:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
287/16: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
287/17:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(y)
287/18:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
287/19:
plt.figure(figsize=(15,8))
sns.heatmap(all_x)
287/20:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
287/21:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
#imp_y_train
287/22:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction)*100)
print("R-square on all features: ", r2_score(all_y_test,lg_model_prediction))
287/23:
# Training and testing on selected features.

lg_model1 = LogisticRegression()
lg_model1.fit(imp_x_train,imp_y_train)
lg_model1_prediction = lg_model1.predict(imp_x_test)
print("Accuracy on Selected features: ", accuracy_score(imp_y_test,lg_model1_prediction)*100)
print("Accuracy on Selected features: ", r2_score(imp_y_test,lg_model1_prediction))
287/24:
confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
287/25:
# Training and testing on all features.

dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction)*100)
print("Accuracy on all features: ", r2_score(all_y_test,dt_model_prediction))
287/26:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test,dt_model1_prediction)*100)
print("Accuracy on selected features: ", r2_score(imp_y_test,dt_model1_prediction))
287/27:
confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')
plt.show()
287/28:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction)*100)
287/29:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction)*100)
287/30:
confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')
plt.show()
287/31:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction)*100)
287/32:
# Training and testing on selected features.

nb_model1 = GaussianNB()
nb_model1.fit(imp_x_train,imp_y_train)
nb_model1_prediction = nb_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, nb_model1_prediction)*100)
287/33:
confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
287/34:
# Training and testing on all features.

svm_model = SVC()
svm_model.fit(all_x_train,all_y_train)
svm_model_prediction = svm_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, svm_model_prediction)*100)
287/35:
# Training and testing on selected features.

svm_model1 = SVC()
svm_model1.fit(imp_x_train,imp_y_train)
svm_model1_prediction = svm_model1.predict(imp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test, svm_model1_prediction)*100)
287/36:
confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
287/37:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

import pickel

from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
287/38:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

import pickle

from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
287/39: warnings.filterwarnings("ignore")
287/40:
# save model
pickle.dump(rf_model,open('Heart_disease_prediction.pickle','wb'))

# Load Model
heart_disease_prediction_model = pickle.load(open('Heart_disease_prediction.pickle','rb'))

y_predict = heart_disease_prediction_model.predict(all_x_test)

print("confusion matrix: \n", confusion_matrix(all_y_test,y_predict))
287/41:
# save model
pickle.dump(rf_model,open('Heart_disease_prediction.pickle','wb'))

# Load Model
heart_disease_prediction_model = pickle.load(open('Heart_disease_prediction.pickle','rb'))

y_predict = heart_disease_prediction_model.predict(all_x_test)

print("confusion matrix: \n", confusion_matrix(all_y_test,y_predict))
accuracy_score(all_y_test,y_predict)
288/1: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/2: pip install flask
288/3: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/4: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/5: clear
288/6: runcell(0, 'F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py')
288/7: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/8: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/9: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/10: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/11: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/12: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
287/42:
# save model
pickle.dump(rf_model,open('model.pkl','wb'))

# Load Model
heart_disease_prediction_model = pickle.load(open('model.pkl','rb'))

y_predict = heart_disease_prediction_model.predict(all_x_test)

print("confusion matrix: \n", confusion_matrix(all_y_test,y_predict))
accuracy_score(all_y_test,y_predict)
288/13: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/14: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/15: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/16: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/17: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/18: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/19: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/20: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/21: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
289/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

import pickle

from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
289/2: warnings.filterwarnings("ignore")
289/3:
data = pd.read_csv("heart_disease.csv")
data
289/4: data.head()
289/5: data.info()
289/6: data.describe()
289/7: data.columns
289/8: data.info()
289/9: data.isnull().sum()
289/10: data.corr()
289/11:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
289/12:
plt.figure(figsize=(15,8))
sns.heatmap(data)
289/13: #sns.pairplot(data, hue="target")
289/14:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
289/15:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
289/16: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
289/17:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(y)
289/18:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
289/19:
plt.figure(figsize=(15,8))
sns.heatmap(all_x)
289/20:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
289/21:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
#imp_y_train
289/22:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction)*100)
print("R-square on all features: ", r2_score(all_y_test,lg_model_prediction))
289/23:
# Training and testing on selected features.

lg_model1 = LogisticRegression()
lg_model1.fit(imp_x_train,imp_y_train)
lg_model1_prediction = lg_model1.predict(imp_x_test)
print("Accuracy on Selected features: ", accuracy_score(imp_y_test,lg_model1_prediction)*100)
print("Accuracy on Selected features: ", r2_score(imp_y_test,lg_model1_prediction))
289/24:
confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
289/25:
# Training and testing on all features.

dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction)*100)
print("Accuracy on all features: ", r2_score(all_y_test,dt_model_prediction))
289/26:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test,dt_model1_prediction)*100)
print("Accuracy on selected features: ", r2_score(imp_y_test,dt_model1_prediction))
289/27:
confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')
plt.show()
289/28:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction)*100)
289/29:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction)*100)
289/30:
confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')
plt.show()
289/31:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction)*100)
289/32:
# Training and testing on selected features.

nb_model1 = GaussianNB()
nb_model1.fit(imp_x_train,imp_y_train)
nb_model1_prediction = nb_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, nb_model1_prediction)*100)
289/33:
confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
289/34:
# Training and testing on all features.

svm_model = SVC()
svm_model.fit(all_x_train,all_y_train)
svm_model_prediction = svm_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, svm_model_prediction)*100)
289/35:
# Training and testing on selected features.

svm_model1 = SVC()
svm_model1.fit(imp_x_train,imp_y_train)
svm_model1_prediction = svm_model1.predict(imp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test, svm_model1_prediction)*100)
289/36:
confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
289/37:
# save model
pickle.dump(rf_model,open('model.pkl','wb'))

# Load Model
heart_disease_prediction_model = pickle.load(open('model.pkl','rb'))

y_predict = heart_disease_prediction_model.predict(all_x_test)

print("confusion matrix: \n", confusion_matrix(all_y_test,y_predict))
accuracy_score(all_y_test,y_predict)
288/22: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
290/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

import pickle

from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
290/2: warnings.filterwarnings("ignore")
290/3:
data = pd.read_csv("heart_disease.csv")
data
290/4: data.head()
290/5: data.info()
290/6: data.describe()
290/7: data.columns
290/8: data.info()
290/9: data.isnull().sum()
290/10: data.corr()
290/11:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
290/12:
plt.figure(figsize=(15,8))
sns.heatmap(data)
290/13: #sns.pairplot(data, hue="target")
290/14:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
290/15:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
290/16: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
290/17:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(y)
290/18:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
290/19:
plt.figure(figsize=(15,8))
sns.heatmap(all_x)
290/20:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
290/21:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
#imp_y_train
290/22:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction)*100)
print("R-square on all features: ", r2_score(all_y_test,lg_model_prediction))
290/23:
# Training and testing on selected features.

lg_model1 = LogisticRegression()
lg_model1.fit(imp_x_train,imp_y_train)
lg_model1_prediction = lg_model1.predict(imp_x_test)
print("Accuracy on Selected features: ", accuracy_score(imp_y_test,lg_model1_prediction)*100)
print("Accuracy on Selected features: ", r2_score(imp_y_test,lg_model1_prediction))
290/24:
confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
290/25:
# Training and testing on all features.

dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction)*100)
print("Accuracy on all features: ", r2_score(all_y_test,dt_model_prediction))
290/26:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test,dt_model1_prediction)*100)
print("Accuracy on selected features: ", r2_score(imp_y_test,dt_model1_prediction))
290/27:
confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')
plt.show()
290/28:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction)*100)
290/29:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction)*100)
290/30:
confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')
plt.show()
290/31:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction)*100)
290/32:
# Training and testing on selected features.

nb_model1 = GaussianNB()
nb_model1.fit(imp_x_train,imp_y_train)
nb_model1_prediction = nb_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, nb_model1_prediction)*100)
290/33:
confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
290/34:
# Training and testing on all features.

svm_model = SVC()
svm_model.fit(all_x_train,all_y_train)
svm_model_prediction = svm_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, svm_model_prediction)*100)
290/35:
# Training and testing on selected features.

svm_model1 = SVC()
svm_model1.fit(imp_x_train,imp_y_train)
svm_model1_prediction = svm_model1.predict(imp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test, svm_model1_prediction)*100)
290/36:
confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
290/37:
# save model
pickle.dump(rf_model,open('model.pkl','wb'))

# Load Model
heart_disease_prediction_model = pickle.load(open('model.pkl','rb'))

y_predict = heart_disease_prediction_model.predict(all_x_test)

print("confusion matrix: \n", confusion_matrix(all_y_test,y_predict))
accuracy_score(all_y_test,y_predict)
291/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

import pickle

from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
291/2: warnings.filterwarnings("ignore")
291/3:
data = pd.read_csv("heart_disease.csv")
data
291/4: data.head()
291/5: data.info()
291/6: data.describe()
291/7: data.columns
291/8: data.info()
291/9: data.isnull().sum()
291/10: data.corr()
291/11:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
291/12:
sns.kdeplot(data[data['target']=='Unwell']['chol'],shade=True,color="orange", label="Unwell", alpha=.7)
sns.kdeplot(data[data['target']=='Healthy']['chol'],shade=True,color="dodgerblue", label="Healthy", alpha=.7)
plt.title('Cholesterol in mg/d for both case')
plt.show()
291/13:
plt.figure(figsize=(15,8))
sns.heatmap(data)
291/14: #sns.pairplot(data, hue="target")
291/15:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
291/16:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
291/17: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
291/18:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(y)
291/19:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
291/20:
plt.figure(figsize=(15,8))
sns.heatmap(all_x)
291/21:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
291/22:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
#imp_y_train
291/23:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction)*100)
print("R-square on all features: ", r2_score(all_y_test,lg_model_prediction))
291/24:
# Training and testing on selected features.

lg_model1 = LogisticRegression()
lg_model1.fit(imp_x_train,imp_y_train)
lg_model1_prediction = lg_model1.predict(imp_x_test)
print("Accuracy on Selected features: ", accuracy_score(imp_y_test,lg_model1_prediction)*100)
print("Accuracy on Selected features: ", r2_score(imp_y_test,lg_model1_prediction))
291/25:
confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
291/26:
# Training and testing on all features.

dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction)*100)
print("Accuracy on all features: ", r2_score(all_y_test,dt_model_prediction))
291/27:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test,dt_model1_prediction)*100)
print("Accuracy on selected features: ", r2_score(imp_y_test,dt_model1_prediction))
291/28:
confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')
plt.show()
291/29:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction)*100)
291/30:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction)*100)
291/31:
confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')
plt.show()
291/32:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction)*100)
291/33:
# Training and testing on selected features.

nb_model1 = GaussianNB()
nb_model1.fit(imp_x_train,imp_y_train)
nb_model1_prediction = nb_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, nb_model1_prediction)*100)
291/34:
confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
291/35:
# Training and testing on all features.

svm_model = SVC()
svm_model.fit(all_x_train,all_y_train)
svm_model_prediction = svm_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, svm_model_prediction)*100)
291/36:
# Training and testing on selected features.

svm_model1 = SVC()
svm_model1.fit(imp_x_train,imp_y_train)
svm_model1_prediction = svm_model1.predict(imp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test, svm_model1_prediction)*100)
291/37:
confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
291/38:
# save model
pickle.dump(rf_model,open('model.pkl','wb'))

# Load Model
heart_disease_prediction_model = pickle.load(open('model.pkl','rb'))

y_predict = heart_disease_prediction_model.predict(all_x_test)

print("confusion matrix: \n", confusion_matrix(all_y_test,y_predict))
accuracy_score(all_y_test,y_predict)
292/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

import pickle

from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
292/2: warnings.filterwarnings("ignore")
292/3:
data = pd.read_csv("heart_disease.csv")
data
292/4: data.head()
292/5: data.info()
292/6: data.describe()
292/7: data.columns
292/8: data.info()
292/9: data.isnull().sum()
292/10: data.corr()
292/11:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
292/12:
sns.kdeplot(data[data['target']==1]['chol'],shade=True,color="orange", label="Unwell", alpha=.7)
sns.kdeplot(data[data['target']==0]['chol'],shade=True,color="dodgerblue", label="Healthy", alpha=.7)
plt.title('Cholesterol in mg/d for both case')
plt.show()
292/13:
plt.figure(figsize=(15,8))
sns.heatmap(data)
292/14: #sns.pairplot(data, hue="target")
292/15:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
292/16:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
292/17: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
292/18:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(y)
292/19:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
292/20:
plt.figure(figsize=(15,8))
sns.heatmap(all_x)
292/21:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
292/22:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
#imp_y_train
292/23:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction)*100)
print("R-square on all features: ", r2_score(all_y_test,lg_model_prediction))
292/24:
# Training and testing on selected features.

lg_model1 = LogisticRegression()
lg_model1.fit(imp_x_train,imp_y_train)
lg_model1_prediction = lg_model1.predict(imp_x_test)
print("Accuracy on Selected features: ", accuracy_score(imp_y_test,lg_model1_prediction)*100)
print("Accuracy on Selected features: ", r2_score(imp_y_test,lg_model1_prediction))
292/25:
confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
292/26:
# Training and testing on all features.

dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction)*100)
print("Accuracy on all features: ", r2_score(all_y_test,dt_model_prediction))
292/27:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test,dt_model1_prediction)*100)
print("Accuracy on selected features: ", r2_score(imp_y_test,dt_model1_prediction))
292/28:
confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')
plt.show()
292/29:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction)*100)
292/30:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction)*100)
292/31:
confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')
plt.show()
292/32:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction)*100)
292/33:
# Training and testing on selected features.

nb_model1 = GaussianNB()
nb_model1.fit(imp_x_train,imp_y_train)
nb_model1_prediction = nb_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, nb_model1_prediction)*100)
292/34:
confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
292/35:
# Training and testing on all features.

svm_model = SVC()
svm_model.fit(all_x_train,all_y_train)
svm_model_prediction = svm_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, svm_model_prediction)*100)
292/36:
# Training and testing on selected features.

svm_model1 = SVC()
svm_model1.fit(imp_x_train,imp_y_train)
svm_model1_prediction = svm_model1.predict(imp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test, svm_model1_prediction)*100)
292/37:
confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
292/38:
# save model
pickle.dump(rf_model,open('model.pkl','wb'))

# Load Model
heart_disease_prediction_model = pickle.load(open('model.pkl','rb'))

y_predict = heart_disease_prediction_model.predict(all_x_test)

print("confusion matrix: \n", confusion_matrix(all_y_test,y_predict))
accuracy_score(all_y_test,y_predict)
288/23: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/24: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/25: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
288/26: runfile('F:/Pythn_prgramming_practice/Machine_Learning/NTCC/app.py', wdir='F:/Pythn_prgramming_practice/Machine_Learning/NTCC')
294/1: import pandas as pd
294/2: df=pd.read_csv('car data.csv')
294/3: df.shape
294/4:
print(df['Seller_Type'].unique())
print(df['Fuel_Type'].unique())
print(df['Transmission'].unique())
print(df['Owner'].unique())
294/5:
##check missing values
df.isnull().sum()
294/6: df.describe()
294/7: final_dataset=df[['Year','Selling_Price','Present_Price','Kms_Driven','Fuel_Type','Seller_Type','Transmission','Owner']]
294/8: final_dataset.head()
294/9: final_dataset['Current Year']=2020
294/10: final_dataset.head()
294/11: final_dataset['no_year']=final_dataset['Current Year']- final_dataset['Year']
294/12: final_dataset.head()
294/13: final_dataset.drop(['Year'],axis=1,inplace=True)
294/14: final_dataset.head()
294/15: final_dataset=pd.get_dummies(final_dataset,drop_first=True)
294/16: final_dataset.head()
294/17: final_dataset.head()
294/18: final_dataset=final_dataset.drop(['Current Year'],axis=1)
294/19: final_dataset.head()
294/20: final_dataset.corr()
294/21: import seaborn as sns
294/22: sns.pairplot(final_dataset)
294/23:

import seaborn as sns
#get correlations of each features in dataset
corrmat = df.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,20))
#plot heat map
g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap="RdYlGn")
294/24:
### Feature Importance

from sklearn.ensemble import ExtraTreesRegressor
import matplotlib.pyplot as plt
model = ExtraTreesRegressor()
model.fit(X,y)
294/25:

import seaborn as sns
#get correlations of each features in dataset
corrmat = df.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,20))
#plot heat map
g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap="RdYlGn")
294/26:
X=final_dataset.iloc[:,1:]
y=final_dataset.iloc[:,0]
294/27: X['Owner'].unique()
294/28: X.head()
294/29: y.head()
294/30:
### Feature Importance

from sklearn.ensemble import ExtraTreesRegressor
import matplotlib.pyplot as plt
model = ExtraTreesRegressor()
model.fit(X,y)
294/31: print(model.feature_importances_)
294/32:
#plot graph of feature importances for better visualization
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(5).plot(kind='barh')
plt.show()
294/33:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
294/34: from sklearn.ensemble import RandomForestRegressor
294/35: regressor=RandomForestRegressor()
294/36:
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
print(n_estimators)
294/37:
from sklearn.model_selection import train_test_split
import numpy as np
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
294/38: from sklearn.ensemble import RandomForestRegressor
294/39: regressor=RandomForestRegressor()
294/40:
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
print(n_estimators)
294/41: from sklearn.model_selection import RandomizedSearchCV
294/42:
 #Randomized Search CV

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
294/43:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
294/44:
# Use the random grid to search for best hyperparameters
# First create the base model to tune
rf = RandomForestRegressor()
294/45:
# Random search of parameters, using 3 fold cross validation, 
# search across 100 different combinations
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
294/46: rf_random.fit(X_train,y_train)
294/47: rf_random.best_params_
294/48: rf_random.best_score_
294/49: predictions=rf_random.predict(X_test)
294/50: sns.distplot(y_test-predictions)
294/51: plt.scatter(y_test,predictions)
294/52: from sklearn import metrics
294/53:
print('MAE:', metrics.mean_absolute_error(y_test, predictions))
print('MSE:', metrics.mean_squared_error(y_test, predictions))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))
297/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
297/2: warnings.filterwarnings("ignore")
297/3: data = pd.read_csv("diabetes.csv")
297/4: data.head()
297/5:
print(data.columns)
data.shape
297/6: data.isnull().sum()
297/7: data.info()
297/8: data.describe()
297/9:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
297/10:
plt.figure(figsize=(15,8))
sns.heatmap(data)
297/11: sns.pairplot(data, hue="Outcome")
297/12: data.corr()
297/13:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
297/14:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
297/15:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
297/16:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
x_data
297/17:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
297/18:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
297/19:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
297/20:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
297/21:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
297/22:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
297/23:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
297/24:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
297/25:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
297/26:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
297/27:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
297/28:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
298/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
298/2: warnings.filterwarnings("ignore")
298/3: data = pd.read_csv("diabetes.csv")
298/4: data.head()
298/5:
print(data.columns)
data.shape
298/6: data.isnull().sum()
298/7: data.info()
298/8: data.describe()
298/9:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
298/10:
plt.figure(figsize=(15,8))
sns.heatmap(data)
298/11: sns.pairplot(data, hue="Outcome")
298/12: data.corr()
298/13:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
298/14:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
298/15:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
298/16:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
x_data
298/17:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
298/18:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
298/19:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
298/20:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
298/21:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
298/22:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
298/23:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
298/24:
 #Randomized Search CV

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
298/25:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
298/26:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
298/27:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
298/28:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
298/29: rf_random.best_params_
299/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
299/2: warnings.filterwarnings("ignore")
299/3: data = pd.read_csv("diabetes.csv")
299/4: data.head()
299/5:
print(data.columns)
data.shape
299/6: data.isnull().sum()
299/7: data.info()
299/8: data.describe()
299/9:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
299/10:
plt.figure(figsize=(15,8))
sns.heatmap(data)
299/11: sns.pairplot(data, hue="Outcome")
300/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
300/2: warnings.filterwarnings("ignore")
300/3: data = pd.read_csv("diabetes.csv")
300/4: data.head()
300/5:
print(data.columns)
data.shape
300/6: data.isnull().sum()
300/7: data.info()
300/8: data.describe()
300/9:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
300/10:
plt.figure(figsize=(15,8))
sns.heatmap(data)
300/11: #sns.pairplot(data, hue="Outcome")
300/12: data.corr()
300/13:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
300/14:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
300/15:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
300/16:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
x_data
300/17:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
300/18:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
300/19:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
300/20:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
300/21:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
300/22:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
300/23:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
300/24:
 #Randomized Search CV

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
300/25:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
300/26:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
300/27: rf_random.fit(X_train,y_train)
300/28: rf_random.fit(x_train,y_train)
300/29: rf_random.best_params_
300/30: predictions=rf_random.predict(X_test)
300/31: predictions=rf_random.predict(x_test)
300/32: accuracy_score(y_test,predictions)
300/33:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
300/34:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
301/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
301/2: warnings.filterwarnings("ignore")
301/3: data = pd.read_csv("diabetes.csv")
301/4: data.head()
301/5:
print(data.columns)
data.shape
301/6: data.isnull().sum()
301/7: data.info()
301/8: data.describe()
301/9:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
301/10:
plt.figure(figsize=(15,8))
sns.heatmap(data)
301/11: #sns.pairplot(data, hue="Outcome")
301/12: data.corr()
301/13:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
301/14:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
301/15:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
301/16:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
x_data
301/17:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
301/18:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
301/19:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
301/20:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
301/21:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
301/22:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
301/23:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
301/24:
 #Randomized Search CV

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
301/25:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
301/26:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
301/27: rf_random.fit(std_x_train,y_train)
301/28: rf_random.best_params_
301/29: predictions=rf_random.predict(std_x_test)
301/30: accuracy_score(y_test,predictions)
301/31:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
301/32:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
301/33:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
301/34:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
304/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
304/2: warnings.filterwarnings("ignore")
304/3: data = pd.read_csv("diabetes.csv")
304/4: data.head()
304/5:
print(data.columns)
data.shape
304/6: data.isnull().sum()
304/7: data.info()
304/8: data.describe()
304/9:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
304/10:
plt.figure(figsize=(15,8))
sns.heatmap(data)
304/11: #sns.pairplot(data, hue="Outcome")
304/12: data.corr()
304/13:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
304/14:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
304/15:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
304/16:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
x_data
304/17:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
304/18:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
304/19:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
304/20:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
304/21:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
304/22:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
304/23:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
304/24:
 #Randomized Search CV

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
304/25:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
304/26:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
304/27: rf_random.fit(std_x_train,y_train)
304/28: rf_random.best_params_
304/29: predictions=rf_random.predict(std_x_test)
304/30: accuracy_score(y_test,predictions)
304/31:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
304/32:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
304/33:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
304/34:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
305/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
305/2: warnings.filterwarnings("ignore")
305/3: data = pd.read_csv("diabetes.csv")
305/4: data.head()
305/5:
print(data.columns)
data.shape
305/6:
data.isnull().sum()
a = data["Insulin"].count(0)
a
305/7:
data.isnull().sum()
a = data.count(data['Insulin']==0)
a
305/8:
data.isnull().sum()
a = data["Insulin"].count(data['Insulin']==0)
a
305/9:
data.isnull().sum()
data.["Insulin"].count(axis=0)
a
305/10:
data.isnull().sum()
data["Insulin"].count(axis=0)
a
305/11:
data.isnull().sum()
data["Insulin"].count(axis=0)
305/12:
data.isnull().sum()
data["Insulin"].count(axis=1)
305/13:
data.isnull().sum()
data.count(axis=1)
305/14:
data.isnull().sum()
a = len(data.loc[data['Insulin']==o])
305/15:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
305/16: data['Insulin'].replace(0, np.nan, inplace=True)
305/17: data.insull.sum()
305/18: data.isnull.sum()
305/19: data.isnull().sum()
305/20: data.info()
305/21: data.head()
305/22: data["Insulin"].fillna(values=data["Insulin"].mean(), inplace=True)
305/23: data["Insulin"].fillna(value=data["Insulin"].mean(), inplace=True)
305/24: data.info()
305/25: data.head()
305/26: data["Insulin"].fillna(value=data["Insulin"].median(), inplace=True)
305/27: data.info()
305/28: data.head()
306/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
306/2: warnings.filterwarnings("ignore")
306/3: data = pd.read_csv("diabetes.csv")
306/4: data.head()
306/5:
print(data.columns)
data.shape
306/6:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
306/7: data['Insulin'].replace(0, np.nan, inplace=True)
306/8: data.isnull().sum()
306/9: data.head()
306/10: data["Insulin"].fillna(value=data["Insulin"].median(), inplace=True)
306/11: data.info()
306/12: data.head()
306/13: data.describe()
306/14:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
306/15:
plt.figure(figsize=(15,8))
sns.heatmap(data)
306/16: #sns.pairplot(data, hue="Outcome")
306/17: data.corr()
306/18:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
306/19:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
306/20:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
306/21:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
x_data
306/22:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
306/23:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
306/24:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
306/25:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
306/26:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
306/27:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
306/28:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
306/29:
 #Randomized Search CV

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
306/30:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
306/31:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
306/32: rf_random.fit(std_x_train,y_train)
306/33: rf_random.best_params_
306/34: predictions=rf_random.predict(std_x_test)
306/35: accuracy_score(y_test,predictions)
306/36:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
306/37:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
306/38:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
306/39:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
306/40:
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
max_depth
306/41:
max_depth = [int(x) for x in np.linspace(5, 30, num = 10)]
max_depth
306/42:
max_depth = [int(x) for x in np.linspace(5, 30, num = 11)]
max_depth
308/1: import pandas as pd
308/2: import pandas as pd
308/3: import pandas as pd
308/4: import jsldhashkjdfasgjk
308/5:
import pandas as pd
import numpy as np
308/6:
import pandas as pd
import numpy as np
308/7:
import pandas as pd
import numpy as np
308/8:
# .csv -> comma seperated values

titatic = pd.read_csv("full.csv")
308/9: titanic
308/10:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")
308/11: titanic
308/12: titanic.head()
308/13: titanic.head(10)
308/14: titanic.head()
308/15: titanic.head(10)
308/16: titanic.head()
308/17: titanic.tail()
308/18: titanic.shape
308/19: titanic.column
308/20: titanic.columns
308/21: data.head()
308/22: titanic.head()
308/23: titanic.info()
308/24: titanic.describe
308/25: titanic.describe()
308/26: titanic.isnull().sum()
308/27: titanic.isnull().any()
308/28: titanic.isnull()
308/29: titanic.isnull()
308/30: titanic.drop(titanic.isnull(),axis=0,inplace=True)
308/31: titanic.drop(titanic["Survived"].isnull(),axis=0,inplace=True)
308/32: titanic.drop(titanic["Survived"].isnull()==True,axis=0,inplace=True)
308/33: titanic.drop(labels=titanic["Survived"].isnull()==True,axis=0,inplace=True)
308/34: titanic.drop(labels=(titanic["Survived"].isnull()==True),axis=0,inplace=True)
308/35: titanic.dropna((titanic["Survived"].isnull()==True),axis=0,inplace=True)
308/36: titanic.dropna(titanic["Survived"].isnull()==True,axis=0,inplace=True)
308/37: titanic.dropna(titanic["Survived"].isnull(),axis=0,inplace=True)
308/38: titanic.dropna("Survived",axis=0,inplace=True)
308/39: titanic.dropna("Survived",inplace=True)
308/40: titanic.dropna(["Survived"],inplace=True)
308/41: titanic.dropna(subset=["Survived"],inplace=True)
308/42: titanic.isnull().sum()
308/43: titanic.info()
309/1:
import pandas as pd
import numpy as np
309/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")
309/3: titanic.head()
309/4: titanic.tail()
309/5: titanic.shape      # to know the shape of the data
309/6: titanic.columns
309/7: titanic.info()
309/8: titanic.describe()
309/9: titanic.isnull().sum()
309/10: titanic.isnull()
310/1:
import pandas as pd
import numpy as np
310/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")
310/3: titanic.head()
310/4: titanic.tail()
310/5: titanic.shape      # to know the shape of the data
310/6: titanic.columns
310/7: titanic.info()
310/8: titanic.describe()
310/9: titanic.isnull().sum()
310/10: titanic.isnull()
310/11: titanic.dropna(subset=["Survived"],inplace=True)
310/12: data.isnull()
310/13: titanic.isnull().sum()
310/14: import seaborn as sns
311/1:
import pandas as pd
import numpy as np
311/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")
311/3: titanic.head()
311/4: titanic.tail()
311/5: titanic.shape      # to know the shape of the data
311/6: titanic.columns
311/7: titanic.info()
311/8: titanic.describe()
311/9: titanic.isnull().sum()
311/10: titanic.isnull()
313/1:
import pandas as pd
import numpy as np
313/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")
313/3: titanic.head()
313/4: titanic.tail()
313/5: titanic.shape      # to know the shape of the data
313/6: titanic.columns
313/7: titanic.info()
313/8: titanic.describe()
313/9: titanic.isnull().sum()
313/10: titanic.isnull()
313/11: titanic.drop(labels=(titanic["Survived"].isnull()==True),axis=0,inplace=True)
313/12: titanic.dropna(subset=["Survived"],inplace=true)
313/13: titanic.dropna(subset=["Survived"],inplace=True)
313/14: titanic.shape
313/15: demo = titanic["Age"]
313/16: demo
313/17: demo.mean()
313/18: demo.isnull().mean()
313/19: demo.median()
313/20: demo.fillna(demo.median(),inplace=True)
313/21: titanic["Age"].std()
313/22: demo.std()
313/23: titanic.isnull().sum()
313/24: titanic.isnull().sum()
314/1:
import pandas as pd
import numpy as np
314/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")
314/3: titanic.head()
314/4: titanic.tail()
314/5: titanic.shape      # to know the shape of the data
314/6: titanic.columns
314/7: titanic.info()
314/8: titanic.describe()
314/9: titanic.isnull().sum()
314/10: titanic.isnull()
314/11: titanic.dropna(subset=["Survived"],inplace=True)
314/12: titanic.isnull().sum()
314/13: demo = titanic["Age"]
314/14: demo.median()
314/15: demo.fillna(demo.median(),inplace=True)
314/16: titanic["Age"].std()
314/17: demo.std()
314/18: titanic.isnull().sum()
314/19:
demo = titanic["Age"]
demo.isnull().sum()
315/1:
import pandas as pd
import numpy as np
315/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")
315/3: titanic.head()
315/4: titanic.tail()
315/5: titanic.shape      # to know the shape of the data
315/6: titanic.columns
315/7: titanic.info()
315/8: titanic.describe()
315/9: titanic.isnull().sum()
315/10: titanic.isnull()
315/11: titanic.dropna(subset=["Survived"],inplace=True)
315/12: titanic.isnull().sum()
315/13:
demo = titanic["Age"]
demo.isnull().sum()
315/14: demo.median()
315/15: demo.fillna(demo.median(),inplace=True)
315/16: titanic["Age"].std()
315/17: demo.std()
315/18: titanic.isnull().sum()
316/1:
import pandas as pd
import numpy as np
316/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")
316/3: titanic.head()
316/4: titanic.tail()
316/5: titanic.shape      # to know the shape of the data
316/6: titanic.columns
316/7: titanic.info()
316/8: titanic.describe()
316/9: titanic.isnull().sum()
316/10: titanic.isnull()
316/11: titanic.dropna(subset=["Survived"],inplace=True)
316/12: titanic.isnull().sum()
316/13:
titanic["new_age"] = titanic["Age"]
titanic["new_age"].isnull().sum()
316/14: titanic["new_age"].median()
316/15: titanic["new_age"].fillna(titanic["new_age"].median(),inplace=True)
316/16: titanic["Age"].std()
316/17: titanic["new_age"].std()
316/18: titanic.isnull().sum()
316/19: titanic.isnull()
317/1:
import pandas as pd
import numpy as np
317/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")
317/3: titanic.head()
317/4: titanic.tail()
317/5: titanic.shape      # to know the shape of the data
317/6: titanic.columns
317/7: titanic.info()
317/8: titanic.describe()
317/9: titanic.isnull().sum()
317/10: titanic.isnull()
319/1:
import pandas as pd
import numpy as np
319/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")
319/3: titanic.head()
319/4: titanic.tail()
319/5: titanic.shape      # to know the shape of the data
319/6: titanic.columns
319/7: titanic.info()
319/8: titanic.describe()
319/9: titanic.isnull().sum()
319/10: titanic.isnull()
319/11: titanic.dropna(subse)
319/12: titanic.dropna(subset=["Survived"],inplace=True)
319/13: titanic.shape
319/14: titanic["Age"].isnull().sum()
319/15: titanic["Age"].isnull().index
319/16: titanic[titanic["Age"].isnull()].index
319/17: df['Age'].dropna().sample(df['Age'].isnull().sum(),random_state=0)
319/18: titanic['Age'].dropna().sample(df['Age'].isnull().sum(),random_state=0)
319/19: titanic['Age'].dropna().sample(titanic['Age'].isnull().sum(),random_state=0)
319/20: titanic['Age'].dropna().sample(titanic['Age'].isnull().sum(),random_state=42)
319/21: titanic["Age"].isnull().any()
319/22: titanic["Age"].isnull().sum()
319/23:
def impute_nan(df,variable,median):
    df[variable+"_median"]=df[variable].fillna(median)
    df[variable+"_random"]=df[variable]
    ##It will have the random sample to fill the na
    random_sample=df[variable].dropna().sample(df[variable].isnull().sum(),random_state=0)
    ##pandas need to have same index in order to merge the dataset
    random_sample.index=df[df[variable].isnull()].index
    df.loc[df[variable].isnull(),variable+'_random']=random_sample
319/24:
impute_nan(titanic,"Age",median)
titanic.head()
319/25:
impute_nan(titanic,"Age",'median')
titanic.head()
319/26: titanic['Age'].std
319/27: titanic['Age'].std()
319/28: titanic['Age_random'].std()
320/1:
import pandas as pd
import numpy as np
320/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")
320/3: titanic.head()
320/4: titanic.tail()
320/5: titanic.shape      # to know the shape of the data
320/6: titanic.columns
320/7: titanic.info()
320/8: titanic.describe()
320/9: titanic.isnull().sum()
320/10: titanic.isnull()
320/11: titanic.dropna(subset=["Survived"],inplace=True)
320/12: titanic.shape
320/13: titanic["Age"].isnull().sum()
320/14: titanic[titanic["Age"].isnull()].index
320/15: titanic['Age'].dropna().sample(titanic['Age'].isnull().sum(),random_state=42)
320/16: titanic["Age"].isnull().sum()
320/17:
def impute_nan(df,variable,median):
    df[variable+"_median"]=df[variable].fillna(median)
    df[variable+"_random"]=df[variable]
    ##It will have the random sample to fill the na
    random_sample=df[variable].dropna().sample(df[variable].isnull().sum(),random_state=0)
    ##pandas need to have same index in order to merge the dataset
    random_sample.index=df[df[variable].isnull()].index
    print(random_sample)
    df.loc[df[variable].isnull(),variable+'_random']=random_sample
320/18:
impute_nan(titanic,"Age",'median')
titanic.head()
320/19: titanic['Age'].std()
320/20: titanic['Age_random'].std()
321/1:
import pandas as pd
import numpy as np
321/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")
321/3: titanic.head()
321/4: titanic.tail()
321/5: titanic.shape      # to know the shape of the data
321/6: titanic.columns
321/7: titanic.info()
321/8: titanic.describe()
321/9: titanic.isnull().sum()
321/10: titanic.isnull()
321/11: titanic.dropna(subset=["Survived"],inplace=True)
321/12: titanic.shape
321/13: titanic["Age"].isnull().sum()
321/14: titanic[titanic["Age"].isnull()].index
321/15: titanic['Age'].dropna().sample(titanic['Age'].isnull().sum(),random_state=42)
321/16: titanic["Age"].isnull().sum()
321/17:
def impute_nan(df,variable,median):
    df[variable+"_median"]=df[variable].fillna(median)
    df[variable+"_random"]=df[variable]
    ##It will have the random sample to fill the na
    random_sample=df[variable].dropna().sample(df[variable].isnull().sum(),random_state=0)
    ##pandas need to have same index in order to merge the dataset
    random_sample.index=df[df[variable].isnull()].index
    print(random_sample.index)
    df.loc[df[variable].isnull(),variable+'_random']=random_sample
321/18:
impute_nan(titanic,"Age",'median')
titanic.head()
321/19: titanic['Age'].std()
321/20: titanic['Age_random'].std()
323/1:
import pandas as pd
import numpy as np
323/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")
323/3: titanic.head()
323/4: titanic.tail()
323/5: titanic.shape      # to know the shape of the data
323/6: titanic.columns
323/7: titanic.info()
323/8: titanic.describe()
323/9: titanic.isnull().sum()
323/10: titanic.isnull()
324/1:
import pandas as pd
import numpy as np
324/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")
324/3: titanic.head()
324/4: titanic.tail()
324/5: titanic.shape      # to know the shape of the data
324/6: titanic.columns
324/7: titanic.info()
324/8: titanic.describe()
324/9: titanic.isnull().sum()
324/10: titanic.isnull()
325/1:
import pandas as pd
import numpy as np
326/1:
import pandas as pd
import numpy as np
326/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")
326/3: titanic.head()
326/4: titanic.tail()
326/5: titanic.shape      # to know the shape of the data
326/6: titanic.columns
326/7: titanic.info()
326/8: titanic.describe()
326/9: titanic.isnull().sum()
326/10: titanic.isnull()
326/11:
import pandas as pd
import numpy as np
326/12:
import pandas as pd
import numpy as np
326/13: titanic
326/14: titanic.head()
326/15:
print(type(titanic))
titanic.head()
326/16:
print(type(titanic))
#titanic.head()
326/17:
#print(type(titanic))
titanic.head()
326/18: titanic.isnull().mean()*100
326/19: titanic.isnull().mean()      # to find the missing value percentage in each column.
326/20: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
326/21: 1309/418
326/22: 418/1309
326/23: (418/1309)*100
326/24: titanic.drop("Class",axis=1)
326/25: titanic.head()
326/26: titanic.drop("Class",axis=1,inplace=True)
326/27: titanic.head()
327/1:
import pandas as pd
import numpy as np
327/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
327/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
327/4: titanic.tail()     # tail() returns last five rows of the whole data.
327/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
327/6: titanic.columns     # to know all the labes, features, columns
327/7: titanic.info()
327/8: titanic.describe()
327/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
327/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
327/11: titanic.drop("Class",axis=1,inplace=True)
327/12: titanic.head()
327/13: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
327/14: titanic.head()
327/15: titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True)
327/16: titanic.head()
327/17: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
327/18: titanic.head()
327/19: titanic.isnull().sum()
327/20: titanic.isnull().mean
327/21: titanic.isnull().mean()*100
327/22: titanic.info()
327/23: titanic.dropna(subset=["Survived"],inplace=True)
327/24: titanic.isnull().mean()*100
327/25: titanic.info()
327/26: titanic.shape
327/27: titanic.isnull().sum()
329/1:
import pandas as pd
import numpy as np
329/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
329/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
329/4: titanic.tail()     # tail() returns last five rows of the whole data.
329/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
329/6: titanic.columns     # to know all the labes, features, columns
329/7: titanic.info()
329/8: titanic.describe()
329/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
329/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
329/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
329/12: titanic.head()
329/13: titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True)
329/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
329/15: titanic.isnull().mean()*100
329/16: titanic.info()
329/17: titanic.dropna(subset=["Survived"],inplace=True)
329/18: titanic.isnull().sum()
329/19: titanic.info()
329/20: titanic.shape
329/21: from sklearn.preprocessing import OneHotEncoder
329/22: sex = pd.get_dummies(df["Sex"])
329/23: sex = pd.get_dummies(titanic["Sex"])
329/24:
sex = pd.get_dummies(titanic["Sex"])
sex
329/25:
sex = pd.get_dummies(titanic["Sex"])
sex.head()
329/26:
encoder = OneHotEncoder(sparse=False)
sex1 = (titanic["Sex"])
329/27: sex1.head()
329/28:
encoder = OneHotEncoder(sparse=False)
sex1 = encoder(titanic["Sex"])
329/29:
encoder = OneHotEncoder(sparse=False)
sex1 = encoder.fit_transform(titanic["Sex"])
329/30:
encoder = OneHotEncoder(sparse=False)
titanic_sex = np.array(titanic["Sex"])
sex1 = encoder.fit_transform(titanic["Sex"])
329/31:
encoder = OneHotEncoder(sparse=False)
titanic_sex = np.array(titanic["Sex"])
titanic_sex
#sex1 = encoder.fit_transform(titanic["Sex"])
329/32:
encoder = OneHotEncoder(sparse=False)
titanic_sex = np.array(titanic["Sex"]).shape(-1,1)
titanic_sex
#sex1 = encoder.fit_transform(titanic["Sex"])
329/33:
encoder = OneHotEncoder(sparse=False)
titanic_sex = np.array(titanic["Sex"])
titanic_sex.shape(-1,1)
#sex1 = encoder.fit_transform(titanic["Sex"])
329/34:
encoder = OneHotEncoder(sparse=False)
titanic_sex = np.array(titanic["Sex"]).reshape(-1,1)
titanic_sex
#sex1 = encoder.fit_transform(titanic["Sex"])
329/35:
encoder = OneHotEncoder(sparse=False)
titanic_sex = np.array(titanic["Sex"]).reshape(-1,1)
titanic_sex
sex1 = encoder.fit_transform(titanic["Sex"])
329/36:
encoder = OneHotEncoder(sparse=False)
titanic_sex = np.array(titanic["Sex"]).reshape(-1,1)
titanic_sex
sex1 = encoder.fit_transform(tianic_sex)
329/37:
encoder = OneHotEncoder(sparse=False)
titanic_sex = np.array(titanic["Sex"]).reshape(-1,1)
titanic_sex
sex1 = encoder.fit_transform(titanic_sex)
329/38:
encoder = OneHotEncoder(sparse=False)
titanic_sex = np.array(titanic["Sex"])
titanic_sex
sex1 = encoder.fit_transform(titanic_sex)
329/39:
encoder = OneHotEncoder(sparse=False)
titanic_sex = np.array(titanic["Sex"]).reshape(-1,1)
titanic_sex
sex1 = encoder.fit_transform(titanic_sex)
329/40:
encoder = OneHotEncoder(sparse=False)
titanic_sex = np.array(titanic["Sex"]).reshape(-1,1)
print(titanic_sex.head())
sex1 = encoder.fit_transform(titanic_sex)
329/41:
encoder = OneHotEncoder(sparse=False)
titanic_sex = np.array(titanic["Sex"]).reshape(-1,1)
print(titanic_sex)
sex1 = encoder.fit_transform(titanic_sex)
329/42: sex1
329/43: sex1.head()
329/44: sex1
329/45: pd.concat([titanic,sex,sex1],axis=1)
329/46: sex1 = pd.DataFrame(data=sex1)
329/47: pd.concat([titanic,sex,sex1],axis=1)
329/48: sex1 = pd.DataFrame(data=sex1,columns=['female','male'])
329/49: pd.concat([titanic,sex,sex1],axis=1)
329/50: sex1 = pd.DataFrame(data=sex1,columns=['female1','male1'])
329/51: pd.concat([titanic,sex,sex1],axis=1)
330/1:
import pandas as pd
import numpy as np
330/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
330/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
330/4: titanic.tail()     # tail() returns last five rows of the whole data.
330/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
330/6: titanic.columns     # to know all the labes, features, columns
330/7: titanic.info()
330/8: titanic.describe()
330/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
330/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
330/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
330/12: titanic.head()
330/13: titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True)
330/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
330/15: titanic.isnull().mean()*100
330/16: titanic.info()
330/17: titanic.dropna(subset=["Survived"],inplace=True)
330/18: titanic.isnull().sum()
330/19: titanic.info()
330/20: titanic.shape
330/21: from sklearn.preprocessing import OneHotEncoder
330/22:
sex = pd.get_dummies(titanic["Sex"])
sex.head()
330/23:
encoder = OneHotEncoder(sparse=False)
titanic_sex = np.array(titanic["Sex"]).reshape(-1,1)
print(titanic_sex)
sex1 = encoder.fit_transform(titanic_sex)
330/24: sex1
330/25: sex1 = pd.DataFrame(data=sex1,columns=['female1','male1'])
330/26: pd.concat([titanic,sex,sex1],axis=1)
331/1:
import pandas as pd
import numpy as np
331/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
331/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
331/4: titanic.tail()     # tail() returns last five rows of the whole data.
331/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
331/6: titanic.columns     # to know all the labes, features, columns
331/7: titanic.info()
331/8: titanic.describe()
331/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
331/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
331/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
331/12: titanic.head()
331/13: titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True)
331/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
331/15: titanic.isnull().mean()*100
331/16: titanic.info()
331/17: titanic.dropna(subset=["Survived"],inplace=True)
331/18: titanic.isnull().sum()
331/19: titanic.info()
331/20: titanic.shape
332/1:
## MEan/ Midean Imputation


titanic.head()
333/1:
import pandas as pd
import numpy as np
333/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
333/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
333/4: titanic.tail()     # tail() returns last five rows of the whole data.
333/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
333/6: titanic.columns     # to know all the labes, features, columns
333/7: titanic.info()
333/8: titanic.describe()
333/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
333/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
333/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
333/12: titanic.head()
333/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
333/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
333/15: titanic.isnull().mean()*100
333/16: titanic.info()
333/17: titanic.dropna(subset=["Survived"],inplace=True)
333/18: titanic.isnull().sum()
333/19: titanic.info()
333/20: titanic.shape
333/21:
## MEan/ Midean Imputation


titanic.head()
333/22:
## MEan/ Midean Imputation


titanic["Age"].hist(bins =50)
333/23:
## Mean/ Midean Imputation

titanic["Age_median"] = titanic['Age']
333/24:
## Mean/ Midean Imputation

titanic["Age_median"] = titanic['Age']
titanic["Age_median"].fillna(titanic["Age_median"].median())
333/25:
## Mean/ Midean Imputation

titanic["Age_median"] = titanic['Age']
titanic["Age_median"].fillna(titanic["Age_median"].median())

titani.head()
333/26:
## Mean/ Midean Imputation

titanic["Age_median"] = titanic['Age']
titanic["Age_median"].fillna(titanic["Age_median"].median())

titanic.head()
333/27:
## Mean/ Midean Imputation

titanic["Age_median"] = titanic['Age']
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True)

titanic.head()
333/28: titanic["Age_median"].median()
333/29: titanic.isnull().sum()
333/30: titanic["Age"].std()
333/31: titanic["Age_median"].std()
333/32:

import matplotlip.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend-handle_labels()
ax.legend(lines,labels,loc = "best")
333/33:

import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend-handle_labels()
ax.legend(lines,labels,loc = "best")
333/34:

import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handle_labels()
ax.legend(lines,labels,loc = "best")
333/35:

import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_lagend_handle_labels()
ax.legend(lines,labels,loc = "best")
333/36:

import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(lines,labels,loc = "best")
333/37:

import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
334/1:
import pandas as pd
import numpy as np
334/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
334/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
334/4: titanic.tail()     # tail() returns last five rows of the whole data.
334/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
334/6: titanic.columns     # to know all the labes, features, columns
334/7: titanic.info()
334/8: titanic.describe()
334/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
334/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
334/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
334/12: titanic.head()
334/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
334/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
334/15: titanic.isnull().mean()*100
334/16: titanic.info()
334/17: titanic.dropna(subset=["Survived"],inplace=True)
334/18: titanic.isnull().sum()
334/19: titanic.info()
334/20: titanic.shape
334/21:
## Mean/ Midean Imputation

titanic["Age_median"] = titanic['Age']
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True)

titanic.head()
334/22: titanic["Age"].std()
334/23: titanic["Age_median"].std()
334/24:

import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
334/25: ## Random sample Imputation
334/26: titanic["Age"].median()
334/27: titanic["Age_median"].median()
334/28: titanic["Age"].std()
334/29: titanic["Age_median"].std()
334/30:
## Random sample Imputation

titanic["Age"].dropna().sample()
334/31:
## Random sample Imputation

titanic["Age"].dropna().sample()
334/32:
## Random sample Imputation

titanic["Age"].dropna().sample()
334/33:
## Random sample Imputation

titanic["Age"].dropna().sample()
334/34:
## Random sample Imputation

titanic["Age"].dropna().sample()
334/35:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum())

random_sample
334/36:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum())

random_samples
334/37: titanic["Age_random"].dropna().sample()
334/38:
titanic["Age_random"].dropna().sample()
titanic["Age_random"].isnull().sum()
334/39:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(177)

random_samples
334/40:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(177)

random_samples
334/41:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum())


random_samples
334/42:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum())


random_samples
334/43:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)


random_samples
334/44:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)


random_samples
334/45:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)


random_samples
334/46:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)


random_samples
334/47:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_sample.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),Age_random]=random_sample

random_samples
334/48:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_sample.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),Age_random]=random_samples

random_samples
334/49:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),Age_random]=random_samples

random_samples
334/50:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

random_samples
334/51:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

titanic.head()
334/52:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

titanic.samples
335/1:
import pandas as pd
import numpy as np
335/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
335/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
335/4: titanic.tail()     # tail() returns last five rows of the whole data.
335/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
335/6: titanic.columns     # to know all the labes, features, columns
335/7: titanic.info()
335/8: titanic.describe()
335/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
335/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
335/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
335/12: titanic.head()
335/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
335/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
335/15: titanic.isnull().mean()*100
335/16: titanic.info()
335/17: titanic.dropna(subset=["Survived"],inplace=True)
335/18: titanic.isnull().sum()
335/19: titanic.info()
335/20: titanic.shape
335/21:
## Mean/ Midean Imputation

titanic["Age_median"] = titanic['Age']
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
335/22: titanic["Age"].std()
335/23: titanic["Age_median"].std()
335/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
335/25:
titanic["Age_random"].dropna().sample()
titanic["Age_random"].isnull().sum()
335/26:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
335/27:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

titanic.samples
335/28:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

random_samples
335/29:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

titanic.info()
335/30:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

random_samples.index
336/1:
import pandas as pd
import numpy as np
336/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
336/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
336/4: titanic.tail()     # tail() returns last five rows of the whole data.
336/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
336/6: titanic.columns     # to know all the labes, features, columns
336/7: titanic.info()
336/8: titanic.describe()
336/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
336/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
336/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
336/12: titanic.head()
336/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
336/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
336/15: titanic.isnull().mean()*100
336/16: titanic.info()
336/17: titanic.dropna(subset=["Survived"],inplace=True)
336/18: titanic.isnull().sum()
336/19: titanic.info()
336/20: titanic.shape
336/21:
## Mean/ Midean Imputation

titanic["Age_median"] = titanic['Age']
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
336/22: titanic["Age"].std()
336/23: titanic["Age_median"].std()
336/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
336/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
336/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

random_samples.index
336/27: from sklearn.preprocessing import OneHotEncoder
336/28: sex = np.array(titanic["Sex"])
336/29: sex
336/30: sex = np.array(titanic["Sex"]).reshape(-1,1)
336/31: sex
336/32:
enc = OneHotEncoder()
sex = enc.fit_transform(sex)
sex
336/33:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
337/1:
import pandas as pd
import numpy as np
337/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
337/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
337/4: titanic.tail()     # tail() returns last five rows of the whole data.
337/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
337/6: titanic.columns     # to know all the labes, features, columns
337/7: titanic.info()
337/8: titanic.describe()
337/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
337/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
337/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
337/12: titanic.head()
337/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
337/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
337/15: titanic.isnull().mean()*100
337/16: titanic.info()
337/17: titanic.dropna(subset=["Survived"],inplace=True)
337/18: titanic.isnull().sum()
337/19: titanic.info()
337/20: titanic.shape
337/21:
## Mean/ Midean Imputation

titanic["Age_median"] = titanic['Age']
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
337/22: titanic["Age"].std()
337/23: titanic["Age_median"].std()
337/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
337/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
337/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

random_samples.index
337/27: from sklearn.preprocessing import OneHotEncoder
337/28: sex = np.array(titanic["Sex"]).reshape(-1,1)
337/29:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
337/30:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
sex
337/31:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
titanic.concat([titanic,sex],axis=1)
337/32:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
df = pd.concat([titanic,sex],axis=1)
337/33:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
titanic = pd.concat([titanic,sex],axis=1)
337/34:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
sex = pd.DataFrame(sex)
titanic = pd.concat([titanic,sex],axis=1)
337/35:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
sex = pd.DataFrame(sex)
titanic = pd.concat([titanic,sex],axis=1)
titanic
337/36:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
sex = pd.DataFrame(sex)
print(sex)
titanic = pd.concat([titanic,sex],axis=1)
titanic
338/1:
import pandas as pd
import numpy as np
338/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
338/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
338/4: titanic.tail()     # tail() returns last five rows of the whole data.
338/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
338/6: titanic.columns     # to know all the labes, features, columns
338/7: titanic.info()
338/8: titanic.describe()
338/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
338/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
338/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
338/12: titanic.head()
338/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
338/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
338/15: titanic.isnull().mean()*100
338/16: titanic.info()
338/17: titanic.dropna(subset=["Survived"],inplace=True)
338/18: titanic.isnull().sum()
338/19: titanic.info()
338/20: titanic.shape
338/21:
## Mean/ Midean Imputation

titanic["Age_median"] = titanic['Age']
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
338/22: titanic["Age"].std()
338/23: titanic["Age_median"].std()
338/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
338/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
338/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

random_samples.index
338/27: from sklearn.preprocessing import OneHotEncoder
338/28: sex = np.array(titanic["Sex"]).reshape(-1,1)
338/29:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
338/30:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
sex
338/31: sex = np.array(titanic["Sex"]).reshape(1,-1)
338/32:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
sex
338/33:
sex = np.array(titanic["Sex"]).reshape(1,-1)
sex
338/34:
sex = np.array(titanic["Sex"]).reshape(-1,1)
sex
338/35:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
sex
338/36:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
sex = pd.DataFrame(data=sex, columns=['male','female'])
sex
338/37:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
sex = pd.DataFrame(data=sex, columns=['male','female'])
titanic = pd.concat([titanic,sex],axis=1)
338/38:
sex = nparray(titanic["Sex"]).reshape(-1,1)
sex
338/39:
sex = np.array(titanic["Sex"])
sex
338/40:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
sex = pd.DataFrame(data=sex, columns=['male','female'])
titanic = pd.concat([titanic,sex],axis=1)
338/41:
sex = np.array(titanic["Sex"])
sex.reshape(-1,1)
sex
338/42:
sex = np.array(titanic["Sex"])
sex.reshape(1,-1)
sex
338/43:
sex = np.array(titanic["Sex"])
sex.reshape(1,-1)
sex
338/44:
sex = np.array(titanic["Sex"])
sex = sex.reshape(1,-1)
sex
338/45:
sex = np.array(titanic["Sex"])
sex = sex.reshape(-1,1)
sex
338/46:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
sex = pd.DataFrame(data=sex, columns=['male','female'])
titanic = pd.concat([titanic,sex],axis=1)
338/47: titanic
338/48:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
sex = pd.DataFrame(data=sex, columns=['female','male'])
titanic = pd.concat([titanic,sex],axis=1)
339/1:
import pandas as pd
import numpy as np
339/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
339/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
339/4: titanic.tail()     # tail() returns last five rows of the whole data.
339/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
339/6: titanic.columns     # to know all the labes, features, columns
339/7: titanic.info()
339/8: titanic.describe()
339/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
339/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
339/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
339/12: titanic.head()
339/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
339/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
339/15: titanic.isnull().mean()*100
339/16: titanic.info()
339/17: titanic.dropna(subset=["Survived"],inplace=True)
339/18: titanic.isnull().sum()
339/19: titanic.info()
339/20: titanic.shape
339/21:
## Mean/ Midean Imputation

titanic["Age_median"] = titanic['Age']
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
339/22: titanic["Age"].std()
339/23: titanic["Age_median"].std()
339/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
339/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
339/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

random_samples.index
339/27: from sklearn.preprocessing import OneHotEncoder
339/28:
sex = np.array(titanic["Sex"])
sex = sex.reshape(-1,1)
sex
339/29:
enc = OneHotEncoder(sparse=False)
sex = enc.fit_transform(sex)
sex = pd.DataFrame(data=sex, columns=['female','male'])
titanic = pd.concat([titanic,sex],axis=1)
339/30: titanic
340/1:
import pandas as pd
import numpy as np
340/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
340/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
340/4: titanic.tail()     # tail() returns last five rows of the whole data.
340/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
340/6: titanic.columns     # to know all the labes, features, columns
340/7: titanic.info()
340/8: titanic.describe()
340/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
340/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
340/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
340/12: titanic.head()
340/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
340/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
340/15: titanic.isnull().mean()*100
340/16: titanic.info()
340/17: titanic.dropna(subset=["Survived"],inplace=True)
340/18: titanic.isnull().sum()
340/19: titanic.info()
340/20: titanic.shape
340/21:
## Mean/ Midean Imputation

titanic["Age_median"] = titanic['Age']
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
340/22: titanic["Age"].std()
340/23: titanic["Age_median"].std()
340/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
340/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
340/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

random_samples.index
341/1:
import pandas as pd
import numpy as np
341/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
341/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
341/4: titanic.tail()     # tail() returns last five rows of the whole data.
341/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
341/6: titanic.columns     # to know all the labes, features, columns
341/7: titanic.info()
341/8: titanic.describe()
341/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
341/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
341/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
341/12: titanic.head()
341/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
341/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
341/15: titanic.isnull().mean()*100
341/16: titanic.info()
341/17: titanic.dropna(subset=["Survived"],inplace=True)
341/18: titanic.isnull().sum()
341/19: titanic.info()
341/20: titanic.shape
341/21:
## Mean/ Midean Imputation

titanic["Age_median"] = titanic['Age']
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
341/22: titanic["Age"].std()
341/23: titanic["Age_median"].std()
341/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
341/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
341/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

random_samples.index
341/27:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
341/28: 891-177
341/29:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

titanic.isnull().sum()
341/30:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

#titanic.isnull().sum()
341/31: titanic["Age"].std()
341/32: titanic["Age_random"].std()
341/33: titanic["Age_median"].std()
341/34:
fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
341/35:
import seaborn as sns


sns.heatmap(titanic.isnull(), yticklabels = False)
341/36:
import seaborn as sns

sns.heatmap(titanic.isnull(), yticklabels = False, cmap="viridis")
341/37:
import seaborn as sns

sns.heatmap(titanic.isnull(), yticklabels = False, cmap="viridis", cbar=False)
343/1:
import pandas as pd
import numpy as np
343/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
343/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
343/4: titanic.tail()     # tail() returns last five rows of the whole data.
343/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
343/6: titanic.columns     # to know all the labes, features, columns
343/7: titanic.info()
343/8: titanic.describe()
343/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
343/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
343/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
343/12: titanic.head()
343/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
343/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
343/15: titanic.isnull().mean()*100
343/16: titanic.info()
343/17: titanic.dropna(subset=["Survived"],inplace=True)
343/18: titanic.isnull().sum()
343/19: titanic.info()
343/20: titanic.shape
343/21:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
343/22: titanic["Age"].std()
343/23: titanic["Age_median"].std()
343/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
343/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
343/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

#titanic.isnull().sum()
343/27: 891-177
343/28: titanic["Age"].std()
343/29: titanic["Age_random"].std()
343/30: titanic["Age_median"].std()
343/31:
fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
343/32: titanic.isnull().sum()
343/33: #titanic.isnull().sum()
343/34:
import seaborn as sns

sns.heatmap(titanic.isnull(),yticklabel=False,cbar=False,cmap="viridic")
343/35:
import seaborn as sns

sns.heatmap(titanic.isnull(),yticklabel=False,cbar=False,cmap="viridis")
343/36:
import seaborn as sns

sns.heatmap(titanic.isnull(),yticklabels=False,cbar=False,cmap="viridis")
343/37:
#
titanic.isnull().sum()
343/38: 2/891
343/39:
import seaborn as sns

sns.heatmap(titanic.isnull(),yticklabels=True,cbar=False,cmap="viridis")
343/40:
import seaborn as sns

sns.heatmap(titanic.isnull(),yticklabels=True,cmap="viridis")
343/41:
import seaborn as sns

sns.heatmap(titanic.isnull(),yticklabels='auto',cmap="viridis")
343/42:
import seaborn as sns

sns.heatmap(titanic.isnull(),yticklabels='auto')
343/43:
import seaborn as sns

sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="viridis")
343/44:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="winter")
343/45:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="viridi")
343/46:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
343/47: titanic["Embarked"].unique()
343/48: titanic["Cabin"].unique()
343/49:
#
titanic.isnull().mean()
343/50: #titanic.isnull()
343/51: titanic["Cabin"].unique().count()    # this show the
343/52: titanic["Cabin"].unique().sum()    # this show the
343/53: titanic["Cabin"].value_count()    # this show the
343/54: titanic["Cabin"].values_count()    # this show the
343/55: titanic["Cabin"].value_counts()    # this show the
343/56: titanic["Cabin"].unique()    # this show the
343/57:
titanic["Age"]=titanic["Age_random"]
titani.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
343/58:
titanic["Age"]=titanic["Age_random"]
titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
343/59:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
343/60:
# Number of person survived and number of persion dies

sns.coumtplot(x='Survived',data=titanic)
343/61:
# Number of person survived and number of persion dies

sns.countplot(x='Survived',data=titanic)
343/62:
# Number of person survived and number of persion dies
print(titanic['Survived'].unique())
sns.countplot(x='Survived',data=titanic)
343/63:
# Number of person survived and number of persion dies
print(titanic['Survived'].value_counts())
sns.countplot(x='Survived',data=titanic)
343/64: sns.countplot(x='Survived', hue='Sex', data=titanic)
343/65:
print(titanic.value_conts())

sns.countplot(x='Survived', hue='Sex', data=titanic)
343/66:
print(titanic["Sex"].value_counts())

sns.countplot(x='Survived', hue='Sex', data=titanic)
343/67: titanic.isnull()
343/68: titanic.isnull().sum()
343/69:
# Imputing the Embarked column.

titanic["Embarked"].unique()
343/70: titanic["Embarked"].mode()
343/71: titanic["Embarked"].mode()[0]
343/72:
# Imputing the Embarked column.

# imputing Catagorical values
#  -> Frequency Imputation/Mode imputation.

titanic["Embarked"].unique()
titanic["Embarked"].fillna(titanic["Embarked"].mode()[0], inplace=True)
343/73: titanic["Embarked"].mode()[0]
343/74: titanic["Embarked"]unique()
343/75: titanic["Embarked"].unique()
343/76: sns.countplot(x='Survived', hue='Embarked', data=titanic)
344/1:
import pandas as pd
import numpy as np
344/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
344/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
344/4: titanic.tail()     # tail() returns last five rows of the whole data.
344/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
344/6: titanic.columns     # to know all the labes, features, columns
344/7: titanic.info()
344/8: titanic.describe()
344/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
344/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
344/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
344/12:
titanic.head()
titanic["Borded"].unique()
344/13:
titanic.head()
titanic["Boarded"].unique()
345/1:
import pandas as pd
import numpy as np
345/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
345/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
345/4: titanic.tail()     # tail() returns last five rows of the whole data.
345/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
345/6: titanic.columns     # to know all the labes, features, columns
345/7: titanic.info()
345/8: titanic.describe()
345/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
345/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
345/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
345/12:
titanic.head()
titanic["Boarded"].unique()
345/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
345/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
345/15: titanic.isnull().mean()*100
345/16: titanic.info()
345/17: titanic.dropna(subset=["Survived"],inplace=True)
345/18: titanic.isnull().sum()
345/19: titanic.info()
345/20: titanic.shape
345/21:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
345/22: titanic["Age"].std()
345/23: titanic["Age_median"].std()
345/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
345/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
345/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

#titanic.isnull().sum()
345/27: 891-177
345/28: titanic["Age"].std()
345/29: titanic["Age_random"].std()
345/30: titanic["Age_median"].std()
345/31:
fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
345/32: titanic.isnull().sum()
345/33:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
345/34: 2/891
345/35: titanic["Cabin"].unique()    # this show the unique values in a particular column.
345/36:
titanic["Age"]=titanic["Age_random"]
titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
345/37:
# Imputing the Embarked column.

# imputing Catagorical values
#  -> Frequency Imputation/Mode imputation.

titanic["Embarked"].unique()
titanic["Embarked"].fillna(titanic["Embarked"].mode()[0], inplace=True)
345/38: titanic["Embarked"].unique()
345/39:
# Number of person survived and number of person dinot survived.
print(titanic['Survived'].value_counts())
sns.countplot(x='Survived',data=titanic)
345/40: sns.countplot(x='Survived', hue='Sex', data=titanic)
345/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)
345/42: titanic["Pclass"].unique()
345/43: sns.contplot(x='Survived',hue="Pclass", data = titanic)
345/44: sns.countplot(x='Survived',hue="Pclass", data = titanic)
345/45:
plt.figure(figsize=(12,10))
sns.boxplot(x='Pclass',y='Age',data=titanic)
345/46:
plt.figure(figsize=(10,10))
sns.boxplot(x='Pclass',y='Age',data=titanic)
345/47:
plt.figure(figsize=(5,10))
sns.boxplot(x='Pclass',y='Age',data=titanic)
345/48:
plt.figure(figsize=(12,8))
sns.boxplot(x='Pclass',y='Age',data=titanic)
345/49:
##plt.figure(figsize=(12,8))
##sns.boxplot(x='Pclass',y='Age',data=titanic)
345/50: titanic.head()
346/1:
import pandas as pd
import numpy as np
346/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                      # overhere titanic is basically a pandas dataframe
346/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
346/4: titanic.tail()     # tail() returns last five rows of the whole data.
346/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
346/6: titanic.columns     # to know all the labes, features, columns
346/7: titanic.info()
346/8: titanic.describe()
346/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
346/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
346/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
346/12:
titanic.head()
titanic["Boarded"].unique()
346/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
346/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
346/15: titanic.isnull().mean()*100
346/16: titanic.info()
346/17: titanic.dropna(subset=["Survived"],inplace=True)
346/18: titanic.isnull().sum()
346/19: titanic.info()
346/20: titanic.shape
346/21:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
346/22: titanic["Age"].std()
346/23: titanic["Age_median"].std()
346/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
346/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
346/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

#titanic.isnull().sum()
346/27: 891-177
346/28: titanic["Age"].std()
346/29: titanic["Age_random"].std()
346/30: titanic["Age_median"].std()
346/31:
fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
346/32: titanic.isnull().sum()
346/33:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
346/34: 2/891
346/35: titanic["Cabin"].unique()    # this show the unique values in a particular column.
346/36:
titanic["Age"]=titanic["Age_random"]
titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
346/37:
# Imputing the Embarked column.

# imputing Catagorical values
#  -> Frequency Imputation/Mode imputation.

titanic["Embarked"].unique()
titanic["Embarked"].fillna(titanic["Embarked"].mode()[0], inplace=True)
346/38: titanic["Embarked"].unique()
346/39:
# Number of person survived and number of person dinot survived.
print(titanic['Survived'].value_counts())
sns.countplot(x='Survived',data=titanic)
346/40: sns.countplot(x='Survived', hue='Sex', data=titanic)
346/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)
346/42: titanic["Pclass"].unique()
346/43: sns.countplot(x='Survived',hue="Pclass", data = titanic)
346/44:
##plt.figure(figsize=(12,8))
##sns.boxplot(x='Pclass',y='Age',data=titanic)
346/45: titanic.head()
346/46: titanic["Ag"].std()
346/47: titanic["Age"].std()
346/48:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
type(titanic)                                  # overhere titanic is basically a pandas dataframe
346/49:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                 # overhere titanic is basically a pandas dataframe
346/50:
import pandas as pd
import numpy as np
346/51:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
346/52:
#print(type(titanic))
titanic.head(10)        # head() returns first first five rows of the whole data.
346/53:
#print(type(titanic))
titanic        # head() returns first first five rows of the whole data.
346/54:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
346/55: titanic     # tail() returns last five rows of the whole data.
346/56: titanic.tail()     # tail() returns last five rows of the whole data.
346/57: type(titanic)
346/58: titanic.info()
346/59: titanic.info()
346/60: titanic.describe()
347/1:
import pandas as pd
import numpy as np
347/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                 # overhere titanic is basically a pandas dataframe
347/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
347/4: titanic.tail()     # tail() returns last five rows of the whole data.
347/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
347/6: titanic.columns     # to know all the labes, features, columns
347/7: titanic.info()
347/8: titanic.describe()
347/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
347/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
347/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
347/12:
titanic.head()
titanic["Boarded"].unique()
347/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
347/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
347/15: titanic.isnull().mean()*100
347/16: titanic.info()
347/17: titanic.dropna(subset=["Survived"],inplace=True)
347/18: titanic.isnull().sum()
347/19: titanic.info()
347/20: titanic.shape
347/21:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
347/22: titanic["Age"].std()
347/23: titanic["Age_median"].std()
347/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
347/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
347/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

#titanic.isnull().sum()
347/27: 891-177
347/28: titanic["Age"].std()
347/29: titanic["Age_random"].std()
347/30: titanic["Age_median"].std()
347/31:
fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
347/32: titanic.isnull().sum()
347/33:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
347/34: 2/891
347/35: titanic["Cabin"].unique()    # this show the unique values in a particular column.
347/36:
titanic["Age"]=titanic["Age_random"]
titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
347/37:
# Imputing the Embarked column.

# imputing Catagorical values
#  -> Frequency Imputation/Mode imputation.

titanic["Embarked"].unique()
titanic["Embarked"].fillna(titanic["Embarked"].mode()[0], inplace=True)
347/38: titanic["Embarked"].unique()
347/39:
# Number of person survived and number of person dinot survived.
print(titanic['Survived'].value_counts())
sns.countplot(x='Survived',data=titanic)
347/40: sns.countplot(x='Survived', hue='Sex', data=titanic)
347/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)
347/42: titanic["Pclass"].unique()
347/43: sns.countplot(x='Survived',hue="Pclass", data = titanic)
347/44:
##plt.figure(figsize=(12,8))
##sns.boxplot(x='Pclass',y='Age',data=titanic)
347/45: titanic.head()
347/46: titanic.corr()
347/47: sns.heatmap(x=titanic.corr())
347/48: sns.heatmap(x=titanic.corr(), data=titanic)
347/49: sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)
347/50: sns.pairplot(titanic)
347/51: sns.heatmap(titanic.corr(), annot=False, linewidth=0.5)
347/52: sns.heatmap(titanic.corr(), annot=False, linewidth=1)
347/53: #sns.pairplot(titanic)
347/54: sns.heatmap(titanic.corr(), annot=False, linewidth=5)
347/55: sns.heatmap(titanic.corr(), annot=True, linewidth=5)
347/56: sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)
347/57:
#
sns.pairplot(titanic)
347/58: #sns.pairplot(titanic)
347/59: sns.pairplot(titanic)
347/60: sns.pairplot(titanic, hue="Survived")
347/61: sns.pairplot(titanic, hue="Survived", vars=["Pclass"])
347/62: sns.pairplot(titanic, hue="Survived", vars=["Pclass","Age"])
347/63: #sns.pairplot(titanic, hue="Survived", vars=["Pclass","Age"])
347/64: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)
347/65: titanic.head()
347/66: # Handeling Categorical features
347/67:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"])
347/68:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"])
Embarked1
347/69:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"])
Embarked1


titanic = titanic.concat([titanic,Embarked1],axis=1)
347/70:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"])
Embarked1


titanic = titanic.Concat([titanic,Embarked1],axis=1)
347/71:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"])
Embarked1


titanic = pd.concat([titanic,Embarked1],axis=1)
347/72: titanic.head
347/73: titanic.head()
347/74: Embarked1
347/75:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"],drop_first=True)
Embarked1


titanic = pd.concat([titanic,Embarked1],axis=1)
347/76: Embarked1
347/77: titanic.head()
348/1:
import pandas as pd
import numpy as np
348/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                 # overhere titanic is basically a pandas dataframe
348/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
348/4: titanic.tail()     # tail() returns last five rows of the whole data.
348/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
348/6: titanic.columns     # to know all the labes, features, columns
348/7: titanic.info()
348/8: titanic.describe()
348/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
348/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
348/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
348/12:
titanic.head()
titanic["Boarded"].unique()
348/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
348/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
348/15: titanic.isnull().mean()*100
348/16: titanic.info()
348/17: titanic.dropna(subset=["Survived"],inplace=True)
348/18: titanic.isnull().sum()
348/19: titanic.info()
348/20: titanic.shape
348/21:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
348/22: titanic["Age"].std()
348/23: titanic["Age_median"].std()
348/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
348/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
348/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

#titanic.isnull().sum()
348/27: 891-177
348/28: titanic["Age"].std()
348/29: titanic["Age_random"].std()
348/30: titanic["Age_median"].std()
348/31:
fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
348/32: titanic.isnull().sum()
348/33:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
348/34: 2/891
348/35: titanic["Cabin"].unique()    # this show the unique values in a particular column.
348/36:
titanic["Age"]=titanic["Age_random"]
titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
348/37:
# Imputing the Embarked column.

# imputing Catagorical values
#  -> Frequency Imputation/Mode imputation.

titanic["Embarked"].unique()
titanic["Embarked"].fillna(titanic["Embarked"].mode()[0], inplace=True)
348/38: titanic["Embarked"].unique()
348/39:
# Number of person survived and number of person dinot survived.
print(titanic['Survived'].value_counts())
sns.countplot(x='Survived',data=titanic)
348/40: sns.countplot(x='Survived', hue='Sex', data=titanic)
348/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)
348/42: titanic["Pclass"].unique()
348/43: sns.countplot(x='Survived',hue="Pclass", data = titanic)
348/44:
##plt.figure(figsize=(12,8))
##sns.boxplot(x='Pclass',y='Age',data=titanic)
348/45: titanic.corr()
348/46: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)
348/47: #sns.pairplot(titanic, hue="Survived", vars=["Pclass","Age"])
348/48: titanic.head()
348/49:
# Handeling Categorical features

# ---> One hot encoding
#  ---> Ordinal number encoding
#  ---> Label encoder
348/50:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"],drop_first=True)
Embarked1


titanic = pd.concat([titanic,Embarked1],axis=1)
348/51: Embarked1
348/52: titanic.head()
348/53: ()
348/54:
sex1 = pd.get_dummies(titanic["Sex"],drop_first=True)

titanic = pd.concat([titanic,sex1],axis=1)
titanic.head()
348/55:
from sklearn.preprocessing import LabelEncoder

labl_encoder = LabelEncoder()
titanic["Embarked"] = labl_encoder.fit_transform(titanic["Embarked"])
titanic.head()
348/56: titanic.drop(["Sex","Embarked"],axis=1,inplace=true)
348/57: titanic.drop(["Sex","Embarked"],axis=1,inplace=True)
348/58: titanic.head()
348/59:
from sklearn.model_selection import train_test_split

y_values = titanic["Survived"]
x_value = titanic.drop(["Survived"])


print(y_value)
348/60:
from sklearn.model_selection import train_test_split

y_values = titanic["Survived"]
x_value = titanic.drop(["Survived"],axis=1)


print(y_value)
348/61:
from sklearn.model_selection import train_test_split

y_value = titanic["Survived"]
x_value = titanic.drop(["Survived"],axis=1)


print(y_value)
348/62:
from sklearn.model_selection import train_test_split

y_value = titanic["Survived"]
x_value = titanic.drop(["Survived"],axis=1)


print(x_value)
348/63:
from sklearn.model_selection import train_test_split

y_value = titanic["Survived"]
x_value = titanic.drop(["Survived"],axis=1)


x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, randon_state = 42)
348/64:
from sklearn.model_selection import train_test_split

y_value = titanic["Survived"]
x_value = titanic.drop(["Survived"],axis=1)


x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)
348/65:
from sklearn.model_selection import train_test_split

y_value = titanic["Survived"]
x_value = titanic.drop(["Survived"],axis=1)


x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)
x_train()
348/66:
from sklearn.model_selection import train_test_split

y_value = titanic["Survived"]
x_value = titanic.drop(["Survived"],axis=1)


x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)
x_train
348/67: 80/100
348/68: 0.8*891
349/1:
import pandas as pd
import numpy as np
349/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                 # overhere titanic is basically a pandas dataframe
349/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
349/4: titanic.tail()     # tail() returns last five rows of the whole data.
349/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
349/6: titanic.columns     # to know all the labes, features, columns
349/7: titanic.info()
349/8: titanic.describe()
349/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
349/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
349/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
349/12:
titanic.head()
titanic["Boarded"].unique()
349/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
349/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
349/15: titanic.isnull().mean()*100
349/16: titanic.info()
349/17: titanic.dropna(subset=["Survived"],inplace=True)
349/18: titanic.isnull().sum()
349/19: titanic.info()
349/20: titanic.shape
349/21:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
349/22: titanic["Age"].std()
349/23: titanic["Age_median"].std()
349/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
349/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
349/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

#titanic.isnull().sum()
349/27: 891-177
349/28: titanic["Age"].std()
349/29: titanic["Age_random"].std()
349/30: titanic["Age_median"].std()
349/31:
fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
349/32: titanic.isnull().sum()
349/33:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
349/34: 2/891
349/35: titanic["Cabin"].unique()    # this show the unique values in a particular column.
349/36:
titanic["Age"]=titanic["Age_random"]
titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
349/37:
# Imputing the Embarked column.

# imputing Catagorical values
#  -> Frequency Imputation/Mode imputation.

titanic["Embarked"].unique()
titanic["Embarked"].fillna(titanic["Embarked"].mode()[0], inplace=True)
349/38: titanic["Embarked"].unique()
349/39:
# Number of person survived and number of person dinot survived.
print(titanic['Survived'].value_counts())
sns.countplot(x='Survived',data=titanic)
349/40: sns.countplot(x='Survived', hue='Sex', data=titanic)
349/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)
349/42: titanic["Pclass"].unique()
349/43: sns.countplot(x='Survived',hue="Pclass", data = titanic)
349/44:
##plt.figure(figsize=(12,8))
##sns.boxplot(x='Pclass',y='Age',data=titanic)
349/45: titanic.corr()
349/46: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)
349/47: #sns.pairplot(titanic, hue="Survived", vars=["Pclass","Age"])
349/48: titanic.head()
349/49:
# Handeling Categorical features

# ---> One hot encoding
#  ---> Ordinal number encoding
#  ---> Label encoder
349/50:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"],drop_first=True)
Embarked1


titanic = pd.concat([titanic,Embarked1],axis=1)
349/51: Embarked1
349/52: titanic.head()
349/53:
sex1 = pd.get_dummies(titanic["Sex"],drop_first=True)

titanic = pd.concat([titanic,sex1],axis=1)
titanic.head()
349/54: titanic.drop(["Sex","Embarked"],axis=1,inplace=True)
349/55: titanic.head()
349/56:
from sklearn.model_selection import train_test_split

y_value = titanic["Survived"]
x_value = titanic.drop(["Survived"],axis=1)


x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)
350/1:
import pandas as pd
import numpy as np
350/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                 # overhere titanic is basically a pandas dataframe
350/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
350/4: titanic.tail()     # tail() returns last five rows of the whole data.
350/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
350/6: titanic.columns     # to know all the labes, features, columns
350/7: titanic.info()
350/8: titanic.describe()
350/9: titanic.isnull()  # count's the number of missing values or NAN values in each column
350/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
350/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
350/12:
titanic.head()
titanic["Boarded"].unique()
350/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
350/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
350/15: titanic.isnull().mean()*100
350/16: titanic.info()
350/17: titanic.dropna(subset=["Survived"],inplace=True)
350/18: titanic.isnull().sum()
350/19: titanic.info()
350/20: titanic.shape
350/21:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
350/22: titanic["Age"].std()
350/23: titanic["Age_median"].std()
350/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
350/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
350/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

#titanic.isnull().sum()
350/27: 891-177
350/28: titanic["Age"].std()
350/29: titanic["Age_random"].std()
350/30: titanic["Age_median"].std()
350/31:
fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
350/32: titanic.isnull().sum()
350/33:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
350/34: 2/891
350/35: titanic["Cabin"].unique()    # this show the unique values in a particular column.
350/36:
titanic["Age"]=titanic["Age_random"]
titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
350/37:
# Imputing the Embarked column.

# imputing Catagorical values
#  -> Frequency Imputation/Mode imputation.

titanic["Embarked"].unique()
titanic["Embarked"].fillna(titanic["Embarked"].mode()[0], inplace=True)
350/38: titanic["Embarked"].unique()
350/39:
# Number of person survived and number of person dinot survived.
print(titanic['Survived'].value_counts())
sns.countplot(x='Survived',data=titanic)
350/40: sns.countplot(x='Survived', hue='Sex', data=titanic)
350/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)
350/42: titanic["Pclass"].unique()
350/43: sns.countplot(x='Survived',hue="Pclass", data = titanic)
350/44:
##plt.figure(figsize=(12,8))
##sns.boxplot(x='Pclass',y='Age',data=titanic)
350/45: titanic.corr()
350/46: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)
350/47: #sns.pairplot(titanic, hue="Survived", vars=["Pclass","Age"])
350/48: titanic.head()
350/49:
# Handeling Categorical features

# ---> One hot encoding
#  ---> Ordinal number encoding
#  ---> Label encoder
350/50:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"],drop_first=True)
Embarked1


titanic = pd.concat([titanic,Embarked1],axis=1)
350/51: Embarked1
350/52: titanic.head()
350/53:
sex1 = pd.get_dummies(titanic["Sex"],drop_first=True)

titanic = pd.concat([titanic,sex1],axis=1)
titanic.head()
350/54: titanic.drop(["Sex","Embarked"],axis=1,inplace=True)
350/55: titanic.head()
350/56:
from sklearn.model_selection import train_test_split

y_value = titanic["Survived"]
x_value = titanic.drop(["Survived"],axis=1)


x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)
350/57: titanic.describe().sum()
351/1:
import pandas as pd
import numpy as np
351/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                 # overhere titanic is basically a pandas dataframe
351/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
351/4: titanic.tail()     # tail() returns last five rows of the whole data.
351/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
351/6: titanic.columns     # to know all the labes, features, columns
351/7: titanic.info()
351/8: titanic.describe().sum()
351/9: titanic.isnull()  # count's the number of missing values or NAN values in each column
351/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
351/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
351/12:
titanic.head()
titanic["Boarded"].unique()
351/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
351/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
351/15: titanic.isnull().mean()*100
351/16: titanic.info()
351/17: titanic.dropna(subset=["Survived"],inplace=True)
351/18: titanic.isnull().sum()
351/19: titanic.info()
351/20: titanic.shape
351/21:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
351/22: titanic["Age"].std()
351/23: titanic["Age_median"].std()
351/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
351/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
351/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

#titanic.isnull().sum()
351/27: 891-177
351/28: titanic["Age"].std()
351/29: titanic["Age_random"].std()
351/30: titanic["Age_median"].std()
351/31:
fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
351/32: titanic.isnull().sum()
351/33:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
351/34: 2/891
351/35: titanic["Cabin"].unique()    # this show the unique values in a particular column.
351/36:
titanic["Age"]=titanic["Age_random"]
titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
351/37:
# Imputing the Embarked column.

# imputing Catagorical values
#  -> Frequency Imputation/Mode imputation.

titanic["Embarked"].unique()
titanic["Embarked"].fillna(titanic["Embarked"].mode()[0], inplace=True)
351/38: titanic["Embarked"].unique()
351/39:
# Number of person survived and number of person dinot survived.
print(titanic['Survived'].value_counts())
sns.countplot(x='Survived',data=titanic)
351/40: sns.countplot(x='Survived', hue='Sex', data=titanic)
351/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)
351/42: titanic["Pclass"].unique()
351/43: sns.countplot(x='Survived',hue="Pclass", data = titanic)
351/44:
##plt.figure(figsize=(12,8))
##sns.boxplot(x='Pclass',y='Age',data=titanic)
351/45: titanic.corr()
351/46: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)
351/47: #sns.pairplot(titanic, hue="Survived", vars=["Pclass","Age"])
351/48: titanic.head()
351/49:
# Handeling Categorical features

# ---> One hot encoding
#  ---> Ordinal number encoding
#  ---> Label encoder
351/50:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"],drop_first=True)
Embarked1


titanic = pd.concat([titanic,Embarked1],axis=1)
351/51: Embarked1
351/52: titanic.head()
351/53:
sex1 = pd.get_dummies(titanic["Sex"],drop_first=True)

titanic = pd.concat([titanic,sex1],axis=1)
titanic.head()
351/54: titanic.drop(["Sex","Embarked"],axis=1,inplace=True)
351/55: titanic.head()
351/56:
from sklearn.model_selection import train_test_split

y_value = titanic["Survived"]
x_value = titanic.drop(["Survived"],axis=1)


x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)
352/1:
import pandas as pd
import numpy as np
352/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                 # overhere titanic is basically a pandas dataframe
352/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
352/4: titanic.tail()     # tail() returns last five rows of the whole data.
352/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
352/6: titanic.columns     # to know all the labes, features, columns
352/7: titanic.info()
352/8: titanic.describe()
352/9: titanic.isnull().sum()  # count's the number of missing values or NAN values in each column
352/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
352/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
352/12:
titanic.head()
titanic["Boarded"].unique()
352/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
352/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
352/15: titanic.isnull().mean()*100
352/16: titanic.info()
352/17: titanic.dropna(subset=["Survived"],inplace=True)
352/18: titanic.isnull().sum()
352/19: titanic.info()
352/20: titanic.shape
352/21:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
352/22: titanic["Age"].std()
352/23: titanic["Age_median"].std()
352/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
352/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
352/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

#titanic.isnull().sum()
352/27: 891-177
352/28: titanic["Age"].std()
352/29: titanic["Age_random"].std()
352/30: titanic["Age_median"].std()
352/31:
fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
352/32: titanic.isnull().sum()
352/33:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
352/34: 2/891
352/35: titanic["Cabin"].unique()    # this show the unique values in a particular column.
352/36:
titanic["Age"]=titanic["Age_random"]
titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
352/37:
# Imputing the Embarked column.

# imputing Catagorical values
#  -> Frequency Imputation/Mode imputation.

titanic["Embarked"].unique()
titanic["Embarked"].fillna(titanic["Embarked"].mode()[0], inplace=True)
352/38: titanic["Embarked"].unique()
352/39:
# Number of person survived and number of person dinot survived.
print(titanic['Survived'].value_counts())
sns.countplot(x='Survived',data=titanic)
352/40: sns.countplot(x='Survived', hue='Sex', data=titanic)
352/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)
352/42: titanic["Pclass"].unique()
352/43: sns.countplot(x='Survived',hue="Pclass", data = titanic)
352/44:
##plt.figure(figsize=(12,8))
##sns.boxplot(x='Pclass',y='Age',data=titanic)
352/45: titanic.corr()
352/46: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)
352/47: #sns.pairplot(titanic, hue="Survived", vars=["Pclass","Age"])
352/48: titanic.head()
352/49:
# Handeling Categorical features

# ---> One hot encoding
#  ---> Ordinal number encoding
#  ---> Label encoder
352/50:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"],drop_first=True)
Embarked1


titanic = pd.concat([titanic,Embarked1],axis=1)
352/51: Embarked1
352/52: titanic.head()
352/53:
sex1 = pd.get_dummies(titanic["Sex"],drop_first=True)

titanic = pd.concat([titanic,sex1],axis=1)
titanic.head()
352/54: titanic.drop(["Sex","Embarked"],axis=1,inplace=True)
352/55: titanic.head()
352/56:
from sklearn.model_selection import train_test_split

y_value = titanic["Survived"]
x_value = titanic.drop(["Survived"],axis=1)


x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)
353/1:
import pandas as pd
import numpy as np
353/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                 # overhere titanic is basically a pandas dataframe
353/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
353/4: titanic.tail()     # tail() returns last five rows of the whole data.
353/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
353/6: titanic.columns     # to know all the labes, features, columns
353/7: titanic.info()
353/8: titanic.describe()
353/9: titanic["Survived"].isnull().sum()  # count's the number of missing values or NAN values in each column
353/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
353/11: titanic.drop("Class",axis=1,inplace=True)   # to drop any column or you can say to remove any colummn.
353/12:
titanic.head()
titanic["Boarded"].unique()
353/13:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
# to drop multiple column at a time.
353/14: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
353/15: titanic.isnull().mean()*100
353/16: titanic.info()
353/17: titanic.dropna(subset=["Survived"],inplace=True)
353/18: titanic.isnull().sum()
353/19: titanic.info()
353/20: titanic.shape
353/21:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
353/22: titanic["Age"].std()
353/23: titanic["Age_median"].std()
353/24:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
353/25:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
353/26:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

#titanic.isnull().sum()
353/27: 891-177
353/28: titanic["Age"].std()
353/29: titanic["Age_random"].std()
353/30: titanic["Age_median"].std()
353/31:
fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
353/32: titanic.isnull().sum()
353/33:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
353/34: 2/891
353/35: titanic["Cabin"].unique()    # this show the unique values in a particular column.
353/36:
titanic["Age"]=titanic["Age_random"]
titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
353/37:
# Imputing the Embarked column.

# imputing Catagorical values
#  -> Frequency Imputation/Mode imputation.

titanic["Embarked"].unique()
titanic["Embarked"].fillna(titanic["Embarked"].mode()[0], inplace=True)
353/38: titanic["Embarked"].unique()
353/39:
# Number of person survived and number of person dinot survived.
print(titanic['Survived'].value_counts())
sns.countplot(x='Survived',data=titanic)
353/40: sns.countplot(x='Survived', hue='Sex', data=titanic)
353/41: sns.countplot(x='Survived', hue='Embarked', data=titanic)
353/42: titanic["Pclass"].unique()
353/43: sns.countplot(x='Survived',hue="Pclass", data = titanic)
353/44:
##plt.figure(figsize=(12,8))
##sns.boxplot(x='Pclass',y='Age',data=titanic)
353/45: titanic.corr()
353/46: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)
353/47: #sns.pairplot(titanic, hue="Survived", vars=["Pclass","Age"])
353/48: titanic.head()
353/49:
# Handeling Categorical features

# ---> One hot encoding
#  ---> Ordinal number encoding
#  ---> Label encoder
353/50:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"],drop_first=True)
Embarked1


titanic = pd.concat([titanic,Embarked1],axis=1)
353/51: Embarked1
353/52: titanic.head()
353/53:
sex1 = pd.get_dummies(titanic["Sex"],drop_first=True)

titanic = pd.concat([titanic,sex1],axis=1)
titanic.head()
353/54: titanic.drop(["Sex","Embarked"],axis=1,inplace=True)
353/55: titanic.head()
353/56:
from sklearn.model_selection import train_test_split

y_value = titanic["Survived"]
x_value = titanic.drop(["Survived"],axis=1)


x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)
354/1:
import pandas as pd
import numpy as np
354/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                 # overhere titanic is basically a pandas dataframe
354/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
354/4: titanic.tail()     # tail() returns last five rows of the whole data.
354/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
354/6: titanic.columns     # to know all the labes, features, columns
354/7: titanic.info()
354/8: titanic.describe()
354/9: titanic["Survived"].isnull().sum()  # count's the number of missing values or NAN values in each column
354/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
354/11: titanic.drop()   # to drop any column or you can say to remove any colummn.
354/12: titanic.drop("Class", axis=1)   # to drop any column or you can say to remove any colummn.
354/13: titanic.head()
354/14: titanic.drop("Class", axis=1, inplace=True)   # to drop any column or you can say to remove any colummn.
354/15: titanic.head()
354/16: #titanic.head()
354/17:
#titanic.head()
titanic["Boarded"].unique()
354/18:
'''titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
 to drop multiple column at a time.'''
354/19:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
#to drop multiple column at a time.
titanic.head()
354/20: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
354/21: titanic.isnull().mean()*100
354/22: titanic.isnull().sum()
354/23: titanic.dropna(subset=["Survived"],inplace=True)
354/24:
#
titanic.isnull().sum()
354/25: titanic.isnull().sum()
354/26:
#
titanic.info()
354/27:
#
titanic.isnull().mean()*100
355/1:
import pandas as pd
import numpy as np
355/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                 # overhere titanic is basically a pandas dataframe
355/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
355/4: titanic.tail()     # tail() returns last five rows of the whole data.
355/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
355/6: titanic.columns     # to know all the labes, features, columns
355/7: titanic.info()
355/8: titanic.describe()
355/9: titanic["Survived"].isnull().sum()  # count's the number of missing values or NAN values in each column
355/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
355/11: titanic.drop("Class", axis=1, inplace=True)   # to drop any column or you can say to remove any colummn.
355/12: #titanic.head()
355/13:
#titanic.head()
titanic["Boarded"].unique()  # to see all the diffrent type of discrit or categorical values prasent inside a particular column.
355/14:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
#to drop multiple column at a time.
titanic.head()
355/15: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
355/16: titanic.isnull().sum()
355/17: #titanic.info()
355/18: titanic.dropna(subset=["Survived"],inplace=True)
355/19:
#
titanic.isnull().mean()*100
355/20:
#
titanic.info()
355/21: #titanic.shape
355/22:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
355/23: titanic["Age"].std()
355/24: titanic["Age_median"].std()
355/25:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
355/26:
titanic["Age_random"].dropna().sample()
titanic["Age_random"].isnull().sum()
356/1:
import pandas as pd
import numpy as np
356/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                 # overhere titanic is basically a pandas dataframe
356/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
356/4: titanic.tail()     # tail() returns last five rows of the whole data.
356/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
356/6: titanic.columns     # to know all the labes, features, columns
356/7: titanic.info()
356/8: titanic.describe()
356/9: titanic["Survived"].isnull().sum()  # count's the number of missing values or NAN values in each column
356/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
356/11: titanic.drop("Class", axis=1, inplace=True)   # to drop any column or you can say to remove any colummn.
356/12: #titanic.head()
356/13:
#titanic.head()
titanic["Boarded"].unique()  # to see all the diffrent type of discrit or categorical values prasent inside a particular column.
356/14:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
#to drop multiple column at a time.
titanic.head()
356/15: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
356/16: titanic.isnull().sum()
356/17: #titanic.info()
356/18: titanic.dropna(subset=["Survived"],inplace=True)
356/19:
#
titanic.isnull().mean()*100
356/20:
#
titanic.info()
356/21: #titanic.shape
356/22:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
356/23: titanic["Age"].std()
356/24: titanic["Age_median"].std()
356/25:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
356/26:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
356/27:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

titanic.isnull().sum()
356/28: 891-177
356/29: titanic["Age"].std()
356/30: titanic["Age_random"].std()
356/31: titanic["Age_median"].std()
356/32:
fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
356/33: #titanic.isnull().sum()
356/34:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
356/35: 2/891
356/36: titanic["Cabin"].unique()    # this show the unique values in a particular column.
356/37:
titanic["Age"]=titanic["Age_random"]
titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
356/38:
# Imputing the Embarked column.

# imputing Catagorical values
#  -> Frequency Imputation/Mode imputation.

titanic["Embarked"].unique()
titanic["Embarked"].fillna(titanic["Embarked"].mode()[0], inplace=True)
356/39: titanic["Embarked"].unique()
356/40:
# Number of person survived and number of person dinot survived.
print(titanic['Survived'].value_counts())
sns.countplot(x='Survived',data=titanic)
356/41: sns.countplot(x='Survived', hue='Sex', data=titanic)
356/42: sns.countplot(x='Survived', hue='Embarked', data=titanic)
356/43: titanic["Pclass"].unique()
356/44: sns.countplot(x='Survived',hue="Pclass", data = titanic)
356/45:
plt.figure(figsize=(12,8))
sns.boxplot(x='Pclass',y='Age',data=titanic)
356/46: titanic.corr()
356/47: sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)
356/48: sns.pairplot(titanic, hue="Survived", vars=["Pclass","Age"])
356/49: titanic.head()
356/50:
# Handeling Categorical features

# ---> One hot encoding
#  ---> Ordinal number encoding
#  ---> Label encoder
356/51:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"],drop_first=True)
Embarked1


#titanic = pd.concat([titanic,Embarked1],axis=1)
356/52: Embarked1
356/53: titanic.head()
356/54:
sex1 = pd.get_dummies(titanic["Sex"],drop_first=True)

titanic = pd.concat([titanic,sex1],axis=1)
titanic.head()
356/55: titanic.drop(["Sex","Embarked"],axis=1,inplace=True)
356/56: titanic.head()
356/57:
from sklearn.model_selection import train_test_split

y_value = titanic["Survived"]
x_value = titanic.drop(["Survived"],axis=1)


x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)
358/1:
import pandas as pd
import numpy as np
358/2:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                 # overhere titanic is basically a pandas dataframe
358/3:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
358/4: titanic.tail()     # tail() returns last five rows of the whole data.
358/5: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
358/6: titanic.columns     # to know all the labes, features, columns
358/7: titanic.info()
358/8: titanic.describe()
358/9: titanic["Survived"].isnull().sum()  # count's the number of missing values or NAN values in each column
358/10: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
358/11: titanic.drop("Class", axis=1, inplace=True)   # to drop any column or you can say to remove any colummn.
358/12: #titanic.head()
358/13:
#titanic.head()
titanic["Boarded"].unique()  # to see all the diffrent type of discrit or categorical values prasent inside a particular column.
358/14:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
#to drop multiple column at a time.
titanic.head()
358/15: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
358/16: titanic.isnull().sum()
358/17: #titanic.info()
358/18: titanic.dropna(subset=["Survived"],inplace=True)
358/19:
#
titanic.isnull().mean()*100
358/20:
#
titanic.info()
358/21: #titanic.shape
358/22:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
358/23: titanic["Age"].std()
358/24: titanic["Age_median"].std()
358/25:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
358/26:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
358/27:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

titanic.isnull().sum()
358/28: 891-177
358/29: titanic["Age"].std()
358/30: titanic["Age_random"].std()
358/31: titanic["Age_median"].std()
358/32:
fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
358/33: #titanic.isnull().sum()
358/34:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
358/35: 2/891
358/36: titanic["Cabin"].unique()    # this show the unique values in a particular column.
358/37:
titanic["Age"]=titanic["Age_random"]
titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
358/38:
# Imputing the Embarked column.

# imputing Catagorical values
#  -> Frequency Imputation/Mode imputation.

titanic["Embarked"].unique()
titanic["Embarked"].fillna(titanic["Embarked"].mode()[0], inplace=True)
358/39: titanic["Embarked"].unique()
358/40:
# Number of person survived and number of person dinot survived.
print(titanic['Survived'].value_counts())
sns.countplot(x='Survived',data=titanic)
358/41: sns.countplot(x='Survived', hue='Sex', data=titanic)
358/42: sns.countplot(x='Survived', hue='Embarked', data=titanic)
358/43: titanic["Pclass"].unique()
358/44: sns.countplot(x='Survived',hue="Pclass", data = titanic)
358/45:
plt.figure(figsize=(12,8))
sns.boxplot(x='Pclass',y='Age',data=titanic)
358/46: titanic.corr()
358/47: sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)
358/48: sns.pairplot(titanic, hue="Survived", vars=["Pclass","Age"])
358/49: titanic.head()
358/50:
# Handeling Categorical features

# ---> One hot encoding
#  ---> Ordinal number encoding
#  ---> Label encoder
358/51:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"],drop_first=True)
Embarked1


#titanic = pd.concat([titanic,Embarked1],axis=1)
358/52: Embarked1
358/53: titanic.head()
358/54:
sex1 = pd.get_dummies(titanic["Sex"],drop_first=True)

titanic = pd.concat([titanic,sex1,Embarked1],axis=1)
titanic.head()
358/55: titanic.drop(["Sex","Embarked"],axis=1,inplace=True)
358/56: titanic.head()
358/57:
from sklearn.model_selection import train_test_split

y_value = titanic["Survived"]
x_value = titanic.drop(["Survived"],axis=1)


x_train,x_text,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)
358/58: titanic.isnull().sum()
358/59: titanic.head()
358/60:
from sklearn.linear_model import LogisticRegression

log_model = LogisticRegression()
log_model.fit_transform(x_train,y_train)
y_predicted = log_model.predict(x_test)
358/61:
from sklearn.linear_model import LogisticRegression

log_model = LogisticRegression()
log_model.fit(x_train,y_train)
y_predicted = log_model.predict(x_test)
358/62:
from sklearn.model_selection import train_test_split

y_value = titanic["Survived"] # depandent variable or to be predicted variable    # 80:20 or 70:30
x_value = titanic.drop(["Survived"],axis=1)  # indepandent variable


x_train,x_test,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)
358/63:
from sklearn.linear_model import LogisticRegression

log_model = LogisticRegression()
log_model.fit(x_train,y_train)
y_predicted = log_model.predict(x_test)
358/64:
import pandas as pd
import numpy as np
import warning
358/65:
import pandas as pd
import numpy as np
import warnings
358/66: warning.filterwarnings("ignore")
358/67: warnings.filterwarnings("ignore")
358/68:
from sklearn.linear_model import LogisticRegression

log_model = LogisticRegression()
log_model.fit(x_train,y_train)
y_predicted = log_model.predict(x_test)
358/69:
from sklearn.linear_model import LogisticRegression

log_model = LogisticRegression()
log_model.fit(x_train,y_train)
y_predicted = log_model.predict(x_test)
y_predict
358/70:
from sklearn.linear_model import LogisticRegression

log_model = LogisticRegression()
log_model.fit(x_train,y_train)
y_predicted = log_model.predict(x_test)
y_predicted
358/71:
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

log_model = LogisticRegression()
log_model.fit(x_train,y_train)
y_predicted = log_model.predict(x_test)
y_predicted
358/72:
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

log_model = LogisticRegression()
log_model.fit(x_train,y_train)
y_predicted = log_model.predict(x_test)
print("Accuracy = ",accuracy_score(y_predicted,y_test))
358/73:
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

log_model = LogisticRegression()
log_model.fit(x_train,y_train)
y_predicted = log_model.predict(x_test)
print("Accuracy = ",accuracy_score(y_predicted,y_test))
print(confusion_matrix)
358/74:
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

log_model = LogisticRegression()
log_model.fit(x_train,y_train)
y_predicted = log_model.predict(x_test)
print("Accuracy = ",accuracy_score(y_predicted,y_test))
print(confusion_matrix(y_predicted,y_test))
358/75:
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

log_model = LogisticRegression()
log_model.fit(x_train,y_train)
y_predicted = log_model.predict(x_test)
print("Accuracy = ",accuracy_score(y_predicted,y_test))*100
print(confusion_matrix(y_predicted,y_test))
358/76:
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

log_model = LogisticRegression()
log_model.fit(x_train,y_train)
y_predicted = log_model.predict(x_test)
print("Accuracy = ",accuracy_score(y_predicted,y_test)*100)
print(confusion_matrix(y_predicted,y_test))
358/77: (89+54)/(89+20+16+54)
358/78: ((89+54)/(89+20+16+54))*100
358/79:
from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier()
dt_model.fit(x_train,y_train)
dt_y_predicted = dt_model.predict(x_test)
print("Accuracy = ",accuracy_score(dt_y_predicted,y_test)*100)
print(confusion_matrix(dt_y_predicted,y_test))
358/80:
from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier()
dt_model.fit(x_train,y_train)
dt_y_predicted = dt_model.predict(x_test)
print("Accuracy = ",accuracy_score(dt_y_predicted,y_test)*100)
print(confusion_matrix(dt_y_predicted,y_test))
358/81:
from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier()
dt_model.fit(x_train,y_train)
dt_y_predicted = dt_model.predict(x_test)
print("Accuracy = ",accuracy_score(dt_y_predicted,y_test)*100)
print(confusion_matrix(dt_y_predicted,y_test))
358/82:
from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier()
dt_model.fit(x_train,y_train)
dt_y_predicted = dt_model.predict(x_test)
print("Accuracy = ",accuracy_score(dt_y_predicted,y_test)*100)
print(confusion_matrix(dt_y_predicted,y_test))
358/83:
from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier()
dt_model.fit(x_train,y_train)
dt_y_predicted = dt_model.predict(x_test)
print("Accuracy = ",accuracy_score(dt_y_predicted,y_test)*100)
print(confusion_matrix(dt_y_predicted,y_test))
358/84:
from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier(random_starte = 42)
dt_model.fit(x_train,y_train)
dt_y_predicted = dt_model.predict(x_test)
print("Accuracy = ",accuracy_score(dt_y_predicted,y_test)*100)
print(confusion_matrix(dt_y_predicted,y_test))
358/85:
from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(x_train,y_train)
dt_y_predicted = dt_model.predict(x_test)
print("Accuracy = ",accuracy_score(dt_y_predicted,y_test)*100)
print(confusion_matrix(dt_y_predicted,y_test))
360/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
360/2: warnings.filterwarnings("ignore")
360/3: data = pd.read_csv("diabetes.csv")
360/4: data.head()
360/5:
print(data.columns)
data.shape
360/6:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
360/7: data['Insulin'].replace(0, np.nan, inplace=True)
360/8: data.isnull().sum()
360/9: data.head()
360/10: data["Insulin"].fillna(value=data["Insulin"].median(), inplace=True)
360/11: data.info()
360/12: data.head()
360/13: data.describe()
360/14:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
360/15:
plt.figure(figsize=(15,8))
sns.heatmap(data)
360/16: #sns.pairplot(data, hue="Outcome")
360/17: data.corr()
360/18:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
360/19:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
360/20:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
360/21:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
x_data
360/22:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
360/23:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
360/24:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
360/25:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
360/26:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
360/27:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
360/28:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
360/29:
 #Randomized Search CV

# Hyperparameter optimization    
    
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
360/30:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
360/31:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
360/32: rf_random.fit(std_x_train,y_train)
360/33: rf_random.best_params_
360/34: predictions=rf_random.predict(std_x_test)
360/35: accuracy_score(y_test,predictions)
360/36:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
360/37:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
360/38:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
360/39:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
358/86:
# randomize search cv

criterion=['gini','entropy']
splitter=['best', 'random']
max_depth=[1,3,5,None]
min_samples_split=[2, 5, 10, 15, 100]
min_samples_leaf=[1, 2, 5, 10]
random_state=42 
    
    
    
random_grid = {'criterion': criterion,
               'splitter': splitter,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
                'random_state'=42 }

print(random_grid)
358/87:
# randomize search cv

criterion=['gini','entropy']
splitter=['best', 'random']
max_depth=[1,3,5,None]
min_samples_split=[2, 5, 10, 15, 100]
min_samples_leaf=[1, 2, 5, 10]
random_state=42 
    
    
    
random_grid = {'criterion': criterion,
               'splitter': splitter,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
                'random_state'= 42 }

print(random_grid)
358/88:
# randomize search cv

criterion=['gini','entropy']
splitter=['best', 'random']
max_depth=[1,3,5,None]
min_samples_split=[2, 5, 10, 15, 100]
min_samples_leaf=[1, 2, 5, 10]
random_state=42 
    
    
    
random_grid = {'criterion': criterion,
               'splitter': splitter,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
                'random_state': 42 }

print(random_grid)
358/89:
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV

tree = DecisionTreeClassifier()

# Instantiate the RandomizedSearchCV object: tree_cv
#tree_cv = RandomizedSearchCV(tree, param_dist, cv=5,  n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)

# Fit it to the data
358/90:
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV

tree = DecisionTreeClassifier()

# Instantiate the RandomizedSearchCV object: tree_cv
tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=random_grid, cv=5,  n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)

# Fit it to the data
358/91:
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV

tree = DecisionTreeClassifier()

# Instantiate the RandomizedSearchCV object: tree_cv
tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=random_grid,n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)

# Fit it to the data
358/92: tree_cv.fit(x_train,y_train)
358/93:
# randomize search cv

criterion=['gini','entropy']
splitter=['best', 'random']
max_depth=[1,3,5,None]
min_samples_split=[2, 5, 10, 15, 100]
min_samples_leaf=[1, 2, 5, 10]
random_state=42 
    
    
    
random_grid = {'criterion': criterion,
               'splitter': splitter,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
                }

print(random_grid)
358/94:
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV

tree = DecisionTreeClassifier(random_state=42)

# Instantiate the RandomizedSearchCV object: tree_cv
tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=random_grid,n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)

# Fit it to the data
358/95: tree_cv.fit(x_train,y_train)
358/96: tree_cv.best_params_
358/97: predictions=tree_cv.predict(x_test)
358/98: accuracy_score(y_test,predictions)
358/99:
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV

tree = DecisionTreeClassifier()

# Instantiate the RandomizedSearchCV object: tree_cv
tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=random_grid,n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)

# Fit it to the data
358/100: tree_cv.fit(x_train,y_train)
358/101: tree_cv.best_params_
358/102: predictions=tree_cv.predict(x_test)
358/103: accuracy_score(y_test,predictions)
361/1:
import pandas as pd
import numpy as np
import warnings
361/2: warnings.filterwarnings("ignore")
361/3:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                 # overhere titanic is basically a pandas dataframe
361/4:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
361/5: titanic.tail()     # tail() returns last five rows of the whole data.
361/6: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
361/7: titanic.columns     # to know all the labes, features, columns
361/8: titanic.info()
361/9: titanic.describe()
361/10: titanic["Survived"].isnull().sum()  # count's the number of missing values or NAN values in each column
361/11: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
361/12: titanic.drop("Class", axis=1, inplace=True)   # to drop any column or you can say to remove any colummn.
361/13: #titanic.head()
361/14:
#titanic.head()
titanic["Boarded"].unique()  # to see all the diffrent type of discrit or categorical values prasent inside a particular column.
361/15:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
#to drop multiple column at a time.
titanic.head()
361/16: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
361/17: titanic.isnull().sum()
361/18: #titanic.info()
361/19: titanic.dropna(subset=["Survived"],inplace=True)
361/20:
#
titanic.isnull().mean()*100
361/21:
#
titanic.info()
361/22: #titanic.shape
361/23:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
361/24: titanic["Age"].std()
361/25: titanic["Age_median"].std()
361/26:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
361/27:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
361/28:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

titanic.isnull().sum()
361/29: 891-177
361/30: titanic["Age"].std()
361/31: titanic["Age_random"].std()
361/32: titanic["Age_median"].std()
361/33:
fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
361/34: #titanic.isnull().sum()
361/35:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
361/36: 2/891
361/37: titanic["Cabin"].unique()    # this show the unique values in a particular column.
361/38:
titanic["Age"]=titanic["Age_random"]
titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
361/39:
# Imputing the Embarked column.

# imputing Catagorical values
#  -> Frequency Imputation/Mode imputation.

titanic["Embarked"].unique()
titanic["Embarked"].fillna(titanic["Embarked"].mode()[0], inplace=True)
361/40: titanic["Embarked"].unique()
361/41:
# Number of person survived and number of person dinot survived.
print(titanic['Survived'].value_counts())
sns.countplot(x='Survived',data=titanic)
361/42: sns.countplot(x='Survived', hue='Sex', data=titanic)
361/43: sns.countplot(x='Survived', hue='Embarked', data=titanic)
361/44: titanic["Pclass"].unique()
361/45: sns.countplot(x='Survived',hue="Pclass", data = titanic)
361/46:
plt.figure(figsize=(12,8))
sns.boxplot(x='Pclass',y='Age',data=titanic)
361/47: titanic.corr()
361/48: sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)
361/49: sns.pairplot(titanic, hue="Survived", vars=["Pclass","Age"])
361/50: titanic.head()
361/51:
# Handeling Categorical features

# ---> One hot encoding
#  ---> Ordinal number encoding
#  ---> Label encoder
361/52:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"],drop_first=True)
Embarked1


#titanic = pd.concat([titanic,Embarked1],axis=1)
361/53: Embarked1
361/54: titanic.head()
361/55:
sex1 = pd.get_dummies(titanic["Sex"],drop_first=True)

titanic = pd.concat([titanic,sex1,Embarked1],axis=1)
titanic.head()
361/56: titanic.drop(["Sex","Embarked"],axis=1,inplace=True)
361/57: titanic.head()
361/58:
from sklearn.model_selection import train_test_split

y_value = titanic["Survived"] # depandent variable or to be predicted variable    # 80:20 or 70:30
x_value = titanic.drop(["Survived"],axis=1)  # indepandent variable


x_train,x_test,y_train,y_test = train_test_split(x_value,y_value, test_size=0.2, random_state = 42)
361/59:
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

log_model = LogisticRegression()
log_model.fit(x_train,y_train)
y_predicted = log_model.predict(x_test)
print("Accuracy = ",accuracy_score(y_predicted,y_test)*100)
print(confusion_matrix(y_predicted,y_test))
361/60: ((89+54)/(89+20+16+54))*100
361/61:
from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(x_train,y_train)
dt_y_predicted = dt_model.predict(x_test)
print("Accuracy = ",accuracy_score(dt_y_predicted,y_test)*100)
print(confusion_matrix(dt_y_predicted,y_test))
361/62:
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(x_train,y_train)
rf_y_predicted = rf_model.predict(x_test)
print("Accuracy = ",accuracy_score(rf_y_predicted,y_test)*100)
print(confusion_matrix(rf_y_predicted,y_test))
361/63:
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=100,random_state=42)
rf_model.fit(x_train,y_train)
rf_y_predicted = rf_model.predict(x_test)
print("Accuracy = ",accuracy_score(rf_y_predicted,y_test)*100)
print(confusion_matrix(rf_y_predicted,y_test))
361/64:
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=200,random_state=42)
rf_model.fit(x_train,y_train)
rf_y_predicted = rf_model.predict(x_test)
print("Accuracy = ",accuracy_score(rf_y_predicted,y_test)*100)
print(confusion_matrix(rf_y_predicted,y_test))
361/65:
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=500,random_state=42)
rf_model.fit(x_train,y_train)
rf_y_predicted = rf_model.predict(x_test)
print("Accuracy = ",accuracy_score(rf_y_predicted,y_test)*100)
print(confusion_matrix(rf_y_predicted,y_test))
361/66:
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=1000,random_state=42)
rf_model.fit(x_train,y_train)
rf_y_predicted = rf_model.predict(x_test)
print("Accuracy = ",accuracy_score(rf_y_predicted,y_test)*100)
print(confusion_matrix(rf_y_predicted,y_test))
361/67:
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=100,random_state=42)
rf_model.fit(x_train,y_train)
rf_y_predicted = rf_model.predict(x_test)
print("Accuracy = ",accuracy_score(rf_y_predicted,y_test)*100)
print(confusion_matrix(rf_y_predicted,y_test))
361/68:
# Randomized searched cv

from sklearn.model_selection import RandomizedSearchCV

criterion=['gini', 'entropy']
splitter=['best','random']
max_depth=[2,5,6,7,8,9,None]
min_samples_split=[2,4,6,8]
min_samples_leaf=[1,3,5,7]


dt_random_grid = {
    'criterion':criterion,
    'splitter':splitter
    'max_depth':max_depth
    'min_samples_split':min_samples_split
    'min_samples_leaf':min_samples_leaf
    
}

print(dt_random_grid)
361/69:
# Randomized searched cv

from sklearn.model_selection import RandomizedSearchCV

criterion=['gini', 'entropy']
splitter=['best','random']
max_depth=[2,5,6,7,8,9,None]
min_samples_split=[2,4,6,8]
min_samples_leaf=[1,3,5,7]


dt_random_grid = {
    'criterion':criterion,
    'splitter':splitter,
    'max_depth':max_depth,
    'min_samples_split':min_samples_split,
    'min_samples_leaf':min_samples_leaf,
    
}

print(dt_random_grid)
361/70:
tree = DecisionTreeClassifier()

tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=dt_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)
361/71: tree_cv.fit(x_train,y_train)
361/72: tree_cv.best_params_
361/73:
prediction = tree_cv.predict(x_test)
print("Accuracy ",accuracy_score(y_test,prediction))
361/74:
prediction = tree_cv.predict(x_test)
print("Accuracy ",accuracy_score(y_test,prediction)*100)
361/75:
# Randomized searched cv

from sklearn.model_selection import RandomizedSearchCV

criterion=['gini', 'entropy']
splitter=['best','random']
max_depth=[2,5,6,8,9,None]
min_samples_split=[2,4,6,8,10,12]
min_samples_leaf=[1,3,5,7,8,9,10]


dt_random_grid = {
    'criterion':criterion,
    'splitter':splitter,
    'max_depth':max_depth,
    'min_samples_split':min_samples_split,
    'min_samples_leaf':min_samples_leaf,
    
}

print(dt_random_grid)
361/76:
tree = DecisionTreeClassifier()

tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=dt_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)
361/77: tree_cv.fit(x_train,y_train)
361/78: tree_cv.best_params_
361/79:
prediction = tree_cv.predict(x_test)
print("Accuracy ",accuracy_score(y_test,prediction)*100)
361/80:
# Randomized searched cv

from sklearn.model_selection import RandomizedSearchCV

criterion=['gini', 'entropy']
splitter=['best','random']
max_depth=[1,3,5,None]
min_samples_split=[2,4,5,10,15,100]
min_samples_leaf=[1,3,5,7,8,9,10]


dt_random_grid = {
    'criterion':criterion,
    'splitter':splitter,
    'max_depth':max_depth,
    'min_samples_split':min_samples_split,
    'min_samples_leaf':min_samples_leaf,
    
}

print(dt_random_grid)
361/81:
tree = DecisionTreeClassifier()

tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=dt_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)
361/82: tree_cv.fit(x_train,y_train)
361/83: tree_cv.best_params_
361/84:
prediction = tree_cv.predict(x_test)
print("Accuracy ",accuracy_score(y_test,prediction)*100)
361/85:
# Randomized searched cv

from sklearn.model_selection import RandomizedSearchCV

criterion=['gini', 'entropy']
splitter=['best','random']
max_depth=[1,3,5,None]
min_samples_split=[2,4,5,10,15,100]
min_samples_leaf=[1,2,5,10]


dt_random_grid = {
    'criterion':criterion,
    'splitter':splitter,
    'max_depth':max_depth,
    'min_samples_split':min_samples_split,
    'min_samples_leaf':min_samples_leaf,
    
}

print(dt_random_grid)
361/86:
tree = DecisionTreeClassifier()

tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=dt_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)
361/87: tree_cv.fit(x_train,y_train)
361/88: tree_cv.best_params_
361/89:
prediction = tree_cv.predict(x_test)
print("Accuracy ",accuracy_score(y_test,prediction)*100)
361/90:
# Randomized searched cv

from sklearn.model_selection import RandomizedSearchCV

criterion=['gini', 'entropy']
splitter=['best','random']
max_depth=[1,3,5,None]
min_samples_split=[2,5,10,15,100]
min_samples_leaf=[1,2,5,10]


dt_random_grid = {
    'criterion':criterion,
    'splitter':splitter,
    'max_depth':max_depth,
    'min_samples_split':min_samples_split,
    'min_samples_leaf':min_samples_leaf,
    
}

print(dt_random_grid)
361/91:
tree = DecisionTreeClassifier()

tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=dt_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)
361/92: tree_cv.fit(x_train,y_train)
361/93: tree_cv.best_params_
361/94:
prediction = tree_cv.predict(x_test)
print("Accuracy ",accuracy_score(y_test,prediction)*100)
361/95:
# Randomized searched cv

from sklearn.model_selection import RandomizedSearchCV

criterion=['gini', 'entropy']
splitter=['best','random']
max_depth=[1,3,5,None]
min_samples_split=[2,5,10,15,100,1000]
min_samples_leaf=[1,2,5,10,15]


dt_random_grid = {
    'criterion':criterion,
    'splitter':splitter,
    'max_depth':max_depth,
    'min_samples_split':min_samples_split,
    'min_samples_leaf':min_samples_leaf,
    
}

print(dt_random_grid)
361/96:
tree = DecisionTreeClassifier()

tree_cv = RandomizedSearchCV(estimator=tree, param_distributions=dt_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)
361/97: tree_cv.fit(x_train,y_train)
361/98: tree_cv.best_params_
361/99:
prediction = tree_cv.predict(x_test)
print("Accuracy ",accuracy_score(y_test,prediction)*100)
361/100: li = for x in np.linespace(start=100,stop=2000,num=15)
361/101: li = [int(x) for x in np.linespace(start=100,stop=2000,num=15)]
361/102: li = [int(x) for x in np.linspace(start=100,stop=2000,num=15)]
361/103: li
361/104: 371-235
361/105: 507-371
361/106:
n_estimators=[int(x) for x in np.linspace(start=100,stop=2000,num=15)]
criterion=['gini','entropy']
max_depth=[int(x) for x in np.linspace(start=2,stop=30,num=10)]
min_samples_split=[2,5,10,15,100,1000]
min_samples_leaf=[1,2,5,10,15]
max_features=['auto','sqrt','log2']


rf_random_grid = {
    'n_estimators':n_estimators,
    'criterion':criterion,
    'splitter':splitter,
    'max_depth':max_depth,
    'min_samples_split':min_samples_split,
    'min_samples_leaf':min_samples_leaf,
    'max_features':max_features
}

print(rf_random_grid)
361/107:
rf_trees = RandomForestClassifier()

tree_cv = RandomizedSearchCV(estimator=rf_trees, param_distributions=rf_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)
361/108: rf_trees.fit(x_train,y_train)
361/109: rf_trees_cv.fit(x_train,y_train)
361/110:
rf_trees = RandomForestClassifier()

rf_tree_cv = RandomizedSearchCV(estimator=rf_trees, param_distributions=rf_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)
361/111: rf_trees_cv.fit(x_train,y_train)
361/112: rf_tree_cv.fit(x_train,y_train)
361/113:
n_estimators=[int(x) for x in np.linspace(start=100,stop=2000,num=15)]
criterion=['gini','entropy']
max_depth=[int(x) for x in np.linspace(start=2,stop=30,num=10)]
min_samples_split=[2,5,10,15,100,1000]
min_samples_leaf=[1,2,5,10,15]
max_features=['auto','sqrt','log2']


rf_random_grid = {
    'n_estimators':n_estimators,
    'criterion':criterion,
    'max_depth':max_depth,
    'min_samples_split':min_samples_split,
    'min_samples_leaf':min_samples_leaf,
    'max_features':max_features
}

print(rf_random_grid)
361/114:
rf_trees = RandomForestClassifier()

rf_tree_cv = RandomizedSearchCV(estimator=rf_trees, param_distributions=rf_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)
361/115: rf_tree_cv.fit(x_train,y_train)
361/116: rf_tree_cv.best_params_
361/117:
rf_prediction = rf_tree_cv.predict(x_test)
print("Accuracy = ",accuracy_score(rf_prediction,y_test)*100)
361/118:
n_estimators=[int(x) for x in np.linspace(start=100,stop=2000,num=20)]
criterion=['gini','entropy']
max_depth=[int(x) for x in np.linspace(start=2,stop=30,num=10)]
min_samples_split=[2,5,10,15,100,1000]
min_samples_leaf=[1,2,5,10,15]
max_features=['auto','sqrt','log2']


rf_random_grid = {
    'n_estimators':n_estimators,
    'criterion':criterion,
    'max_depth':max_depth,
    'min_samples_split':min_samples_split,
    'min_samples_leaf':min_samples_leaf,
    'max_features':max_features
}

print(rf_random_grid)
361/119:
rf_trees = RandomForestClassifier()

rf_tree_cv = RandomizedSearchCV(estimator=rf_trees, param_distributions=rf_random_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=1)
361/120: rf_tree_cv.fit(x_train,y_train)
361/121: rf_tree_cv.best_params_
361/122:
rf_prediction = rf_tree_cv.predict(x_test)
print("Accuracy = ",accuracy_score(rf_prediction,y_test)*100)
364/1:
import pandas as pd
import numpy as np
import warnings
364/2: warnings.filterwarnings("ignore")
364/3:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                 # overhere titanic is basically a pandas dataframe
364/4:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
364/5: titanic.tail()     # tail() returns last five rows of the whole data.
364/6: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
364/7: titanic.columns     # to know all the labes, features, columns
364/8: titanic.info()
364/9: titanic.describe()
364/10: titanic["Survived"].isnull().sum()  # count's the number of missing values or NAN values in each column
364/11: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
364/12: titanic.drop("Class", axis=1, inplace=True)   # to drop any column or you can say to remove any colummn.
364/13: #titanic.head()
364/14:
#titanic.head()
titanic["Boarded"].unique()  # to see all the diffrent type of discrit or categorical values prasent inside a particular column.
364/15:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
#to drop multiple column at a time.
titanic.head()
364/16: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
364/17: titanic.isnull().sum()
364/18: #titanic.info()
364/19: titanic.dropna(subset=["Survived"],inplace=True)
364/20:
#
titanic.isnull().mean()*100
364/21:
#
titanic.info()
364/22: #titanic.shape
364/23:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
364/24: titanic["Age"].std()
364/25: titanic["Age_median"].std()
364/26:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
364/27:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
364/28:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

titanic.isnull().sum()
364/29: 891-177
364/30: titanic["Age"].std()
364/31: titanic["Age_random"].std()
364/32: titanic["Age_median"].std()
364/33:
fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
364/34: #titanic.isnull().sum()
364/35:
import seaborn as sns
sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
364/36: 2/891
364/37: titanic["Cabin"].unique()    # this show the unique values in a particular column.
364/38:
titanic["Age"]=titanic["Age_random"]
titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
364/39:
# Imputing the Embarked column.

# imputing Catagorical values
#  -> Frequency Imputation/Mode imputation.

titanic["Embarked"].unique()
titanic["Embarked"].fillna(titanic["Embarked"].mode()[0], inplace=True)
364/40: titanic["Embarked"].unique()
364/41:
# Number of person survived and number of person dinot survived.
print(titanic['Survived'].value_counts())
sns.countplot(x='Survived',data=titanic)
364/42: sns.countplot(x='Survived', hue='Sex', data=titanic)
364/43: sns.countplot(x='Survived', hue='Embarked', data=titanic)
364/44: titanic["Pclass"].unique()
364/45: sns.countplot(x='Survived',hue="Pclass", data = titanic)
364/46:
plt.figure(figsize=(12,8))
sns.boxplot(x='Pclass',y='Age',data=titanic)
364/47: titanic.corr()
364/48: sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)
364/49: sns.pairplot(titanic, hue="Survived", vars=["Pclass","Age"])
364/50: titanic.head()
364/51:
# Handeling Categorical features

# ---> One hot encoding
#  ---> Ordinal number encoding
#  ---> Label encoder
364/52:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"],drop_first=True)
Embarked1

titanic["Embarked2"] = titanic["Embarked"]


#titanic = pd.concat([titanic,Embarked1],axis=1)
364/53: Embarked1
364/54: titanic.head()
364/55:
sex1 = pd.get_dummies(titanic["Sex"],drop_first=True)

titanic = pd.concat([titanic,sex1,Embarked1],axis=1)
titanic.head()
364/56: titanic.drop(["Sex","Embarked"],axis=1,inplace=True)
364/57: titanic.head()
365/1:
import pandas as pd
import numpy as np
import warnings
365/2: warnings.filterwarnings("ignore")
365/3:
# .csv -> comma seperated values

titanic = pd.read_csv("full.csv")     # to load the dataset
                                 # overhere titanic is basically a pandas dataframe
365/4:
#print(type(titanic))
titanic.head()        # head() returns first first five rows of the whole data.
365/5: titanic.tail()     # tail() returns last five rows of the whole data.
365/6: titanic.shape      # to know the shape of the data, size, dimention or to be more specific you can say number of rows and columns.
365/7: titanic.columns     # to know all the labes, features, columns
365/8: titanic.info()
365/9: titanic.describe()
365/10: titanic["Survived"].isnull().sum()  # count's the number of missing values or NAN values in each column
365/11: titanic.isnull().mean()*100      # to find the missing value percentage in each column.
365/12: titanic.drop("Class", axis=1, inplace=True)   # to drop any column or you can say to remove any colummn.
365/13: #titanic.head()
365/14:
#titanic.head()
titanic["Boarded"].unique()  # to see all the diffrent type of discrit or categorical values prasent inside a particular column.
365/15:
titanic.drop(["WikiId","Name_wiki","Age_wiki","Hometown","Boarded","Destination","Lifeboat","Body"],axis=1,inplace=True) 
#to drop multiple column at a time.
titanic.head()
365/16: titanic.drop(["PassengerId","Name","Ticket"],axis=1,inplace=True)
365/17: titanic.isnull().sum()
365/18: #titanic.info()
365/19: titanic.dropna(subset=["Survived"],inplace=True)
365/20:
#
titanic.isnull().mean()*100
365/21:
#
titanic.info()
365/22: #titanic.shape
365/23:
## Mean/ Midean Imputation

# we try to replace the missing values or we impute the missing values using the mean or the meandian.

titanic["Age_median"] = titanic['Age'] 
titanic["Age_median"].fillna(titanic["Age_median"].median(),inplace=True) # i am replacing the missing values with the median()

titanic.head()
365/24: titanic["Age"].std()
365/25: titanic["Age_median"].std()
365/26:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
titanic["Age"].plot(kind = "kde", ax = ax)
titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
365/27:
#titanic["Age_random"].dropna().sample()
#titanic["Age_random"].isnull().sum()
365/28:
## Random sample Imputation

titanic["Age_random"] = titanic["Age"] # we are copying all the values present inside the age column into the age_random column
random_samples = titanic["Age_random"].dropna().sample(titanic["Age_random"].isnull().sum(), random_state=42)
random_samples.index = titanic[titanic["Age_random"].isnull()].index
titanic.loc[titanic["Age_random"].isnull(),"Age_random"]=random_samples

titanic.isnull().sum()
365/29: 891-177
365/30: titanic["Age"].std()
365/31: titanic["Age_random"].std()
365/32: titanic["Age_median"].std()
365/33:
#fig = plt.figure()
#ax = fig.add_subplot(111)
#titanic["Age"].plot(kind = "kde", ax = ax)
#titanic["Age_median"].plot(kind="kde", ax=ax, color="green")
#titanic["Age_random"].plot(kind="kde", ax=ax, color="red")
#line, labels = ax.get_legend_handles_labels()
#ax.legend(line,labels,loc = "best")
365/34: #titanic.isnull().sum()
365/35:
import seaborn as sns
#sns.heatmap(titanic.isnull(),yticklabels='auto',cbar=False,cmap="brg")
365/36: 2/891
365/37: titanic["Cabin"].unique()    # this show the unique values in a particular column.
365/38:
titanic["Age"]=titanic["Age_random"]
titanic.drop(['Cabin','Age_random','Age_median'],axis=1,inplace=True)
365/39:
# Imputing the Embarked column.

# imputing Catagorical values
#  -> Frequency Imputation/Mode imputation.

titanic["Embarked"].unique()
titanic["Embarked"].fillna(titanic["Embarked"].mode()[0], inplace=True)
365/40: titanic["Embarked"].unique()
365/41:
# Number of person survived and number of person dinot survived.
#print(titanic['Survived'].value_counts())
#sns.countplot(x='Survived',data=titanic)
365/42: #sns.countplot(x='Survived', hue='Sex', data=titanic)
365/43: #sns.countplot(x='Survived', hue='Embarked', data=titanic)
365/44: titanic["Pclass"].unique()
365/45: #sns.countplot(x='Survived',hue="Pclass", data = titanic)
365/46:
#plt.figure(figsize=(12,8))
#sns.boxplot(x='Pclass',y='Age',data=titanic)
365/47: #titanic.corr()
365/48: #sns.heatmap(titanic.corr(), annot=True, linewidth=0.5)
365/49: sns.pairplot(titanic, hue="Survived", vars=["Pclass","Age"])
365/50: titanic.head()
365/51:
# Handeling Categorical features

# ---> One hot encoding
#  ---> Ordinal number encoding
#  ---> Label encoder
365/52:
# one hot encodding


Embarked1 = pd.get_dummies(titanic["Embarked"],drop_first=True)
Embarked1

titanic["Embarked2"] = titanic["Embarked"]


#titanic = pd.concat([titanic,Embarked1],axis=1)
365/53: Embarked1
365/54: titanic.head()
365/55:
sex1 = pd.get_dummies(titanic["Sex"],drop_first=True)

titanic = pd.concat([titanic,sex1,Embarked1],axis=1)
titanic.head()
365/56: #titanic.drop(["Sex","Embarked"],axis=1,inplace=True)
365/57: titanic.head()
365/58: from sklearn.preprocessing import OrdinalEncoder
365/59:
o_encoder = OrdinalEncoder
result = o_encoder.fit_transform(titanic["Embarked2"])
result
365/60:
o_encoder = OrdinalEncoder
x = np.array(titanic["Embarked2"])
#result = o_encoder.fit_transform()
#result
x
365/61:
o_encoder = OrdinalEncoder
x = np.array(titanic["Embarked2"])
result = o_encoder.fit_transform(x)
result
365/62:
o_encoder = OrdinalEncoder
x = np.array(titanic["Embarked2"]).reshape(1,-1)
print(x)
result = o_encoder.fit_transform(x)
result
365/63:
o_encoder = OrdinalEncoder
x = np.array(titanic["Embarked2"]).reshape(-1,1)
print(x)
result = o_encoder.fit_transform(x)
result
365/64:
o_encoder = OrdinalEncoder
x = np.array(titanic["Embarked2"]).reshape(-1,1)
#print(x)
result = o_encoder.fit_transform(x)
result
365/65:
o_encoder = OrdinalEncoder
x = np.array(titanic["Embarked2"]).reshape(-1,1)
print(x)
result = o_encoder.fit_transform(x)
result
365/66:
o_encoder = OrdinalEncoder
x = np.array(titanic["Embarked2"]).reshape(-1,1)
#print(x)
result = o_encoder.fit_transform(x)
print(result)
365/67:
o_encoder = OrdinalEncoder()
x = np.array(titanic["Embarked2"]).reshape(-1,1)
#print(x)
result = o_encoder.fit_transform(x)
print(result)
365/68:
o_encoder = OrdinalEncoder()
x = np.array(titanic["Embarked2"]).reshape(-1,1)
#print(x)
embarked2 = o_encoder.fit_transform(x)

embarked2 = pd.DataFrame(data=embarked2,columns=['embarked2'])

titanic = pd.concat([titanic,embarked2],axis=1)
365/69: titanic.head()
367/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

import pickle

from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
367/2: warnings.filterwarnings("ignore")
367/3:
data = pd.read_csv("heart_disease.csv")
data
367/4: data.head()
367/5: data.info()
367/6: data.describe()
367/7: data.columns
367/8: data.info()
367/9: data.isnull().sum()
367/10: data.corr()
367/11:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
367/12:
sns.kdeplot(data[data['target']==1]['chol'],shade=True,color="orange", label="Unwell", alpha=.7)
sns.kdeplot(data[data['target']==0]['chol'],shade=True,color="dodgerblue", label="Healthy", alpha=.7)
plt.title('Cholesterol in mg/d for both case')
plt.show()
367/13:
plt.figure(figsize=(15,8))
sns.heatmap(data)
367/14: #sns.pairplot(data, hue="target")
367/15:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
367/16:
data_2 = data.drop(["target"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.target).index, data_2.corrwith(data.target))
367/17: sns.pairplot(data, hue = "target", vars = ["cp","restecg","thalach","slope"])
367/18:
# Selecting all the features

all_x = data.drop(("target"), axis=1)
all_x = np.array(all_x)

# Selecting only importent features

imp_x = data[["cp","restecg","thalach","slope"]]
imp_x = np.array(imp_x)


y = data["target"]
y = np.array(y).reshape(-1,1)

print(all_x)
print(imp_x)
print(y)
367/19:
std_scl = StandardScaler()

all_x = std_scl.fit_transform(all_x)
imp_x = std_scl.fit_transform(imp_x)

all_x
367/20:
plt.figure(figsize=(15,8))
sns.heatmap(all_x)
367/21:
all_x_train,all_x_test,all_y_train,all_y_test = train_test_split(all_x, y, test_size = 0.2, random_state = 42)
all_x_train
#all_y_train
367/22:
imp_x_train,imp_x_test,imp_y_train,imp_y_test = train_test_split(imp_x, y, test_size = 0.2, random_state = 42)
imp_x_train
#imp_y_train
367/23:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction)*100)
print("R-square on all features: ", r2_score(all_y_test,lg_model_prediction))
367/24:
# Training and testing on selected features.

lg_model1 = LogisticRegression()
lg_model1.fit(imp_x_train,imp_y_train)
lg_model1_prediction = lg_model1.predict(imp_x_test)
print("Accuracy on Selected features: ", accuracy_score(imp_y_test,lg_model1_prediction)*100)
print("Accuracy on Selected features: ", r2_score(imp_y_test,lg_model1_prediction))
367/25:
confu_matrix = confusion_matrix(all_y_test, lg_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
367/26:
# Training and testing on all features.

dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(all_x_train,all_y_train)
dt_model_prediction = dt_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,dt_model_prediction)*100)
print("Accuracy on all features: ", r2_score(all_y_test,dt_model_prediction))
367/27:
# Training and testing on selected features.

dt_model1 = DecisionTreeClassifier(random_state = 42)
dt_model1.fit(imp_x_train,imp_y_train)
dt_model1_prediction = dt_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test,dt_model1_prediction)*100)
print("Accuracy on selected features: ", r2_score(imp_y_test,dt_model1_prediction))
367/28:
confu_matrix = confusion_matrix(all_y_test, dt_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True, cbar=False, fmt='g')
plt.show()
367/29:
# Training and testing on all features.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(all_x_train,all_y_train)
rf_model_prediction = rf_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, rf_model_prediction)*100)
367/30:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction)*100)
367/31:
confu_matrix = confusion_matrix(all_y_test, rf_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True,cbar=False, fmt='g')
plt.show()
367/32:
# Training and testing on all features.

nb_model = GaussianNB()
nb_model.fit(all_x_train,all_y_train)
nb_model_prediction = nb_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, nb_model_prediction)*100)
367/33:
# Training and testing on selected features.

nb_model1 = GaussianNB()
nb_model1.fit(imp_x_train,imp_y_train)
nb_model1_prediction = nb_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, nb_model1_prediction)*100)
367/34:
confu_matrix = confusion_matrix(all_y_test, nb_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
367/35:
# Training and testing on all features.

svm_model = SVC()
svm_model.fit(all_x_train,all_y_train)
svm_model_prediction = svm_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test, svm_model_prediction)*100)
367/36:
# Training and testing on selected features.

svm_model1 = SVC()
svm_model1.fit(imp_x_train,imp_y_train)
svm_model1_prediction = svm_model1.predict(imp_x_test)
print("Accuracy on all features: ", accuracy_score(imp_y_test, svm_model1_prediction)*100)
367/37:
confu_matrix = confusion_matrix(all_y_test, svm_model_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
367/38:
# save model
pickle.dump(rf_model,open('model.pkl','wb'))

# Load Model
heart_disease_prediction_model = pickle.load(open('model.pkl','rb'))

y_predict = heart_disease_prediction_model.predict(all_x_test)

print("confusion matrix: \n", confusion_matrix(all_y_test,y_predict))
accuracy_score(all_y_test,y_predict)
367/39:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

import pickle

from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
367/40:
# Training and testing on selected features.

rf_model1 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model1.fit(imp_x_train,imp_y_train)
rf_model1_prediction = rf_model1.predict(imp_x_test)
print("Accuracy on selected features: ", accuracy_score(imp_y_test, rf_model1_prediction)*100)
367/41:
cross_val = cross_val_score(estimator=rf_model, X=all_x_train, y=all_y_train)
print("Cross validation accuracy of Random Forest Classifier: ",cross_val)
367/42:
cross_val = cross_val_score(estimator=rf_model, X=all_x_train, y=all_y_train)
print("Cross validation accuracy of Random Forest Classifier: ",cross_val)
print("Cross validation mean accuracy of Random Forest Classifier: ", cross_val.mean())
369/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix
369/2: data = pd.read_csv("Breast_cancer_dataset.csv")
369/3: data.head()
369/4: data.columns
369/5: data.describe()
369/6: data.info()
369/7: data.isnull().sum() # looking for null values
369/8: data.drop(["id","Unnamed: 32"], axis=1, inplace = True) # droping the unwanted columns
369/9: data.head()
369/10: #sns.countplot(data["diagnosis"])
369/11: #sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
369/12:
print(data["diagnosis"].unique())
label_encoder = preprocessing.LabelEncoder()
data["diagnosis"] = label_encoder.fit_transform(data["diagnosis"])
print(data["diagnosis"].unique())
369/13: sns.countplot(data["diagnosis"])
369/14:
data.head()      # M = 1 (It means the the tumor is cancerous or having breast cancer.)
                 # B = 0 ( It means the tumor is normal and not a cancerous type.)
369/15:
plt.figure(figsize=(15,8))              #HEAT MAP
sns.heatmap(data)
369/16: #sns.pairplot(data,hue = "diagnosis")
369/17:
plt.figure(figsize=(20,8))           
sns.countplot(data["radius_mean"])
369/18: data.corr()
369/19:
f,ax = plt.subplots(figsize=(18, 18))    #CORRELATION HEAT MAP
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
369/20:
sns.pairplot(data, hue = "diagnosis", vars = ["radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean"])
# 1 = M, 0 = B
369/21:
data_x_labels = data.drop(("diagnosis"), axis=1)
#data_x_labels.head()

data_y_label = data["diagnosis"]
369/22:
stnd_scl = StandardScaler()
data_x_labels = stnd_scl.fit_transform(data_x_labels)
369/23: x_train,x_test,y_train,y_test = train_test_split(data_x_labels, data_y_label, test_size = 0.2, random_state = 42)
369/24:
#print(x_train)
# print(y_train.head())
369/25:
model_1 = LogisticRegression()
model_1.fit(x_train,y_train)
model_1_y_prediction = model_1.predict(x_test)
print("Accuracy of Logistic Regrassion: ",accuracy_score(y_test, model_1_y_prediction))
369/26:
model_2 = DecisionTreeClassifier(random_state = 45)
model_2.fit(x_train,y_train)
model_2_y_prediction = model_2.predict(x_test)
print("Accuracy of Decission Tree Classifier: ",accuracy_score(y_test, model_2_y_prediction))
369/27:
model_3 = RandomForestClassifier(n_estimators=100, random_state=45)
model_3.fit(x_train, y_train)
model_3_y_prediction = model_3.predict(x_test)
print("Accuracy of Random Forest Classifier: ",accuracy_score(y_test, model_1_y_prediction))
369/28:
confu_matrix = confusion_matrix(y_test, model_3_y_prediction)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
369/29:
cross_val = cross_val_score(estimator=model_3, X=x_train, y=y_train)
print("Cross validation accuracy of Random Forest Classifier: ",cross_val)
print("Cross validation mean accuracy of Random Forest Classifier: ", cross_val.mean())
369/30:
stnd_scl = StandardScaler()
data_x_labels = stnd_scl.fit_transform(data_x_labels)

data_x_labels_labels
369/31:
stnd_scl = StandardScaler()
data_x_labels = stnd_scl.fit_transform(data_x_labels)

data_x_labels
369/32:
stnd_scl = StandardScaler()
data_x_labels = stnd_scl.fit_transform(data_x_labels)

plt.figure(figsize=(15,8))              #HEAT MAP
sns.heatmap(data_x_labels)
370/1: import pandas as pd
370/2: df = pd.read_csv("full.csv", usecols = ["Cabin","Survived"])
370/3:
df = pd.read_csv("full.csv", usecols = ["Cabin","Survived"])
df.head()
370/4: df["Survived"] = df["Survived"].astype(int)
370/5: df["Survived"] = df["Survived"].astype(int32)
370/6: df["Survived"] = df["Survived"].astype('int')
370/7: df["Survived"] = df["Survived"].astype('int64')
370/8: df["Survived"] = df["Survived"].astype('int64').dtypes
370/9: df["Survived"] = df["Survived"].astype('int32').dtypes
370/10: df.isnull().sum()
370/11: df.dropna()
370/12: df.isnull().sum()
370/13: df["Survived"].dropna(inplace = True)
370/14: df.isnull().sum()
370/15: df["Survived"].dropna(inplace = True)
370/16: df.isnull().sum()
370/17: df.dropna(subset = ["Survived"], inplace = True)
370/18: df.isnull().sum()
370/19: df["Survived"] = df["Survived"].astype("int64").dtypes
370/20: df.head()
370/21:
df = pd.read_csv("full.csv", usecols = ["Cabin","Survived"])
df.head()
370/22: #df.isnull().sum()
370/23: df.dropna(subset = ["Survived"], inplace = True)
370/24: df.head()
370/25: df["Cabin"].fillna("Missing",inplace=True)
370/26: df.head()
370/27: sad
370/28:
df["Cabin"].fillna("Missing",inplace=True)
df.head()
370/29: dtype(df["Cabin"])
370/30: dtypes(df["Cabin"])
370/31: types(df["Cabin"])
370/32: type(df["Cabin"])
370/33: df.Cabin.dtype
370/34: df.Cabin.dtypes
370/35: df.dtypes
370/36: df["Cabin"] = df["Cabin"].astype(str)
370/37: df.dtypes
370/38: df["Cabin"] = df["Cabin"].astype(str).str[0]
370/39: df.head()
370/40: df.dtypes
370/41: df.Cabin.Unique()
370/42: df.Cabin.Unique
370/43: df.Cabin.unique
370/44: df["Cabin"].unique
370/45: df["Cabin"].unique
370/46: df["Cabin"].unique()
370/47: df.groupby(["Cabin"])
370/48: df.groupby(["Cabin"])["Survived"]
370/49: df.groupby(["Cabin"])["Survived"].mean()
370/50: df.groupby(["Cabin"])["Survived"].mean().value_sort()
370/51: df.groupby(["Cabin"])["Survived"].mean().sort_values()
370/52: df["Cabin"] = df["Cabin"].astype(str).str[0]
370/53:
df["Cabin"] = df["Cabin"].astype(str).str[0]
df.head()
370/54: labes = df.groupby(["Cabin"])["Survived"].mean().sort_values().index
370/55:
labes = df.groupby(["Cabin"])["Survived"].mean().sort_values().index
labels
370/56:
labels = df.groupby(["Cabin"])["Survived"].mean().sort_values().index
labels
370/57: ordinal_labels = {key:value for key,value in enumerate(labels,0)}
370/58:
ordinal_labels = {key:value for key,value in enumerate(labels,0)}
ordinal_labels
370/59:
ordinal_labels = {key:value for value,key in enumerate(labels,0)}
ordinal_labels
370/60: df["Encoded_value"] = df["Cabin"].map(ordinal_labels)
370/61: df.head()
371/1: import pandas as pd
371/2:
df = pd.read_csv("full.csv", usecols = ["Cabin","Survived"])
df.head()
371/3: #df.isnull().sum()
371/4: df.dropna(subset = ["Survived"], inplace = True)
371/5:
df["Cabin"].fillna("Missing",inplace=True)
df.head()
371/6: df.dtypes
371/7:
df["Cabin"] = df["Cabin"].astype(str).str[0]
df.head()
371/8: df["Cabin"].unique()
371/9: df.groupby(["Cabin"])["Survived"].mean()
371/10: df.groupby(["Cabin"])["Survived"].mean().sort_values()
371/11:
labels = df.groupby(["Cabin"])["Survived"].mean().sort_values().index
labels
371/12:
ordinal_labels = {key:value for value,key in enumerate(labels,0)}
ordinal_labels
371/13: df["Encoded_value"] = df["Cabin"].map(ordinal_labels)
371/14: df.head()
372/1: import pandas as pd
372/2:
df = pd.read_csv("full.csv", usecols = ["Sex"])
df.head()
372/3: pd.getdummies(df.["Sex"])
372/4: pd.getdummies(df["Sex"])
372/5: pd.get_dummies(df["Sex"])
372/6: one_hot = pd.get_dummies(df["Sex"],drop_first=True)
372/7:
one_hot = pd.get_dummies(df["Sex"],drop_first=True)
one_hot
372/8:
one_hot = pd.get_dummies(df["Sex"],drop_first=True)
#one_hot
df = df.concat([df,one_hot],axis=1)
372/9:
one_hot = pd.get_dummies(df["Sex"],drop_first=True)
#one_hot
df = pd.concat([df,one_hot],axis=1)
372/10:
one_hot = pd.get_dummies(df["Sex"],drop_first=True)
#one_hot
df = pd.concat([df,one_hot],axis=1)
df.head()
373/1: import pandas as pd
373/2:
df = pd.read_csv("full.csv", usecols = ["Sex"])
df.head()
373/3: pd.get_dummies(df["Sex"])
373/4:
one_hot = pd.get_dummies(df["Sex"],drop_first=True)
#one_hot
df = pd.concat([df,one_hot],axis=1)
df.head()
373/5:
one_hot = pd.get_dummies(df["Sex"],drop_first=True)
#one_hot
df = pd.concat([df,one_hot],axis=1)
df.head()
374/1: import pandas as pd
374/2:
df = pd.read_csv("full.csv", usecols = ["Sex"])
df.head()
374/3: pd.get_dummies(df["Sex"])
374/4:
one_hot = pd.get_dummies(df["Sex"],drop_first=True)
#one_hot
df = pd.concat([df,one_hot],axis=1)
df.head()
374/5: df.rename(columns={"male":"One_hot_encoded"})
374/6:
df2 = pd.read_csv("full.csv",usecols=["Sex"])
df2.head()
374/7:
import pandas as pd
import numpy as np
374/8:
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop='first',spares=False)
sex = np.array(df2["Sex"]).reshape(-1,1)
sex = encoder.fit_transform(sex)
sex = pd.DataFrame(data=sex,columns=["One_hot_encoded"])
df2 = pd.concat([df2,sex],axis=1)
374/9:
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop='first',sparse=False)
sex = np.array(df2["Sex"]).reshape(-1,1)
sex = encoder.fit_transform(sex)
sex = pd.DataFrame(data=sex,columns=["One_hot_encoded"])
df2 = pd.concat([df2,sex],axis=1)
374/10:
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop='first',sparse=False)
sex = np.array(df2["Sex"]).reshape(-1,1)
sex = encoder.fit_transform(sex)
sex = pd.DataFrame(data=sex,columns=["One_hot_encoded"])
df2 = pd.concat([df2,sex],axis=1)


df2.head()
375/1:
import pandas as pd
import numpy as np
375/2:
df = pd.read_csv("full.csv", usecols = ["Sex"])
df.head()
375/3: pd.get_dummies(df["Sex"])
375/4:
one_hot = pd.get_dummies(df["Sex"],drop_first=True)
#one_hot
df = pd.concat([df,one_hot],axis=1)
df.head()
375/5: df.rename(columns={"male":"One_hot_encoded"})    # to rename any columns in dataframe
375/6:
df2 = pd.read_csv("full.csv",usecols=["Sex"])
df2.head()
375/7:
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop='first',sparse=False)
sex = np.array(df2["Sex"]).reshape(-1,1)
sex = encoder.fit_transform(sex)
sex = pd.DataFrame(data=sex,columns=["One_hot_encoded"])
df2 = pd.concat([df2,sex],axis=1)


df2.head()
375/8: df2.head()
376/1:
import pandas as pd
import numpy as np
376/2:
df = pd.read_csv("full.csv", usecols = ["Sex"])
df.head()
376/3: pd.get_dummies(df["Sex"])
376/4:
one_hot = pd.get_dummies(df["Sex"],drop_first=True)
#one_hot
df = pd.concat([df,one_hot],axis=1)
df.head()
376/5: df.rename(columns={"male":"One_hot_encoded"})    # to rename any columns in dataframe
376/6:
df2 = pd.read_csv("full.csv",usecols=["Sex"])
df2.head()
376/7:
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop='first',sparse=False)
sex = np.array(df2["Sex"]).reshape(-1,1)
sex = encoder.fit_transform(sex)
sex = pd.DataFrame(data=sex,columns=["One_hot_encoded"])
df2 = pd.concat([df2,sex],axis=1)
376/8: df2.head()
376/9:
df.rename(columns={"male":"One_hot_encoded"})  # to rename any columns in dataframe
df.head()
376/10:
df.rename(columns={"male":"One_hot_encoded"}, inplace=True)  # to rename any columns in dataframe
df.head()
377/1:
import pandas as pd
import datetime
377/2: today = datetime.datetime.today()
377/3:
today = datetime.datetime.today()
today
377/4: datetime.datetime.today()
377/5: datetime.datetime.today() - datetime.timedelta(3)
377/6:
days = [datetime.datetime.today() - datetime.timedelta(i) for i in range(20)]
days
377/7:
df = pd.DataFrame(data=days,columns=["Date"])
df.head
377/8:
df = pd.DataFrame(data=days,columns=["Date"])
df.head()
377/9:
df = pd.DataFrame(data=days,columns=["Date"])
df.head()
377/10:
df = pd.DataFrame(data=days,columns=["Date"])
df.head()
377/11: df["Week_days"] = df["Date"].dt.weekday_name
377/12: df["Week_days"] = df["Date"].dt.day_name()
377/13: df.head()
377/14: df["Week_days"] = df["Date"].dt.day_name()
377/15: df.head()
377/16: df["Week_days"] = df["Date"].dt.day_name()
377/17: df.head()
377/18: df["Week_days"] = df["Date"].dt.day_name()
377/19: df.head()
377/20: days = df["Week_days"].to_dict()
377/21: days
377/22: days = {"Monday":1, "Tuesday":2, "Wednesday":3, "Thursday":4, "Friday":5, "Saturday":6, "Sunday":7}
377/23: days
377/24:
days = {"Monday":1, "Tuesday":2, "Wednesday":3, "Thursday":4, "Friday":5, "Saturday":6, "Sunday":7}
days
377/25:
df["Ordinal_encoded"] = df["Week_days"].map(days)
df.head()
378/1:
import pandas as pd
import datetime
378/2: datetime.datetime.today()
378/3: datetime.datetime.today() - datetime.timedelta(3) # to find the three days back date information
378/4:
days = [datetime.datetime.today() - datetime.timedelta(i) for i in range(20)]
days
378/5:
df = pd.DataFrame(data=days,columns=["Date"])
df.head()
378/6: df["Week_days"] = df["Date"].dt.day_name() # to get day name for all days
378/7: df.head()
378/8:
days_name = {"Monday":1, "Tuesday":2, "Wednesday":3, "Thursday":4, "Friday":5, "Saturday":6, "Sunday":7}
days_name
378/9:
df["Ordinal_encoded"] = df["Week_days"].map(days_name)
df.head()
378/10:
days = [datetime.datetime.today() - datetime.timedelta(i) for i in range(20)]
days
378/11:
days = [datetime.datetime.today() - datetime.timedelta(i) for i in range(20)]
#days
378/12:
df2 = pd.DataFrame(data=days,columns=["Date"])
df2.head()
378/13:
df2 = pd.DataFrame(data=days,columns=["Date"])
df["Week_days"] = df["Date"].dt.day_name()
df2.head()
378/14:
df2 = pd.DataFrame(data=days,columns=["Date"])
df2["Week_days"] = df["Date"].dt.day_name()
df2.head()
378/15: from sklearn.preprocessing import OrdinalEncoder
378/16:
import pandas as pd
import numpy as np
import datetime
378/17:
o-encoder = OrdinalEncoder()
x = np.array(df2["Week_daysk"]).reshape(-1,1)
378/18:
o-encoder = OrdinalEncoder()
x = np.array(df2["Week_days"]).reshape(-1,1)
378/19:
o_encoder = OrdinalEncoder()
x = np.array(df2["Week_days"]).reshape(-1,1)
378/20:
o_encoder = OrdinalEncoder()
x = np.array(df2["Week_days"]).reshape(-1,1)
days_column = o_encoder.fit_transform(x)
days_column = pd.DataFrame(data=days_column, columns=["Ordinal_encoded"])
df2 = pd.concat([df2,days_column],axis=1)
378/21: df2.head()
378/22: df2.head(20)
378/23: df2["Week_days"].unique()
378/24: df2["Week_days"].unique().sort()
378/25: df2["Week_days"].unique().sort()
378/26:
c = df2["Week_days"].unique().sort()
c
378/27:
c = df2["Week_days"].unique()
c
378/28:
c = df2["Week_days"].unique().value_sort()
c
378/29:
c = df2["Week_days"].unique().values_sort()
c
378/30:
c = df2["Week_days"].unique().sort_values()
c
378/31:
c = list(df2["Week_days"].unique())
c
378/32:
c = list(df2["Week_days"].unique())
c.sort()
378/33:
c = list(df2["Week_days"].unique())
c.sort()
378/34:
c = list(df2["Week_days"].unique())
c.sort
378/35:
c = list(df2["Week_days"].unique())
c.sort()
print(c)
379/1:
import pandas as pd
import numpy as np
import datetime
379/2: datetime.datetime.today()
379/3: datetime.datetime.today() - datetime.timedelta(3) # to find the three days back date information
379/4:
days = [datetime.datetime.today() - datetime.timedelta(i) for i in range(20)]
days
379/5:
df = pd.DataFrame(data=days,columns=["Date"])
df.head()
379/6: df["Week_days"] = df["Date"].dt.day_name() # to get day name for all days
379/7: df.head()
379/8:
days_name = {"Monday":1, "Tuesday":2, "Wednesday":3, "Thursday":4, "Friday":5, "Saturday":6, "Sunday":7}
days_name
379/9:
df["Ordinal_encoded"] = df["Week_days"].map(days_name)
df.head()
379/10:
days = [datetime.datetime.today() - datetime.timedelta(i) for i in range(20)]
#days
379/11:
df2 = pd.DataFrame(data=days,columns=["Date"])
df2["Week_days"] = df["Date"].dt.day_name()
df2.head()
379/12:
c = list(df2["Week_days"].unique())
c.sort()
print(c)
379/13: from sklearn.preprocessing import OrdinalEncoder
379/14:
o_encoder = OrdinalEncoder()
x = np.array(df2["Week_days"]).reshape(-1,1)
days_column = o_encoder.fit_transform(x)
days_column = pd.DataFrame(data=days_column, columns=["Ordinal_encoded"])
df2 = pd.concat([df2,days_column],axis=1)
379/15: df2.head(20)
380/1:
import pandas as pd
import numpy as np
380/2:
df = pd.read_csv("adult.csv")
df.head()
381/1:
import pandas as pd
import numpy as np
381/2:
df = pd.read_csv("adult.csv")
df.head()
381/3:
for i in range(1,16):
    print(df[i].unique)
381/4: df[1].unique
381/5: df[1].unique()
381/6: df[1]
381/7: df[1]
381/8: df["1"]
381/9:
for i in range(1,16):
    print(df[str(i)].unique)
381/10:
for i in range(1,16):
    print(df[str(i)].unique())
381/11: df.info()
381/12:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
        print(df[str(i)].unique())
381/13:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
        print("Column {}".format(i,df[str(i)].unique()))
381/14:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
        print("Column {} {}".format(i,df[str(i)].unique()))
381/15:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
        print("Column {} {} /n".format(i,df[str(i)].unique()))
381/16:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
        print("Column {} {} \n".format(i,df[str(i)].unique()))
381/17: df[1].unique
381/18: df["1"].unique
381/19: df["1"].unique()
381/20:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
       df[str(i)].replace("?","Unknown")
381/21:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
        print("Column {} {} \n".format(i,df[str(i)].unique()))
381/22:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
       df[str(i)].replace(" ?","Unknown")
381/23:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
        print("Column {} {} \n".format(i,df[str(i)].unique()))
381/24: df["1"].replace("?","Unknown")
381/25: df["2"].replace("?","Unknown")
381/26: df["2"].unique()
381/27: df["2"].replace(" ?","Unknown")
381/28: df["2"].unique()
381/29:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
       df[str(i)].replace(" ?","Unknown",inplace=True)
381/30:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
        print("Column {} {} \n".format(i,df[str(i)].unique()))
381/31: df["2"].unique()
381/32:
columns = []
for in in range(1,16):
    columns.append(str(i))
    
columns
381/33:
columns = []
for i in range(1,16):
    columns.append(str(i))
    
columns
381/34: df = df[columns]
381/35:
column = []
for i in range(1,16):
    column.append(str(i))
column
381/36:
df = df[column]
df.columns
381/37:
column = []
for i in range(1,16):
    if df[str(i)].dtype!="int64":
       column.append(str(i))
381/38:
column = []
for i in range(1,16):
    if df[str(i)].dtype!="int64":
       column.append(str(i))
column
381/39:
column = []
for i in range(1,15):
    if df[str(i)].dtype!="int64":
        column.append(str(i))
column
381/40:
df = df[column]
df.columns=["Employe","Degree","Status","Post","Family_status","Races","Gender","Country"]
381/41:
df = df[column]
df.columns=["Employe","Degree","Status","Post","Family_status","Races","Gender","Country"]
df.head()
381/42:
df = df[column]
df.column=["Employe","Degree","Status","Post","Family_status","Races","Gender","Country"]
df.head()
381/43:
column = []
for i in range(1,15):
    if df[str(i)].dtype!="int64":
        column.append(str(i))
column
381/44:
df = df[column]
df.column=["Employe","Degree","Status","Post","Family_status","Races","Gender","Country"]
df.head()
381/45:
df = df[column]
df.columns=["Employe","Degree","Status","Post","Family_status","Races","Gender","Country"]
df.head()
381/46:
column = []
for i in range(1,15):
    if df[str(i)].dtype!="int64":
        column.append(str(i))
column
382/1:
import pandas as pd
import numpy as np
382/2:
df = pd.read_csv("adult.csv")
df.head()
382/3: df.info()
382/4:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
        print("Column {} {} \n".format(i,df[str(i)].unique()))
382/5:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
       df[str(i)].replace(" ?","Unknown",inplace=True)
382/6:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
        print("Column {} {} \n".format(i,df[str(i)].unique()))
382/7:
column = []
for i in range(1,15):
    if df[str(i)].dtype!="int64":
        column.append(str(i))
column
382/8:
df = df[column]
df.columns=["Employe","Degree","Status","Post","Family_status","Races","Gender","Country"]
df.head()
382/9:
for column in df.columns[:]:
    print("{}, : ,{}, values".format(column,len(df[column].unique())))
382/10:
for column in df.columns[:]:
    print("{}, : {}, values".format(column,len(df[column].unique())))
382/11:
for column in df.columns[:]:
    print("{}, : {} values".format(column,len(df[column].unique())))
382/12:
for column in df.columns[:]:
    print("{} : {} values".format(column,len(df[column].unique())))
382/13: df["Country"].unique()
382/14: df["Country"].value_count()
382/15: df["Country"].values_count()
382/16: df["Country"].count_values()
382/17: df["Country"].value_counts()
382/18: dictt = df["Country"].value_counts().to_dict()
382/19: dictt
382/20:
dictt = df["Country"].value_counts().to_dict()
dictt
382/21:
df["Country"] = df["Country"].map(dictt)
df.head()
383/1:
import pandas as pd
import numpy as np
383/2:
df = pd.read_csv("adult.csv")
df.head()
383/3: df.info()
383/4:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
        print("Column {} {} \n".format(i,df[str(i)].unique()))
383/5:
for i in range(1,16):
    if df[str(i)].dtype!="int64":
       df[str(i)].replace(" ?","Unknown",inplace=True)
383/6:
column = []
for i in range(1,15):
    if df[str(i)].dtype!="int64":
        column.append(str(i))
column
383/7:
df = df[column]
df.columns=["Employe","Degree","Status","Post","Family_status","Races","Gender","Country"]
df.head()
383/8:
for column in df.columns[:]:
    print("{} : {} values".format(column,len(df[column].unique())))
383/9: df["Country"].unique()
383/10: df["Country"].value_counts()
383/11:
dictt = df["Country"].value_counts().to_dict()
dictt
383/12:
df["Country"] = df["Country"].map(dictt)
df.head()
387/1:
import pandas as pd
df = read_csv("full.csv", use)
387/2:
import pandas as pd
df = read_csv("full.csv")
387/3:
import pandas as pd
df = pd.read_csv("full.csv")
387/4:
import pandas as pd
df = pd.read_csv("full.csv")
df.head()
387/5:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Faire"])
df.head()
387/6:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Fare"])
df.head()
387/7: df.isnull().sum()
387/8: df.fillna(df.median(),inplace=True)
387/9: df.isnull().sum()
387/10: df.head()
388/1:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Fare"])
df.head()
388/2: df.fillna(df.median(),inplace=True)
388/3: df.head()
388/4: from sklearn.pre
388/5: from sklearn.preprocessing import StandardScaler
388/6:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df = std_scaler.fit_transform(df)
df.head()
388/7:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df = std_scaler.fit_transform(df)
df
388/8:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df = std_scaler.fit_transform(df)
pd.DataFrame(df)
389/1:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Fare"])
df.head()
389/2: df.fillna(df.median(),inplace=True)
389/3: df.head()
389/4:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df_scal = std_scaler.fit_transform(df)
pd.DataFrame(df_scal)
390/1: import matplotlib.pyplot as plt
391/1:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Fare"])
df.head()
391/2: df.fillna(df.median(),inplace=True)
391/3: df.head()
391/4:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df_scal = std_scaler.fit_transform(df)
pd.DataFrame(df_scal)
391/5: import matplotlib.pyplot as plt
391/6: plt.hist(df["2"])
391/7: plt.hist(df["Fare"],bins=50)
391/8: plt.hist(df["Fare"],bins=20)
391/9: plt.hist(df_scal["Fare"],bins=20)
391/10: plt.hist(df_scal[2],bins=20)
391/11: plt.hist(df_scal["2"],bins=20)
391/12: plt.hist(df_scal[:,"2"],bins=20)
391/13: plt.hist(df_scal[1],bins=20)
391/14: plt.hist(df_scal[0],bins=20)
391/15: plt.hist(df["Age"],bins=20)
391/16: plt.hist(df["Age"],bins=20,density=True)
391/17: plt.hist(df["Age"],bins=20)
391/18:
import matplotlib.pyplot as plt
import seaborn as sns
391/19:
sns.distplot(df['Age'], hist = False, kde = True,
                 kde_kws = {'shade': True, 'linewidth': 3}, 
                  label = airline)
391/20: sns.distplot(df['Age'], hist = False, kde = True,kde_kws = {'shade': True, 'linewidth': 3})
391/21: sns.distplot(df['Age'],bins=20, hist = True, kde = True,kde_kws = {'shade': True, 'linewidth': 3})
391/22: sns.distplot(df['Age'],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
392/1:
import matplotlib.pyplot as plt
import seaborn as sns
392/2:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Fare"])
df.head()
392/3: df.fillna(df.median(),inplace=True)
392/4: df.head()
392/5:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df_scal = std_scaler.fit_transform(df)
pd.DataFrame(df_scal)
392/6: plt.hist(df["Age"],bins=20)
392/7: sns.distplot(df['Age'],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
392/8: sns.distplot(df_scal[2],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
392/9: sns.distplot(df_scal["2"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
392/10: sns.distplot(df_scal[0],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
392/11: sns.distplot(df_scal[:,2],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
392/12: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
392/13: sns.distplot(df["Age"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
392/14: from sklearn.preprocessing import MinMaxScaler
392/15:
mm_scal = MinMaxScaler()
df_minmax = mm_scal.fit_transform(df)
pd.DataFrame(df_minmax)
392/16: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
392/17: from sklearn.preprocessing import RobustScaler
392/18:
r_scal = RobustScaler()
df_r_scal = r_scal.fit_transform(df)
pd.DataFrame(df_r_scal)
df_r_scal.head()
392/19:
r_scal = RobustScaler()
df_r_scal = r_scal.fit_transform(df)
df_r_scal = pd.DataFrame(df_r_scal)
df_r_scal.head()
392/20:
mm_scal = MinMaxScaler()
df_minmax = mm_scal.fit_transform(df)
df_minmax = pd.DataFrame(df_minmax)
df_minmax.head()
393/1:
import matplotlib.pyplot as plt
import seaborn as sns
393/2:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Fare"])
df.head()
393/3: df.fillna(df.median(),inplace=True)
393/4: df.head()
393/5:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df_scal = std_scaler.fit_transform(df)
pd.DataFrame(df_scal)
393/6: plt.hist(df["Age"],bins=20)
393/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
393/8: sns.distplot(df["Age"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
393/9: from sklearn.preprocessing import MinMaxScaler
393/10:
mm_scal = MinMaxScaler()
df_minmax = mm_scal.fit_transform(df)
df_minmax = pd.DataFrame(df_minmax)
df_minmax.head()
393/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
394/1:
import matplotlib.pyplot as plt
import seaborn as sns
394/2:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Fare"])
df.head()
394/3: df.fillna(df.median(),inplace=True)
394/4: df.head()
394/5:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df_scal = std_scaler.fit_transform(df)
pd.DataFrame(df_scal)
394/6: plt.hist(df["Age"],bins=20)
394/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
394/8: sns.distplot(df["Age"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
394/9: from sklearn.preprocessing import MinMaxScaler
394/10:
mm_scal = MinMaxScaler()
df_minmax = mm_scal.fit_transform(df)
pd.DataFrame(df_minmax)
394/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
394/12: from sklearn.preprocessing import RobustScaler
394/13:
r_scal = RobustScaler()
df_r_scal = r_scal.fit_transform(df)
pd.DataFrame(df_r_scal)
394/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
394/15: sns.distplot(df_r_scal[:,2],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
394/16: sns.distplot(df_minmax[:,2],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
394/17: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
394/18: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
395/1:
import matplotlib.pyplot as plt
import seaborn as sns
395/2:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Fare"])
df.head()
395/3: df.fillna(df.median(),inplace=True)
395/4: df.head()
395/5:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df_scal = std_scaler.fit_transform(df)
pd.DataFrame(df_scal)
395/6: plt.hist(df["Age"],bins=20)
395/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
395/8: sns.distplot(df["Age"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
395/9: from sklearn.preprocessing import MinMaxScaler
395/10:
mm_scal = MinMaxScaler()
df_minmax = mm_scal.fit_transform(df)
pd.DataFrame(df_minmax)
395/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
395/12: from sklearn.preprocessing import RobustScaler
395/13:
r_scal = RobustScaler()
df_r_scal = r_scal.fit_transform(df)
pd.DataFrame(df_r_scal)
395/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
397/1:
import matplotlib.pyplot as plt
import seaborn as sns
397/2:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Fare"])
df.head()
397/3: df.fillna(df.median(),inplace=True)
397/4: df.head()
397/5:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df_scal = std_scaler.fit_transform(df)
pd.DataFrame(df_scal)
397/6: plt.hist(df["Age"],bins=20)
397/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
397/8: sns.distplot(df["Age"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
397/9: from sklearn.preprocessing import MinMaxScaler
397/10:
mm_scal = MinMaxScaler()
df_minmax = mm_scal.fit_transform(df)
pd.DataFrame(df_minmax)
397/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
397/12: from sklearn.preprocessing import RobustScaler
397/13:
r_scal = RobustScaler()
df_r_scal = r_scal.fit_transform(df)
pd.DataFrame(df_r_scal)
397/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
397/15: dic = {"Jamshedpur":["ranchi","jharkhand","jugasalai"]}
397/16: dic["Jamshedpur"]
397/17: dic["Jamshedpur"][0]
397/18: dic["Jamshedpur"][1]
397/19: dic["Jamshedpur"][2]
397/20:
jdf = pd.read_csv("Book1.csv")
jdf.head()
397/21:
jdf = pd.read_csv("Book1.csv",encoding= 'unicode_escape')
jdf.head()
397/22: jdf.to_dict()
397/23: ddf = pd.read_excel("vidal-hosp-list.xlsx")
397/24: ddf = pd.read_csv("vidal-hosp-list.xlsx")
397/25: ddf = pd.read_csv("vidal-hosp-list.csv")
397/26:
ddf = pd.read_csv("vidal-hosp-list.csv",encoding= 'unicode_escape')
ddf.head()
397/27: ddf["State"].Unique()
397/28: ddf["State"].Unique
397/29: ddf["State"].unique
397/30: ddf["State"].unique()
397/31: ddf["State"]["Jharkhand"].unique()
397/32: ddf["State"].unique()
397/33: ddf.duplicated()
397/34: df = ddf.duplicated()
397/35: pd.DataFrame(df)
397/36: ddf[ddf.duplicated()]
398/1:
import matplotlib.pyplot as plt
import seaborn as sns
398/2:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Fare"])
df.head()
398/3: df.fillna(df.median(),inplace=True)
398/4: df.head()
398/5:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df_scal = std_scaler.fit_transform(df)
pd.DataFrame(df_scal)
398/6: plt.hist(df["Age"],bins=20)
398/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
398/8: sns.distplot(df["Age"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
398/9: from sklearn.preprocessing import MinMaxScaler
398/10:
mm_scal = MinMaxScaler()
df_minmax = mm_scal.fit_transform(df)
pd.DataFrame(df_minmax)
398/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
398/12: from sklearn.preprocessing import RobustScaler
398/13:
r_scal = RobustScaler()
df_r_scal = r_scal.fit_transform(df)
pd.DataFrame(df_r_scal)
398/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
398/15: dic = {"Jamshedpur":["ranchi","jharkhand","jugasalai"]}
398/16: dic["Jamshedpur"][2]
398/17:
jdf = pd.read_csv("Book1.csv",encoding= 'unicode_escape')
jdf.head()
398/18:
ddf = pd.read_csv("vidal-hosp-list.csv",encoding= 'unicode_escape')
ddf.head()
398/19: ddf["State"].unique()
398/20:
df = ddf[ddf.duplicated()]
df
398/21:
df = ddf[ddf.duplicated("State")]
df
398/22:
df = ddf[ddf.duplicated("State"=="Jharkhand")]
df
398/23:
df = ddf[ddf.duplicated("Jharkhand")]
df
398/24:
df = ddf[ddf.duplicated(ddf["State"]["Jharkhand"])]
df
398/25:
df = ddf[ddf.duplicated(ddf["State"])]
df
398/26:
df = ddf[ddf.duplicated("State")]
df
398/27: d = ddf.groupby("State")
398/28: d
398/29:
for stet, stt in d:
    print(stet)
398/30:
for stet, stt in d:
    print(stet)
    print(stt)
398/31: d.get_group("Jharkhand")
398/32: d.get_group("Bihar")
398/33: ddf = d.get_group("Bihar")
398/34: ddf.head()
398/35: ddf["City"].unique()
398/36: ddf.to_csv("Bihar.csv")
398/37:
ddf["City"].unique()
a = "Jharkhand"
398/38: ddf.to_csv(a+".csv")
398/39: import folium
398/40: import folium
398/41: ddf.head()
398/42:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
map = folium.Map(location=[20.5937, 78.9629] )
398/43: map
398/44: map.save("map1.html")
398/45:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
map = folium.Map(location=[28.70, 77.10] )
398/46: map.save("map1.html")
398/47:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
map = folium.Map(location=[28.70, 77.10], zoom_start=4)
398/48: map.save("map1.html")
398/49:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
map = folium.Map(location=[28.70, 77.10], zoom_start=5)
398/50: map.save("map1.html")
398/51:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
map = folium.Map(location=[28.70, 77.10], zoom_start=15)
398/52: map.save("map1.html")
398/53:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
map = folium.Map(location=[28.70, 77.10], zoom_start=10)
398/54: map.save("map1.html")
398/55:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
map = folium.Map(location=[28.70, 77.10], zoom_start=10,width=50)
398/56: map.save("map1.html")
398/57:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
map = folium.Map(location=[28.70, 77.10], zoom_start=10,width=50%)
398/58:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
map = folium.Map(location=[28.70, 77.10], zoom_start=10,width='50%'')
398/59:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
map = folium.Map(location=[28.70, 77.10], zoom_start=10,width='50%')
398/60: map.save("map1.html")
398/61:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
map = folium.Map(location=[28.70, 77.10], zoom_start=10,width='50%',height='50%')
398/62: map.save("map1.html")
398/63:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
map = folium.Map(location=[28.70, 77.10], zoom_start=10,width='50%',height='50%',top='10%')
398/64: map.save("map1.html")
398/65: from IPython.display import HTML
398/66: HTML(filename='map.html')
398/67: HTML(filename='map1.html')
398/68:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
map = folium.Map(location=[28.70, 77.10], zoom_start=10,width='50%',height='50%',top='10%')
398/69: map.save("map1.html")
398/70: from IPython.display import HTML
398/71: HTML(filename='map1.html')
398/72: map.save("map1.html")
398/73:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
map = folium.Map(location=[28.70, 77.10], zoom_start=10)
398/74: map.save("map1.html")
398/75:
fg = folium.FeatureGroup(name="My map")
for i,j in zip(lon,lat):
    fg.add_child(folium.Marker(location=[i,j],icon=folium.Icon(color="green")))
map.add_child(fg)
398/76:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
map = folium.Map(location=[28.70, 77.10], zoom_start=10)
398/77:
fg = folium.FeatureGroup(name="My map")
for i,j in zip(lon,lat):
    fg.add_child(folium.Marker(location=[i,j],icon=folium.Icon(color="green")))
map.add_child(fg)
398/78:
fg = folium.FeatureGroup(name="My map")
for i,j in zip(lon,lat):
    fg.add_child(folium.Marker(location=[j,i],icon=folium.Icon(color="green")))
map.add_child(fg)
398/79:
lat = ddf["LATITUDE"]
lon = ddf["LONGITUDE"]
print(lat)
map = folium.Map(location=[28.70, 77.10], zoom_start=10)
399/1:
import matplotlib.pyplot as plt
import seaborn as sns
399/2:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Fare"])
df.head()
399/3: df.fillna(df.median(),inplace=True)
399/4: df.head()
399/5:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df_scal = std_scaler.fit_transform(df)
pd.DataFrame(df_scal)
399/6: plt.hist(df["Age"],bins=20)
399/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
399/8: sns.distplot(df["Age"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
399/9: from sklearn.preprocessing import MinMaxScaler
399/10:
mm_scal = MinMaxScaler()
df_minmax = mm_scal.fit_transform(df)
pd.DataFrame(df_minmax)
399/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
399/12: from sklearn.preprocessing import RobustScaler
399/13:
r_scal = RobustScaler()
df_r_scal = r_scal.fit_transform(df)
pd.DataFrame(df_r_scal)
399/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
399/15: dic = {"Jamshedpur":["ranchi","jharkhand","jugasalai"]}
399/16: dic["Jamshedpur"][2]
399/17:
jdf = pd.read_csv("Book1.csv",encoding= 'unicode_escape')
jdf.head()
399/18:
ddf = pd.read_csv("vidal-hosp-list.csv",encoding= 'unicode_escape')
ddf.head()
399/19: ddf["State"].unique()
399/20: d = ddf.groupby("State")
399/21:
for stet, stt in d:
    print(stet)
    print(stt)
399/22: ddf = d.get_group("Bihar")
399/23:
ddf["City"].unique()
a = "Jharkhand"
399/24: ddf.to_csv(a+".csv")
399/25: import folium
399/26: ddf.head()
399/27:
lat = list(ddf["LATITUDE"])
lon = list(ddf["LONGITUDE"])
#print(lat)
map = folium.Map(location=[28.70, 77.10], zoom_start=10)
399/28:
fg = folium.FeatureGroup(name="My map")
for i,j in zip(lon,lat):
    fg.add_child(folium.Marker(location=[i,j],icon=folium.Icon(color="green")))
map.add_child(fg)
399/29: map.save("map1.html")
399/30:
fg = folium.FeatureGroup(name="My map")
for i,j in zip(lat,lon):
    fg.add_child(folium.Marker(location=[i,j],icon=folium.Icon(color="green")))
map.add_child(fg)
399/31:
fg = folium.FeatureGroup(name="My map")
for i,j in zip(lat,lon):
    fg.add_child(folium.Marker(location=[i,j],popup="Hey there",icon=folium.Icon(color="green")))
map.add_child(fg)
399/32:
lat = list(ddf["LATITUDE"])
lon = list(ddf["LONGITUDE"])
#print(lat)
map = folium.Map(location=[25.09, 85.31], zoom_start=10)
399/33:
fg = folium.FeatureGroup(name="My map")
for i,j in zip(lat,lon):
    fg.add_child(folium.Marker(location=[i,j],popup="Hey there",icon=folium.Icon(color="green")))
map.add_child(fg)
399/34:
lat = list(ddf["LATITUDE"])
lon = list(ddf["LONGITUDE"])
h_name = list(ddf["Hospital Name"])
#print(lat)
map = folium.Map(location=[25.09, 85.31], zoom_start=10)
399/35:
fg = folium.FeatureGroup(name="My map")
for i,j,k in zip(lat,lon,h_name):
    fg.add_child(folium.Marker(location=[i,j],popup=k,icon=folium.Icon(color="green")))
map.add_child(fg)
399/36: ddf.head(20)
399/37:
lat = list(ddf["LATITUDE"])
lon = list(ddf["LONGITUDE"])
h_name = list(ddf["Hospital Name"])
#print(lat)
map = folium.Map(location=[lat[0], lon[0]], zoom_start=10)
399/38:
fg = folium.FeatureGroup(name="My map")
for i,j,k in zip(lat,lon,h_name):
    fg.add_child(folium.Marker(location=[i,j],popup=k,icon=folium.Icon(color="green")))
map.add_child(fg)
399/39:
fg = folium.FeatureGroup(name="My map")
for i,j,k in zip(lat,lon,h_name):
    fg.add_child(folium.Marker(location=[i,j],popup=k +str(i)+str(j),icon=folium.Icon(color="green")))
map.add_child(fg)
399/40:
lat = list(ddf["LATITUDE"])
lon = list(ddf["LONGITUDE"])
h_name = list(ddf["Hospital Name"])
print(lat[0],lon[0])
map = folium.Map(location=[lat[0], lon[0]], zoom_start=10)
399/41:
fg = folium.FeatureGroup(name="My map")
for i,j,k in zip(lat,lon,h_name):
    fg.add_child(folium.Marker(location=[i,j],popup=k +"\n"+str(i)+str(j),icon=folium.Icon(color="green")))
map.add_child(fg)
399/42: ddf.head(2)
399/43: h = list(ddf["Hospital Name"])
399/44:
h = list(ddf["Hospital Name"])
h
399/45:
h = list(ddf["Hospital Name"])

dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf["Contact Perso"],ddf["Email"])
    
dic
399/46:
h = list(ddf["Hospital Name"])

dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf["Contact Perso","Email"])
    
dic
399/47:
h = list(ddf["Hospital Name"])

dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf["Contact Perso"])
    
dic
399/48:
h = list(ddf["Hospital Name"])

dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf["Phone No."])
    
dic
399/49:
h = list(ddf["Hospital Name"])

dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf[591])
    
dic
399/50:
h = list(ddf["Hospital Name"])

dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf["591"])
    
dic
399/51:
h = list(ddf["Hospital Name"])

dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[591])
    
dic
399/52:
h = list(ddf["Hospital Name"])

dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[1])
    
dic
399/53:
h = list(ddf["Hospital Name"])

dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[1])
    i+=1
dic
399/54:
h = list(ddf["Hospital Name"])
count=1
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[count])
    count+=1
dic
399/55:
h = list(ddf["Hospital Name"])
count=1
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[1])
    count+=1
dic
399/56:
h = list(ddf["Hospital Name"])
count=1
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[0])
    count+=1
dic
399/57:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index])
    
dic
399/58:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index])
    index+=1
dic
399/59:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10]])
    index+=1
dic
399/60:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10]])
    index+=1
399/61:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
dic
399/62:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
dic['Bishnupur Multispeciality Hospital']
399/63:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
dic['Bishnupur Multispeciality Hospital'][0]
399/64:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
dic['Bishnupur Multispeciality Hospital']
399/65:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,12,11]])
    index+=1
dic['Bishnupur Multispeciality Hospital']
399/66:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
dic['Bishnupur Multispeciality Hospital']
399/67:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
dic['Bishnupur Multispeciality Hospital']
dic.keys
399/68:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
dic['Bishnupur Multispeciality Hospital']
dic.keys()
399/69:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
dic['Bishnupur Multispeciality Hospital']
dic.keys()
399/70:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
dic['Bishnupur Multispeciality Hospital']
399/71:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
dic['Bishnupur Multispeciality Hospital']
399/72: map = folium.Map(location=[lat[0], lon[0]], zoom_start=10)
399/73:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name + df[h_name][0] + df[h_name][1]
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
399/74:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
399/75:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name + dic[h_name][1] 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
399/76:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name + str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
399/77:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name + str(dic[h_name][0]) + str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
399/78:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name+"\n" +str(dic[h_name][0])"\n" + str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
399/79:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name+"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
399/80:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name+"\n\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
399/81:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=h_name+"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
399/82:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],popup=str(h_name)+"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
399/83:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],
                               popup=h_name +"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
399/84: map = folium.Map(location=[lat[0], lon[0]], zoom_start=7)
399/85:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],
                               popup=h_name +"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
399/86:
states = list(ddf["State"].unique())
states
400/1:
import matplotlib.pyplot as plt
import seaborn as sns
400/2:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Fare"])
df.head()
400/3: df.fillna(df.median(),inplace=True)
400/4: df.head()
400/5:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df_scal = std_scaler.fit_transform(df)
pd.DataFrame(df_scal)
400/6: plt.hist(df["Age"],bins=20)
400/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
400/8: sns.distplot(df["Age"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
400/9: from sklearn.preprocessing import MinMaxScaler
400/10:
mm_scal = MinMaxScaler()
df_minmax = mm_scal.fit_transform(df)
pd.DataFrame(df_minmax)
400/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
400/12: from sklearn.preprocessing import RobustScaler
400/13:
r_scal = RobustScaler()
df_r_scal = r_scal.fit_transform(df)
pd.DataFrame(df_r_scal)
400/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
400/15: dic = {"Jamshedpur":["ranchi","jharkhand","jugasalai"]}
400/16: dic["Jamshedpur"][2]
400/17:
jdf = pd.read_csv("Book1.csv",encoding= 'unicode_escape')
jdf.head()
400/18:
ddf = pd.read_csv("vidal-hosp-list.csv",encoding= 'unicode_escape')
ddf.head()
400/19:
states = list(ddf["State"].unique())
states
400/20: d = ddf.groupby("State")
400/21:
for stet, stt in d:
    print(stet)
    print(stt)
400/22: ddf = d.get_group("Bihar")
400/23:
ddf["City"].unique()
a = "Jharkhand"
400/24: ddf.to_csv(a+".csv")
400/25: import folium
400/26: ddf.head(2)
400/27:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
dic['Bishnupur Multispeciality Hospital']
400/28: map = folium.Map(location=[lat[0], lon[0]], zoom_start=7)
400/29: map = folium.Map(location=[25.45, 86.27], zoom_start=7)
400/30:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],
                               popup=h_name +"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
400/31:
for state in states:
    if state == "Jharkhand":
        ddf = d.get_group(state)
400/32:
ddf["City"].unique()
a = "Jharkhand"
400/33: ddf.to_csv(a+".csv")
400/34: import folium
400/35: ddf.head(2)
400/36:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
dic['Bishnupur Multispeciality Hospital']
400/37: map = folium.Map(location=[25.45, 86.27], zoom_start=7)
400/38:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
400/39: map = folium.Map(location=[25.45, 86.27], zoom_start=7)
400/40:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],
                               popup=h_name +"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
400/41: ddf
401/1:
import matplotlib.pyplot as plt
import seaborn as sns
401/2:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Fare"])
df.head()
401/3: df.fillna(df.median(),inplace=True)
401/4: df.head()
401/5:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df_scal = std_scaler.fit_transform(df)
pd.DataFrame(df_scal)
401/6: plt.hist(df["Age"],bins=20)
401/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
401/8: sns.distplot(df["Age"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
401/9: from sklearn.preprocessing import MinMaxScaler
401/10:
mm_scal = MinMaxScaler()
df_minmax = mm_scal.fit_transform(df)
pd.DataFrame(df_minmax)
401/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
401/12: from sklearn.preprocessing import RobustScaler
401/13:
r_scal = RobustScaler()
df_r_scal = r_scal.fit_transform(df)
pd.DataFrame(df_r_scal)
401/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
401/15: dic = {"Jamshedpur":["ranchi","jharkhand","jugasalai"]}
401/16: dic["Jamshedpur"][2]
401/17:
jdf = pd.read_csv("Book1.csv",encoding= 'unicode_escape')
jdf.head()
401/18:
ddf = pd.read_csv("vidal-hosp-list.csv",encoding= 'unicode_escape')
ddf.head()
401/19:
states = list(ddf["State"].unique())
states
401/20: d = ddf.groupby("State")
401/21:
for state in states:
    if state == "Jharkhand":
        ddf = d.get_group(state)
401/22:
ddf["City"].unique()
a = "Jharkhand"
401/23: ddf.to_csv(a+".csv")
401/24: import folium
401/25: ddf
401/26:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
401/27: map = folium.Map(location=[25.45, 86.27], zoom_start=7)
401/28:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],
                               popup=h_name +"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
401/29: map.save("map1.html")
401/30:
for state in states:
    if state == "West Bengal":
        ddf = d.get_group(state)
401/31: ddf["City"].unique()
401/32: ddf.to_csv(a+".csv")
401/33: import folium
401/34: ddf
401/35:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
401/36: map = folium.Map(location=[25.45, 86.27], zoom_start=7)
401/37:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],
                               popup=h_name +"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
401/38:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],
                               popup=h_name +"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
401/39: len(ddf["Hospital Name"].unique())
401/40:
for state in states:
    if state == "Bihar":
        ddf = d.get_group(state)
401/41: ddf["City"].unique()
401/42: ddf.to_csv(a+".csv")
401/43: import folium
401/44: len(ddf["Hospital Name"].unique())
401/45:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
401/46: map = folium.Map(location=[25.45, 86.27], zoom_start=7)
401/47:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],
                               popup=h_name +"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
401/48:
for state in states:
    if state == "Jharkhand":
        ddf = d.get_group(state)
401/49: ddf["City"].unique()
401/50: ddf.to_csv(a+".csv")
401/51: import folium
401/52: len(ddf["Hospital Name"].unique())
401/53:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
401/54: map = folium.Map(location=[25.45, 86.27], zoom_start=7)
401/55:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],
                               popup=h_name +"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
401/56:
for state in states:
    if state == "Odisha":
        ddf = d.get_group(state)
401/57: ddf["City"].unique()
401/58: ddf.to_csv(a+".csv")
401/59: import folium
401/60: len(ddf["Hospital Name"].unique())
401/61:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
401/62: map = folium.Map(location=[25.45, 86.27], zoom_start=7)
401/63:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],
                               popup=h_name +"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
401/64:
for state in states:
    if state == "Jharkhand":
        ddf = d.get_group(state)
401/65: ddf["City"].unique()
401/66: ddf.to_csv(a+".csv")
401/67: import folium
401/68: len(ddf["Hospital Name"].unique())
401/69:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
401/70: map = folium.Map(location=[25.45, 86.27], zoom_start=7)
401/71:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],
                               popup=h_name +"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
402/1: dic = {"Jamshedpur":["ranchi","jharkhand","jugasalai"]}
402/2: dic["Jamshedpur"][2]
402/3:
jdf = pd.read_csv("Book1.csv",encoding= 'unicode_escape')
jdf.head()
402/4:
ddf = pd.read_csv("vidal-hosp-list.csv",encoding= 'unicode_escape')
ddf.head()
402/5:
states = list(ddf["State"].unique())
states
402/6: d = ddf.groupby("State")
402/7:
for state in states:
    if state == "Jharkhand":
        ddf = d.get_group(state)
403/1:
import matplotlib.pyplot as plt
import seaborn as sns
403/2:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Fare"])
df.head()
403/3: df.fillna(df.median(),inplace=True)
403/4: df.head()
403/5:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df_scal = std_scaler.fit_transform(df)
pd.DataFrame(df_scal)
403/6: plt.hist(df["Age"],bins=20)
403/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
403/8: sns.distplot(df["Age"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
403/9: from sklearn.preprocessing import MinMaxScaler
403/10:
mm_scal = MinMaxScaler()
df_minmax = mm_scal.fit_transform(df)
pd.DataFrame(df_minmax)
403/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
403/12: from sklearn.preprocessing import RobustScaler
403/13:
r_scal = RobustScaler()
df_r_scal = r_scal.fit_transform(df)
pd.DataFrame(df_r_scal)
403/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
404/1: import pandas as pd
404/2: dic = {"Jamshedpur":["ranchi","jharkhand","jugasalai"]}
404/3: dic["Jamshedpur"][2]
404/4:
jdf = pd.read_csv("Book1.csv",encoding= 'unicode_escape')
jdf.head()
404/5:
ddf = pd.read_csv("vidal-hosp-list.csv",encoding= 'unicode_escape')
ddf.head()
404/6:
states = list(ddf["State"].unique())
states
404/7: d = ddf.groupby("State")
404/8:
for state in states:
    if state == "Jharkhand":
        ddf = d.get_group(state)
404/9: ddf["City"].unique()
404/10: ddf.to_csv(a+".csv")
405/1: import pandas as pd
405/2: dic = {"Jamshedpur":["ranchi","jharkhand","jugasalai"]}
405/3: dic["Jamshedpur"][2]
405/4:
jdf = pd.read_csv("Book1.csv",encoding= 'unicode_escape')
jdf.head()
405/5:
ddf = pd.read_csv("vidal-hosp-list.csv",encoding= 'unicode_escape')
ddf.head()
405/6:
states = list(ddf["State"].unique())
states
405/7: d = ddf.groupby("State")
405/8:
for state in states:
    if state == "Jharkhand":
        ddf = d.get_group(state)
405/9: ddf["City"].unique()
405/10: import folium
405/11: len(ddf["Hospital Name"].unique())
405/12:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
405/13: map = folium.Map(location=[25.45, 86.27], zoom_start=7)
405/14:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],
                               popup=h_name +"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
403/15: 24*60
403/16: 3000/1440
403/17: 30*70
406/1:
import matplotlib.pyplot as plt
import seaborn as sns
406/2:
import pandas as pd
df = pd.read_csv("full.csv",usecols=["Pclass","Age","Fare"])
df.head()
406/3: df.fillna(df.median(),inplace=True)
406/4: df.head()
406/5:
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
df_scal = std_scaler.fit_transform(df)
pd.DataFrame(df_scal)
406/6: plt.hist(df["Age"],bins=20)
406/7: sns.distplot(df_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
406/8: sns.distplot(df["Age"],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
406/9: from sklearn.preprocessing import MinMaxScaler
406/10:
mm_scal = MinMaxScaler()
df_minmax = mm_scal.fit_transform(df)
pd.DataFrame(df_minmax)
406/11: sns.distplot(df_minmax[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
406/12: from sklearn.preprocessing import RobustScaler
406/13:
r_scal = RobustScaler()
df_r_scal = r_scal.fit_transform(df)
pd.DataFrame(df_r_scal)
406/14: sns.distplot(df_r_scal[:,1],bins=20, hist = True, kde = True,kde_kws = {'shade': False, 'linewidth': 2})
406/15: 24*60
406/16: 30*70
407/1:
import pandas as pd
import numpy as np
407/2: df = pd.read_csv("full.csv")
407/3: df.head()
407/4:
df = pd.read_csv("full.csv")
df.head()
407/5:
df = pd.read_csv("full.csv")
df.head(5)
407/6: df["Age"].isnull().sum()
407/7: df["Age"].fillna(100)
407/8: import seaborn as sns
407/9: sns.distplot(df)
407/10: sns.distplot(df["Age"])
407/11: df["Age"].fillna(100,inplace=True)
407/12: import seaborn as sns
407/13: sns.distplot(df["Age"])
407/14: df.boxplot(column="Age")
407/15: sns.distplot(df["Age"].fillna(100))
408/1:
import pandas as pd
import numpy as np
408/2:
df = pd.read_csv("full.csv")
df.head(5)
408/3: df["Age"].isnull().sum()
408/4: import seaborn as sns
408/5: sns.distplot(df["Age"].fillna(100))
408/6: df.boxplot(column="Age")
406/17:
from flask import Flask, render_template
from flask_googlemaps import GoogleMaps
from flask_googlemaps import Map

app = Flask(__name__, template_folder=".")
GoogleMaps(app)

@app.route("/")
def mapview():
    # creating a map in the view
    mymap = Map(
        identifier="view-side",
        lat=37.4419,
        lng=-122.1419,
        markers=[(37.4419, -122.1419)]
    )
    sndmap = Map(
        identifier="sndmap",
        lat=37.4419,
        lng=-122.1419,
        markers=[
          {
             'icon': 'http://maps.google.com/mapfiles/ms/icons/green-dot.png',
             'lat': 37.4419,
             'lng': -122.1419,
             'infobox': "<b>Hello World</b>"
          },
          {
             'icon': 'http://maps.google.com/mapfiles/ms/icons/blue-dot.png',
             'lat': 37.4300,
             'lng': -122.1400,
             'infobox': "<b>Hello World from other place</b>"
          }
        ]
    )
    return render_template('example.html', mymap=mymap, sndmap=sndmap)

if __name__ == "__main__":
    app.run(debug=True)
410/1: import pandas as pd
410/2: dic = {"Jamshedpur":["ranchi","jharkhand","jugasalai"]}
410/3: dic["Jamshedpur"][2]
410/4:
jdf = pd.read_csv("Book1.csv",encoding= 'unicode_escape')
jdf.head()
410/5:
ddf = pd.read_csv("vidal-hosp-list.csv",encoding= 'unicode_escape')
ddf.head()
410/6:
states = list(ddf["State"].unique())
states
410/7: d = ddf.groupby("State")
410/8:
for state in states:
    if state == "Jharkhand":
        ddf = d.get_group(state)
410/9: ddf["City"].unique()
410/10: import folium
410/11: len(ddf["Hospital Name"].unique())
410/12:
h = list(ddf["Hospital Name"])
index=0
dic = dict()
for i in h:
    dic[i] = list()
    dic[i].extend(ddf.iloc[index,[6,10,11,12]])
    index+=1
410/13: map = folium.Map(location=[25.45, 86.27], zoom_start=7)
410/14:
fg = folium.FeatureGroup(name="My map")
for h_name in dic.keys():
    fg.add_child(folium.Marker(location=[dic[h_name][2],dic[h_name][3]],
                               popup=h_name +"\n"+str(dic[h_name][0])+"\n"+str(dic[h_name][1]) 
                               ,icon=folium.Icon(color="green")))
map.add_child(fg)
410/15:
d = ['abcdefghijklmnopqrstuvwxyz']
alpha = list(set(d))
alpha
410/16:
d = ['abcdefghijklmnopqrstuvwxyz']
alpha = list(d.set())
alpha
410/17:
d = ['abcdefghijklmnopqrstuvwxyz']
alpha = (d.set())
alpha
410/18:
d = 'abcdefghijklmnopqrstuvwxyz'
alpha = list(d.set())
alpha
410/19:
d = 'abcdefghijklmnopqrstuvwxyz'
alpha = list(set(d))
alpha
410/20:
d = 'abcdefghijklmnopqrstuvwxyz'
alpha = list(set(d))
alpha.sort()
410/21:
d = 'abcdefghijklmnopqrstuvwxyz'
alpha = list(set(d))
alpha.sort()
alpha
410/22: alpha[::-1]
410/23: alpha[::-2]
410/24: alpha[::-3]
410/25: alpha[:-1]
410/26: alpha[:-2]
410/27: alpha[:1:-1]
410/28: alpha[:0:-1]
410/29: alpha[::-1]
410/30: alpha[0::-1]
410/31: alpha[25::-1]
410/32: alpha[24::-1]
410/33: alpha[::-1]
410/34: alpha[:1:-1]
410/35: alpha[::-1]
410/36: alpha[::]
410/37: mat = alpha[0::]
410/38:
mat = alpha[0::]
mat
410/39:
mat = alpha[1::]
mat
410/40:
mat = alpha[1::]
mat = mat.append(aplha[1])
410/41:
mat = alpha[1::]
mat = mat.append(alpha[1])
410/42:
mat = alpha[1::]
mat = mat.append(alpha[1])
mat
410/43:
mat = alpha[1::]
mat = mat.append(alpha[1])
mat
410/44:
mat = alpha[1::]
mat = mat.append(alpha[1])
print(mat)
410/45:
mat = alpha[1::]
print(type(mat))
mat = mat.append(alpha[1])
410/46:
mat = alpha[1::]
print(type(mat))
mat.append(alpha[1])
mat
410/47:
mat = alpha[1::]
print(type(mat))
mat.append(alpha[0])
mat
410/48:
mat = alpha[1::]
print(type(mat))
mat.append(alpha[0:0])
mat
410/49:
mat = alpha[1::]
print(type(mat))
mat.append(alpha[0:])
mat
410/50:
mat = alpha[1::]
print(type(mat))
mat.append(alpha[0:1])
mat
410/51:
mat = alpha[1::]
print(type(mat))
mat.extend(alpha[0:1])
mat
410/52:
mat = alpha[1::]
print(type(mat))
mat.extend(alpha[0:2])
mat
410/53:
matrix = []

for i in range(26):
    a = []
    for j in range(26):
        a = alpha[::]
    matrix.append(a)
410/54: matrix
410/55:
for i in range(26):
    for j in range(26):
        print(matrix[i][j], end=" ")
    print()
410/56:
matrix = []

for i in range(26):
    a = []
    if i!=0:
        a = alpha[::]
        matrix.append(a)
    else:
        a = alpha[i::]
        a.extend(alpha[0:i+1])
        matrix.append(a)
410/57:
for i in range(26):
    for j in range(26):
        print(matrix[i][j], end=" ")
    print()
410/58:
d = 'abcdefghijklmnopqrstuvwxyz'
alpha = list(set(d))
alpha.sort()
alpha
410/59:
mat = alpha[1::]
print(type(mat))
mat.extend(alpha[0:2])
mat
410/60:
matrix = []

for i in range(26):
    a = []
    if i!=0:
        a = alpha[::]
        matrix.append(a)
    else:
        a = alpha[i::]
        a.extend(alpha[0:i+1])
        matrix.append(a)
410/61:
for i in range(26):
    for j in range(26):
        print(matrix[i][j], end=" ")
    print()
410/62:
matrix = []

for i in range(26):
    a = []
    if i==0:
        a = alpha[::]
        matrix.append(a)
    else:
        a = alpha[i::]
        a.extend(alpha[0:i+1])
        matrix.append(a)
410/63:
for i in range(26):
    for j in range(26):
        print(matrix[i][j], end=" ")
    print()
410/64:
plane_text = "we are discovered save yourself"
key = "deceptive"
len_key = len(key)
410/65:
plane_text = "we are discovered save yourself"
key = "deceptive"
len_key = len(key)
print(len_key)
flag = 0
410/66:
d = 'abcdefghijklmnopqrstuvwxyz'
alpha = list(set(d))
alpha.sort()
alpha["a"].index()
410/67:
d = 'abcdefghijklmnopqrstuvwxyz'
alpha = list(set(d))
alpha.sort()
alpha.index("a")
410/68:
cypher_text = []
for i in plane_text:
    if flag==len_key:
        flag=0
    cypher_text.append(matrix[alpha.index[i],alpha.index[key[flag]]])
410/69:
cypher_text = []
for i in plane_text:
    if flag==len_key:
        flag=0
    cypher_text.append(matrix[alpha.index[i],alpha.index[key[flag]]])
    flag+=1
410/70:
for i in plane_text:
    print(alpha.index[i])
410/71:
for i in plane_text:
    if i!=" ":
        print(alpha.index[i])
410/72:
for i in plane_text:
    print(i)
    if i!=" ":
        print(alpha.index[i])
410/73:
for i in plane_text:
    print(type(i))
    if i!=" ":
        print(alpha.index[i])
410/74:
for i in plane_text:
    print(type(" "))
    if i!=" ":
        print(alpha.index[i])
410/75:
for i in plane_text:
    print(type(i))
    if i!=" ":
        print(alpha.index[i])
410/76:
for i in plane_text:
    print(type(i))
    if i!=" ":
        print(alpha.index["w"])
410/77:
for i in plane_text:
    print(type(i))
    if i!=" ":
        print(alpha.index(i))
410/78:
cypher_text = []
for i in plane_text:
    if i!=" ":
        if flag==len_key:
            flag=0
            cypher_text.append(matrix[alpha.index(i),alpha.index(key[flag]))
            flag+=1
410/79:
cypher_text = []
for i in plane_text:
    if i!=" ":
        if flag==len_key:
            flag=0
            cypher_text.append(matrix[alpha.index(i),alpha.index(key[flag]])
            flag+=1
410/80:
cypher_text = []
for ele in plane_text:
    if ele!=" ":
        if flag==len_key:
            flag=0
        i = alpha.index(ele)
        j = alpha.index(key[flag])
        cypher_text.append(matrix[i][j])
        flag+=1
410/81: cypher_text
410/82: cypher_text = "".join(cypher_text)
410/83:
cypher_text = "".join(cypher_text)
cypher_text
414/1: df["Age"].isnull().sum()
414/2:
import pandas as pd
import numpy as np
414/3:
df = pd.read_csv("full.csv")
df.head(5)
414/4: df["Age"].isnull().sum()
414/5: import seaborn as sns
414/6: sns.distplot(df["Age"].fillna(100))
414/7: df.boxplot(column="Age")
414/8:
import pandas as pd
import numpy as np
414/9:
df = pd.read_csv("full.csv")
df.head(5)
414/10: df["Age"].isnull().sum()
414/11: import seaborn as sns
414/12: sns.distplot(df["Age"].fillna(100))
414/13: df.boxplot(column="Age")
414/14:
import pandas as pd
import numpy as np
414/15:
df = pd.read_csv("full.csv")
df.head(5)
414/16: df["Age"].isnull().sum()
414/17: import seaborn as sns
414/18: sns.distplot(df["Age"].fillna(100))
414/19: df.boxplot(column="Age")
414/20:
import pandas as pd
import numpy as np
414/21:
df = pd.read_csv("full.csv")
df.head(5)
414/22: df["Age"].isnull().sum()
414/23: import seaborn as sns
414/24: sns.distplot(df["Age"].fillna(100))
414/25: df.boxplot(column="Age")
415/1:
import pandas as pd
import numpy as np
415/2:
df = pd.read_csv("full.csv")
df.head(5)
415/3: df["Age"].isnull().sum()
415/4: import seaborn as sns
415/5: sns.distplot(df["Age"].fillna(100))
415/6: df.boxplot(column="Age")
415/7: sns.distplot(df["Age"].fillna(100))
416/1:
import pandas as pd
import numpy as np
416/2:
df = pd.read_csv("full.csv")
df.head(5)
416/3: df["Age"].isnull().sum()
416/4: import seaborn as sns
416/5: sns.distplot(df["Age"].fillna(100))
416/6: df.boxplot(column="Age")
417/1: import this
420/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
420/2: warnings.filterwarnings("ignore")
420/3: data = pd.read_csv("diabetes.csv")
420/4: data.head()
420/5:
print(data.columns)
data.shape
420/6:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
420/7: data['Insulin'].replace(0, np.nan, inplace=True)
420/8: data.isnull().sum()
420/9: data.head()
420/10: data["Insulin"].fillna(value=data["Insulin"].median(), inplace=True)
420/11: data.info()
420/12: data.head()
420/13: data.describe()
420/14:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
420/15:
plt.figure(figsize=(15,8))
sns.heatmap(data)
420/16: #sns.pairplot(data, hue="Outcome")
420/17: data.corr()
420/18:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
420/19:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
420/20:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
420/21:
impute = SimpleImputer(missing_values=0, strategy="mean")
x_data = impute.fit_transform(x_data)
x_data
420/22:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
420/23:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
420/24:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
420/25:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
420/26:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
420/27:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
420/28:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
420/29:
 #Randomized Search CV

# Hyperparameter optimization    
    
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
420/30:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
420/31:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
420/32: rf_random.fit(std_x_train,y_train)
420/33: rf_random.best_params_
420/34: predictions=rf_random.predict(std_x_test)
420/35: accuracy_score(y_test,predictions)
420/36:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
420/37:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
420/38:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
420/39:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
421/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
421/2: warnings.filterwarnings("ignore")
421/3: data = pd.read_csv("diabetes.csv")
421/4: data.head()
421/5:
print(data.columns)
data.shape
421/6:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
421/7: data['Insulin'].replace(0, np.nan, inplace=True)
421/8: data.isnull().sum()
421/9: data.head()
421/10: data["Insulin"].fillna(value=data["Insulin"].median(), inplace=True)
421/11: data.info()
421/12: data.head()
421/13: data.describe()
421/14:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
421/15:
plt.figure(figsize=(15,8))
sns.heatmap(data)
421/16: #sns.pairplot(data, hue="Outcome")
421/17: data.corr()
421/18:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
421/19:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
421/20:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
421/21:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
421/22:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
421/23:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
421/24:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
421/25:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
421/26:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
421/27:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
421/28:
 #Randomized Search CV

# Hyperparameter optimization    
    
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
421/29:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
421/30:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
421/31: rf_random.fit(std_x_train,y_train)
421/32: rf_random.best_params_
421/33: predictions=rf_random.predict(std_x_test)
421/34: accuracy_score(y_test,predictions)
421/35:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
421/36:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
421/37:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
421/38:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
421/39:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3, random_state=42)
x_train
421/40:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.3, random_state=42)
std_x_train
421/41:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
421/42:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
421/43:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
421/44:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
421/45:
 #Randomized Search CV

# Hyperparameter optimization    
    
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
421/46:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
421/47:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
421/48: rf_random.fit(std_x_train,y_train)
421/49: rf_random.best_params_
421/50: predictions=rf_random.predict(std_x_test)
421/51: accuracy_score(y_test,predictions)
421/52:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
421/53:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
421/54:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
421/55:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
421/56:
data['Insulin'].replace(0, np.nan, inplace=True)
data['SkinThickness'].replace(0, np.nan, inplace=True)
421/57: data = pd.read_csv("diabetes.csv")
421/58: data.head()
421/59:
print(data.columns)
data.shape
421/60:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
421/61:
data['Insulin'].replace(0, np.nan, inplace=True)
data['SkinThickness'].replace(0, np.nan, inplace=True)
421/62: data.isnull().sum()
421/63:
columns_name = data.columns
columns_name
421/64:
columns_name = data.columns
columns_name[0]
421/65:
columns_name = data.columns
for ele in columns_name:
    print(data[ele].unique())
421/66:
columns_name = data.columns
for ele in columns_name:
    print(ele,data[ele].unique())
421/67:
columns_name = data.columns
for ele in columns_name:
    print(ele,"/n",data[ele].unique())
421/68:
columns_name = data.columns
for ele in columns_name:
    print(ele,"\n",data[ele].unique())
421/69:
columns_name = data.columns
for ele in columns_name:
    print(data[ele].unique().count(0))
421/70:
columns_name = data.columns
for ele in columns_name:
    print(ele,"\n",data[ele].unique())
421/71: data = pd.read_csv("diabetes.csv")
421/72: data.head()
421/73:
print(data.columns)
data.shape
421/74:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
421/75:
columns_name = data.columns
for ele in columns_name:
    print(ele,"\n",data[ele].unique())
421/76:
columns_name = data.columns
for ele in columns_name:
    if 0 in data[ele].unique():
        print(ele,":","True")
421/77:
for ele in columns_name:
    if ele!="Pregnancies" or ele!="Outcome":
        data[ele].replace(0, np.nan, inplace=True)
421/78: data.isnull().sum()
421/79:
for ele in columns_name:
    if ele!="Pregnancies" and ele!="Outcome":
        data[ele].replace(0, np.nan, inplace=True)
421/80: data.isnull().sum()
421/81: data = pd.read_csv("diabetes.csv")
421/82: data.head()
421/83:
print(data.columns)
data.shape
421/84:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
421/85:
columns_name = data.columns
for ele in columns_name:
    if 0 in data[ele].unique():
        print(ele,":","True")
421/86:
for ele in columns_name:
    if ele!="Pregnancies" and ele!="Outcome":
        data[ele].replace(0, np.nan, inplace=True)
421/87: data.isnull().sum()
421/88: data.head()
421/89:
for ele in columns_name:
    data[ele].fillna(value=data["Insulin"].median(), inplace=True)
421/90: data.head()
421/91: data.info()
421/92: data.head()
421/93: data.describe()
421/94:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
421/95:
plt.figure(figsize=(15,8))
sns.heatmap(data)
421/96: #sns.pairplot(data, hue="Outcome")
421/97: data.corr()
421/98:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
421/99:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
421/100:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
421/101:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
421/102:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3, random_state=42)
x_train
421/103:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.3, random_state=42)
std_x_train
421/104:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
421/105:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
421/106:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
421/107:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
422/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
422/2: warnings.filterwarnings("ignore")
422/3: data = pd.read_csv("diabetes.csv")
422/4: data.head()
422/5:
print(data.columns)
data.shape
422/6:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
422/7:
columns_name = data.columns
for ele in columns_name:
    if 0 in data[ele].unique():
        print(ele,":","True")
422/8:
for ele in columns_name:
    if ele!="Pregnancies" and ele!="Outcome":
        data[ele].replace(0, np.nan, inplace=True)
422/9: data.isnull().sum()
422/10: data.head()
422/11:
for ele in columns_name:
    data[ele].fillna(value=data[ele].median(), inplace=True)
422/12: data.info()
422/13: data.head()
422/14: data.describe()
422/15:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
422/16:
plt.figure(figsize=(15,8))
sns.heatmap(data)
422/17: #sns.pairplot(data, hue="Outcome")
422/18: data.corr()
422/19:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
422/20:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
422/21:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
422/22:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
422/23:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3, random_state=42)
x_train
422/24:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.3, random_state=42)
std_x_train
422/25:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
422/26:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
422/27:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
422/28:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
422/29:
 #Randomized Search CV

# Hyperparameter optimization    
    
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
422/30:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
422/31:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
422/32: rf_random.fit(std_x_train,y_train)
422/33: rf_random.best_params_
422/34: predictions=rf_random.predict(std_x_test)
422/35: accuracy_score(y_test,predictions)
422/36:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
422/37:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
422/38:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
422/39:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
422/40: sns.boxplot(data["Age"])
422/41: data["Age"].unique()
422/42: sns.boxplot(data["SkinThickness"])
423/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
423/2: warnings.filterwarnings("ignore")
423/3: data = pd.read_csv("diabetes.csv")
423/4: data.head()
423/5:
print(data.columns)
data.shape
423/6:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
423/7:
columns_name = data.columns
for ele in columns_name:
    if 0 in data[ele].unique():
        print(ele,":","True")
423/8:
for ele in columns_name:
    if ele!="Pregnancies" and ele!="Outcome":
        data[ele].replace(0, np.nan, inplace=True)
423/9: data.isnull().sum()
423/10: data.head()
423/11:
for ele in columns_name:
    data[ele].fillna(value=data[ele].median(), inplace=True)
423/12: data["Age"].unique()
423/13: sns.boxplot(data["SkinThickness"])
423/14: data.info()
423/15: data.head()
423/16: data.describe()
423/17:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
423/18:
plt.figure(figsize=(15,8))
sns.heatmap(data)
423/19: #sns.pairplot(data, hue="Outcome")
423/20: data.corr()
423/21:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
423/22:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
423/23:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
423/24:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
423/25:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3, random_state=42)
x_train
423/26:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.3, random_state=42)
std_x_train
423/27:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
423/28:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
423/29:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
423/30:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
423/31:
 #Randomized Search CV

# Hyperparameter optimization    
    
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
423/32:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
423/33:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
423/34: rf_random.fit(std_x_train,y_train)
423/35: rf_random.best_params_
423/36: predictions=rf_random.predict(std_x_test)
423/37: accuracy_score(y_test,predictions)
423/38:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
423/39:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
423/40:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
423/41:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
424/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
424/2: warnings.filterwarnings("ignore")
424/3: data = pd.read_csv("diabetes.csv")
424/4: data.head()
424/5:
print(data.columns)
data.shape
424/6:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
424/7:
columns_name = data.columns
for ele in columns_name:
    if 0 in data[ele].unique():
        print(ele,":","True")
424/8:
for ele in columns_name:
    if ele!="Pregnancies" and ele!="Outcome":
        data[ele].replace(0, np.nan, inplace=True)
424/9: data.isnull().sum()
424/10: data.head()
424/11:
for ele in columns_name:
    data[ele].fillna(value=data[ele].median(), inplace=True)
424/12: data["Age"].unique()
424/13: sns.boxplot(data["SkinThickness"])
424/14: data.info()
424/15: data.head()
424/16: data.describe()
424/17:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
424/18:
plt.figure(figsize=(15,8))
sns.heatmap(data)
424/19: #sns.pairplot(data, hue="Outcome")
424/20: data.corr()
424/21:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
424/22:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
424/23:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
424/24:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
424/25:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3, random_state=42)
x_train
424/26:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.3, random_state=42)
std_x_train
424/27:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
424/28:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
424/29:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
424/30:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
424/31:
 #Randomized Search CV

# Hyperparameter optimization    
    
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
424/32:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
424/33:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
424/34: rf_random.fit(std_x_train,y_train)
424/35: rf_random.best_params_
424/36: predictions=rf_random.predict(std_x_test)
424/37: accuracy_score(y_test,predictions)
424/38:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
424/39:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
424/40:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
424/41:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
424/42:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
data["Age"].plot(kind = "kde", ax = ax)

line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
424/43:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
data["SkinThickness"].plot(kind = "kde", ax = ax)

line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
424/44:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
data["Glucose"].plot(kind = "kde", ax = ax)

line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
425/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
425/2: warnings.filterwarnings("ignore")
425/3: data = pd.read_csv("diabetes.csv")
425/4: data.head()
425/5:
print(data.columns)
data.shape
425/6:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
425/7:
columns_name = data.columns
for ele in columns_name:
    if 0 in data[ele].unique():
        print(ele,":","True")
425/8:
for ele in columns_name:
    if ele!="Pregnancies" and ele!="Outcome":
        data[ele].replace(0, np.nan, inplace=True)
425/9: data.isnull().sum()
425/10: data.head()
425/11:
for ele in columns_name:
    data[ele].fillna(value=data[ele].median(), inplace=True)
425/12: data["Age"].unique()
425/13:
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
data["Glucose"].plot(kind = "kde", ax = ax)

line, labels = ax.get_legend_handles_labels()
ax.legend(line,labels,loc = "best")
425/14: data.info()
425/15: data.head()
425/16: data.describe()
425/17:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
425/18:
plt.figure(figsize=(15,8))
sns.heatmap(data)
425/19: #sns.pairplot(data, hue="Outcome")
425/20: data.corr()
425/21:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
425/22:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
425/23:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
425/24:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
425/25:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
425/26:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
425/27:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
425/28:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
425/29:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
425/30:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
425/31:
 #Randomized Search CV

# Hyperparameter optimization    
    
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
425/32:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
425/33:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
425/34: rf_random.fit(std_x_train,y_train)
425/35: rf_random.best_params_
425/36: predictions=rf_random.predict(std_x_test)
425/37: accuracy_score(y_test,predictions)
425/38:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
425/39:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
425/40:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
425/41:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
425/42:
plt.figure(figsize=(15,8))
sns.heatmap(data)
425/43:
data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(9,9))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
425/44:
data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
425/45:
data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(9,9))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
425/46:
data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
425/47:
from sklearn.utils import resample

df_majority = data.loc[data.Outcome == 0].copy()
df_minority = data.loc[data.Outcome == 1].copy()
df_minority_upsampled = resample(df_minority,
                             replace=True,  # sample with replacement
                            n_samples=500,  # to match majority class
                            random_state=123) 
data = pd.concat([df_majority, df_minority_upsampled])
425/48:
data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
425/49: #### Now our data set look balanced
425/50:
data1=data.drop('Outcome',axis=1)
data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))
plt.show()
425/51:
for ele in columns_name:
    data[ele].fillna(value=data[ele].mean(), inplace=True)
426/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
426/2: warnings.filterwarnings("ignore")
426/3: data = pd.read_csv("diabetes.csv")
426/4: data.head()
426/5:
print(data.columns)
data.shape
426/6:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
426/7:
columns_name = data.columns
for ele in columns_name:
    if 0 in data[ele].unique():
        print(ele,":","True")
426/8:
for ele in columns_name:
    if ele!="Pregnancies" and ele!="Outcome":
        data[ele].replace(0, np.nan, inplace=True)
426/9: data.isnull().sum()
426/10: data.head()
426/11:
for ele in columns_name:
    data[ele].fillna(value=data[ele].mean(), inplace=True)
426/12: data["Age"].unique()
426/13:
# import matplotlib.pyplot as plt

# fig = plt.figure()
# ax = fig.add_subplot(111)
# data["Glucose"].plot(kind = "kde", ax = ax)

# line, labels = ax.get_legend_handles_labels()
# ax.legend(line,labels,loc = "best")
426/14: data.info()
426/15: data.head()
426/16: data.describe()
426/17:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
426/18:
data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
426/19:
from sklearn.utils import resample

df_majority = data.loc[data.Outcome == 0].copy()
df_minority = data.loc[data.Outcome == 1].copy()
df_minority_upsampled = resample(df_minority,
                             replace=True,  # sample with replacement
                            n_samples=500,  # to match majority class
                            random_state=123) 
data = pd.concat([df_majority, df_minority_upsampled])
426/20:
data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
426/21:
data1=data.drop('Outcome',axis=1)
data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))
plt.show()
426/22:
plt.figure(figsize=(15,8))
sns.heatmap(data)
426/23: #sns.pairplot(data, hue="Outcome")
426/24: data.corr()
426/25:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
426/26:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
426/27:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
426/28:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
426/29:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
426/30:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
426/31:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
426/32:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
426/33:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
426/34:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
426/35:
 #Randomized Search CV

# Hyperparameter optimization    
    
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
426/36:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
426/37:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
426/38: rf_random.fit(std_x_train,y_train)
426/39: rf_random.best_params_
426/40: predictions=rf_random.predict(std_x_test)
426/41: accuracy_score(y_test,predictions)
426/42:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
426/43:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
426/44:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
426/45:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
426/46:
from sklearn.ensemble import IsolationForest
from collections import Counter
rs=np.random.RandomState(0)
clf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) 
clf.fit(data)
y_pred_train = clf.predict(data)
sayı = Counter(y_pred_train)
print(sayı)
426/47:
from collections import Counter
def detect_outliers(data,features):
    outlier_indices = []
    for c in features:
        # 1st quartile
        Q1 = np.percentile(data[c],25)
        # 3rd quartile
        Q3 = np.percentile(data[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index
        # store indeces
        outlier_indices.extend(outlier_list_col)
    
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)
    
    return multiple_outliers
426/48:
data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])]
426/49: multiple_outliers
426/50:
from collections import Counter
def detect_outliers(data,features):
    outlier_indices = []
    for c in features:
        # 1st quartile
        Q1 = np.percentile(data[c],25)
        # 3rd quartile
        Q3 = np.percentile(data[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index
        # store indeces
        outlier_indices.extend(outlier_list_col)
    
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)
    print(type(multiple_outliers))
    return multiple_outliers
426/51:
data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])]
426/52:
from collections import Counter
def detect_outliers(data,features):
    outlier_indices = []
    for c in features:
        # 1st quartile
        Q1 = np.percentile(data[c],25)
        # 3rd quartile
        Q3 = np.percentile(data[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index
        # store indeces
        outlier_indices.extend(outlier_list_col)
    
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)
    print(type(multiple_outliers))
    print(multiple_outliers)
    return multiple_outliers
426/53:
data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])]
426/54:
from collections import Counter
def detect_outliers(data,features):
    outlier_indices = []
    for c in features:
        # 1st quartile
        Q1 = np.percentile(data[c],25)
        # 3rd quartile
        Q3 = np.percentile(data[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index
        # store indeces
        outlier_indices.extend(outlier_list_col)
    
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)

    print(multiple_outliers)
    return multiple_outliers
426/55:
data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])]
426/56:
data = data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, 
                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,
                  748, 655, 584, 485, 618, 661, 45, 243])
426/57:
data1=data.drop('Outcome',axis=1)
data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))
plt.show()
426/58:
data = data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, 
                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,
                  748, 655, 584, 485, 618, 661, 45, 243], axis=0)
427/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
427/2: warnings.filterwarnings("ignore")
427/3: data = pd.read_csv("diabetes.csv")
427/4: data.head()
427/5:
print(data.columns)
data.shape
427/6:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
427/7:
columns_name = data.columns
for ele in columns_name:
    if 0 in data[ele].unique():
        print(ele,":","True")
427/8:
for ele in columns_name:
    if ele!="Pregnancies" and ele!="Outcome":
        data[ele].replace(0, np.nan, inplace=True)
427/9: data.isnull().sum()
427/10: data.head()
427/11:
for ele in columns_name:
    data[ele].fillna(value=data[ele].mean(), inplace=True)
427/12: data["Age"].unique()
427/13:
# import matplotlib.pyplot as plt

# fig = plt.figure()
# ax = fig.add_subplot(111)
# data["Glucose"].plot(kind = "kde", ax = ax)

# line, labels = ax.get_legend_handles_labels()
# ax.legend(line,labels,loc = "best")
427/14: data.info()
427/15: data.head()
427/16: data.describe()
427/17:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
427/18:
data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
427/19:
from sklearn.utils import resample

df_majority = data.loc[data.Outcome == 0].copy()
df_minority = data.loc[data.Outcome == 1].copy()
df_minority_upsampled = resample(df_minority,
                             replace=True,  # sample with replacement
                            n_samples=500,  # to match majority class
                            random_state=123) 
data = pd.concat([df_majority, df_minority_upsampled])
427/20:
data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
427/21:
data1=data.drop('Outcome',axis=1)
data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))
plt.show()
427/22:
from sklearn.ensemble import IsolationForest
from collections import Counter
rs=np.random.RandomState(0)
clf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) 
clf.fit(data)
y_pred_train = clf.predict(data)
sayı = Counter(y_pred_train)
print(sayı)

## -1 = Outliers
427/23:
from collections import Counter
def detect_outliers(data,features):
    outlier_indices = []
    for c in features:
        # 1st quartile
        Q1 = np.percentile(data[c],25)
        # 3rd quartile
        Q3 = np.percentile(data[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index
        # store indeces
        outlier_indices.extend(outlier_list_col)
    
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)

    print(multiple_outliers)
    return multiple_outliers
427/24:
data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])]
427/25:
data = data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, 
                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,
                  748, 655, 584, 485, 618, 661, 45, 243], axis=0)
427/26:
data1=data.drop('Outcome',axis=1)
data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))
plt.show()
427/27:
plt.figure(figsize=(15,8))
sns.heatmap(data)
427/28: #sns.pairplot(data, hue="Outcome")
427/29: data.corr()
427/30:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
427/31:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
427/32:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
427/33:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
427/34:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
427/35:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
427/36:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
427/37:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
427/38:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
427/39:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
427/40:
 #Randomized Search CV

# Hyperparameter optimization    
    
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
427/41:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
427/42:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
427/43: rf_random.fit(std_x_train,y_train)
427/44: rf_random.best_params_
427/45: predictions=rf_random.predict(std_x_test)
427/46: accuracy_score(y_test,predictions)
427/47:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
427/48:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
427/49:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
427/50:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
427/51: data.shape()
428/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
428/2: warnings.filterwarnings("ignore")
428/3: data = pd.read_csv("diabetes.csv")
428/4: data.head()
428/5:
print(data.columns)
data.shape
428/6:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
428/7:
columns_name = data.columns
for ele in columns_name:
    if 0 in data[ele].unique():
        print(ele,":","True")
428/8:
for ele in columns_name:
    if ele!="Pregnancies" and ele!="Outcome":
        data[ele].replace(0, np.nan, inplace=True)
428/9: data.isnull().sum()
428/10: data.head()
428/11:
for ele in columns_name:
    data[ele].fillna(value=data[ele].mean(), inplace=True)
428/12: data["Age"].unique()
428/13:
# import matplotlib.pyplot as plt

# fig = plt.figure()
# ax = fig.add_subplot(111)
# data["Glucose"].plot(kind = "kde", ax = ax)

# line, labels = ax.get_legend_handles_labels()
# ax.legend(line,labels,loc = "best")
428/14: data.info()
428/15: data.head()
428/16: data.describe()
428/17:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
428/18:
data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
428/19:
from sklearn.utils import resample

df_majority = data.loc[data.Outcome == 0].copy()
df_minority = data.loc[data.Outcome == 1].copy()
df_minority_upsampled = resample(df_minority,
                             replace=True,  # sample with replacement
                            n_samples=500,  # to match majority class
                            random_state=123) 
data = pd.concat([df_majority, df_minority_upsampled])
428/20:
data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
428/21:
data1=data.drop('Outcome',axis=1)
data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))
plt.show()
428/22:
from sklearn.ensemble import IsolationForest
from collections import Counter
rs=np.random.RandomState(0)
clf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) 
clf.fit(data)
y_pred_train = clf.predict(data)
sayı = Counter(y_pred_train)
print(sayı)

## -1 = Outliers
428/23:
from collections import Counter
def detect_outliers(data,features):
    outlier_indices = []
    for c in features:
        # 1st quartile
        Q1 = np.percentile(data[c],25)
        # 3rd quartile
        Q3 = np.percentile(data[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index
        # store indeces
        outlier_indices.extend(outlier_list_col)
    
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)

    print(multiple_outliers)
    return multiple_outliers
428/24:
data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])]
428/25:
data = data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, 
                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,
                  748, 655, 584, 485, 618, 661, 45, 243], axis=0)
428/26: data.shape()
428/27:
data = data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, 
                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,
                  748, 655, 584, 485, 618, 661, 45, 243], axis=0,inplace=True)
429/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
429/2: warnings.filterwarnings("ignore")
429/3: data = pd.read_csv("diabetes.csv")
429/4: data.head()
429/5:
print(data.columns)
data.shape
429/6:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
429/7:
columns_name = data.columns
for ele in columns_name:
    if 0 in data[ele].unique():
        print(ele,":","True")
429/8:
for ele in columns_name:
    if ele!="Pregnancies" and ele!="Outcome":
        data[ele].replace(0, np.nan, inplace=True)
429/9: data.isnull().sum()
429/10: data.head()
429/11:
for ele in columns_name:
    data[ele].fillna(value=data[ele].mean(), inplace=True)
429/12: data["Age"].unique()
429/13:
# import matplotlib.pyplot as plt

# fig = plt.figure()
# ax = fig.add_subplot(111)
# data["Glucose"].plot(kind = "kde", ax = ax)

# line, labels = ax.get_legend_handles_labels()
# ax.legend(line,labels,loc = "best")
429/14: data.info()
429/15: data.head()
429/16: data.describe()
429/17:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
429/18:
data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
429/19:
from sklearn.utils import resample

df_majority = data.loc[data.Outcome == 0].copy()
df_minority = data.loc[data.Outcome == 1].copy()
df_minority_upsampled = resample(df_minority,
                             replace=True,  # sample with replacement
                            n_samples=500,  # to match majority class
                            random_state=123) 
data = pd.concat([df_majority, df_minority_upsampled])
429/20:
data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
429/21:
data1=data.drop('Outcome',axis=1)
data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))
plt.show()
429/22:
from sklearn.ensemble import IsolationForest
from collections import Counter
rs=np.random.RandomState(0)
clf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) 
clf.fit(data)
y_pred_train = clf.predict(data)
sayı = Counter(y_pred_train)
print(sayı)

## -1 = Outliers
429/23:
from collections import Counter
def detect_outliers(data,features):
    outlier_indices = []
    for c in features:
        # 1st quartile
        Q1 = np.percentile(data[c],25)
        # 3rd quartile
        Q3 = np.percentile(data[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index
        # store indeces
        outlier_indices.extend(outlier_list_col)
    
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)

    print(multiple_outliers)
    return multiple_outliers
429/24:
data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])]
429/25:
data = data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, 
                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,
                  748, 655, 584, 485, 618, 661, 45, 243], axis=0,inplace=True)
429/26: data.shape()
429/27: data
429/28: data
429/29: print(data)
430/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
430/2: warnings.filterwarnings("ignore")
430/3: data = pd.read_csv("diabetes.csv")
430/4: data.head()
430/5:
print(data.columns)
data.shape
430/6:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
430/7:
columns_name = data.columns
for ele in columns_name:
    if 0 in data[ele].unique():
        print(ele,":","True")
430/8:
for ele in columns_name:
    if ele!="Pregnancies" and ele!="Outcome":
        data[ele].replace(0, np.nan, inplace=True)
430/9: data.isnull().sum()
430/10: data.head()
430/11:
for ele in columns_name:
    data[ele].fillna(value=data[ele].mean(), inplace=True)
430/12: data["Age"].unique()
430/13:
# import matplotlib.pyplot as plt

# fig = plt.figure()
# ax = fig.add_subplot(111)
# data["Glucose"].plot(kind = "kde", ax = ax)

# line, labels = ax.get_legend_handles_labels()
# ax.legend(line,labels,loc = "best")
430/14: data.info()
430/15: data.head()
430/16: data.describe()
430/17:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
430/18:
data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
430/19:
from sklearn.utils import resample

df_majority = data.loc[data.Outcome == 0].copy()
df_minority = data.loc[data.Outcome == 1].copy()
df_minority_upsampled = resample(df_minority,
                             replace=True,  # sample with replacement
                            n_samples=500,  # to match majority class
                            random_state=123) 
data = pd.concat([df_majority, df_minority_upsampled])
430/20:
data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
430/21:
data1=data.drop('Outcome',axis=1)
data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))
plt.show()
430/22:
from sklearn.ensemble import IsolationForest
from collections import Counter
rs=np.random.RandomState(0)
clf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) 
clf.fit(data)
y_pred_train = clf.predict(data)
sayı = Counter(y_pred_train)
print(sayı)

## -1 = Outliers
430/23:
from collections import Counter
def detect_outliers(data,features):
    outlier_indices = []
    for c in features:
        # 1st quartile
        Q1 = np.percentile(data[c],25)
        # 3rd quartile
        Q3 = np.percentile(data[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index
        # store indeces
        outlier_indices.extend(outlier_list_col)
    
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)

    print(multiple_outliers)
    return multiple_outliers
430/24:
data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])]
430/25:
data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, 
                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,
                  748, 655, 584, 485, 618, 661, 45, 243], axis=0,inplace=True)
430/26: print(data)
430/27:
plt.figure(figsize=(15,8))
sns.heatmap(data)
430/28: #sns.pairplot(data, hue="Outcome")
430/29: data.corr()
430/30:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
430/31:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
430/32:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
430/33:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
430/34:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
430/35:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
430/36:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
430/37:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
430/38:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
430/39:
#  standardize data
sd_tree = DecisionTreeClassifier(random_state=42)
sd_tree.fit(std_x_train,y_train)
sd_predict2 = sd_tree.predict(std_x_test)
print("Accuracy of standardize data in Deccission Tree Classifier:",accuracy_score(y_test,sd_predict2))
430/40:
 #Randomized Search CV

# Hyperparameter optimization    
    
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
430/41:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
430/42:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
430/43: rf_random.fit(std_x_train,y_train)
430/44: rf_random.best_params_
430/45: predictions=rf_random.predict(std_x_test)
430/46: accuracy_score(y_test,predictions)
430/47:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
430/48:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
430/49:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
430/50:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
430/51: data
430/52: newdata=data.drop(['BloodPressure', 'SkinThickness', 'Insulin','DiabetesPedigreeFunction'],axis=1)
430/53:
newdata=data.drop(['BloodPressure', 'SkinThickness', 'Insulin','DiabetesPedigreeFunction'],axis=1)
newdata
430/54: data.drop(['BloodPressure', 'SkinThickness', 'Insulin','DiabetesPedigreeFunction'],axis=1,inplace=True)
430/55:
data.drop(['BloodPressure', 'SkinThickness', 'Insulin','DiabetesPedigreeFunction'],axis=1,inplace=True)
data
431/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings

from sklearn.metrics import accuracy_score, confusion_matrix
431/2: warnings.filterwarnings("ignore")
431/3: data = pd.read_csv("diabetes.csv")
431/4: data.head()
431/5:
print(data.columns)
data.shape
431/6:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
431/7:
columns_name = data.columns
for ele in columns_name:
    if 0 in data[ele].unique():
        print(ele,":","True")
431/8:
for ele in columns_name:
    if ele!="Pregnancies" and ele!="Outcome":
        data[ele].replace(0, np.nan, inplace=True)
431/9: data.isnull().sum()
431/10: data.head()
431/11:
for ele in columns_name:
    data[ele].fillna(value=data[ele].mean(), inplace=True)
431/12: data["Age"].unique()
431/13:
# import matplotlib.pyplot as plt

# fig = plt.figure()
# ax = fig.add_subplot(111)
# data["Glucose"].plot(kind = "kde", ax = ax)

# line, labels = ax.get_legend_handles_labels()
# ax.legend(line,labels,loc = "best")
431/14: data.info()
431/15: data.head()
431/16: data.describe()
431/17:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
431/18:
data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
431/19:
from sklearn.utils import resample

df_majority = data.loc[data.Outcome == 0].copy()
df_minority = data.loc[data.Outcome == 1].copy()
df_minority_upsampled = resample(df_minority,
                             replace=True,  # sample with replacement
                            n_samples=500,  # to match majority class
                            random_state=123) 
data = pd.concat([df_majority, df_minority_upsampled])
431/20:
data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
431/21:
data1=data.drop('Outcome',axis=1)
data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))
plt.show()
431/22:
from sklearn.ensemble import IsolationForest
from collections import Counter
rs=np.random.RandomState(0)
clf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) 
clf.fit(data)
y_pred_train = clf.predict(data)
sayı = Counter(y_pred_train)
print(sayı)

## -1 = Outliers
431/23:
from collections import Counter
def detect_outliers(data,features):
    outlier_indices = []
    for c in features:
        # 1st quartile
        Q1 = np.percentile(data[c],25)
        # 3rd quartile
        Q3 = np.percentile(data[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index
        # store indeces
        outlier_indices.extend(outlier_list_col)
    
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)

    print(multiple_outliers)
    return multiple_outliers
431/24:
data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])]
431/25:
data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, 
                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,
                  748, 655, 584, 485, 618, 661, 45, 243], axis=0,inplace=True)
431/26: data
431/27:
plt.figure(figsize=(15,8))
sns.heatmap(data)
431/28: #sns.pairplot(data, hue="Outcome")
431/29: data.corr()
431/30:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
431/31:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
431/32:
data.drop(['BloodPressure', 'SkinThickness', 'Insulin','DiabetesPedigreeFunction'],axis=1,inplace=True)
data
431/33:
from sklearn.preprocessing import StandardScaler
X = data.iloc[:, 0:4]
Y = data.iloc[:, 4]
nd = StandardScaler()
nd.fit(X)
X =nd.transform(X)
print(Y)
431/34:
x_data = data.iloc[:,:8].values
y_data = data.iloc[:,8:].values
431/35:
std_scl = StandardScaler()
std_scl_x = std_scl.fit_transform(x_data)
#std_scl_x
431/36:
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2, random_state=42)
x_train
431/37:
std_x_train,std_x_test,y_train,y_test = train_test_split(std_scl_x,y_data,test_size=0.2, random_state=42)
std_x_train
431/38:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
431/39:
from sklearn.preprocessing import StandardScaler

x_data = data.iloc[:, 0:4]
y_data = data.iloc[:, 4]
nd = StandardScaler()
nd.fit(x_data)
x_data =nd.transform(x_data)
print(y_data)
431/40:
from sklearn.preprocessing import StandardScaler
X = data.iloc[:, 0:4]
Y = data.iloc[:, 4]
nd = StandardScaler()
nd.fit(X)
X =nd.transform(X)
print(Y)
431/41:
x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2, random_state=42)
x_train
431/42:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1))
431/43:
# standardize data
slog_reg1 = LogisticRegression()
slog_reg1.fit(std_x_train,y_train)
slog_predict2 = slog_reg1.predict(std_x_test)
print(accuracy_score(y_test,slog_predict2))
431/44:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1))
431/45:
#  #Randomized Search CV

# # Hyperparameter optimization    
    
# # Number of trees in random forest
# n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# # Number of features to consider at every split
# max_features = ['auto', 'sqrt']
# # Maximum number of levels in tree
# max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# # max_depth.append(None)
# # Minimum number of samples required to split a node
# min_samples_split = [2, 5, 10, 15, 100]
# # Minimum number of samples required at each leaf node
# min_samples_leaf = [1, 2, 5, 10]
431/46:
# # Create the random grid
# random_grid = {'n_estimators': n_estimators,
#                'max_features': max_features,
#                'max_depth': max_depth,
#                'min_samples_split': min_samples_split,
#                'min_samples_leaf': min_samples_leaf}

# print(random_grid)
431/47:
# rf = RandomForestClassifier()
# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
431/48: # rf_random.fit(std_x_train,y_train)
431/49: # rf_random.best_params_
431/50: # predictions=rf_random.predict(std_x_test)
431/51: # accuracy_score(y_test,predictions)
431/52:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1))
431/53:
# standardize data
sr_forest = RandomForestClassifier(n_estimators=100, random_state=42)
sr_forest.fit(std_x_train,y_train)
sr_predict2 = sr_forest.predict(std_x_test)
print("Accuracy of standardize data in Random Forest Classifier:",accuracy_score(y_test,sr_predict2))
431/54:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
431/55:
cross_val = cross_val_score(estimator=log_reg, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
431/56:
#  #Randomized Search CV

# # Hyperparameter optimization    
    
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
431/57:
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)
431/58:
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
431/59: rf_random.fit(std_x_train,y_train)
431/60: rf_random.best_params_
431/61: predictions=rf_random.predict(std_x_test)
431/62: accuracy_score(y_test,predictions)
431/63: predictions=rf_random.predict(x_test)
431/64: rf_random.fit(x_train,y_train)
431/65: rf_random.best_params_
431/66: predictions=rf_random.predict(x_test)
431/67: accuracy_score(y_test,predictions)
431/68:
confu_matrix = confusion_matrix(y_test, rf_random)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
431/69: r = accuracy_score(y_test,predictions)
431/70:
confu_matrix = confusion_matrix(y_test, r)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
431/71:
confu_matrix = confusion_matrix(y_test, predictions)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
431/72:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
431/73:
confu_matrix = confusion_matrix(y_test, r_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
431/74:
confu_matrix = confusion_matrix(y_test, d_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
431/75:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1)*100)
431/76:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1)*100)
431/77:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1)*100)
431/78:
cross_val = cross_val_score(estimator=rf_random, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
431/79:
cross_val = cross_val_score(estimator=rf, X=x_train, y=y_train)
print("Cross validation accuracy of Logistic Regrassior: ",cross_val)
print("Cross validation mean accuracy of Logistic Regrassion: ", cross_val.mean())
431/80:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings
import pickle

from sklearn.metrics import accuracy_score, confusion_matrix
431/81:

# save model
pickle.dump(rf_random,open('model.pkl','wb'))

# Load Model
dP = pickle.load(open('model.pkl','rb'))

y_predict = dp.predict(all_x_test)

print("confusion matrix: \n", confusion_matrix(all_y_test,y_predict))
accuracy_score(all_y_test,y_predict)
431/82:

# save model
pickle.dump(rf_random,open('model.pkl','wb'))

# Load Model
dp = pickle.load(open('model.pkl','rb'))

y_predict = dp.predict(all_x_test)

print("confusion matrix: \n", confusion_matrix(all_y_test,y_predict))
accuracy_score(all_y_test,y_predict)
431/83:

# save model
pickle.dump(rf_random,open('model.pkl','wb'))

# Load Model
dp = pickle.load(open('model.pkl','rb'))

y_predict = dp.predict(x_test)

print("confusion matrix: \n", confusion_matrix(y_test,y_predict))
accuracy_score(y_test,y_predict)
431/84:

# save model
pickle.dump(rf_random,open('model.pkl','wb'))

# Load Model
dp = pickle.load(open('model.pkl','rb'))

y_predict = dp.predict([1,89.0,28.1,21])
print(y_predict)
#print("confusion matrix: \n", confusion_matrix(y_test,y_predict))
#accuracy_score(y_test,y_predict)
431/85:

# save model
pickle.dump(rf_random,open('model.pkl','wb'))

# Load Model
dp = pickle.load(open('model.pkl','rb'))

y_predict = dp.predict(np.array([1,89.0,28.1,21]))
print(y_predict)
#print("confusion matrix: \n", confusion_matrix(y_test,y_predict))
#accuracy_score(y_test,y_predict)
431/86:

# save model
pickle.dump(rf_random,open('model.pkl','wb'))

# Load Model
dp = pickle.load(open('model.pkl','rb'))

y_predict = dp.predict(np.array([1,89.0,28.1,21].reshape(-1,-1)))
print(y_predict)
#print("confusion matrix: \n", confusion_matrix(y_test,y_predict))
#accuracy_score(y_test,y_predict)
431/87:

# save model
pickle.dump(rf_random,open('model.pkl','wb'))

# Load Model
dp = pickle.load(open('model.pkl','rb'))

y_predict = dp.predict(np.array([1,89.0,28.1,21]).reshape(-1,-1))
print(y_predict)
#print("confusion matrix: \n", confusion_matrix(y_test,y_predict))
#accuracy_score(y_test,y_predict)
431/88:

# save model
pickle.dump(rf_random,open('model.pkl','wb'))

# Load Model
dp = pickle.load(open('model.pkl','rb'))

y_predict = dp.predict(np.array([1,89.0,28.1,21]).reshape(1,-1))
print(y_predict)
#print("confusion matrix: \n", confusion_matrix(y_test,y_predict))
#accuracy_score(y_test,y_predict)
431/89:

# save model
pickle.dump(rf_random,open('model.pkl','wb'))

# Load Model
dp = pickle.load(open('model.pkl','rb'))

y_predict = dp.predict(np.array([3,171,33.3,24]).reshape(1,-1))
print(y_predict)
#print("confusion matrix: \n", confusion_matrix(y_test,y_predict))
#accuracy_score(y_test,y_predict)
431/90:

# save model
pickle.dump(r_forest,open('model.pkl','wb'))

# Load Model
dp = pickle.load(open('model.pkl','rb'))

y_predict = dp.predict(np.array([3,171,33.3,24]).reshape(1,-1))
print(y_predict)
#print("confusion matrix: \n", confusion_matrix(y_test,y_predict))
#accuracy_score(y_test,y_predict)
431/91:

# save model
pickle.dump(rf_random,open('model.pkl','wb'))

# Load Model
dp = pickle.load(open('model.pkl','rb'))

y_predict = dp.predict(x_test)

print("confusion matrix: \n", confusion_matrix(y_test,y_predict))
accuracy_score(y_test,y_predict)
431/92:

# save model
pickle.dump(r_forest,open('model.pkl','wb'))

# Load Model
dp = pickle.load(open('model.pkl','rb'))

y_predict = dp.predict(x_test)

print("confusion matrix: \n", confusion_matrix(y_test,y_predict))
accuracy_score(y_test,y_predict)
432/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

import warnings
import pickle

from sklearn.metrics import accuracy_score, confusion_matrix
432/2: warnings.filterwarnings("ignore")
432/3: data = pd.read_csv("diabetes.csv")
432/4: data.head()
432/5:
print(data.columns)
data.shape
432/6:
data.isnull().sum()
a = len(data.loc[data['Insulin']==0])
a
432/7:
columns_name = data.columns
for ele in columns_name:
    if 0 in data[ele].unique():
        print(ele,":","True")
432/8:
for ele in columns_name:
    if ele!="Pregnancies" and ele!="Outcome":
        data[ele].replace(0, np.nan, inplace=True)
432/9: data.isnull().sum()
432/10: data.head()
432/11:
for ele in columns_name:
    data[ele].fillna(value=data[ele].mean(), inplace=True)
432/12: data["Age"].unique()
432/13:
# import matplotlib.pyplot as plt

# fig = plt.figure()
# ax = fig.add_subplot(111)
# data["Glucose"].plot(kind = "kde", ax = ax)

# line, labels = ax.get_legend_handles_labels()
# ax.legend(line,labels,loc = "best")
432/14: data.info()
432/15: data.head()
432/16: data.describe()
432/17:
print(data.groupby(["Outcome"]).count())
sns.countplot(data["Outcome"])

# 0=No diabeties, 1=Diabeties
432/18:
data['Outcome'].value_counts().plot(kind='pie',colors=['#2C4373', '#F2A74B'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
432/19:
from sklearn.utils import resample

df_majority = data.loc[data.Outcome == 0].copy()
df_minority = data.loc[data.Outcome == 1].copy()
df_minority_upsampled = resample(df_minority,
                             replace=True,  # sample with replacement
                            n_samples=500,  # to match majority class
                            random_state=123) 
data = pd.concat([df_majority, df_minority_upsampled])
432/20:
data['Outcome'].value_counts().plot(kind='pie',colors=['#F2A74B', '#cd919e'],autopct='%1.1f%%',figsize=(5,5))
plt.show
varValue = data.Outcome.value_counts()
print(varValue)
432/21:
data1=data.drop('Outcome',axis=1)
data1.plot(kind='box', subplots=True, layout=(4,4), sharex=False,sharey=False ,figsize =(15,15))
plt.show()
432/22:
from sklearn.ensemble import IsolationForest
from collections import Counter
rs=np.random.RandomState(0)
clf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) 
clf.fit(data)
y_pred_train = clf.predict(data)
sayı = Counter(y_pred_train)
print(sayı)

## -1 = Outliers
432/23:
from collections import Counter
def detect_outliers(data,features):
    outlier_indices = []
    for c in features:
        # 1st quartile
        Q1 = np.percentile(data[c],25)
        # 3rd quartile
        Q3 = np.percentile(data[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index
        # store indeces
        outlier_indices.extend(outlier_list_col)
    
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)

    print(multiple_outliers)
    return multiple_outliers
432/24:
data.loc[detect_outliers(data,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])]
432/25:
data.drop([298, 125, 177, 43, 254, 293, 595, 693, 458, 16, 39, 409, 
                  579, 370, 323, 659, 220, 31, 6, 402, 715, 359, 195, 425, 109,
                  748, 655, 584, 485, 618, 661, 45, 243], axis=0,inplace=True)
432/26: data
432/27:
plt.figure(figsize=(15,8))
sns.heatmap(data)
432/28: #sns.pairplot(data, hue="Outcome")
432/29: data.corr()
432/30:
plt.figure(figsize=(18,8))
sns.heatmap(data.corr(), annot=True, linewidths=1)
432/31:
data_2 = data.drop(["Outcome"],axis=1)
plt.figure(figsize=(15,8))
sns.barplot(data_2.corrwith(data.Outcome).index, data_2.corrwith(data.Outcome))
432/32:
data.drop(['BloodPressure', 'SkinThickness', 'Insulin','DiabetesPedigreeFunction'],axis=1,inplace=True)
data
432/33:

X = data.iloc[:, 0:4]
Y = data.iloc[:, 4]
432/34:
x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2, random_state=42)
x_train
432/35:
# Non standardize data
log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)
log_predict1 = log_reg.predict(x_test)
print("Accuracy of Non standardize data in Logistic regrassion:",accuracy_score(y_test,log_predict1)*100)
432/36:
confu_matrix = confusion_matrix(y_test, log_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
432/37:
# Non standardize data
d_tree = DecisionTreeClassifier(random_state=42)
d_tree.fit(x_train,y_train)
d_predict1 = d_tree.predict(x_test)
print("Accuracy of Non standardize data in Deccission Tree Classifier:",accuracy_score(y_test,d_predict1)*100)
432/38:
confu_matrix = confusion_matrix(y_test, d_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
432/39:
# Non standardize data
r_forest = RandomForestClassifier(n_estimators=100, random_state=42)
r_forest.fit(x_train,y_train)
r_predict1 = r_forest.predict(x_test)
print("Accuracy of Non standardize data in Random Forest Classifier:",accuracy_score(y_test,r_predict1)*100)
432/40:
confu_matrix = confusion_matrix(y_test, r_predict1)
plt.title("Confusion Matrix", fontsize=10)
sns.heatmap(confu_matrix, annot=True)
plt.show()
432/41:

# save model
pickle.dump(r_forest,open('model.pkl','wb'))

# Load Model
dp = pickle.load(open('model.pkl','rb'))

y_predict = dp.predict(x_test)

print("confusion matrix: \n", confusion_matrix(y_test,y_predict))
accuracy_score(y_test,y_predict)
433/1:
from googleplaces import GooglePlaces, types, lang
import requests
import json
  
# This is the way to make api requests
# using python requests library
  
# send_url = 'http://freegeoip.net/json'
# r = requests.get(send_url)
# j = json.loads(r.text)
# print(j)
# lat = j['latitude']
# lon = j['longitude']
  
# Generate an API key by going to this location
# https://cloud.google.com /maps-platform/places/?apis =
# places in the google developers
  
# Use your own API key for making api request calls
API_KEY = 'AIzaSyAU14VOIJDGbRPGzIpUXfKP40AmvLfcVIQ'
  
# Initialising the GooglePlaces constructor
google_places = GooglePlaces(API_KEY)
  
# call the function nearby search with
# the parameters as longitude, latitude,
# radius and type of place which needs to be searched of 
# type can be HOSPITAL, CAFE, BAR, CASINO, etc
query_result = google_places.nearby_search(
        # lat_lng ={'lat': 46.1667, 'lng': -1.15},
        lat_lng ={'lat': 28.4089, 'lng': 77.3178},
        radius = 5000,
        # types =[types.TYPE_HOSPITAL] or
        # [types.TYPE_CAFE] or [type.TYPE_BAR]
        # or [type.TYPE_CASINO])
        types =[types.TYPE_HOSPITAL])
  
# If any attributions related 
# with search results print them
if query_result.has_attributions:
    print (query_result.html_attributions)
  
  
# Iterate over the search results
for place in query_result.places:
    # print(type(place))
    # place.get_details()
    print (place.name)
    print("Latitude", place.geo_location['lat'])
    print("Longitude", place.geo_location['lng'])
    print()
433/2: !pip install googleplaces
433/3: !pip install python-google-places
433/4:
from googleplaces import GooglePlaces, types, lang
import requests
import json
  
# This is the way to make api requests
# using python requests library
  
# send_url = 'http://freegeoip.net/json'
# r = requests.get(send_url)
# j = json.loads(r.text)
# print(j)
# lat = j['latitude']
# lon = j['longitude']
  
# Generate an API key by going to this location
# https://cloud.google.com /maps-platform/places/?apis =
# places in the google developers
  
# Use your own API key for making api request calls
API_KEY = 'AIzaSyAU14VOIJDGbRPGzIpUXfKP40AmvLfcVIQ'
  
# Initialising the GooglePlaces constructor
google_places = GooglePlaces(API_KEY)
  
# call the function nearby search with
# the parameters as longitude, latitude,
# radius and type of place which needs to be searched of 
# type can be HOSPITAL, CAFE, BAR, CASINO, etc
query_result = google_places.nearby_search(
        # lat_lng ={'lat': 46.1667, 'lng': -1.15},
        lat_lng ={'lat': 28.4089, 'lng': 77.3178},
        radius = 5000,
        # types =[types.TYPE_HOSPITAL] or
        # [types.TYPE_CAFE] or [type.TYPE_BAR]
        # or [type.TYPE_CASINO])
        types =[types.TYPE_HOSPITAL])
  
# If any attributions related 
# with search results print them
if query_result.has_attributions:
    print (query_result.html_attributions)
  
  
# Iterate over the search results
for place in query_result.places:
    # print(type(place))
    # place.get_details()
    print (place.name)
    print("Latitude", place.geo_location['lat'])
    print("Longitude", place.geo_location['lng'])
    print()
435/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

import pickle

from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
435/2: warnings.filterwarnings("ignore")
435/3:
data = pd.read_csv("heart_disease.csv")
data
435/4:
print("Shape:")
print(data.shape())
print("-"*100)
data.info()
435/5:
print("Shape:")
print(data.shape)
print("-"*100)
data.info()
435/6:
print("Shape:")
print(data.shape)
print("-"*100)
print("Columns:")
print(data.columns())
print("-"*100)
data.info()
435/7:
print("Shape:")
print(data.shape)
print("-"*100)
print("Columns:")
print(data.column())
print("-"*100)
data.info()
435/8:
print("Shape:")
print(data.shape)
print("-"*100)
print("Columns:")
columns = data.columns
print(columns)
print("-"*100)
data.info()
435/9:
print("Shape:")
print(data.shape)
print("-"*100)
print("Columns Name:")
columns = data.columns
print(columns)
print("-"*100)
print("Data Information:")
data.info()
print("-"*100)
print("Data Description: ")
print(data.describe())
print("-"*100)
print("Counting Null Values:")
print(data.isnull().sum())
print("-"*100)
435/10:
print("Shape:")
print(data.shape)
print("-"*100)
print("Columns Name:")
columns = data.columns
print(columns)
print("-"*100)
print("Data Information:")
data.info()
print("-"*100)
print("Data Description: ")
print(data.describe)
print("-"*100)
print("Counting Null Values:")
print(data.isnull().sum())
print("-"*100)
435/11:
data = pd.read_csv("heart_disease.csv")
data.head()
435/12:
print("Shape:")
print(data.shape)
print("-"*100)
print("Columns Name:")
columns = data.columns
print(columns)
print("-"*100)
print("Data Information:")
data.info()
print("-"*100)
print("Data Description: ")
print(data.describe())
print("-"*100)
print("Counting Null Values:")
print(data.isnull().sum())
print("-"*100)
435/13:
print("Shape:")
print(data.shape)
print("-"*100)
print("Columns Name:")
columns = data.columns
print(columns)
print("-"*100)
print("Data Information:")
data.info()
print("-"*100)
print("Data Description: ")
print(data.describe())
print("-"*100)
print("Counting Null Values:")
print(data.isnull().sum())
print("-"*100)
print("Data correlation: ")
print(data.corr)
print("-"*100)
435/14:
print("Shape:")
print(data.shape)
print("-"*100)
print("Columns Name:")
columns = data.columns
print(columns)
print("-"*100)
print("Data Information:")
data.info()
print("-"*100)
print("Data Description: ")
print(data.describe())
print("-"*100)
print("Counting Null Values:")
print(data.isnull().sum())
print("-"*100)
print("Data correlation: ")
print(data.corr())
print("-"*100)
435/15:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='RdYlGn', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
435/16:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='RdYl', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
435/17:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='Blues', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
435/18:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='Blue', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
435/19:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='BuPu_r', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
"""'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',
'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 
'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 
'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 
'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 
'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',
'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 
'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',
'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',
'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 
'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 
'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',
'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',
'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 
'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',
'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r"""
435/20:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='BrBG', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
"""'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',
'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 
'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 
'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 
'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 
'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',
'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 
'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',
'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',
'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 
'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 
'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',
'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',
'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 
'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',
'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r"""
435/21:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='rainbow', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
"""'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',
'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 
'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 
'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 
'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 
'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',
'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 
'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',
'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',
'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 
'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 
'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',
'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',
'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 
'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',
'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r"""
435/22:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='rainbow', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
"""'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',
'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 
'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 
'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 
'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 
'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',
'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 
'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',
'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',
'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 
'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 
'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',
'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',
'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 
'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',
'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r'"""
435/23:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='RdYlGn', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
"""'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',
'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 
'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 
'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 
'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 
'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',
'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 
'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',
'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',
'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 
'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 
'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',
'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',
'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 
'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',
'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r'"""
435/24:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='viridis', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
"""'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',
'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 
'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 
'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 
'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 
'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',
'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 
'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',
'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',
'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 
'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 
'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',
'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',
'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 
'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',
'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r'"""
435/25:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='vlag', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
"""'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',
'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 
'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 
'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 
'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 
'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',
'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 
'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',
'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',
'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 
'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 
'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',
'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',
'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 
'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',
'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r'"""
435/26:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='viridis_r', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
"""'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',
'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 
'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 
'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 
'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 
'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',
'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 
'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',
'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',
'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 
'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 
'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',
'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',
'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 
'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',
'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r'"""
435/27:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
"""'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2',
'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 
'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 
'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 
'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 
'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r',
'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 
'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix',
'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',
'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 
'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 
'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean',
'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r',
'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 
'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted',
'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r'"""
435/28:
data.hit(figure(20,20))
plt.show
435/29:
data.hit(figure(20,20))
plt.show()
435/30:
data.hist(figure(20,20))
plt.show()
435/31:
data.hist(figsize(20,20))
plt.show()
435/32:
data.hist(figsize=(20,20))
plt.show()
435/33: sns.pairplot(data, hue="target")
435/34:
X = data.drop("target",axis=1)
y = data("target")
435/35:
X = data.drop("target",axis=1)
y = data["target"]
435/36:
std_scl = StandardScaler()

X = std_scl.fit_transform(X)
435/37:
std_scl = StandardScaler()

X = std_scl.fit_transform(X)
print(X)
435/38:
plt.figure(figsize=(15,8))
sns.heatmap(X)
435/39: x_train,x_test,y_tarin,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
435/40:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier

import pickle

from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
435/41: !pip install xgboost
436/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier

import pickle

from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
436/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, cross_val_score
436/3: warnings.filterwarnings("ignore")
436/4:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, cross_val_score
436/5:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
436/6: warnings.filterwarnings("ignore")
436/7:
data = pd.read_csv("heart_disease.csv")
data.head()
436/8:
print("Shape:")
print(data.shape)
print("-"*100)
print("Columns Name:")
columns = data.columns
print(columns)
print("-"*100)
print("Data Information:")
data.info()
print("-"*100)
print("Data Description: ")
print(data.describe())
print("-"*100)
print("Counting Null Values:")
print(data.isnull().sum())
print("-"*100)
print("Data correlation: ")
print(data.corr())
print("-"*100)
436/9:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
436/10:
data.hist(figsize=(20,20))
plt.show()
436/11:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
436/12:
sns.kdeplot(data[data['target']==1]['chol'],shade=True,color="orange", label="Unwell", alpha=.7)
sns.kdeplot(data[data['target']==0]['chol'],shade=True,color="dodgerblue", label="Healthy", alpha=.7)
plt.title('Cholesterol in mg/d for both case')
plt.show()
436/13:
plt.figure(figsize=(15,8))
sns.heatmap(data)
436/14: sns.pairplot(data, hue="target")
436/15:
X = data.drop("target",axis=1)
y = data["target"]
436/16:
std_scl = StandardScaler()

X = std_scl.fit_transform(X)
print(X)
436/17:
plt.figure(figsize=(15,8))
sns.heatmap(X)
436/18: x_train,x_test,y_tarin,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
436/19:
lr = LogisticRegression(solver="liblinear").fit(x_train,y_train)
gnb = GaussianNB().fit(x_train,y_train)
knnc = KNeighborsClassifier().fit(x_train,y_train)
dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)
rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)
gbc = GradientBoostingClassifier(verbose=False).fit(x_train,y_train)
xgbc = XGBClassifier().fit(x_train,y_train)
svmc = SVC().fit(x_train,y_train)
catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)
436/20: x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
436/21:
lr = LogisticRegression(solver="liblinear").fit(x_train,y_train)
gnb = GaussianNB().fit(x_train,y_train)
knnc = KNeighborsClassifier().fit(x_train,y_train)
dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)
rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)
gbc = GradientBoostingClassifier(verbose=False).fit(x_train,y_train)
xgbc = XGBClassifier().fit(x_train,y_train)
svmc = SVC().fit(x_train,y_train)
catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)
436/22:
lr = LogisticRegression(solver="liblinear").fit(x_train,y_train)
gnb = GaussianNB().fit(x_train,y_train)
knnc = KNeighborsClassifier().fit(x_train,y_train)
dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)
rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)
xgbc = XGBClassifier().fit(x_train,y_train)
svmc = SVC().fit(x_train,y_train)
catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)
436/23: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,svmc,catbc]
436/24:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    error = -cross_val_score(model,x_test,y_test,cv=10,scoring="neg_mean_squared_error",verbose=False).mean()
    print(name + ": ")
    print("-" * 50)
    print("Accuracy Score: ",accuracy_score(y_test,predict))
    print("Cross Validation Score: ",CV)
    print(np.sqrt(error))
    print("R-square value: ",r2_score(y_test,predict))
    print("Confusion matrix: ")
    print(confusion_matrix(y_test,predict))
    print("-" * 30)
436/25:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    error = -cross_val_score(model,x_test,y_test,cv=10,scoring="neg_mean_squared_error",verbose=False).mean()
    print(name + ": ")
    print("-" * 50)
    print("Accuracy Score: ",accuracy_score(y_test,predict))
    print("Cross Validation Score: ",CV)
    print(np.sqrt(error))
    print("R-square value: ",r2_score(y_test,predict))
    print("Confusion matrix: ")
    print(confusion_matrix(y_test,predict))
    print("-" * 100)
437/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
437/2: warnings.filterwarnings("ignore")
437/3:
data = pd.read_csv("heart_disease.csv")
data.head()
437/4:
print("Shape:")
print(data.shape)
print("-"*100)
print("Columns Name:")
columns = data.columns
print(columns)
print("-"*100)
print("Data Information:")
data.info()
print("-"*100)
print("Data Description: ")
print(data.describe())
print("-"*100)
print("Counting Null Values:")
print(data.isnull().sum())
print("-"*100)
print("Data correlation: ")
print(data.corr())
print("-"*100)
437/5:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
437/6:
data.hist(figsize=(20,20))
plt.show()
437/7:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
437/8:
sns.kdeplot(data[data['target']==1]['chol'],shade=True,color="orange", label="Unwell", alpha=.7)
sns.kdeplot(data[data['target']==0]['chol'],shade=True,color="dodgerblue", label="Healthy", alpha=.7)
plt.title('Cholesterol in mg/d for both case')
plt.show()
437/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
437/10: sns.pairplot(data, hue="target")
437/11:
X = data.drop("target",axis=1)
y = data["target"]
437/12:
std_scl = StandardScaler()

X = std_scl.fit_transform(X)
print(X)
437/13:
plt.figure(figsize=(15,8))
sns.heatmap(X)
437/14: x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
437/15:
lr = LogisticRegression(solver="liblinear").fit(x_train,y_train)
gnb = GaussianNB().fit(x_train,y_train)
knnc = KNeighborsClassifier().fit(x_train,y_train)
dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)
rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)
xgbc = XGBClassifier().fit(x_train,y_train)
svmc = SVC().fit(x_train,y_train)
catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)
437/16: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,svmc,catbc]
437/17:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    error = -cross_val_score(model,x_test,y_test,cv=10,scoring="neg_mean_squared_error",verbose=False).mean()
    print(name + ": ")
    print("-" * 50)
    print("Accuracy Score: ",accuracy_score(y_test,predict))
    print("Cross Validation Score: ",CV)
    print("Error: ",np.sqrt(error))
    print("R-square value: ",r2_score(y_test,predict))
    print("Confusion matrix: ")
    print(confusion_matrix(y_test,predict))
    print("-" * 100)
437/18:
# Training and testing on all features.

lg_model = LogisticRegression()
lg_model.fit(all_x_train,all_y_train)
lg_model_prediction = lg_model.predict(all_x_test)
print("Accuracy on all features: ", accuracy_score(all_y_test,lg_model_prediction)*100)
print("R-square on all features: ", r2_score(all_y_test,lg_model_prediction))
437/19: r = pd.DataFrame(columns=["MODELS","R2CV"])
437/20:
r = pd.DataFrame(columns=["MODELS","R2CV"])
r
437/21:
r = pd.DataFrame(columns=["MODELS","Accuracy"])
r
437/22:
df = pd.DataFrame(columns=["MODELS","R2CV"])
for model in model_names:
    name = model.__class__.__name__
    accuracy = accuracy_score(y_test,predict)
    result = pd.DataFrame([[name,accuracy*100]],columns=["MODELS","Accuracy"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="Accuracy",y="MODELS",data=r,color="k")
plt.xlabel("ACCURACY")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL ACCURACY COMPARISON")
plt.show()
437/23:
df = pd.DataFrame(columns=["MODELS","Accuracy"])
for model in model_names:
    name = model.__class__.__name__
    accuracy = accuracy_score(y_test,predict)
    result = pd.DataFrame([[name,accuracy*100]],columns=["MODELS","Accuracy"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="Accuracy",y="MODELS",data=r,color="k")
plt.xlabel("ACCURACY")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL ACCURACY COMPARISON")
plt.show()
437/24:
df = pd.DataFrame(columns=["MODELS","Accuracy"])
for model in model_names:
    name = model.__class__.__name__
    accuracy = accuracy_score(y_test,predict)
    result = pd.DataFrame([[name,accuracy*100]],columns=["MODELS","Accuracy"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="Accuracy",y="MODELS",data=df,color="k")
plt.xlabel("ACCURACY")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL ACCURACY COMPARISON")
plt.show()
438/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
438/2: warnings.filterwarnings("ignore")
438/3:
data = pd.read_csv("heart_disease.csv")
data.head()
438/4:
print("Shape:")
print(data.shape)
print("-"*100)
print("Columns Name:")
columns = data.columns
print(columns)
print("-"*100)
print("Data Information:")
data.info()
print("-"*100)
print("Data Description: ")
print(data.describe())
print("-"*100)
print("Counting Null Values:")
print(data.isnull().sum())
print("-"*100)
print("Data correlation: ")
print(data.corr())
print("-"*100)
438/5:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
438/6:
data.hist(figsize=(20,20))
plt.show()
438/7:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
438/8:
sns.kdeplot(data[data['target']==1]['chol'],shade=True,color="orange", label="Unwell", alpha=.7)
sns.kdeplot(data[data['target']==0]['chol'],shade=True,color="dodgerblue", label="Healthy", alpha=.7)
plt.title('Cholesterol in mg/d for both case')
plt.show()
438/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
438/10: sns.pairplot(data, hue="target")
438/11:
X = data.drop("target",axis=1)
y = data["target"]
438/12:
std_scl = StandardScaler()

X = std_scl.fit_transform(X)
print(X)
438/13:
plt.figure(figsize=(15,8))
sns.heatmap(X)
438/14: x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
438/15:
lr = LogisticRegression(solver="liblinear").fit(x_train,y_train)
gnb = GaussianNB().fit(x_train,y_train)
knnc = KNeighborsClassifier().fit(x_train,y_train)
dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)
rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)
xgbc = XGBClassifier().fit(x_train,y_train)
svmc = SVC().fit(x_train,y_train)
catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)
438/16: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,svmc,catbc]
438/17:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    error = -cross_val_score(model,x_test,y_test,cv=10,scoring="neg_mean_squared_error",verbose=False).mean()
    print(name + ": ")
    print("-" * 50)
    print("Accuracy Score: ",accuracy_score(y_test,predict))
    print("Cross Validation Score: ",CV)
    print("Error: ",np.sqrt(error))
    print("R-square value: ",r2_score(y_test,predict))
    print("Confusion matrix: ")
    print(confusion_matrix(y_test,predict))
    print("-" * 100)
438/18:
r = pd.DataFrame(columns=["MODELS","Accuracy"])
r
438/19:
df = pd.DataFrame(columns=["MODELS","Accuracy"])
for model in model_names:
    name = model.__class__.__name__
    accuracy = accuracy_score(y_test,predict)
    result = pd.DataFrame([[name,accuracy*100]],columns=["MODELS","Accuracy"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="Accuracy",y="MODELS",data=df,color="k")
plt.xlabel("ACCURACY")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL ACCURACY COMPARISON")
plt.show()
438/20: df
438/21:
df = pd.DataFrame(columns=["MODELS","Accuracy"])
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    accuracy = accuracy_score(y_test,predict)
    result = pd.DataFrame([[name,accuracy*100]],columns=["MODELS","Accuracy"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="Accuracy",y="MODELS",data=df,color="k")
plt.xlabel("ACCURACY")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL ACCURACY COMPARISON")
plt.show()
438/22: df
438/23:
df = pd.DataFrame(columns=["MODELS","CV"])
for model in model_names:
    name = model.__class__.__name__
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    result = pd.DataFrame([[name,CV*100]],columns=["MODELS","CV"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="CV",y="MODELS",data=df,color="k")
plt.xlabel("CV")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL CROSS VALIDATION COMPARISON")
plt.show()
438/24:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score
438/25: r_prob = [0 for _ in range(len(y_test))]
438/26:
r_prob = [0 for _ in range(len(y_test))]
r_prob
438/27: r_prob = [0 for _ in range(len(y_test))]
438/28:
r_prob = [0 for _ in range(len(y_test))]

rf = rf.predict_proba(x_test)
rf
438/29:
r_prob = [0 for _ in range(len(y_test))]

rfc = rfc.predict_proba(x_test)
rfc
438/30: rfc_prob = rfc_prob[:.1]
438/31: rfc = rfc[:.1]
438/32: rfc = rfc[:,1]
438/33:
rfc = rfc[:,1]
rfc
438/34: rfc
438/35:
r_prob = [0 for _ in range(len(y_test))]

rfc = rfc.predict_proba(x_test)[1]
rfc
438/36:
r_prob = [0 for _ in range(len(y_test))]

rfc = rfc.predict_proba(x_test)[:,1]
rfc
438/37:
r_prob = [0 for _ in range(len(y_test))]

rfc = rfc.predict_proba(x_test)
rfc
439/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score
439/2: warnings.filterwarnings("ignore")
439/3:
data = pd.read_csv("heart_disease.csv")
data.head()
439/4:
print("Shape:")
print(data.shape)
print("-"*100)
print("Columns Name:")
columns = data.columns
print(columns)
print("-"*100)
print("Data Information:")
data.info()
print("-"*100)
print("Data Description: ")
print(data.describe())
print("-"*100)
print("Counting Null Values:")
print(data.isnull().sum())
print("-"*100)
print("Data correlation: ")
print(data.corr())
print("-"*100)
439/5:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
439/6:
data.hist(figsize=(20,20))
plt.show()
439/7:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
439/8:
sns.kdeplot(data[data['target']==1]['chol'],shade=True,color="orange", label="Unwell", alpha=.7)
sns.kdeplot(data[data['target']==0]['chol'],shade=True,color="dodgerblue", label="Healthy", alpha=.7)
plt.title('Cholesterol in mg/d for both case')
plt.show()
439/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
439/10: sns.pairplot(data, hue="target")
439/11:
X = data.drop("target",axis=1)
y = data["target"]
439/12:
std_scl = StandardScaler()

X = std_scl.fit_transform(X)
print(X)
439/13:
plt.figure(figsize=(15,8))
sns.heatmap(X)
439/14: x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
439/15:
lr = LogisticRegression(solver="liblinear").fit(x_train,y_train)
gnb = GaussianNB().fit(x_train,y_train)
knnc = KNeighborsClassifier().fit(x_train,y_train)
dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)
rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)
xgbc = XGBClassifier().fit(x_train,y_train)
svmc = SVC().fit(x_train,y_train)
catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)
439/16: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,svmc,catbc]
439/17:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    error = -cross_val_score(model,x_test,y_test,cv=10,scoring="neg_mean_squared_error",verbose=False).mean()
    print(name + ": ")
    print("-" * 50)
    print("Accuracy Score: ",accuracy_score(y_test,predict))
    print("Cross Validation Score: ",CV)
    print("Error: ",np.sqrt(error))
    print("R-square value: ",r2_score(y_test,predict))
    print("Confusion matrix: ")
    print(confusion_matrix(y_test,predict))
    print("-" * 100)
439/18:
df = pd.DataFrame(columns=["MODELS","Accuracy"])
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    accuracy = accuracy_score(y_test,predict)
    result = pd.DataFrame([[name,accuracy*100]],columns=["MODELS","Accuracy"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="Accuracy",y="MODELS",data=df,color="k")
plt.xlabel("ACCURACY")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL ACCURACY COMPARISON")
plt.show()
439/19:
df = pd.DataFrame(columns=["MODELS","CV"])
for model in model_names:
    name = model.__class__.__name__
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    result = pd.DataFrame([[name,CV*100]],columns=["MODELS","CV"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="CV",y="MODELS",data=df,color="k")
plt.xlabel("CV")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL CROSS VALIDATION COMPARISON")
plt.show()
439/20:
r_prob = [0 for _ in range(len(y_test))]

rfc = rfc.predict_proba(x_test)
rfc
439/21: rfc = rfc[:,1]
439/22: rfc
439/23:
r_prob = [0 for _ in range(len(y_test))]

rfcc = rfc.predict_proba(x_test)[:,1]
rfcc
439/24:
r_prob = [0 for _ in range(len(y_test))]

rfcca = rfc.predict_proba(x_test)[:,1]
rfcca
439/25: r_prob = [0 for _ in range(len(y_test))]
439/26:
r_prob = [0 for _ in range(len(y_test))]

rfcc = rfc.predict_proba(x_test)
439/27:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    error = -cross_val_score(model,x_test,y_test,cv=10,scoring="neg_mean_squared_error",verbose=False).mean()
    print(name + ": ")
    print("-" * 50)
    print("Accuracy Score: ",accuracy_score(y_test,predict))
    print("Cross Validation Score: ",CV)
    print("Error: ",np.sqrt(error))
    print("R-square value: ",r2_score(y_test,predict))
    print("Confusion matrix: ")
    print(confusion_matrix(y_test,predict))
    print("-" * 100)
439/28:
df = pd.DataFrame(columns=["MODELS","Accuracy"])
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    accuracy = accuracy_score(y_test,predict)
    result = pd.DataFrame([[name,accuracy*100]],columns=["MODELS","Accuracy"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="Accuracy",y="MODELS",data=df,color="k")
plt.xlabel("ACCURACY")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL ACCURACY COMPARISON")
plt.show()
439/29:
df = pd.DataFrame(columns=["MODELS","CV"])
for model in model_names:
    name = model.__class__.__name__
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    result = pd.DataFrame([[name,CV*100]],columns=["MODELS","CV"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="CV",y="MODELS",data=df,color="k")
plt.xlabel("CV")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL CROSS VALIDATION COMPARISON")
plt.show()
439/30:
r_prob = [0 for _ in range(len(y_test))]

rfcc = rfc.predict_proba(x_test)
439/31:
r_prob = [0 for _ in range(len(y_test))]

rfcc = rfc.predict_proba(x_test)
439/32: rfcc
439/33:
r_prob = [0 for _ in range(len(y_test))]

rfcc = rfc.predict_proba(x_test)
440/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score
440/2: warnings.filterwarnings("ignore")
440/3:
data = pd.read_csv("heart_disease.csv")
data.head()
440/4:
print("Shape:")
print(data.shape)
print("-"*100)
print("Columns Name:")
columns = data.columns
print(columns)
print("-"*100)
print("Data Information:")
data.info()
print("-"*100)
print("Data Description: ")
print(data.describe())
print("-"*100)
print("Counting Null Values:")
print(data.isnull().sum())
print("-"*100)
print("Data correlation: ")
print(data.corr())
print("-"*100)
440/5:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
440/6:
data.hist(figsize=(20,20))
plt.show()
440/7:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
440/8:
sns.kdeplot(data[data['target']==1]['chol'],shade=True,color="orange", label="Unwell", alpha=.7)
sns.kdeplot(data[data['target']==0]['chol'],shade=True,color="dodgerblue", label="Healthy", alpha=.7)
plt.title('Cholesterol in mg/d for both case')
plt.show()
440/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
440/10: sns.pairplot(data, hue="target")
440/11:
X = data.drop("target",axis=1)
y = data["target"]
440/12:
std_scl = StandardScaler()

X = std_scl.fit_transform(X)
print(X)
440/13:
plt.figure(figsize=(15,8))
sns.heatmap(X)
440/14: x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
440/15:
lr = LogisticRegression(solver="liblinear").fit(x_train,y_train)
gnb = GaussianNB().fit(x_train,y_train)
knnc = KNeighborsClassifier().fit(x_train,y_train)
dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)
rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)
xgbc = XGBClassifier().fit(x_train,y_train)
svmc = SVC().fit(x_train,y_train)
catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)
440/16: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,svmc,catbc]
440/17:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    error = -cross_val_score(model,x_test,y_test,cv=10,scoring="neg_mean_squared_error",verbose=False).mean()
    print(name + ": ")
    print("-" * 50)
    print("Accuracy Score: ",accuracy_score(y_test,predict))
    print("Cross Validation Score: ",CV)
    print("Error: ",np.sqrt(error))
    print("R-square value: ",r2_score(y_test,predict))
    print("Confusion matrix: ")
    print(confusion_matrix(y_test,predict))
    print("-" * 100)
440/18:
df = pd.DataFrame(columns=["MODELS","Accuracy"])
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    accuracy = accuracy_score(y_test,predict)
    result = pd.DataFrame([[name,accuracy*100]],columns=["MODELS","Accuracy"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="Accuracy",y="MODELS",data=df,color="k")
plt.xlabel("ACCURACY")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL ACCURACY COMPARISON")
plt.show()
440/19:
df = pd.DataFrame(columns=["MODELS","CV"])
for model in model_names:
    name = model.__class__.__name__
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    result = pd.DataFrame([[name,CV*100]],columns=["MODELS","CV"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="CV",y="MODELS",data=df,color="k")
plt.xlabel("CV")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL CROSS VALIDATION COMPARISON")
plt.show()
440/20: r_prob = [0 for _ in range(len(y_test))]
440/21: rfcc
440/22:
r_prob = [0 for _ in range(len(y_test))]
rcc = rfc.predict_proba(x_test)
440/23: rcc
440/24:
r_prob = [0 for _ in range(len(y_test))]
rcc = rfc.predict_proba(x_test)
440/25:
r_prob = [0 for _ in range(len(y_test))]
rcc = rfc.predict_proba(x_test)[:,1]
440/26: rcc
440/27:
df = pd.DataFrame(columns=["MODELS","Roc_auc_score"])
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)
    auroc_score = roc_auc_score(y_test,predict)
    print(name,"score: ",auroc_score)
    print("-"*50)
440/28:
df = pd.DataFrame(columns=["MODELS","Roc_auc_score"])
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)
    auroc_score = roc_auc_score(y_test,predict)
    print(name+"score: ",auroc_score)
    print("-"*50)
440/29:
df = pd.DataFrame(columns=["MODELS","Roc_auc_score"])
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    auroc_score = roc_auc_score(y_test,predict)
    print(name+"score: ",auroc_score)
    print("-"*50)
441/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score
441/2: warnings.filterwarnings("ignore")
441/3:
data = pd.read_csv("heart_disease.csv")
data.head()
441/4:
print("Shape:")
print(data.shape)
print("-"*100)
print("Columns Name:")
columns = data.columns
print(columns)
print("-"*100)
print("Data Information:")
data.info()
print("-"*100)
print("Data Description: ")
print(data.describe())
print("-"*100)
print("Counting Null Values:")
print(data.isnull().sum())
print("-"*100)
print("Data correlation: ")
print(data.corr())
print("-"*100)
441/5:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
441/6:
data.hist(figsize=(20,20))
plt.show()
441/7:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
441/8:
sns.kdeplot(data[data['target']==1]['chol'],shade=True,color="orange", label="Unwell", alpha=.7)
sns.kdeplot(data[data['target']==0]['chol'],shade=True,color="dodgerblue", label="Healthy", alpha=.7)
plt.title('Cholesterol in mg/d for both case')
plt.show()
441/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
441/10: sns.pairplot(data, hue="target")
441/11:
X = data.drop("target",axis=1)
y = data["target"]
441/12:
std_scl = StandardScaler()

X = std_scl.fit_transform(X)
print(X)
441/13:
plt.figure(figsize=(15,8))
sns.heatmap(X)
441/14: x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
441/15:
lr = LogisticRegression(solver="liblinear").fit(x_train,y_train)
gnb = GaussianNB().fit(x_train,y_train)
knnc = KNeighborsClassifier().fit(x_train,y_train)
dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)
rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)
xgbc = XGBClassifier().fit(x_train,y_train)
catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)
441/16: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,catbc]
441/17:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    error = -cross_val_score(model,x_test,y_test,cv=10,scoring="neg_mean_squared_error",verbose=False).mean()
    print(name + ": ")
    print("-" * 50)
    print("Accuracy Score: ",accuracy_score(y_test,predict))
    print("Cross Validation Score: ",CV)
    print("Error: ",np.sqrt(error))
    print("R-square value: ",r2_score(y_test,predict))
    print("Confusion matrix: ")
    print(confusion_matrix(y_test,predict))
    print("-" * 100)
441/18:
df = pd.DataFrame(columns=["MODELS","Accuracy"])
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    accuracy = accuracy_score(y_test,predict)
    result = pd.DataFrame([[name,accuracy*100]],columns=["MODELS","Accuracy"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="Accuracy",y="MODELS",data=df,color="k")
plt.xlabel("ACCURACY")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL ACCURACY COMPARISON")
plt.show()
441/19:
df = pd.DataFrame(columns=["MODELS","CV"])
for model in model_names:
    name = model.__class__.__name__
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    result = pd.DataFrame([[name,CV*100]],columns=["MODELS","CV"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="CV",y="MODELS",data=df,color="k")
plt.xlabel("CV")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL CROSS VALIDATION COMPARISON")
plt.show()
441/20:
r_prob = [0 for _ in range(len(y_test))]
r_auc = roc_auc_score(y_test,r_prob)
441/21:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    auroc_score = roc_auc_score(y_test,predict)
    print(name+"score: ",auroc_score)
    print("-"*50)
441/22: rfc
441/23:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    auroc_score = roc_auc_score(y_test,predict)
    print(name+" score: ",auroc_score)
    print("-"*50)
441/24:
r_fpt,r_tpr,_= roc_curve(y_test,r_prob)
model_dict={}

for model in model_names:
    name = model.__class__.name
    predict = model.predict_proba(x_test)[:,1]
    fpt,tpr,_= roc_curve(y_test,predict)
    model_dict[name]=[fpt,tpr]
    
model_dict
441/25:
r_fpt,r_tpr,_= roc_curve(y_test,r_prob)
model_dict={}

for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    fpt,tpr,_= roc_curve(y_test,predict)
    model_dict[name]=[fpt,tpr]
    
model_dict
441/26:
r_fpt,r_tpr,_= roc_curve(y_test,r_prob)
model_dict={}

for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    fpt,tpr,_= roc_curve(y_test,predict)
    model_dict[name]=[fpt,tpr]
    
model_dict["LogisticRegression"][0]
441/27:
r_fpt,r_tpr,_= roc_curve(y_test,r_prob)
model_dict={}

for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    fpt,tpr,_= roc_curve(y_test,predict)
    model_dict[name]=[fpt,tpr]
    
model_dict["LogisticRegression"][1]
441/28:
r_fpt,r_tpr,_= roc_curve(y_test,r_prob)
model_dict={}

for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    fpt,tpr,_= roc_curve(y_test,predict)
    model_dict[name]=[fpt,tpr]
441/29: model_dict["LogisticRegression"][1]
441/30: model_dict["LogisticRegression"][0]
441/31:
r_fpr,r_tpr,_= roc_curve(y_test,r_prob)
model_dict={}

for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    fpr,tpr,_= roc_curve(y_test,predict)
    model_dict[name]=[fpt,tpr]
441/32: model_dict["LogisticRegression"][0]
441/33:
r_fpr,r_tpr,_= roc_curve(y_test,r_prob)
model_dict={}

for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    fpr,tpr,_= roc_curve(y_test,predict)
    model_dict[name]=[fpt,tpr]
441/34: model_dict["LogisticRegression"][0]
441/35: model_dict["LogisticRegression"][1]
441/36: model_dict["LogisticRegression"][0]
441/37:
plt.plot(r_fpr,r_tpr,linestyle="--")
plt.title("ROC plot")
plt.xlabel("False positive rate.")
plt.ylabel("True positive rate.")
plt.show()
441/38:
plt.plot(r_fpr,r_tpr,linestyle="--")
plt.plot(model_dict["LogisticRegression"][0],model_dict["LogisticRegression"][1],linestyle='.')






plt.title("ROC plot")
plt.xlabel("False positive rate.")
plt.ylabel("True positive rate.")
plt.show()
441/39:
plt.plot(r_fpr,r_tpr,linestyle="--")
plt.plot(*model_dict["LogisticRegression"][0],*model_dict["LogisticRegression"][1],linestyle='.')






plt.title("ROC plot")
plt.xlabel("False positive rate.")
plt.ylabel("True positive rate.")
plt.show()
441/40: *model_dict["LogisticRegression"][0]
441/41: model_dict["LogisticRegression"][0]
441/42: r_fpr
441/43:
plt.plot(r_fpr,r_tpr,linestyle="--")
plt.plot(model_dict["LogisticRegression"][0],model_dict["LogisticRegression"][1],linestyle='.')






plt.title("ROC plot")
plt.xlabel("False positive rate.")
plt.ylabel("True positive rate.")
plt.show()
441/44: r_tpr
441/45: model_dict["LogisticRegression"][1]
441/46: len(model_dict["LogisticRegression"][1])
441/47: len(model_dict["LogisticRegression"][0])
442/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score
442/2: warnings.filterwarnings("ignore")
442/3:
data = pd.read_csv("heart_disease.csv")
data.head()
442/4:
print("Shape:")
print(data.shape)
print("-"*100)
print("Columns Name:")
columns = data.columns
print(columns)
print("-"*100)
print("Data Information:")
data.info()
print("-"*100)
print("Data Description: ")
print(data.describe())
print("-"*100)
print("Counting Null Values:")
print(data.isnull().sum())
print("-"*100)
print("Data correlation: ")
print(data.corr())
print("-"*100)
442/5:
figure = plt.figure(figsize=(10,8))
sns.heatmap(data.corr(method="pearson"),annot=True,cmap='YlGnBu_r', vmin=-1, vmax=+1)
plt.title("PEARSON")
plt.xlabel("COLUMNS")
plt.ylabel("COLUMNS")
plt.show()
442/6:
data.hist(figsize=(20,20))
plt.show()
442/7:
sns.countplot(data["target"])
print(data.groupby(["target"]).count())

# 0 -> No Heart Disease, 1 -> Heart Disease
442/8:
sns.kdeplot(data[data['target']==1]['chol'],shade=True,color="orange", label="Unwell", alpha=.7)
sns.kdeplot(data[data['target']==0]['chol'],shade=True,color="dodgerblue", label="Healthy", alpha=.7)
plt.title('Cholesterol in mg/d for both case')
plt.show()
442/9:
plt.figure(figsize=(15,8))
sns.heatmap(data)
442/10: sns.pairplot(data, hue="target")
442/11:
X = data.drop("target",axis=1)
y = data["target"]
442/12:
std_scl = StandardScaler()

X = std_scl.fit_transform(X)
print(X)
442/13:
plt.figure(figsize=(15,8))
sns.heatmap(X)
442/14: x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
442/15:
lr = LogisticRegression(solver="liblinear").fit(x_train,y_train)
gnb = GaussianNB().fit(x_train,y_train)
knnc = KNeighborsClassifier().fit(x_train,y_train)
dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)
rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)
xgbc = XGBClassifier().fit(x_train,y_train)
catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)
442/16: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,catbc]
442/17:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    error = -cross_val_score(model,x_test,y_test,cv=10,scoring="neg_mean_squared_error",verbose=False).mean()
    print(name + ": ")
    print("-" * 50)
    print("Accuracy Score: ",accuracy_score(y_test,predict))
    print("Cross Validation Score: ",CV)
    print("Error: ",np.sqrt(error))
    print("R-square value: ",r2_score(y_test,predict))
    print("Confusion matrix: ")
    print(confusion_matrix(y_test,predict))
    print("-" * 100)
442/18:
df = pd.DataFrame(columns=["MODELS","Accuracy"])
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    accuracy = accuracy_score(y_test,predict)
    result = pd.DataFrame([[name,accuracy*100]],columns=["MODELS","Accuracy"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="Accuracy",y="MODELS",data=df,color="k")
plt.xlabel("ACCURACY")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL ACCURACY COMPARISON")
plt.show()
442/19:
df = pd.DataFrame(columns=["MODELS","CV"])
for model in model_names:
    name = model.__class__.__name__
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    result = pd.DataFrame([[name,CV*100]],columns=["MODELS","CV"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="CV",y="MODELS",data=df,color="k")
plt.xlabel("CV")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL CROSS VALIDATION COMPARISON")
plt.show()
442/20:
r_prob = [0 for _ in range(len(y_test))]
r_auc = roc_auc_score(y_test,r_prob)
442/21:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    auroc_score = roc_auc_score(y_test,predict)
    print(name+" score: ",auroc_score)
    print("-"*50)
442/22:
r_fpr,r_tpr,_= roc_curve(y_test,r_prob)
model_dict={}

for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    fpr,tpr,_= roc_curve(y_test,predict)
    model_dict[name]=[fpt,tpr]
442/23:
r_fpr,r_tpr,_= roc_curve(y_test,r_prob)
model_dict={}

for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    fpr,tpr,_= roc_curve(y_test,predict)
    model_dict[name]=[fpr,tpr]
442/24:
plt.plot(r_fpr,r_tpr,linestyle="--")
plt.plot(model_dict["LogisticRegression"][0],model_dict["LogisticRegression"][1],linestyle='dotted')
plt.plot(model_dict["GaussianNB"][0],model_dict["GaussianNB"][1],linestyle='dotted')
plt.plot(model_dict["KNeighborsClassifier"][0],model_dict["KNeighborsClassifier"][1],linestyle='dotted')
plt.plot(model_dict["DecisionTreeClassifier"][0],model_dict["DecisionTreeClassifier"][1],linestyle='dotted')
plt.plot(model_dict["RandomForestClassifier"][0],model_dict["RandomForestClassifier"][1],linestyle='dotted')
plt.plot(model_dict["XGBClassifier"][0],model_dict["XGBClassifier"][1],linestyle='dotted')
plt.plot(model_dict["CatBoostClassifier"][0],model_dict["CatBoostClassifier"][1],linestyle='dotted')

plt.title("ROC plot")
plt.xlabel("False positive rate.")
plt.ylabel("True positive rate.")
plt.show()
442/25:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score
442/26: warnings.filterwarnings("ignore")
442/27:
data = pd.read_csv("heart_disease.csv")
data.head()
442/28:
from dataprep.datasets import load_dataset
from dataprep.eda import create_report
df = load_datatset("heart_disease")
create_report(df)
442/29:
import dtale
import seaborn as sns
442/30:
df = sns.load_dataset("Heart_disease")
dtale.show(df)
442/31:
df = sns.load_dataset("heart_disease")
dtale.show(df)
442/32:
import dtale
import seaborn as sns
import pandas as pd
442/33:
df = pd.read_csv("heart_disease")
dtale.show(df)
442/34:
df = pd.read_csv("heart_disease.csv")
dtale.show(df)
442/35:
from pandas_profiling import ProfileReport
profile=ProfileReport(df,explorative=True)
profile.to_file("output.html")
442/36:
import sweetviz as sv
report = sv.analyze(df)
report.show_html("report.html")
443/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
443/2: df = pd.read_csv("StudentsPerformance.csv")
443/3: df.head()
443/4:
print("Shape: ")
print(df.shape)
443/5:
print("Shape: ")
print(df.shape)
print("-"*100)
443/6:
print("Shape: ")
print(df.shape)
print("-"*100)
print("Columns: ")
columns = df.columns()
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Finding Null values: ")
print(df.isnull().sum())
print("-"*100)
print("Data discription: ")
print(df.describe())
print("-"*100)
443/7:
print("Shape: ")
print(df.shape)
print("-"*100)
print("Columns: ")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Finding Null values: ")
print(df.isnull().sum())
print("-"*100)
print("Data discription: ")
print(df.describe())
print("-"*100)
443/8:
def unique_values(columns):
    print(columns+": ",df[columns].unique())
    print("-"*100)
443/9:
for ele in columns:
    unique_values(ele)
443/10:
for ele in columns:
    unique_values(ele)
443/11: df["Percentage"] = round((df["math score"]+df["reading score"]+df["writing score"])/3,2)
443/12: df.head()
443/13: df["CGPA"] = round((df["Percentage"])/9.5,2)
443/14: df.head()
443/15:
# 8.0 - 10.0 --> A+
# 7.0 - <8.0 --> A
# 6.0 - <7.0 --> B+
# 5.5 - <6.0 --> B
# 4.5 - <5.5 --> C+
# 4.0 - <4.5 --> C
# 0.0 - <4.0 --> F

df["CGPA"][0]
443/16:
# 8.0 - 10.0 --> A+
# 7.0 - <8.0 --> A
# 6.0 - <7.0 --> B+
# 5.5 - <6.0 --> B
# 4.5 - <5.5 --> C+
# 4.0 - <4.5 --> C
# 0.0 - <4.0 --> F

df["CGPA"][1]
443/17:
# 8.0 - 10.0 --> A+
# 7.0 - <8.0 --> A
# 6.0 - <7.0 --> B+
# 5.5 - <6.0 --> B
# 4.5 - <5.5 --> C+
# 4.0 - <4.5 --> C
# 0.0 - <4.0 --> F

df["CGPA"][1000]
443/18:
# 8.0 - 10.0 --> A+
# 7.0 - <8.0 --> A
# 6.0 - <7.0 --> B+
# 5.5 - <6.0 --> B
# 4.5 - <5.5 --> C+
# 4.0 - <4.5 --> C
# 0.0 - <4.0 --> F

df["CGPA"][999]
443/19: df.tail()
443/20: df["Grade"]
443/21: df.head()
443/22: df["Grade"]=0
443/23: df.head()
443/24:
# 8.0 - 10.0 --> A+
# 7.0 - <8.0 --> A
# 6.0 - <7.0 --> B+
# 5.5 - <6.0 --> B
# 4.5 - <5.5 --> C+
# 4.0 - <4.5 --> C
# 0.0 - <4.0 --> F
def grade(i):
    if df["CGPA"][i]>=8.0:
        df["Grade"][i] = "A+"
    elif df["CGPA"][i]>=7.0 and df["CGPA"][i]<8.0:
        df["Grade"][i] = "A"
    elif df["CGPA"][i]>=6.0 and df["CGPA"][i]<7.0:
        df["Grade"][i] = "B+"
    elif df["CGPA"][i]>=5.5 and df["CGPA"][i]<6.0:
        df["Grade"][i] = "B"
    elif df["CGPA"][i]>=4.5 and df["CGPA"][i]<5.5:
        df["Grade"][i] = "C+"
    elif df["CGPA"][i]>=4.0 and df["CGPA"][i]<4.5:
        df["Grade"][i] = "C"
    else:
        df["Grade"] = "F"
443/25:
df["Grade"]="nan"
for i in range(1000):
    grade(i)
443/26:
df["Grade"]=nan
for i in range(1000):
    grade(i)
443/27:
df["Grade"]=0
for i in range(1000):
    grade(i)
443/28:
df["Grade"]="nan"
for i in range(1000):
    grade(i)
444/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
444/2: df = pd.read_csv("StudentsPerformance.csv")
444/3: df.head()
444/4:
print("Shape: ")
print(df.shape)
print("-"*100)
print("Columns: ")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Finding Null values: ")
print(df.isnull().sum())
print("-"*100)
print("Data discription: ")
print(df.describe())
print("-"*100)
444/5:
def unique_values(columns):
    print(columns+": ",df[columns].unique())
    print("-"*100)
444/6:
for ele in columns:
    unique_values(ele)
444/7: df["Percentage"] = round((df["math score"]+df["reading score"]+df["writing score"])/3,2)
444/8: df.head()
444/9: df["CGPA"] = round((df["Percentage"])/9.5,2)
444/10:
# 8.0 - 10.0 --> A+
# 7.0 - <8.0 --> A
# 6.0 - <7.0 --> B+
# 5.5 - <6.0 --> B
# 4.5 - <5.5 --> C+
# 4.0 - <4.5 --> C
# 0.0 - <4.0 --> F
def grade(i):
    if df["CGPA"][i]>=8.0:
        df["Grade"][i] = "A+"
    elif df["CGPA"][i]>=7.0 and df["CGPA"][i]<8.0:
        df["Grade"][i] = "A"
    elif df["CGPA"][i]>=6.0 and df["CGPA"][i]<7.0:
        df["Grade"][i] = "B+"
    elif df["CGPA"][i]>=5.5 and df["CGPA"][i]<6.0:
        df["Grade"][i] = "B"
    elif df["CGPA"][i]>=4.5 and df["CGPA"][i]<5.5:
        df["Grade"][i] = "C+"
    elif df["CGPA"][i]>=4.0 and df["CGPA"][i]<4.5:
        df["Grade"][i] = "C"
    else:
        df["Grade"] = "F"
444/11:
df["Grade"]="nan"
for i in range(1000):
    grade(i)
444/12: df.head()
444/13:
df["Grade"]=0
for i in range(1000):
    grade(i)
444/14:
# 8.0 - 10.0 --> A+
# 7.0 - <8.0 --> A
# 6.0 - <7.0 --> B+
# 5.5 - <6.0 --> B
# 4.5 - <5.5 --> C+
# 4.0 - <4.5 --> C
# 0.0 - <4.0 --> F
def grade(i):
    if df["CGPA"][i]>=8.0:
        df["Grade"][i] = "A+"
    elif df["CGPA"][i]>=7.0 and df["CGPA"][i]<8.0:
        df["Grade"][i] = "A"
    elif df["CGPA"][i]>=6.0 and df["CGPA"][i]<7.0:
        df["Grade"][i] = "B+"
    elif df["CGPA"][i]>=5.5 and df["CGPA"][i]<6.0:
        df["Grade"][i] = "B"
    elif df["CGPA"][i]>=4.5 and df["CGPA"][i]<5.5:
        df["Grade"][i] = "C+"
    elif df["CGPA"][i]>=4.0 and df["CGPA"][i]<4.5:
        df["Grade"][i] = "C"
    else:
        df["Grade"] = "F"
444/15: df.head()
445/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
445/2: df = pd.read_csv("StudentsPerformance.csv")
445/3: df.head()
445/4:
print("Shape: ")
print(df.shape)
print("-"*100)
print("Columns: ")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Finding Null values: ")
print(df.isnull().sum())
print("-"*100)
print("Data discription: ")
print(df.describe())
print("-"*100)
445/5:
def unique_values(columns):
    print(columns+": ",df[columns].unique())
    print("-"*100)
445/6:
for ele in columns:
    unique_values(ele)
445/7: df["Percentage"] = round((df["math score"]+df["reading score"]+df["writing score"])/3,2)
445/8: df.head()
445/9: df["CGPA"] = round((df["Percentage"])/9.5,2)
445/10:
# 8.0 - 10.0 --> A+
# 7.0 - <8.0 --> A
# 6.0 - <7.0 --> B+
# 5.5 - <6.0 --> B
# 4.5 - <5.5 --> C+
# 4.0 - <4.5 --> C
# 0.0 - <4.0 --> F
def grade(i):
    if df["CGPA"][i]>=8.0:
        df["Grade"][i] = "A+"
    elif df["CGPA"][i]>=7.0 and df["CGPA"][i]<8.0:
        df["Grade"][i] = "A"
    elif df["CGPA"][i]>=6.0 and df["CGPA"][i]<7.0:
        df["Grade"][i] = "B+"
    elif df["CGPA"][i]>=5.5 and df["CGPA"][i]<6.0:
        df["Grade"][i] = "B"
    elif df["CGPA"][i]>=4.5 and df["CGPA"][i]<5.5:
        df["Grade"][i] = "C+"
    elif df["CGPA"][i]>=4.0 and df["CGPA"][i]<4.5:
        df["Grade"][i] = "C"
    else:
        df["Grade"] = "F"
445/11:
df["Grade"]=0
for i in range(1000):
    grade(i)
445/12: df.head()
445/13: df["Grade"][0].replacee("F","Jay")
445/14: df["Grade"][0].replace("F","Jay")
445/15: df.head()
445/16: df["Grade"][0].replace("F","Jay",inplace=True)
445/17: df["Grade"][0].replace(to_replace="F",value="Jay",inplace=True)
445/18: df["Grade"][1].replace(to_replace="F",value="Jay",inplace=True)
445/19: df.head()
445/20: df["Grade"][1].replace(to_replace="F",value=Jay,inplace=True)
445/21: df["Grade"][1].replace(to_replace="F",value="Jay",inplace=True)
445/22: type(df["Grade"])
445/23: df.info()
445/24: df["Grade"][1]
445/25: df["Grade"][1].replace(to_replace="F",value="Jay")
445/26: df["Grade"][1].replace(to_replace="F",value="Jay")
445/27: df["Grade"][1]
445/28: df["Grade"][1].replace(to_replace="F",value="Jay")
445/29: df["Grade"][3].replace(to_replace="F",value="Jay")
445/30: df["Grade"][1].replace(to_replace="Jay",value="F",inplace=True)
445/31: df.at[0,"Grade"]='jay'
445/32: df.head()
446/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
446/2: df = pd.read_csv("StudentsPerformance.csv")
446/3: df.head()
446/4:
print("Shape: ")
print(df.shape)
print("-"*100)
print("Columns: ")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Finding Null values: ")
print(df.isnull().sum())
print("-"*100)
print("Data discription: ")
print(df.describe())
print("-"*100)
446/5:
def unique_values(columns):
    print(columns+": ",df[columns].unique())
    print("-"*100)
446/6:
for ele in columns:
    unique_values(ele)
446/7: df["Percentage"] = round((df["math score"]+df["reading score"]+df["writing score"])/3,2)
446/8: df.head()
446/9: df["CGPA"] = round((df["Percentage"])/9.5,2)
446/10:
# 8.0 - 10.0 --> A+
# 7.0 - <8.0 --> A
# 6.0 - <7.0 --> B+
# 5.5 - <6.0 --> B
# 4.5 - <5.5 --> C+
# 4.0 - <4.5 --> C
# 0.0 - <4.0 --> F
def grade(i):
    if df["CGPA"][i]>=8.0:
        df.at[i,"Grade"] = "A+"
    elif df["CGPA"][i]>=7.0 and df["CGPA"][i]<8.0:
        df.at[i,"Grade"] = "A"
    elif df["CGPA"][i]>=6.0 and df["CGPA"][i]<7.0:
        df.at[i,"Grade"] = "B+"
    elif df["CGPA"][i]>=5.5 and df["CGPA"][i]<6.0:
        df.at[i,"Grade"] = "B"
    elif df["CGPA"][i]>=4.5 and df["CGPA"][i]<5.5:
        df.at[i,"Grade"] = "C+"
    elif df["CGPA"][i]>=4.0 and df["CGPA"][i]<4.5:
        df.at[i,"Grade"] = "C"
    else:
        df.at[i,"Grade"] = "F"
446/11:
df["Grade"]="nan"
for i in range(1000):
    grade(i)
446/12: df.head()
446/13: df.shape
446/14: df.shape()
446/15: df.shape
446/16: plt.bar(df["gender"],df["Grade"])
446/17:
plt.bar(df["gender"],df["Grade"])
plt.show()
446/18:
plt.bar(df["Grade"],df["gender"])
plt.show()
446/19:
sns.catplot(x='gender', kind='count', hue='Grade', data=df)
plt.show()
446/20:
sns.catplot(x='gender', kind='bar', hue='Grade', data=df)
plt.show()
446/21:
sns.catplot(x='gender', kind='count', hue='Grade', data=df)
plt.show()
446/22:
sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df)
plt.show()
446/23:
sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df)
#plt.show()
446/24:
plt.figure(figsize=(12,12))
sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df)
#plt.show()
446/25:
plt.figure(figsize=(12,5))
sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df)
#plt.show()
446/26:
plt.figure(figsize=(12,5))
sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df)
plt.show()
446/27:
plt.figure(figsize=(12,15))
sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df)
plt.show()
446/28:
from pandas_profiling import ProfileReport
profile=ProfileReport(df,explorative=True)
profile.to_file("output.html")
446/29: df["gender"].value_count()
446/30: df["gender"].values_count()
446/31: df["gender"].value_counts()
446/32: df["Grade"].value_counts()
446/33:
print(df["Grade"].value_counts())
sns.catplot(x="Grade",kind="count",hue="Grade")
446/34:
print(df["Grade"].value_counts())
sns.catplot(x="Grade",kind="count",hue="Grade",data=df)
446/35:
print(df["Grade"].value_counts())
sns.catplot(x="Grade",kind="count",data=df)
446/36:
print(df["Grade"].value_counts())
sns.catplot(x="Grade",kind="count",data=df)
plt.subplot(2,2)
sns.catplot(x="Grade",kind="count",data=df)
446/37:
print(df["Grade"].value_counts())
sns.catplot(x="Grade",kind="count",data=df)
plt.subplot(1,2,2)
sns.catplot(x="Grade",kind="count",data=df)
446/38:
print(df["Grade"].value_counts())
sns.catplot(x="Grade",kind="count",data=df)
plt.subplot(1,2,1)
sns.catplot(x="Grade",kind="count",data=df)
446/39:
print(df["Grade"].value_counts())
sns.catplot(x="Grade",kind="count",data=df)
plt.subplot(1,1,2)
sns.catplot(x="Grade",kind="count",data=df)
446/40:
print(df["Grade"].value_counts())
sns.catplot(x="Grade",kind="count",data=df)
plt.subplot(1,1,1)
sns.catplot(x="Grade",kind="count",data=df)
446/41:
print(df["Grade"].value_counts())
sns.catplot(x="Grade",kind="count",data=df)
446/42:
print(df["Grade"].value_counts())
sns.catplot(x="test preparation course",kind="count",hue="Grade",data=df)
446/43: sns.catplot(x="test preparation course",kind="count",hue="Grade",data=df)
446/44: sns.catplot(x="test preparation course",kind="count",hue="Grade",data=df, palette="ch:.25")
446/45:
print(df["Grade"].value_counts())
sns.catplot(x="Grade",kind="count",data=df,palette="ch:.25")
446/46: sns.catplot(x="lunch",kind="bar",hue="Grade",data=df,palette="ch:.25")
446/47: sns.catplot(x="lunch",kind="count",hue="Grade",data=df,palette="ch:.25")
446/48: sns.catplot(x="race/ethnicity", kind="count", hue="Grade")
446/49: sns.catplot(x="race/ethnicity", kind="count", hue="Grade",data=df)
446/50: sns.catplot(x="lunch",kind="count",hue="Grade",data=df,palette="ch:.29")
446/51: sns.catplot(x="lunch",kind="count",hue="Grade",data=df,palette="ch:.50")
446/52: sns.catplot(x="lunch",kind="count",hue="Grade",data=df,palette="ch:1")
446/53: sns.catplot(x="lunch",kind="count",hue="Grade",data=df,palette="ch:1.2")
446/54: sns.catplot(x="race/ethnicity", kind="count", hue="Grade",data=df,palette="ch:1.5")
446/55: sns.catplot(x="race/ethnicity", kind="count", hue="Grade",data=df,palette="ch:2.5")
446/56: sns.catplot(x="race/ethnicity", kind="count", hue="Grade",data=df,palette="ch:2.5", col="race/ethnicity")
446/57: sns.catplot(x="race/ethnicity", kind="count", hue="Grade",data=df,palette="ch:2.5", row="race/ethnicity")
446/58:
sns.catplot(x="race/ethnicity",
            kind="count", hue="Grade",
            data=df,palette="ch:2.5", 
            col="race/ethnicity",
           col_order=["group A","group B","group C","group D"])
446/59:
plt.figure(figsize=(12,15))
sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, col="parental level of education")
plt.show()
446/60:
plt.figure(figsize=(12,15))
sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row="parental level of education")
plt.show()
446/61:
plt.figure(figsize=(12,15))
sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row="parental level of education")
plt.show()
446/62: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row="parental level of education")
446/63: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, col="parental level of education")
446/64: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, col="parental level of education")
446/65: sns.catplot(kind='count', hue='Grade', data=df, col="parental level of education")
446/66: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, col="parental level of education")
446/67: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row="parental level of education")
446/68: sns.catplot(x='parental level of education', kind='box', hue='Grade', data=df, row="parental level of education")
446/69: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row="parental level of education")
446/70: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row="parental level of education",aspect=2:4)
446/71: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row="parental level of education",aspect=4)
446/72: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df, row="parental level of education",aspect=3)
446/73: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)
446/74:
sns.set(style="darkgrid",font_scale=2)
sns.catplot(x='gender', kind='count', hue='Grade', data=df)
plt.show()
446/75: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)
446/76:
sns.set(style="darkgrid",font_scale=1)
sns.catplot(x='gender', kind='count', hue='Grade', data=df)
plt.show()
446/77: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)
446/78:
sns.set(style="darkgrid",font_scale=1.2)
sns.catplot(x='gender', kind='count', hue='Grade', data=df)
plt.show()
446/79: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)
446/80:
print(df["Grade"].value_counts())
sns.catplot(x="Grade",kind="count",data=df,palette="ch:.25")
446/81: sns.catplot(x="test preparation course",kind="count",hue="Grade",data=df, palette="ch:.25")
446/82: sns.catplot(x="lunch",kind="count",hue="Grade",data=df,palette="ch:1.2")
446/83:
sns.catplot(x="race/ethnicity",
            kind="count",
            hue="Grade",
            data=df,
            palette="ch:2.5", 
            col="race/ethnicity",
           col_order=["group A","group B","group C","group D"])
446/84:
sns.catplot(x="race/ethnicity",
            kind="count",
            hue="Grade",
            data=df,
            palette="ch:2.5", 
            col="race/ethnicity",
           col_order=["group A","group B","group C","group D"],
           aspect=3)
446/85:
sns.catplot(x="race/ethnicity",
            kind="count",
            hue="Grade",
            data=df,
            palette="ch:2.5", 
            row="race/ethnicity",
            col_order=["group A","group B","group C","group D"],
            aspect=3)
446/86: sns.countplot(x="race/ethnicity",hue="parental level of education",kind="count")
446/87: sns.countplot(x="race/ethnicity",hue="parental level of education",kind="count",data=df)
446/88: sns.catplot(x="race/ethnicity",hue="parental level of education",kind="count",data=df)
446/89: sns.catplot(x="race/ethnicity",hue="parental level of education",kind="count",data=df,col="parental level of education")
446/90: sns.catplot(x="race/ethnicity",hue="parental level of education",kind="count",data=df,col="parental level of education",col_wrap=3)
446/91: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3,col_wrap=3)
446/92: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)
446/93:
sns.catplot(x="race/ethnicity",
            hue="parental level of education",
            kind="count",
            data=df,
            col="parental level of education",
            col_wrap=3,
           palette="ch:0.2")
446/94:
sns.catplot(x="race/ethnicity",
            kind="count",
            hue="Grade",
            data=df,
            palette="ch:2.5", 
            row="race/ethnicity",
            row_order=["group A","group B","group C","group D"],
            aspect=3)
446/95: sns.catplot(x="math score",data=df)
446/96: sns.catplot(x="math score",data=df,kind="bar")
446/97: sns.catplot(y="math score",data=df,kind="bar")
446/98: sns.catplot(y="math score",data=df,kind="hist")
446/99: sns.displot()
446/100: sns.distplot()
446/101: sns.distplot(df["math score"])
446/102: sns.distplot(df["math score"],fit=norm)
446/103:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
446/104: sns.distplot(df["math score"],fit=norm)
446/105: sns.distplot(df["reading score"],fit=norm)
446/106: sns.distplot(df["writing score"],fit=norm)
446/107: sns.countplot(x="Grade",data=df)
446/108: sns.countplot(x="Percentage",data=df)
446/109: df.corr()
446/110:
corr = df.corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})
446/111: sns.heatmap(corr)
446/112: sns.heatmap(corr,annot=True)
446/113:
corr = df.corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5},annote=True)
446/114:
corr = df.corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5},annot=True)
446/115:
corr = df.corr(method='pearson')

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5},annot=True)
446/116: sns.heatmap(df.corr(method='pearson'),annot=True,)
446/117: sns.heatmap(df.corr(method='spherman'),annot=True,)
446/118: sns.heatmap(df.corr(method='spearman'),annot=True,)
446/119: sns.heatmap(df.corr(method='pearson'),annot=True)
446/120: sns.heatmap(df.corr(method='spearman'),annot=True)
446/121: sns.heatmap(df.corr(method='pearson'),annot=True,linewidths=0.5)
446/122: sns.heatmap(df.corr(method='spearman'),annot=True,linewidths=0.5)
447/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
447/2: df = pd.read_csv("StudentsPerformance.csv")
447/3: df.head()
447/4:
print("Shape: ")
print(df.shape)
print("-"*100)
print("Columns: ")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Finding Null values: ")
print(df.isnull().sum())
print("-"*100)
print("Data discription: ")
print(df.describe())
print("-"*100)
447/5:
def unique_values(columns):
    print(columns+": ",df[columns].unique())
    print("-"*100)
447/6:
for ele in columns:
    unique_values(ele)
447/7: df["Percentage"] = round((df["math score"]+df["reading score"]+df["writing score"])/3,2)
447/8: df.head()
447/9: df["CGPA"] = round((df["Percentage"])/9.5,2)
447/10:
# 8.0 - 10.0 --> A+
# 7.0 - <8.0 --> A
# 6.0 - <7.0 --> B+
# 5.5 - <6.0 --> B
# 4.5 - <5.5 --> C+
# 4.0 - <4.5 --> C
# 0.0 - <4.0 --> F
def grade(i):
    if df["CGPA"][i]>=8.0:
        df.at[i,"Grade"] = "A+"
    elif df["CGPA"][i]>=7.0 and df["CGPA"][i]<8.0:
        df.at[i,"Grade"] = "A"
    elif df["CGPA"][i]>=6.0 and df["CGPA"][i]<7.0:
        df.at[i,"Grade"] = "B+"
    elif df["CGPA"][i]>=5.5 and df["CGPA"][i]<6.0:
        df.at[i,"Grade"] = "B"
    elif df["CGPA"][i]>=4.5 and df["CGPA"][i]<5.5:
        df.at[i,"Grade"] = "C+"
    elif df["CGPA"][i]>=4.0 and df["CGPA"][i]<4.5:
        df.at[i,"Grade"] = "C"
    else:
        df.at[i,"Grade"] = "F"
447/11:
df["Grade"]="nan"
for i in range(1000):
    grade(i)
447/12:
sns.set(style="darkgrid",font_scale=1.2)
sns.catplot(x='gender', kind='count', hue='Grade', data=df)
plt.show()
447/13: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)
447/14:
from pandas_profiling import ProfileReport
profile=ProfileReport(df,explorative=True)
profile.to_file("output.html")
447/15:
print(df["Grade"].value_counts())
sns.catplot(x="Grade",kind="count",data=df,palette="ch:.25")
447/16: sns.catplot(x="test preparation course",kind="count",hue="Grade",data=df, palette="ch:.25")
447/17: sns.catplot(x="lunch",kind="count",hue="Grade",data=df,palette="ch:1.2")
447/18:
sns.catplot(x="race/ethnicity",
            kind="count",
            hue="Grade",
            data=df,
            palette="ch:2.5", 
            row="race/ethnicity",
            row_order=["group A","group B","group C","group D"],
            aspect=3)
447/19:
sns.catplot(x="race/ethnicity",
            hue="parental level of education",
            kind="count",
            data=df,
            col="parental level of education",
            col_wrap=3,
           palette="ch:0.2")
447/20: sns.distplot(df["math score"],fit=norm)
447/21: sns.distplot(df["reading score"],fit=norm)
447/22: sns.distplot(df["writing score"],fit=norm)
447/23: df.corr()
447/24:
corr = df.corr(method='pearson')

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5},annot=True)
447/25: sns.heatmap(df.corr(method='pearson'),annot=True,linewidths=0.5)
447/26: sns.heatmap(df.corr(method='spearman'),annot=True,linewidths=0.5,)
447/27: df.hist()
447/28: df.hist(figsize=(20,20))
447/29: df.head()
447/30: sex=pd.get_dummies(df["gender"],drop_first=True)
447/31: sex
447/32: df = pd.concat([df,sex],axis=1)
447/33: df.head()
447/34:
li = []
for i in range(1000):
    value = df["parental level of education"][i]
    if value not in li:
        li.append(value)
        
li
447/35:
li = []
for i in range(1000):
    value = df["race/ethnicity"][i]
    if value == "goup A" and value not in li:
        li.append(value)
        
li
447/36:
li = []
for i in range(1000):
    value = df["race/ethnicity"][i]
    if value == "goup A":
        education = df["parental level of education"][i]
        if education not in li:
            li.append(value)
        
li
447/37:
li = []
for i in range(1000):
    value = df["race/ethnicity"][i]
    if value == "group A":
        education = df["parental level of education"][i]
        if education not in li:
            li.append(value)
        
li
447/38:
li = []
for i in range(1000):
    value = df["race/ethnicity"][i]
    if value == "group A":
        education = df["parental level of education"][i]
        if education not in li:
            li.append(education)
        
li
447/39:
li = []
for i in range(1000):
    value = df["race/ethnicity"][i]
    if value == "group B":
        education = df["parental level of education"][i]
        if education not in li:
            li.append(education)
        
li
447/40:
race = {"group A":0, "group B":1, "group C":2, "group D": 3, "group E": 4}
df["Education"] = df["race/ethnicity"].map(race)
df.head()
447/41: df["parental level of education"].unique()
448/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
448/2: df = pd.read_csv("StudentsPerformance.csv")
448/3: df.head()
448/4:
print("Shape: ")
print(df.shape)
print("-"*100)
print("Columns: ")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Finding Null values: ")
print(df.isnull().sum())
print("-"*100)
print("Data discription: ")
print(df.describe())
print("-"*100)
448/5:
def unique_values(columns):
    print(columns+": ",df[columns].unique())
    print("-"*100)
448/6:
for ele in columns:
    unique_values(ele)
448/7: df["Percentage"] = round((df["math score"]+df["reading score"]+df["writing score"])/3,2)
448/8: df.head()
448/9: df["CGPA"] = round((df["Percentage"])/9.5,2)
448/10:
# 8.0 - 10.0 --> A+
# 7.0 - <8.0 --> A
# 6.0 - <7.0 --> B+
# 5.5 - <6.0 --> B
# 4.5 - <5.5 --> C+
# 4.0 - <4.5 --> C
# 0.0 - <4.0 --> F
def grade(i):
    if df["CGPA"][i]>=8.0:
        df.at[i,"Grade"] = "A+"
    elif df["CGPA"][i]>=7.0 and df["CGPA"][i]<8.0:
        df.at[i,"Grade"] = "A"
    elif df["CGPA"][i]>=6.0 and df["CGPA"][i]<7.0:
        df.at[i,"Grade"] = "B+"
    elif df["CGPA"][i]>=5.5 and df["CGPA"][i]<6.0:
        df.at[i,"Grade"] = "B"
    elif df["CGPA"][i]>=4.5 and df["CGPA"][i]<5.5:
        df.at[i,"Grade"] = "C+"
    elif df["CGPA"][i]>=4.0 and df["CGPA"][i]<4.5:
        df.at[i,"Grade"] = "C"
    else:
        df.at[i,"Grade"] = "F"
448/11:
df["Grade"]="nan"
for i in range(1000):
    grade(i)
448/12:
sns.set(style="darkgrid",font_scale=1.2)
sns.catplot(x='gender', kind='count', hue='Grade', data=df)
plt.show()
448/13: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)
448/14:
from pandas_profiling import ProfileReport
profile=ProfileReport(df,explorative=True)
profile.to_file("output.html")
448/15:
print(df["Grade"].value_counts())
sns.catplot(x="Grade",kind="count",data=df,palette="ch:.25")
448/16: df.hist(figsize=(20,20))
448/17: sns.catplot(x="test preparation course",kind="count",hue="Grade",data=df, palette="ch:.25")
448/18: sns.catplot(x="lunch",kind="count",hue="Grade",data=df,palette="ch:1.2")
448/19:
sns.catplot(x="race/ethnicity",
            kind="count",
            hue="Grade",
            data=df,
            palette="ch:2.5", 
            row="race/ethnicity",
            row_order=["group A","group B","group C","group D"],
            aspect=3)
448/20:
sns.catplot(x="race/ethnicity",
            hue="parental level of education",
            kind="count",
            data=df,
            col="parental level of education",
            col_wrap=3,
           palette="ch:0.2")
448/21: sns.distplot(df["math score"],fit=norm)
448/22: sns.distplot(df["reading score"],fit=norm)
448/23: sns.distplot(df["writing score"],fit=norm)
448/24: df.corr()
448/25:
corr = df.corr(method='pearson')

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5},annot=True)
448/26: sns.heatmap(df.corr(method='pearson'),annot=True,linewidths=0.5)
448/27: sns.heatmap(df.corr(method='spearman'),annot=True,linewidths=0.5,)
448/28: df.head()
448/29: sex=pd.get_dummies(df["gender"],drop_first=True)
448/30:
df = pd.concat([df,sex],axis=1)
# female --> 0
# male --> 1
448/31:
race = {"group A": 0, "group B": 1, "group C": 2, "group D": 3, "group E": 4}
df["Group"] = df["race/ethnicity"].map(race)
df.head()
448/32:
education = {"bachelor's degree": 0, "some college": 1, "master's degree": 2, "master's degree": 3, "associate's degree": 4, 
       "high school": 5, "some high school": 6}
df["Education"] = df["race/ethnicity"].map(education)
df.head()
448/33: df["parental level of education"].unique()
449/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport
449/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
449/3:
profile = ProfileReport(df,explorative=True)
profile.to_file("output.html")
452/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport
452/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
452/3:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
print(df.columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Check for null values: ")
print(df.isnull())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
452/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
print(df.columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe)
print("-"*100)
print("Check for null values: ")
print(df.isnull())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
452/5:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
print(df.columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Check for null values: ")
print(df.isnull())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
452/6:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
print(df.columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Check for null values: ")
print(df.isnull())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
452/7:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
print(df.columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Check for null values: ")
print(df.isnull)
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
452/8:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
print(df.columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
452/9:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
452/10:
for ele in columns:
    if ele!="id":
        print(ele+" : ",df[ele].unique)
452/11:
for ele in columns:
    if ele!="id":
        print(ele+" : ",df[ele].unique())
452/12: df["age"]
452/13: type(df["age"])
452/14: df["age"].dtypes
452/15: df["gender"].unique()
452/16: df["gender"].unique
452/17: df["gender"].unique()
452/18: print("Gender: "df["gender"].unique())
452/19: print("Gender: ",df["gender"].unique())
452/20:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
452/21:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
452/22:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
452/23:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
452/24:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
452/25:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
452/26:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
452/27: sns.catplot(x=df["gender"], kind="count", data=df)
452/28: sns.catplot(x=df["gender"], kind="count", data=df)
452/29: sns.catplot(x=df["gender"], kind="count", data=df, palette="ch:.25")
459/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport
459/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
459/3:
profile = ProfileReport(df,explorative=True)
profile.to_file("output.html")
459/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
459/5:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
459/6: sns.catplot(x=df["gender"], kind="count", data=df, palette="ch:.25")
459/7:
sns.set(style="darkgrid")
sns.catplot(x=df["gender"], kind="count", data=df, palette="ch:.25")
460/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score
460/2: warnings.filterwarnings("ignore")
460/3:
data = pd.read_csv("heart_disease.csv")
data.head()
461/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
461/2: df = pd.read_csv("StudentsPerformance.csv")
461/3: df.head()
461/4:
print("Shape: ")
print(df.shape)
print("-"*100)
print("Columns: ")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Finding Null values: ")
print(df.isnull().sum())
print("-"*100)
print("Data discription: ")
print(df.describe())
print("-"*100)
461/5:
def unique_values(columns):
    print(columns+": ",df[columns].unique())
    print("-"*100)
461/6:
for ele in columns:
    unique_values(ele)
461/7: df["Percentage"] = round((df["math score"]+df["reading score"]+df["writing score"])/3,2)
461/8: df.head()
461/9: df["CGPA"] = round((df["Percentage"])/9.5,2)
461/10:
# 8.0 - 10.0 --> A+
# 7.0 - <8.0 --> A
# 6.0 - <7.0 --> B+
# 5.5 - <6.0 --> B
# 4.5 - <5.5 --> C+
# 4.0 - <4.5 --> C
# 0.0 - <4.0 --> F
def grade(i):
    if df["CGPA"][i]>=8.0:
        df.at[i,"Grade"] = "A+"
    elif df["CGPA"][i]>=7.0 and df["CGPA"][i]<8.0:
        df.at[i,"Grade"] = "A"
    elif df["CGPA"][i]>=6.0 and df["CGPA"][i]<7.0:
        df.at[i,"Grade"] = "B+"
    elif df["CGPA"][i]>=5.5 and df["CGPA"][i]<6.0:
        df.at[i,"Grade"] = "B"
    elif df["CGPA"][i]>=4.5 and df["CGPA"][i]<5.5:
        df.at[i,"Grade"] = "C+"
    elif df["CGPA"][i]>=4.0 and df["CGPA"][i]<4.5:
        df.at[i,"Grade"] = "C"
    else:
        df.at[i,"Grade"] = "F"
461/11:
df["Grade"]="nan"
for i in range(1000):
    grade(i)
461/12:
sns.set(style="darkgrid",font_scale=1.2)
sns.catplot(x='gender', kind='count', hue='Grade', data=df)
plt.show()
461/13: sns.catplot(x='parental level of education', kind='count', hue='Grade', data=df,aspect=3)
461/14:
from pandas_profiling import ProfileReport
profile=ProfileReport(df,explorative=True)
profile.to_file("output.html")
461/15:
print(df["Grade"].value_counts())
sns.catplot(x="Grade",kind="count",data=df,palette="ch:.25")
461/16: df.hist(figsize=(20,20))
461/17: sns.catplot(x="test preparation course",kind="count",hue="Grade",data=df, palette="ch:.25")
461/18: sns.catplot(x="lunch",kind="count",hue="Grade",data=df,palette="ch:1.2")
461/19:
sns.catplot(x="race/ethnicity",
            kind="count",
            hue="Grade",
            data=df,
            palette="ch:2.5", 
            row="race/ethnicity",
            row_order=["group A","group B","group C","group D"],
            aspect=3)
461/20:
sns.catplot(x="race/ethnicity",
            hue="parental level of education",
            kind="count",
            data=df,
            col="parental level of education",
            col_wrap=3,
           palette="ch:0.2")
461/21: sns.distplot(df["math score"],fit=norm)
461/22: sns.distplot(df["reading score"],fit=norm)
461/23: sns.distplot(df["writing score"],fit=norm)
461/24: df.corr()
461/25:
corr = df.corr(method='pearson')

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5},annot=True)
461/26: sns.heatmap(df.corr(method='pearson'),annot=True,linewidths=0.5)
461/27: sns.heatmap(df.corr(method='spearman'),annot=True,linewidths=0.5,)
461/28: df.head()
461/29: sex=pd.get_dummies(df["gender"],drop_first=True)
461/30:
df = pd.concat([df,sex],axis=1)
# female --> 0
# male --> 1
461/31:
race = {"group A": 0, "group B": 1, "group C": 2, "group D": 3, "group E": 4}
df["Group"] = df["race/ethnicity"].map(race)
df.head()
461/32:
education = {"bachelor's degree": 0, "some college": 1, "master's degree": 2, "master's degree": 3, "associate's degree": 4, 
       "high school": 5, "some high school": 6}
df["Education"] = df["race/ethnicity"].map(education)
df.head()
461/33: df["parental level of education"].unique()
461/34:
sns.set(style="darkgrid",font_scale=1.2)
sns.catplot(x='gender', kind='count', data=df)
plt.show()
461/35:
sns.set(style="darkgrid",font_scale=1.2)
sns.catplot(x='gender', kind='count', hue='Grade', data=df)
plt.show()
459/8:
sns.set(style="darkgrid")
sns.catplot(x=df["gender"], kind="count", data=df, palette="ch:.25")
sns.catplot(x=df["gender"], kind="count", data=df, palette="ch:.25")
459/9:
sns.set(style="darkgrid")
ptint(sns.catplot(x=df["gender"], kind="count", data=df, palette="ch:.25"))
sns.catplot(x=df["gender"], kind="count", data=df, palette="ch:.25")
459/10:
sns.set(style="darkgrid")
print(sns.catplot(x=df["gender"], kind="count", data=df, palette="ch:.25"))
sns.catplot(x=df["gender"], kind="count", data=df, palette="ch:.25")
459/11:
sns.set(style="darkgrid")
sns.catplot(x=df["gender"], kind="count", data=df, palette="ch:.25")
459/12: df["gender"].values_count()
459/13: df["gender"].value_counts()
459/14:
sns.set(style="darkgrid")
sns.catplot(x=df["gender"], kind="cout", data=df, palette="ch:.25")
459/15:
sns.set(style="darkgrid")
sns.catplot(x=df["gender"], kind="count", data=df, palette="ch:.25")
459/16:
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
459/17: sns.catplot(x=df["hypertension"],kind="count", hue=df["stroke"], data=df)
459/18: sns.countplot(x=df["hypertension"],kind="count", hue=df["stroke"], data=df)
459/19: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
463/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport
463/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
463/3:
profile = ProfileReport(df,explorative=True)
profile.to_file("output.html")
463/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
463/5:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
463/6:
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
463/7: df["gender"].value_counts()
463/8: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
463/9: sns.contplot(x=df["gender"], hue=df["stroke"], data=df)
463/10: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
463/11: sns.countplot(x=df["gender"], hue=df["stroke"], data=df, palette="ch:.24")
463/12: sns.countplot(x=df["gender"], hue=df["stroke"], data=df, palette="ch:.25")
463/13: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df, palette="ch:.50")
463/14: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df, palette="ch:1")
463/15: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df, palette="ch:1.26")
463/16: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
463/17: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
463/18: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
463/19: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
463/20: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
463/21: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
463/22: sns.distplot(x=df["age"])
463/23: sns.distplot(df["age"])
463/24:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
463/25: sns.distplot(df["age"], fit=norm)
463/26: sns.distplot(df["bmi"], fit=norm)
463/27: sns.distplot(df["avg_glucose_level"], fit=norm)
463/28: sns.countplot(x=df["stroke"], data=df)
463/29: sns.boxenplot(x=df["age"], data=df)
463/30: sns.boxplot(x=df["age"], data=df)
463/31: sns.boxplot(x=df["bmi"], data=df)
463/32: sns.boxplot(x=df["avg_glucose_level"], data=df)
463/33:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
463/34:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
463/35:
print(df["heart_disease"])
sns.countplot(x=df["heart_disease"], data=df)
463/36:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
463/37:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
463/38:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
463/39:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
463/40:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
463/41: print("Length of bmi before: ",len(df["bmi"]))
463/42: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
463/43: df["bmi"]=df["bmi"].fillna(value=df["bmi"].mean())
463/44: df["bmi"]=df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
463/45: df["bmi"].isnull().sum()
463/46: df.head()
465/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
465/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
465/3:
profile = ProfileReport(df,explorative=True)
profile.to_file("output.html")
465/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
465/5:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
465/6:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
465/7:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
465/8:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
465/9:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
465/10:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
465/11:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
465/12:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
465/13: sns.countplot(x=df["stroke"], data=df)
465/14: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
465/15: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
465/16: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
465/17: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
465/18: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
465/19: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
465/20: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
465/21: sns.distplot(df["age"], fit=norm)
465/22: sns.distplot(df["bmi"], fit=norm)
465/23: sns.distplot(df["avg_glucose_level"], fit=norm)
465/24: sns.boxplot(x=df["age"], data=df)
465/25: sns.boxplot(x=df["bmi"], data=df)
465/26: sns.boxplot(x=df["avg_glucose_level"], data=df)
465/27: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
465/28: df["bmi"]=df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
465/29: df["bmi"].isnull().sum()
465/30: df.head()
465/31: df["agee"].mean()
465/32: df["age"].mean()
466/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
466/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
466/3:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
466/4:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
466/5:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
466/6:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
466/7:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
466/8:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
466/9:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
466/10:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
466/11:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
466/12: sns.countplot(x=df["stroke"], data=df)
466/13: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
466/14: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
466/15: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
466/16: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
466/17: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
466/18: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
466/19: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
466/20: sns.distplot(df["age"], fit=norm)
466/21: sns.distplot(df["bmi"], fit=norm)
466/22: sns.distplot(df["avg_glucose_level"], fit=norm)
466/23: sns.boxplot(x=df["age"], data=df)
466/24: sns.boxplot(x=df["bmi"], data=df)
466/25: sns.boxplot(x=df["avg_glucose_level"], data=df)
466/26: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
466/27: df["bmi"].fillna(value=df["bmi"].mean, inplace=True)
466/28: df["bmi"].isnull().sum()
466/29: df.head()
466/30: df["age"].mean()
466/31: df.head()
466/32: df.head(5)
466/33: df["age"].mean()
466/34: df.head()
467/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
467/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
467/3:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
467/4:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
467/5:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
467/6:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
467/7:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
467/8:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
467/9:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
467/10:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
467/11:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
467/12: sns.countplot(x=df["stroke"], data=df)
467/13: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
467/14: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
467/15: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
467/16: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
467/17: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
467/18: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
467/19: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
467/20: sns.distplot(df["age"], fit=norm)
467/21: sns.distplot(df["bmi"], fit=norm)
467/22: sns.distplot(df["avg_glucose_level"], fit=norm)
467/23: sns.boxplot(x=df["age"], data=df)
467/24: sns.boxplot(x=df["bmi"], data=df)
467/25: sns.boxplot(x=df["avg_glucose_level"], data=df)
467/26: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
467/27: df["bmi"].fillna(value=df["bmi"].mean, inplace=True)
467/28: df["bmi"].isnull().sum()
467/29: df["age"].mean()
467/30: df.head()
467/31: df["age"]
467/32: df.head()
467/33: df.tail()
469/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
469/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
469/3:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
469/4:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
469/5:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
469/6:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
469/7:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
469/8:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
469/9:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
469/10:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
469/11:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
469/12: sns.countplot(x=df["stroke"], data=df)
469/13: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
469/14: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
469/15: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
469/16: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
469/17: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
469/18: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
469/19: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
469/20: sns.distplot(df["age"], fit=norm)
469/21: sns.distplot(df["bmi"], fit=norm)
469/22: sns.distplot(df["avg_glucose_level"], fit=norm)
469/23: sns.boxplot(x=df["age"], data=df)
469/24: sns.boxplot(x=df["bmi"], data=df)
469/25: sns.boxplot(x=df["avg_glucose_level"], data=df)
469/26: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
469/27: df["bmi"].fillna(value=df["bmi"].mean, inplace=True)
469/28: df["bmi"].isnull().sum()
469/29: df["age"].mean()
469/30: df.head()
470/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
470/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
470/3: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
470/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
470/5:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
470/6:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
470/7:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
470/8:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
470/9:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
470/10:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
470/11:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
470/12:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
470/13: sns.countplot(x=df["stroke"], data=df)
470/14: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
470/15: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
470/16: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
470/17: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
470/18: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
470/19: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
470/20: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
470/21: sns.distplot(df["age"], fit=norm)
470/22: sns.distplot(df["bmi"], fit=norm)
470/23: sns.distplot(df["avg_glucose_level"], fit=norm)
470/24: sns.boxplot(x=df["age"], data=df)
470/25: sns.boxplot(x=df["bmi"], data=df)
470/26: sns.boxplot(x=df["avg_glucose_level"], data=df)
470/27: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
470/28: df["bmi"].fillna(value=df["bmi"].mean, inplace=True)
470/29: df["bmi"].isnull().sum()
470/30: df["age"].mean()
470/31: df.head()
470/32: df["age"].mean
470/33: df["age"].mean()
471/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
471/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
471/3:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
471/4:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
471/5:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
471/6:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
471/7:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
471/8:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
471/9:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
471/10:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
471/11:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
471/12: sns.countplot(x=df["stroke"], data=df)
471/13: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
471/14: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
471/15: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
471/16: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
471/17: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
471/18: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
471/19: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
471/20: sns.distplot(df["age"], fit=norm)
471/21: sns.distplot(df["bmi"], fit=norm)
471/22: sns.distplot(df["avg_glucose_level"], fit=norm)
471/23: sns.boxplot(x=df["age"], data=df)
471/24: sns.boxplot(x=df["bmi"], data=df)
471/25: sns.boxplot(x=df["avg_glucose_level"], data=df)
471/26: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
471/27: df.head()
471/28: df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
471/29: df["bmi"].isnull().sum()
471/30: df["age"].mean()
471/31: df.head()
471/32: print("Length of bmi after: ",["bmi"].isnull().sum())
471/33: print("Length of bmi after: ",df["bmi"].isnull().sum())
471/34: sns.boxplot(x=df["bmi"], data=df)
471/35: sns.distplot(df["bmi"], fit=norm)
471/36:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["stroke"], data=df)
471/37:
print(df["stroke"].value_counts())
sns.countplot(x=df["stroke"], data=df)
471/38:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
471/39:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
471/40:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound {}, lower bound {}".formate(upper_bound,lower_bound))
471/41:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound {}, lower bound {}".format(upper_bound,lower_bound))
471/42:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound: {}, lower bound: {}".format(upper_bound,lower_bound))
471/43: df["bmi"][0]
471/44: sns.boxplot(x=df["bmi"], data=df)
471/45:
for i in range(len(df["bmi"])):
    if df["bmi"][i] > upper_bound or df["bmi"][i] < lower_bound:
        df.drop([df["bmi"].index[i]], inplace=True)
471/46: sns.boxplot(x=df["bmi"], data=df)
471/47: df.drop(df[df['bmi'] > upper_bound or df["bmi"]<lower_bound].index, inplace = True)
471/48:
df.drop(df[df['bmi'] > upper_bound].index, inplace = True)
df.drop(df[df['bmi'] < lower_bound].index, inplace = True)
471/49: sns.boxplot(x=df["bmi"], data=df)
472/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
472/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
472/3:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
472/4:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
472/5:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
472/6:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
472/7:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
472/8:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
472/9:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
472/10:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
472/11:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
472/12:
print(df["stroke"].value_counts())
sns.countplot(x=df["stroke"], data=df)
472/13: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
472/14: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
472/15: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
472/16: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
472/17: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
472/18: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
472/19: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
472/20: sns.distplot(df["age"], fit=norm)
472/21: sns.distplot(df["bmi"], fit=norm)
472/22: sns.distplot(df["bmi"], fit=norm)
472/23: sns.distplot(df["avg_glucose_level"], fit=norm)
472/24: sns.boxplot(x=df["age"], data=df)
472/25: sns.boxplot(x=df["bmi"], data=df)
472/26: sns.boxplot(x=df["bmi"], data=df)
472/27: sns.boxplot(x=df["avg_glucose_level"], data=df)
472/28: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
472/29: df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
472/30: print("Length of bmi after: ",df["bmi"].isnull().sum())
472/31:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound: {}, lower bound: {}".format(upper_bound,lower_bound))
472/32:
df.drop(df[df['bmi'] > upper_bound].index, inplace = True)
df.drop(df[df['bmi'] < lower_bound].index, inplace = True)
472/33: sns.boxplot(x=df["bmi"], data=df)
472/34: print(df["stroke"].value_counts())
472/35: sns.boxplot(x=df["bmi"], data=df)
472/36:
print("After outlier removal")
sns.boxplot(x=df["bmi"], data=df)
472/37: sns.distplot(df["bmi"], fit=norm)
472/38:
print("After outlier removal")
fig, ax = pplt.subplot(1,2)
sns.boxplot(x=df["bmi"], data=df)
sns.distplot(df["bmi"], fit=norm)
472/39:
print("After outlier removal")
fig, ax = plt.subplot(1,2)
sns.boxplot(x=df["bmi"], data=df)
sns.distplot(df["bmi"], fit=norm)
472/40:
print("After outlier removal")
fig, ax = plt.subplot(1,2)
sns.boxplot(x=df["bmi"], data=df, ax=ax)
sns.distplot(df["bmi"], fit=norm, ax=ax)
472/41:
print("After outlier removal")
fig, ax = plt.subplot(1,2,figsize=(10,5))
sns.boxplot(x=df["bmi"], data=df, ax=ax)
sns.distplot(df["bmi"], fit=norm, ax=ax)
472/42:
print("After outlier removal")
fig, ax = plt.subplot(2,2,1)
sns.boxplot(x=df["bmi"], data=df, ax=ax)
sns.distplot(df["bmi"], fit=norm, ax=ax)
472/43:
print("After outlier removal")
fig, ax = plt.subplot(2,2,1)
sns.boxplot(x=df["bmi"], data=df)
sns.distplot(df["bmi"], fit=norm)
472/44:
print("After outlier removal")
fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,5))
axes[0].sns.boxplot(x=df["bmi"], data=df)
axes[1].sns.distplot(df["bmi"], fit=norm)
472/45:
print("After outlier removal")
fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,5))
sns.boxplot(x=df["bmi"], data=df)axes[0]
axes[1].sns.distplot(df["bmi"], fit=norm)
472/46:
print("After outlier removal")
fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,5))
sns.boxplot(x=df["bmi"], data=df).axes[0]
axes[1].sns.distplot(df["bmi"], fit=norm)
472/47:
print("After outlier removal")
fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,5))
sns.boxplot(x=df["bmi"], data=df, ax=axes[0])
axes[1].sns.distplot(df["bmi"], fit=norm)
472/48:
print("After outlier removal")
fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,5))
sns.boxplot(x=df["bmi"], data=df, ax = axes[0])
sns.distplot(df["bmi"], fit=norm, ax = axes[1])
472/49:
print("After outlier removal")
fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,10))
sns.boxplot(x=df["bmi"], data=df, ax = axes[0])
sns.distplot(df["bmi"], fit=norm, ax = axes[1])
472/50:
print("After outlier removal")
fig, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x=df["bmi"], data=df, ax = axes[0])
sns.distplot(df["bmi"], fit=norm, ax = axes[1])
472/51:
data = df.corr(method='pearson')
sns.heatmap(data,annot=True,cbar=True)
472/52:
data = df.corr(method='pearson')
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
472/53:
data = df.corr(method='pearson')
fig = plt.figure(figsize=(10,5))
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
472/54:
data = df.corr(method='pearson')
fig = plt.figure(figsize=(15,5))
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
472/55:
data = df.corr(method='pearson')
fig = plt.figure(figsize=(15,8))
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
472/56: df.head()
472/57: df.describe()
474/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
474/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
474/3:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
474/4:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
474/5:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
474/6:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
474/7:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
474/8:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
474/9:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
474/10:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
474/11:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
474/12:
print(df["stroke"].value_counts())
sns.countplot(x=df["stroke"], data=df)
474/13: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
474/14: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
474/15: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
474/16: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
474/17: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
474/18: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
474/19: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
474/20: sns.distplot(df["age"], fit=norm)
474/21: sns.distplot(df["bmi"], fit=norm)
474/22: sns.distplot(df["avg_glucose_level"], fit=norm)
474/23: sns.boxplot(x=df["age"], data=df)
474/24: sns.boxplot(x=df["bmi"], data=df)
474/25: sns.boxplot(x=df["avg_glucose_level"], data=df)
474/26:
data = df.corr(method='pearson')
fig = plt.figure(figsize=(15,8))
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
474/27: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
474/28: df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
474/29: print("Length of bmi after: ",df["bmi"].isnull().sum())
474/30:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound: {}, lower bound: {}".format(upper_bound,lower_bound))
474/31:
df.drop(df[df['bmi'] > upper_bound].index, inplace = True)
df.drop(df[df['bmi'] < lower_bound].index, inplace = True)
474/32:
print("After outlier removal")
fig, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x=df["bmi"], data=df, ax = axes[0])
sns.distplot(df["bmi"], fit=norm, ax = axes[1])
474/33: print(df["stroke"].value_counts())
474/34:
# There is only one "other" category in the gender. So, we should remove it.
df["gender"].drop([df["gender"]=="other"].index, inplace=True)
474/35:
# There is only one "other" category in the gender. So, we should remove it.
df["gender"].drop(df[df["gender"]=="other"].index, inplace=True)
474/36:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
474/37:
# There is only one "other" category in the gender. So, we should remove it.
df["gender"].drop(df[df["gender"]=="other"].index, inplace=True)
474/38:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
474/39:
# There is only one "other" category in the gender. So, we should remove it.
df["gender"].drop(df[df["gender"]=="Other"].index, inplace=True)
474/40:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
474/41: df["gender"].value_counts()
474/42:
# There is only one "other" category in the gender. So, we should remove it.
df.drop(df[df["gender"]=="Other"].index, inplace=True)
474/43: df["gender"].value_counts()
474/44:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
474/45:
sex = pd.get_dummies(df["gender"], drop_first=True)
df = pd.concat([df,sex],axis=1)
df.head()
474/46:
# For Ever_married colummn.
married_status = pd.get_dummies(df["ever_married"], drop_first=True)
df = pd.concat([df, married_status], axis=1)
474/47: df.head()
474/48:
# For Residence type column.
residence = pd.get_dummies(df["Residence_type"], drop_first=True)
df = pd.concat([df, residence], axis=1)
474/49: df.head()
474/50: df["smoking_status"].unique()
474/51:
df["Work_Type"] = df["work_type"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})
df["Smoking_Status"] = df["smoking_status"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})
474/52: df.head()
474/53:
# Now since we have converted all the categorical values to numerical, now we should drop all those columns.

df = df.drop(["id","gender","ever_married","work_type","Residence_type","smoking_status"],axis=1, inplace=True)
df.head()
474/54:
# Now since we have converted all the categorical values to numerical, now we should drop all those columns.

df.drop(["id","gender","ever_married","work_type","Residence_type","smoking_status"],axis=1, inplace=True)
df.head()
475/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
475/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
475/3: df.head()
475/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
475/5:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
475/6:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
475/7:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
475/8:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
475/9:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
475/10:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
475/11:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
475/12:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
475/13:
print(df["stroke"].value_counts())
sns.countplot(x=df["stroke"], data=df)
475/14: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
475/15: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
475/16: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
475/17: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
475/18: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
475/19: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
475/20: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
475/21: sns.distplot(df["age"], fit=norm)
475/22: sns.distplot(df["bmi"], fit=norm)
475/23: sns.distplot(df["avg_glucose_level"], fit=norm)
475/24: sns.boxplot(x=df["age"], data=df)
475/25: sns.boxplot(x=df["bmi"], data=df)
475/26: sns.boxplot(x=df["avg_glucose_level"], data=df)
475/27:
data = df.corr(method='pearson')
fig = plt.figure(figsize=(15,8))
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
475/28: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
475/29: df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
475/30: print("Length of bmi after: ",df["bmi"].isnull().sum())
475/31:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound: {}, lower bound: {}".format(upper_bound,lower_bound))
475/32:
df.drop(df[df['bmi'] > upper_bound].index, inplace = True)
df.drop(df[df['bmi'] < lower_bound].index, inplace = True)
475/33:
print("After outlier removal")
fig, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x=df["bmi"], data=df, ax = axes[0])
sns.distplot(df["bmi"], fit=norm, ax = axes[1])
475/34:
# There is only one "other" category in the gender. So, we should remove it.
df.drop(df[df["gender"]=="Other"].index, inplace=True)
475/35:
# For gender column.
sex = pd.get_dummies(df["gender"], drop_first=True)
df = pd.concat([df,sex],axis=1)
475/36:
# For Ever_married colummn.
married_status = pd.get_dummies(df["ever_married"], drop_first=True)
df = pd.concat([df, married_status], axis=1)
475/37:
# For Residence type column.
residence = pd.get_dummies(df["Residence_type"], drop_first=True)
df = pd.concat([df, residence], axis=1)
475/38:
df["Work_Type"] = df["work_type"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})
df["Smoking_Status"] = df["smoking_status"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})
475/39: df.head()
475/40:
# Now since we have converted all the categorical values to numerical, now we should drop all those columns.

df.drop(["id","gender","ever_married","work_type","Residence_type","smoking_status"],axis=1, inplace=True)
df.head()
475/41: print(df["stroke"].value_counts())
475/42: df["gender"].value_counts()
475/43:
df.rename(columns={"Male":"Gender","Yes":"Ever_Married","Urban":"Residence_type"})
df.head()
475/44:
df.rename(columns={"Male":"Gender","Yes":"Ever_Married","Urban":"Residence_type"}, inplace=True)
df.head()
475/45: from imblearn.over_sampling import SMOTE
475/46: !pip install imblearn
475/47: from imblearn.over_sampling import SMOTE
475/48: from imblearn.over_sampling import SMOTE
475/49: from imblearn.over_sampling import SMOTE
477/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
477/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
477/3: df.head()
477/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
477/5:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
477/6:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
477/7:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
477/8:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
477/9:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
477/10:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
477/11:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
477/12:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
477/13:
print(df["stroke"].value_counts())
sns.countplot(x=df["stroke"], data=df)
477/14: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
477/15: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
477/16: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
477/17: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
477/18: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
477/19: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
477/20: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
477/21: sns.distplot(df["age"], fit=norm)
477/22: sns.distplot(df["bmi"], fit=norm)
477/23: sns.distplot(df["avg_glucose_level"], fit=norm)
477/24: sns.boxplot(x=df["age"], data=df)
477/25: sns.boxplot(x=df["bmi"], data=df)
477/26: sns.boxplot(x=df["avg_glucose_level"], data=df)
477/27:
data = df.corr(method='pearson')
fig = plt.figure(figsize=(15,8))
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
477/28: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
477/29: df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
477/30: print("Length of bmi after: ",df["bmi"].isnull().sum())
477/31:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound: {}, lower bound: {}".format(upper_bound,lower_bound))
477/32:
df.drop(df[df['bmi'] > upper_bound].index, inplace = True)
df.drop(df[df['bmi'] < lower_bound].index, inplace = True)
477/33:
print("After outlier removal")
fig, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x=df["bmi"], data=df, ax = axes[0])
sns.distplot(df["bmi"], fit=norm, ax = axes[1])
477/34:
# There is only one "other" category in the gender. So, we should remove it.
df.drop(df[df["gender"]=="Other"].index, inplace=True)
477/35:
# For gender column.
sex = pd.get_dummies(df["gender"], drop_first=True)
df = pd.concat([df,sex],axis=1)
477/36:
# For Ever_married colummn.
married_status = pd.get_dummies(df["ever_married"], drop_first=True)
df = pd.concat([df, married_status], axis=1)
477/37:
# For Residence type column.
residence = pd.get_dummies(df["Residence_type"], drop_first=True)
df = pd.concat([df, residence], axis=1)
477/38:
df["Work_Type"] = df["work_type"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})
df["Smoking_Status"] = df["smoking_status"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})
477/39: df.head()
477/40:
df.drop(["id","gender","ever_married","work_type","Residence_type","smoking_status"],axis=1, inplace=True)
df.head()
477/41:
df.rename(columns={"Male":"Gender","Yes":"Ever_Married","Urban":"Residence_type"}, inplace=True)
df.head()
477/42: from imblearn.over_sampling import SMOTE
478/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
478/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
478/3: df.head()
478/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
478/5:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
478/6:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
478/7:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
478/8:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
478/9:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
478/10:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
478/11:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
478/12:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
478/13:
print(df["stroke"].value_counts())
sns.countplot(x=df["stroke"], data=df)
478/14: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
478/15: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
478/16: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
478/17: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
478/18: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
478/19: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
478/20: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
478/21: sns.distplot(df["age"], fit=norm)
478/22: sns.distplot(df["bmi"], fit=norm)
478/23: sns.distplot(df["avg_glucose_level"], fit=norm)
478/24: sns.boxplot(x=df["age"], data=df)
478/25: sns.boxplot(x=df["bmi"], data=df)
478/26: sns.boxplot(x=df["avg_glucose_level"], data=df)
478/27:
data = df.corr(method='pearson')
fig = plt.figure(figsize=(15,8))
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
478/28: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
478/29: df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
478/30: print("Length of bmi after: ",df["bmi"].isnull().sum())
478/31:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound: {}, lower bound: {}".format(upper_bound,lower_bound))
478/32:
df.drop(df[df['bmi'] > upper_bound].index, inplace = True)
df.drop(df[df['bmi'] < lower_bound].index, inplace = True)
478/33:
print("After outlier removal")
fig, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x=df["bmi"], data=df, ax = axes[0])
sns.distplot(df["bmi"], fit=norm, ax = axes[1])
478/34:
# There is only one "other" category in the gender. So, we should remove it.
df.drop(df[df["gender"]=="Other"].index, inplace=True)
478/35:
# For gender column.
sex = pd.get_dummies(df["gender"], drop_first=True)
df = pd.concat([df,sex],axis=1)
478/36:
# For Ever_married colummn.
married_status = pd.get_dummies(df["ever_married"], drop_first=True)
df = pd.concat([df, married_status], axis=1)
478/37:
# For Residence type column.
residence = pd.get_dummies(df["Residence_type"], drop_first=True)
df = pd.concat([df, residence], axis=1)
478/38:
df["Work_Type"] = df["work_type"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})
df["Smoking_Status"] = df["smoking_status"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})
478/39: df.head()
478/40:
df.drop(["id","gender","ever_married","work_type","Residence_type","smoking_status"],axis=1, inplace=True)
df.head()
478/41:
df.rename(columns={"Male":"Gender","Yes":"Ever_Married","Urban":"Residence_type"}, inplace=True)
df.head()
478/42: from imblearn.over_sampling import SMOTE
478/43:
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state = 42)
X = df.drop(['stroke'],axis=1)
y = df[['stroke']]
X,y= smote.fit_resample(X,y['stroke'].values.ravel())
y = pd.DataFrame({'stroke':y})
sns.countplot(data = y, x = 'stroke', y= None)
plt.show()
479/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
479/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
479/3: df.head()
479/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
479/5:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
479/6:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
479/7:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
479/8:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
479/9:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
479/10:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
479/11:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
479/12:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
479/13:
print(df["stroke"].value_counts())
sns.countplot(x=df["stroke"], data=df)
479/14: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
479/15: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
479/16: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
479/17: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
479/18: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
479/19: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
479/20: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
479/21: sns.distplot(df["age"], fit=norm)
479/22: sns.distplot(df["bmi"], fit=norm)
479/23: sns.distplot(df["avg_glucose_level"], fit=norm)
479/24: sns.boxplot(x=df["age"], data=df)
479/25: sns.boxplot(x=df["bmi"], data=df)
479/26: sns.boxplot(x=df["avg_glucose_level"], data=df)
479/27:
data = df.corr(method='pearson')
fig = plt.figure(figsize=(15,8))
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
479/28: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
479/29: df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
479/30: print("Length of bmi after: ",df["bmi"].isnull().sum())
479/31:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound: {}, lower bound: {}".format(upper_bound,lower_bound))
479/32:
df.drop(df[df['bmi'] > upper_bound].index, inplace = True)
df.drop(df[df['bmi'] < lower_bound].index, inplace = True)
479/33:
print("After outlier removal")
fig, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x=df["bmi"], data=df, ax = axes[0])
sns.distplot(df["bmi"], fit=norm, ax = axes[1])
479/34:
# There is only one "other" category in the gender. So, we should remove it.
df.drop(df[df["gender"]=="Other"].index, inplace=True)
479/35:
# For gender column.
sex = pd.get_dummies(df["gender"], drop_first=True)
df = pd.concat([df,sex],axis=1)
479/36:
# For Ever_married colummn.
married_status = pd.get_dummies(df["ever_married"], drop_first=True)
df = pd.concat([df, married_status], axis=1)
479/37:
# For Residence type column.
residence = pd.get_dummies(df["Residence_type"], drop_first=True)
df = pd.concat([df, residence], axis=1)
479/38:
df["Work_Type"] = df["work_type"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})
df["Smoking_Status"] = df["smoking_status"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})
479/39: df.head()
479/40:
df.drop(["id","gender","ever_married","work_type","Residence_type","smoking_status"],axis=1, inplace=True)
df.head()
479/41:
df.rename(columns={"Male":"Gender","Yes":"Ever_Married","Urban":"Residence_type"}, inplace=True)
df.head()
479/42:
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state = 42)
X = df.drop(['stroke'],axis=1)
y = df['stroke']
X,y= smote.fit_resample(X,y['stroke'])
sns.countplot(data = y, x = 'stroke', y= None)
plt.show()
479/43:
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state = 42)
X = df.drop(['stroke'],axis=1)
y = df['stroke']
X,y= smote.fit_resample(X,y)
sns.countplot(data = y, x = 'stroke', y= None)
plt.show()
480/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
480/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
480/3: df.head()
480/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
480/5:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
480/6:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
480/7:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
480/8:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
480/9:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
480/10:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
480/11:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
480/12:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
480/13:
print(df["stroke"].value_counts())
sns.countplot(x=df["stroke"], data=df)
480/14: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
480/15: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
480/16: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
480/17: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
480/18: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
480/19: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
480/20: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
480/21: sns.distplot(df["age"], fit=norm)
480/22: sns.distplot(df["bmi"], fit=norm)
480/23: sns.distplot(df["avg_glucose_level"], fit=norm)
480/24: sns.boxplot(x=df["age"], data=df)
480/25: sns.boxplot(x=df["bmi"], data=df)
480/26: sns.boxplot(x=df["avg_glucose_level"], data=df)
480/27:
data = df.corr(method='pearson')
fig = plt.figure(figsize=(15,8))
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
480/28: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
480/29: df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
480/30: print("Length of bmi after: ",df["bmi"].isnull().sum())
480/31:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound: {}, lower bound: {}".format(upper_bound,lower_bound))
480/32:
df.drop(df[df['bmi'] > upper_bound].index, inplace = True)
df.drop(df[df['bmi'] < lower_bound].index, inplace = True)
480/33:
print("After outlier removal")
fig, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x=df["bmi"], data=df, ax = axes[0])
sns.distplot(df["bmi"], fit=norm, ax = axes[1])
480/34:
# There is only one "other" category in the gender. So, we should remove it.
df.drop(df[df["gender"]=="Other"].index, inplace=True)
480/35:
# For gender column.
sex = pd.get_dummies(df["gender"], drop_first=True)
df = pd.concat([df,sex],axis=1)
480/36:
# For Ever_married colummn.
married_status = pd.get_dummies(df["ever_married"], drop_first=True)
df = pd.concat([df, married_status], axis=1)
480/37:
# For Residence type column.
residence = pd.get_dummies(df["Residence_type"], drop_first=True)
df = pd.concat([df, residence], axis=1)
480/38:
df["Work_Type"] = df["work_type"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})
df["Smoking_Status"] = df["smoking_status"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})
480/39: df.head()
480/40:
df.drop(["id","gender","ever_married","work_type","Residence_type","smoking_status"],axis=1, inplace=True)
df.head()
480/41:
df.rename(columns={"Male":"Gender","Yes":"Ever_Married","Urban":"Residence_type"}, inplace=True)
df.head()
480/42:
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state = 42)
X = df.drop(['stroke'],axis=1)
y = df['stroke']
X,y= smote.fit_resample(X,y)
y = pd.DataFrame({'stroke':y})
sns.countplot(data = y, x = 'stroke', y= None)
plt.show()
481/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
481/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
481/3: df.head()
481/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
481/5:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
481/6:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
481/7:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
481/8:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
481/9:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
481/10:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
481/11:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
481/12:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
481/13:
print(df["stroke"].value_counts())
sns.countplot(x=df["stroke"], data=df)
481/14: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
481/15: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
481/16: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
481/17: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
481/18: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
481/19: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
481/20: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
481/21: sns.distplot(df["age"], fit=norm)
481/22: sns.distplot(df["bmi"], fit=norm)
481/23: sns.distplot(df["avg_glucose_level"], fit=norm)
481/24: sns.boxplot(x=df["age"], data=df)
481/25: sns.boxplot(x=df["bmi"], data=df)
481/26: sns.boxplot(x=df["avg_glucose_level"], data=df)
481/27:
data = df.corr(method='pearson')
fig = plt.figure(figsize=(15,8))
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
481/28: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
481/29: df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
481/30: print("Length of bmi after: ",df["bmi"].isnull().sum())
481/31:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound: {}, lower bound: {}".format(upper_bound,lower_bound))
481/32:
df.drop(df[df['bmi'] > upper_bound].index, inplace = True)
df.drop(df[df['bmi'] < lower_bound].index, inplace = True)
481/33:
print("After outlier removal")
fig, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x=df["bmi"], data=df, ax = axes[0])
sns.distplot(df["bmi"], fit=norm, ax = axes[1])
481/34:
# There is only one "other" category in the gender. So, we should remove it.
df.drop(df[df["gender"]=="Other"].index, inplace=True)
481/35:
# For gender column.
sex = pd.get_dummies(df["gender"], drop_first=True)
df = pd.concat([df,sex],axis=1)
481/36:
# For Ever_married colummn.
married_status = pd.get_dummies(df["ever_married"], drop_first=True)
df = pd.concat([df, married_status], axis=1)
481/37:
# For Residence type column.
residence = pd.get_dummies(df["Residence_type"], drop_first=True)
df = pd.concat([df, residence], axis=1)
481/38:
df["Work_Type"] = df["work_type"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})
df["Smoking_Status"] = df["smoking_status"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})
481/39: df.head()
481/40:
df.drop(["id","gender","ever_married","work_type","Residence_type","smoking_status"],axis=1, inplace=True)
df.head()
481/41:
df.rename(columns={"Male":"Gender","Yes":"Ever_Married","Urban":"Residence_type"}, inplace=True)
df.head()
481/42:
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state = 42)
X = df.drop(['stroke'],axis=1)
y = df['stroke']
X,y= smote.fit_resample(X,y)
y = pd.DataFrame({'stroke':y})
sns.countplot(data = y, x = 'stroke', y= None)
plt.show()
print(y.value_counts())
481/43: df["stroke"].value_counts()
481/44:
df = pd.concat([X,y],axis = 1)
df.head()
481/45: df["stroke"].value_counts()
481/46:
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
df = ss.fit_transform(df)
481/47: df.head()
481/48:
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
df = ss.fit_transform(df)
df = pd.Dataframe(df)
481/49:
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
df = ss.fit_transform(df)
df = np.Dataframe(df)
481/50: df.head()
482/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
482/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
482/3: df.head()
482/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
482/5:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
482/6:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
482/7:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
482/8:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
482/9:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
482/10:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
482/11:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
482/12:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
482/13:
print(df["stroke"].value_counts())
sns.countplot(x=df["stroke"], data=df)
482/14: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
482/15: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
482/16: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
482/17: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
482/18: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
482/19: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
482/20: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
482/21: sns.distplot(df["age"], fit=norm)
482/22: sns.distplot(df["bmi"], fit=norm)
482/23: sns.distplot(df["avg_glucose_level"], fit=norm)
482/24: sns.boxplot(x=df["age"], data=df)
482/25: sns.boxplot(x=df["bmi"], data=df)
482/26: sns.boxplot(x=df["avg_glucose_level"], data=df)
482/27:
data = df.corr(method='pearson')
fig = plt.figure(figsize=(15,8))
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
482/28: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
482/29: df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
482/30: print("Length of bmi after: ",df["bmi"].isnull().sum())
482/31:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound: {}, lower bound: {}".format(upper_bound,lower_bound))
482/32:
df.drop(df[df['bmi'] > upper_bound].index, inplace = True)
df.drop(df[df['bmi'] < lower_bound].index, inplace = True)
482/33:
print("After outlier removal")
fig, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x=df["bmi"], data=df, ax = axes[0])
sns.distplot(df["bmi"], fit=norm, ax = axes[1])
482/34:
# There is only one "other" category in the gender. So, we should remove it.
df.drop(df[df["gender"]=="Other"].index, inplace=True)
482/35:
# For gender column.
sex = pd.get_dummies(df["gender"], drop_first=True)
df = pd.concat([df,sex],axis=1)
482/36:
# For Ever_married colummn.
married_status = pd.get_dummies(df["ever_married"], drop_first=True)
df = pd.concat([df, married_status], axis=1)
482/37:
# For Residence type column.
residence = pd.get_dummies(df["Residence_type"], drop_first=True)
df = pd.concat([df, residence], axis=1)
482/38:
df["Work_Type"] = df["work_type"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})
df["Smoking_Status"] = df["smoking_status"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})
482/39: df.head()
482/40:
df.drop(["id","gender","ever_married","work_type","Residence_type","smoking_status"],axis=1, inplace=True)
df.head()
482/41:
df.rename(columns={"Male":"Gender","Yes":"Ever_Married","Urban":"Residence_type"}, inplace=True)
df.head()
482/42:
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state = 42)
X = df.drop(['stroke'],axis=1)
y = df['stroke']
X,y= smote.fit_resample(X,y)
y = pd.DataFrame({'stroke':y})
sns.countplot(data = y, x = 'stroke', y= None)
plt.show()
print(y.value_counts())
482/43:
df = pd.concat([X,y],axis = 1)
df.head()
482/44: X = df.drop(["stroke"], axis=1)
482/45: x
482/46: X
482/47:
X = df.drop(["stroke"], axis=1)
y = df["stroke"]
482/48: X
482/49: X.head()
482/50:
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
X = ss.fit_transform(X)
482/51: X
485/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
485/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
485/3: df.head()
485/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
485/5:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
485/6:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
485/7:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
485/8:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
485/9:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
485/10:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
485/11:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
485/12:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
485/13:
print(df["stroke"].value_counts())
sns.countplot(x=df["stroke"], data=df)
485/14: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
485/15: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
485/16: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
485/17: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
485/18: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
485/19: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
485/20: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
485/21: sns.distplot(df["age"], fit=norm)
485/22: sns.distplot(df["bmi"], fit=norm)
485/23: sns.distplot(df["avg_glucose_level"], fit=norm)
485/24: sns.boxplot(x=df["age"], data=df)
485/25: sns.boxplot(x=df["bmi"], data=df)
485/26: sns.boxplot(x=df["avg_glucose_level"], data=df)
485/27:
data = df.corr(method='pearson')
fig = plt.figure(figsize=(15,8))
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
485/28: print("Length of bmi before: ",len(df["bmi"])-df["bmi"].isnull().sum())
485/29: df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
485/30: print("Length of bmi after: ",df["bmi"].isnull().sum())
485/31:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound: {}, lower bound: {}".format(upper_bound,lower_bound))
485/32:
df.drop(df[df['bmi'] > upper_bound].index, inplace = True)
df.drop(df[df['bmi'] < lower_bound].index, inplace = True)
485/33:
print("After outlier removal")
fig, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x=df["bmi"], data=df, ax = axes[0])
sns.distplot(df["bmi"], fit=norm, ax = axes[1])
485/34:
# There is only one "other" category in the gender. So, we should remove it.
df.drop(df[df["gender"]=="Other"].index, inplace=True)
485/35:
# For gender column.
sex = pd.get_dummies(df["gender"], drop_first=True)
df = pd.concat([df,sex],axis=1)
485/36:
# For Ever_married colummn.
married_status = pd.get_dummies(df["ever_married"], drop_first=True)
df = pd.concat([df, married_status], axis=1)
485/37:
# For Residence type column.
residence = pd.get_dummies(df["Residence_type"], drop_first=True)
df = pd.concat([df, residence], axis=1)
485/38:
df["Work_Type"] = df["work_type"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})
df["Smoking_Status"] = df["smoking_status"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})
485/39: df.head()
485/40:
df.drop(["id","gender","ever_married","work_type","Residence_type","smoking_status"],axis=1, inplace=True)
df.head()
485/41:
df.rename(columns={"Male":"Gender","Yes":"Ever_Married","Urban":"Residence_type"}, inplace=True)
df.head()
485/42:
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state = 42)
X = df.drop(['stroke'],axis=1)
y = df['stroke']
X,y= smote.fit_resample(X,y)
y = pd.DataFrame({'stroke':y})
sns.countplot(data = y, x = 'stroke', y= None)
plt.show()
print(y.value_counts())
485/43:
df = pd.concat([X,y],axis = 1)
df.head()
485/44:
X = df.drop(["stroke"], axis=1)
y = df["stroke"]
485/45:
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
X = ss.fit_transform(X)
485/46:
lr = LogisticRegression(solver="liblinear").fit(x_train,y_train)
gnb = GaussianNB().fit(x_train,y_train)
knnc = KNeighborsClassifier().fit(x_train,y_train)
dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)
rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)
xgbc = XGBClassifier().fit(x_train,y_train)
catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)
485/47: x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)
485/48:
lr = LogisticRegression(solver="liblinear").fit(x_train,y_train)
gnb = GaussianNB().fit(x_train,y_train)
knnc = KNeighborsClassifier().fit(x_train,y_train)
dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)
rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)
xgbc = XGBClassifier().fit(x_train,y_train)
catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)
485/49: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,catbc]
485/50:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    error = -cross_val_score(model,x_test,y_test,cv=10,scoring="neg_mean_squared_error",verbose=False).mean()
    print(name + ": ")
    print("-" * 50)
    print("Accuracy Score: ",accuracy_score(y_test,predict))
    print("Cross Validation Score: ",CV)
    print("Error: ",np.sqrt(error))
    print("R-square value: ",r2_score(y_test,predict))
    print("Confusion matrix: ")
    print(confusion_matrix(y_test,predict))
    print("-" * 100)
485/51:
df = pd.DataFrame(columns=["MODELS","Accuracy"])
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    accuracy = accuracy_score(y_test,predict)
    result = pd.DataFrame([[name,accuracy*100]],columns=["MODELS","Accuracy"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="Accuracy",y="MODELS",data=df,color="k")
plt.xlabel("ACCURACY")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL ACCURACY COMPARISON")
plt.show()
485/52:
df = pd.DataFrame(columns=["MODELS","CV"])
for model in model_names:
    name = model.__class__.__name__
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    result = pd.DataFrame([[name,CV*100]],columns=["MODELS","CV"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="CV",y="MODELS",data=df,color="k")
plt.xlabel("CV")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL CROSS VALIDATION COMPARISON")
plt.show()
485/53:
r_prob = [0 for _ in range(len(y_test))]
r_auc = roc_auc_score(y_test,r_prob)
485/54:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    auroc_score = roc_auc_score(y_test,predict)
    print(name+" score: ",auroc_score)
    print("-"*50)
485/55:
r_fpr,r_tpr,_= roc_curve(y_test,r_prob)
model_dict={}

for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    fpr,tpr,_= roc_curve(y_test,predict)
    model_dict[name]=[fpr,tpr]
485/56:
plt.plot(r_fpr,r_tpr,linestyle="--")
plt.plot(model_dict["LogisticRegression"][0],model_dict["LogisticRegression"][1],linestyle='dotted')
plt.plot(model_dict["GaussianNB"][0],model_dict["GaussianNB"][1],linestyle='dotted')
plt.plot(model_dict["KNeighborsClassifier"][0],model_dict["KNeighborsClassifier"][1],linestyle='dotted')
plt.plot(model_dict["DecisionTreeClassifier"][0],model_dict["DecisionTreeClassifier"][1],linestyle='dotted')
plt.plot(model_dict["RandomForestClassifier"][0],model_dict["RandomForestClassifier"][1],linestyle='dotted')
plt.plot(model_dict["XGBClassifier"][0],model_dict["XGBClassifier"][1],linestyle='dotted')
plt.plot(model_dict["CatBoostClassifier"][0],model_dict["CatBoostClassifier"][1],linestyle='dotted')

plt.title("ROC plot")
plt.xlabel("False positive rate.")
plt.ylabel("True positive rate.")
plt.show()
485/57:
plt.plot(r_fpr,r_tpr,linestyle="--")
plt.plot(model_dict["LogisticRegression"][0],model_dict["LogisticRegression"][1],linestyle='dotted')
plt.plot(model_dict["GaussianNB"][0],model_dict["GaussianNB"][1],linestyle='dotted')
plt.plot(model_dict["KNeighborsClassifier"][0],model_dict["KNeighborsClassifier"][1],linestyle='dotted')
plt.plot(model_dict["DecisionTreeClassifier"][0],model_dict["DecisionTreeClassifier"][1],linestyle='dotted')
plt.plot(model_dict["RandomForestClassifier"][0],model_dict["RandomForestClassifier"][1],linestyle='dotted')
plt.plot(model_dict["XGBClassifier"][0],model_dict["XGBClassifier"][1],linestyle='dotted')
plt.plot(model_dict["CatBoostClassifier"][0],model_dict["CatBoostClassifier"][1],linestyle='dotted')

plt.title("ROC plot")
plt.xlabel("False positive rate.")
plt.ylabel("True positive rate.")
plt.legend()
plt.show()
485/58:
plt.plot(r_fpr,r_tpr,linestyle="--")
plt.plot(model_dict["LogisticRegression"][0],model_dict["LogisticRegression"][1],linestyle='dotted', label="LogisticRegression")
plt.plot(model_dict["GaussianNB"][0],model_dict["GaussianNB"][1],linestyle='dotted')
plt.plot(model_dict["KNeighborsClassifier"][0],model_dict["KNeighborsClassifier"][1],linestyle='dotted')
plt.plot(model_dict["DecisionTreeClassifier"][0],model_dict["DecisionTreeClassifier"][1],linestyle='dotted')
plt.plot(model_dict["RandomForestClassifier"][0],model_dict["RandomForestClassifier"][1],linestyle='dotted')
plt.plot(model_dict["XGBClassifier"][0],model_dict["XGBClassifier"][1],linestyle='dotted')
plt.plot(model_dict["CatBoostClassifier"][0],model_dict["CatBoostClassifier"][1],linestyle='dotted')

plt.title("ROC plot")
plt.xlabel("False positive rate.")
plt.ylabel("True positive rate.")
plt.legend()
plt.show()
485/59:
plt.plot(r_fpr,r_tpr,linestyle="--")
plt.plot(model_dict["LogisticRegression"][0],model_dict["LogisticRegression"][1],linestyle='dotted', label="LogisticRegression")
plt.plot(model_dict["GaussianNB"][0],model_dict["GaussianNB"][1],linestyle='dotted',label="GaussianNB")
plt.plot(model_dict["KNeighborsClassifier"][0],model_dict["KNeighborsClassifier"][1],linestyle='dotted', label="KNeighborsClassifier")
plt.plot(model_dict["DecisionTreeClassifier"][0],model_dict["DecisionTreeClassifier"][1],linestyle='dotted', label="DecisionTreeClassifier")
plt.plot(model_dict["RandomForestClassifier"][0],model_dict["RandomForestClassifier"][1],linestyle='dotted', label="RandomForestClassifier")
plt.plot(model_dict["XGBClassifier"][0],model_dict["XGBClassifier"][1],linestyle='dotted', label="XGBClassifier")
plt.plot(model_dict["CatBoostClassifier"][0],model_dict["CatBoostClassifier"][1],linestyle='dotted', label="CatBoostClassifier")

plt.title("ROC plot")
plt.xlabel("False positive rate.")
plt.ylabel("True positive rate.")
plt.legend()
plt.show()
485/60:
plt.figure(figsize=(15,8))
plt.plot(r_fpr,r_tpr,linestyle="--")
plt.plot(model_dict["LogisticRegression"][0],model_dict["LogisticRegression"][1],linestyle='dotted', label="LogisticRegression")
plt.plot(model_dict["GaussianNB"][0],model_dict["GaussianNB"][1],linestyle='dotted',label="GaussianNB")
plt.plot(model_dict["KNeighborsClassifier"][0],model_dict["KNeighborsClassifier"][1],linestyle='dotted', label="KNeighborsClassifier")
plt.plot(model_dict["DecisionTreeClassifier"][0],model_dict["DecisionTreeClassifier"][1],linestyle='dotted', label="DecisionTreeClassifier")
plt.plot(model_dict["RandomForestClassifier"][0],model_dict["RandomForestClassifier"][1],linestyle='dotted', label="RandomForestClassifier")
plt.plot(model_dict["XGBClassifier"][0],model_dict["XGBClassifier"][1],linestyle='dotted', label="XGBClassifier")
plt.plot(model_dict["CatBoostClassifier"][0],model_dict["CatBoostClassifier"][1],linestyle='dotted', label="CatBoostClassifier")

plt.title("ROC plot")
plt.xlabel("False positive rate.")
plt.ylabel("True positive rate.")
plt.legend()
plt.show()
485/61: print("Missing values in bmi after: ",df["bmi"].isnull().sum())
486/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
486/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
486/3: df.head()
486/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
486/5:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
486/6:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
486/7:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
486/8:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
486/9:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
486/10:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
486/11:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
486/12:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
486/13:
print(df["stroke"].value_counts())
sns.countplot(x=df["stroke"], data=df)
486/14: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
486/15: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
486/16: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
486/17: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
486/18: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
486/19: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
486/20: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
486/21: sns.distplot(df["age"], fit=norm)
486/22: sns.distplot(df["bmi"], fit=norm)
486/23: sns.distplot(df["avg_glucose_level"], fit=norm)
486/24: sns.boxplot(x=df["age"], data=df)
486/25: sns.boxplot(x=df["bmi"], data=df)
486/26: sns.boxplot(x=df["avg_glucose_level"], data=df)
486/27:
data = df.corr(method='pearson')
fig = plt.figure(figsize=(15,8))
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
486/28: print("Missing values in bmi before: ",df["bmi"].isnull().sum())
486/29: df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
486/30: print("Missing values in bmi after: ",df["bmi"].isnull().sum())
486/31:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound: {}, lower bound: {}".format(upper_bound,lower_bound))
486/32:
df.drop(df[df['bmi'] > upper_bound].index, inplace = True)
df.drop(df[df['bmi'] < lower_bound].index, inplace = True)
486/33:
print("After outlier removal")
fig, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x=df["bmi"], data=df, ax = axes[0])
sns.distplot(df["bmi"], fit=norm, ax = axes[1])
486/34:
# There is only one "other" category in the gender. So, we should remove it.
df.drop(df[df["gender"]=="Other"].index, inplace=True)
486/35:
# For gender column.
sex = pd.get_dummies(df["gender"], drop_first=True)
df = pd.concat([df,sex],axis=1)
486/36:
# For Ever_married colummn.
married_status = pd.get_dummies(df["ever_married"], drop_first=True)
df = pd.concat([df, married_status], axis=1)
486/37:
# For Residence type column.
residence = pd.get_dummies(df["Residence_type"], drop_first=True)
df = pd.concat([df, residence], axis=1)
486/38:
df["Work_Type"] = df["work_type"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})
df["Smoking_Status"] = df["smoking_status"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})
486/39: df.head()
486/40:
df.drop(["id","gender","ever_married","work_type","Residence_type","smoking_status"],axis=1, inplace=True)
df.head()
486/41:
df.rename(columns={"Male":"Gender","Yes":"Ever_Married","Urban":"Residence_type"}, inplace=True)
df.head()
486/42:
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state = 42)
X = df.drop(['stroke'],axis=1)
y = df['stroke']
X,y= smote.fit_resample(X,y)
y = pd.DataFrame({'stroke':y})
sns.countplot(data = y, x = 'stroke', y= None)
plt.show()
print(y.value_counts())
486/43:
df = pd.concat([X,y],axis = 1)
df.head()
486/44:
X = df.drop(["stroke"], axis=1)
y = df["stroke"]
486/45:
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
X = ss.fit_transform(X)
486/46: x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)
486/47:
lr = LogisticRegression(solver="liblinear").fit(x_train,y_train)
gnb = GaussianNB().fit(x_train,y_train)
knnc = KNeighborsClassifier().fit(x_train,y_train)
dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)
rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)
xgbc = XGBClassifier().fit(x_train,y_train)
catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)
486/48: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,catbc]
486/49:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    error = -cross_val_score(model,x_test,y_test,cv=10,scoring="neg_mean_squared_error",verbose=False).mean()
    print(name + ": ")
    print("-" * 50)
    print("Accuracy Score: ",accuracy_score(y_test,predict))
    print("Cross Validation Score: ",CV)
    print("Error: ",np.sqrt(error))
    print("R-square value: ",r2_score(y_test,predict))
    print("Confusion matrix: ")
    print(confusion_matrix(y_test,predict))
    print("-" * 100)
486/50:
df = pd.DataFrame(columns=["MODELS","Accuracy"])
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    accuracy = accuracy_score(y_test,predict)
    result = pd.DataFrame([[name,accuracy*100]],columns=["MODELS","Accuracy"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="Accuracy",y="MODELS",data=df,color="k")
plt.xlabel("ACCURACY")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL ACCURACY COMPARISON")
plt.show()
486/51:
df = pd.DataFrame(columns=["MODELS","CV"])
for model in model_names:
    name = model.__class__.__name__
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    result = pd.DataFrame([[name,CV*100]],columns=["MODELS","CV"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="CV",y="MODELS",data=df,color="k")
plt.xlabel("CV")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL CROSS VALIDATION COMPARISON")
plt.show()
486/52:
r_prob = [0 for _ in range(len(y_test))]
r_auc = roc_auc_score(y_test,r_prob)
486/53:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    auroc_score = roc_auc_score(y_test,predict)
    print(name+" score: ",auroc_score)
    print("-"*50)
486/54:
r_fpr,r_tpr,_= roc_curve(y_test,r_prob)
model_dict={}

for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    fpr,tpr,_= roc_curve(y_test,predict)
    model_dict[name]=[fpr,tpr]
486/55:
plt.figure(figsize=(15,8))
plt.plot(r_fpr,r_tpr,linestyle="--")
plt.plot(model_dict["LogisticRegression"][0],model_dict["LogisticRegression"][1],linestyle='dotted', label="LogisticRegression")
plt.plot(model_dict["GaussianNB"][0],model_dict["GaussianNB"][1],linestyle='dotted',label="GaussianNB")
plt.plot(model_dict["KNeighborsClassifier"][0],model_dict["KNeighborsClassifier"][1],linestyle='dotted', label="KNeighborsClassifier")
plt.plot(model_dict["DecisionTreeClassifier"][0],model_dict["DecisionTreeClassifier"][1],linestyle='dotted', label="DecisionTreeClassifier")
plt.plot(model_dict["RandomForestClassifier"][0],model_dict["RandomForestClassifier"][1],linestyle='dotted', label="RandomForestClassifier")
plt.plot(model_dict["XGBClassifier"][0],model_dict["XGBClassifier"][1],linestyle='dotted', label="XGBClassifier")
plt.plot(model_dict["CatBoostClassifier"][0],model_dict["CatBoostClassifier"][1],linestyle='dotted', label="CatBoostClassifier")

plt.title("ROC plot")
plt.xlabel("False positive rate.")
plt.ylabel("True positive rate.")
plt.legend()
plt.show()
489/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/List/TM2_list_q1_20846636.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/List')
489/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/List/TM2_list_q1_20846636.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/List')
489/3: clear
492/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/List/TM2_list_q1_20846636.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/List')
492/2: clear
494/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments')
494/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments')
494/3: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')
494/4: clear
494/5: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')
494/6: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')
494/7: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')
494/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')
494/9: clear
494/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')
494/11: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')
494/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')
494/13: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')
494/14: clear
494/15: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')
494/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')
494/17: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')
494/18: runcell(0, 'C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q5.py')
494/19:
for i in range(len(Sample_list)):
    temp = list(Sample_list[i])
    temp[-1] = 100
    temp = tuple(temp)
    Sample_list[i]=temp
494/20: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')
494/21: clear
494/22: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')
494/23: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples/tuple_q3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Tuples')
495/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/3: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/4: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/5: clear
495/6: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/7: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/9: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/11: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/13: clear
495/14: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/15: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/17: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/18: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/19: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/20: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/21: clear
495/22: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/23: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Set/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Set')
495/24: clear
498/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/3: clear
498/4: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/5: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/6: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/7: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/9: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/11: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/13: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/14: clear
498/15: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/17: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/18: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/19: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/20: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/21: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/22: clear
498/23: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/24: clear
498/25: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/26: clear
498/27: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/28: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/29: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/30: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/31: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/32: clear
498/33: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/34: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/35: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/36: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/37: clear
498/38: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/39: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/40: clear
498/41: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/42: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/43: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/44: clear
498/45: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/46: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/47: clear
498/48: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/49: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/50: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/51: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/52: clear
498/53: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/54: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/55: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/56: clear
498/57: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
498/58: clear
500/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
500/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
500/3: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
500/4: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
500/5: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
500/6: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
500/7: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
500/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
500/9: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
500/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
500/11: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
500/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/String/String_q4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/String')
501/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/3: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/4: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/5: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/6: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/7: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/9: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/11: runcell(0, 'C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled1.py')
501/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/13: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/14: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/15: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/17: clear
501/18: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/19: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/20: clear
501/21: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/22: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/23: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/24: clear
501/25: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/26: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Data_Structure')
501/27: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/28: clear
501/29: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/30: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/31: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/32: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled7.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/33: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled7.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/34: clear
501/35: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled7.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/36: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled7.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/37: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled7.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/38: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled7.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/39: clear
501/40: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled8.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/41: clear
501/42: clear
501/43: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled8.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/44: clear
501/45: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled8.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/46: clear
501/47: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled9.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/48: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled9.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/49: clear
501/50: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled9.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/51: runcell(0, 'C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py')
501/52: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/53: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/54: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/55: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/56: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/57: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/58: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/59: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled10.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/60: clear
501/61: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled11.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/62: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled11.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/63: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled11.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/64: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/65: clear
501/66: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/67: clear
501/68: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/69: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/70: clear
501/71: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/72: clear
501/73: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/74: clear
501/75: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/76: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/77: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/78: clear
501/79: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/80: clear
501/81: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/82: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/83: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/untitled13.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
501/84: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/Question_6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
502/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/Project_2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
502/2: clear
502/3: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages/Project_2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Function_Modules_Packages')
502/4: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument/Question_1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument')
502/5: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument/Question_1.py', args='10 20', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument')
502/6: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument/Question_1.py', args='10 20', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument')
502/7: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument/Question_1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Command_Line_Argument')
502/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
502/9: clear
502/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
502/11: clear
502/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/Question_1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
502/13: clear
502/14: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
502/15: clear
502/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
502/17: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
502/18: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
502/19: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
502/20: clear
503/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/3: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/4: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/5: clear
503/6: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/7: clear
503/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/9: clear
503/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/11: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/12: clear
503/13: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/14: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/15: clear
503/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/17: clear
503/18: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled0.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/19: clear
503/20: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/21: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/22: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/23: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/24: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/25: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/26: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/27: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/28: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/29: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/30: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/31: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/32: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/33: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/34: clear
503/35: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/36: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/37: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/38: clear
503/39: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/40: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/41: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/42: clear
503/43: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/44: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/45: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/46: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/47: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
503/48: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
505/1: runfile('C:/Users/Jay/untitled0.py', wdir='C:/Users/Jay')
505/2: runfile('C:/Users/Jay/untitled0.py', wdir='C:/Users/Jay')
505/3: runfile('C:/Users/Jay/untitled0.py', wdir='C:/Users/Jay')
505/4: runfile('C:/Users/Jay/untitled0.py', wdir='C:/Users/Jay')
505/5: runfile('C:/Users/Jay/untitled0.py', wdir='C:/Users/Jay')
505/6: runfile('C:/Users/Jay/untitled0.py', wdir='C:/Users/Jay')
505/7: runfile('C:/Users/Jay/untitled0.py', wdir='C:/Users/Jay')
505/8: clear
505/9: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/11: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/13: clear
505/14: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/15: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled1.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/17: clear
505/18: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/19: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled2.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/20: clear
505/21: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/22: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/23: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled3.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/24: clear
505/25: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/26: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/27: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/28: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/29: clear
505/30: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/31: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/32: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/33: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations/Question_6.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/IO_Operations')
505/34: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/35: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/36: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/37: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/38: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/39: clear
505/40: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
505/41: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/1: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/2: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/3: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/4: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/5: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/6: clear
506/7: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/8: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/9: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/10: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/11: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/12: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/13: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/14: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/15: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/16: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/17: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/18: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/19: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/20: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/21: clear
506/22: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/23: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/24: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/25: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/26: clear
506/27: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/28: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/29: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/30: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/31: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/32: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/33: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/34: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/35: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/36: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled5.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/37: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/38: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/39: clear
506/40: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/41: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/42: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/43: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/44: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/45: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/46: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/47: clear
506/48: runfile('C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling/untitled4.py', wdir='C:/Users/Jay/Desktop/Python_PBL/Assignments/Exception_handeling')
506/49: clear
507/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
507/2: data1 = pd.read_csv("country_wise_latest.csv")
507/3: data1.head()
507/4:
data1 = pd.read_csv("country_wise_latest.csv")
data2 = pd.read_csv("covid_19_clean_complete.csv")
507/5: data2.head()
507/6: data2
507/7: data2["Country/Region"] == "India"
507/8: data2
507/9:
data1 = pd.read_csv("country_wise_latest.csv")
data2 = pd.read_csv("covid_19_clean_complete.csv")
data3 = pd.read_csv("full_grouped.csv")
507/10: data3.head()
507/11:
data1 = pd.read_csv("country_wise_latest.csv")
data2 = pd.read_csv("covid_19_clean_complete.csv")
data3 = pd.read_csv("full_grouped.csv")
data4 = pd.read_csv("day_wise.csv")
507/12: data4.head()
507/13: data3.head(186)
507/14: data3.head(188)
507/15: data1.head(188)
507/16: data1.size()
507/17: data1.size
507/18: data1.head(10)
507/19: data1.shape()
507/20: data1.shape
507/21: data1.head(188)
507/22: data1.isna().sum()
507/23: data2.head(188)
507/24: data2.tail()
507/25: data2.shape
507/26: data2.head()
507/27: data2["Country/Region"].isunique()
507/28: data2["Country/Region"].unique()
507/29: data2["Country/Region"].unique().sum()
507/30: data2["Country/Region"].unique()
507/31: len(data2["Country/Region"].unique())
507/32: data2["Country/Region"].unique()
507/33: data2.head()
507/34: data2.isna().sum()
507/35: data3.shape
507/36: data2.head().shape
507/37: data2.head()
507/38: data2.shape
507/39: new_data = pd.read_csv("https://github.com/owid/covid-19-data/blob/master/public/data/latest/owid-covid-latest.csv")
507/40:
vactin_data = https://github.com/owid/covid-19-data/blob/master/public/data/latest/owid-covid-latest.csv
new_data = pd.read_csv("vactin_data")
507/41:
vactin_data = 'https://github.com/owid/covid-19-data/blob/master/public/data/latest/owid-covid-latest.csv'
new_data = pd.read_csv("vactin_data")
507/42:
vactin_data = 'https://github.com/owid/covid-19-data/blob/master/public/data/latest/owid-covid-latest.csv'
#new_data = pd.read_csv("vactin_data")
507/43: vactin_data
507/44:
vactin_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.csv'
#new_data = pd.read_csv("vactin_data")
507/45: vactin_data
507/46:
vactin_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.csv'
new_data = pd.read_csv("vactin_data")
507/47:
vactin_data = "https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.csv"
new_data = pd.read_csv("vactin_data")
507/48:
vactin_data = "https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.csv"
new_data = pd.read_csv(vactin_data)
507/49:
vactin_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.csv'
new_data = pd.read_csv(vactin_data)
507/50:
vactin_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.csv'
new_data = pd.read_csv(vactin_data)
new_data
507/51:
v_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv'
pd.read_csv(v_data)
507/52:
v_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv'
pd.read_csv(v_data)
507/53: v_data['date'].unique()
507/54: v_data['date'].unique
507/55: v_data['location'].unique
507/56:
v_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv'
pd.read_csv(v_data)
print(type(v_data))
507/57:
v_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv'
pd.read_csv(v_data)
507/58: v_data['location']
507/59:
v_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv'
v_data = pd.read_csv(v_data)
v_data.head()
507/60: v_data['location'].unique
507/61: v_data['location'].unique()
507/62: v_data['date'].unique()
507/63: columns = v_data['date'].unique()
507/64:
columns = v_data['date'].unique()
columns
507/65: data_frame = pd.DataFrame(0, columns=[date_columns])
507/66: date_columns = v_data['date'].unique()
507/67: data_frame = pd.DataFrame(0, columns=[date_columns])
507/68: data_frame = pd.DataFrame(0, columns=[for col in date_colmns])
507/69: data_frame = pd.DataFrame(0, columns=[col, for col in date_colmns])
507/70: data_frame = pd.DataFrame(0, columns=[col, for col in date_colmns:])
507/71: data_frame = pd.DataFrame(0, columns=date_columns)
507/72: date_columns = v_data['date'].unique()
507/73:
date_columns = v_data['date'].unique()
date_columns
507/74:
v_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv'
v_data = pd.read_csv(v_data)
v_data
507/75:
date_columns = v_data['date'].unique()
date_columns.sort()
date_column
507/76:
date_columns = v_data['date'].unique()
date_columns.sort()
date_columns
507/77: data_frame[date_columns]
507/78: v_data_1 = v_data.pivot_table('total_vaccinations','location','date')
507/79: v_data_1
507/80: v_data_1 = v_data.pivot_table('total_vaccinations'*10000,'location','date')
507/81: v_data_1 = v_data.pivot_table('total_vaccinations','location','date')
507/82: v_data_1
507/83: v_data_1['Afghanistan']['2021-09-16']
507/84: v_data_1
507/85: v_data['2021-09-16']['Afghanistan']
507/86: v_data['Zimbabwe']
507/87: v_data['location']['Zimbabwe']
507/88: v_data['location'].iloc[1]
507/89: v_data['location'].iloc[1:]
507/90: v_data['location'].iloc[1]
507/91: v_data[:].iloc[1]
507/92: v_data[:].iloc[50237]
507/93: v_df_1 = pd.DataFrame()
507/94:
v_df_1 = pd.DataFrame()
v_df_1
507/95:
v_df_1 = pd.DataFrame()
v_df_1[date_columns]
507/96: v_df_1 = pd.DataFrame(columns=date_columns)
507/97:
v_df_1 = pd.DataFrame(columns=date_columns)
v_df_1
507/98:
v_df_1 = pd.DataFrame(columns=['location',date_columns])
v_df_1
507/99:
v_df_1 = pd.DataFrame(columns='location',date_columns)
v_df_1
507/100:
v_df_1 = pd.DataFrame(columns=('location',date_columns))
v_df_1
507/101:
v_df_1 = pd.DataFrame(columns=date_columns)
v_df_1 = pd.DataFrame(loc=idx, column='location')
v_df_1
507/102:
v_df_1 = pd.DataFrame(columns=date_columns)
v_df_1 = pd.DataFrame(loc=0, column='location')
v_df_1
507/103:
v_df_1 = pd.DataFrame(columns=date_columns)
v_df_1.insert(loc=0, column='location')
v_df_1
507/104:
v_df_1 = pd.DataFrame(columns=date_columns)
v_df_1.insert(loc=0, column='location',value=0)
v_df_1
507/105:
location_column = v_data['location'].unique().sort()
location_column
507/106:
location_column = v_data['location'].unique().sort()
location_column
507/107:
location_column = v_data['location'].unique().sort()
print(location_column)
507/108:
location_column = v_data['location'].unique()
print(location_column)
507/109:
location_column = v_data['location'].unique()
location_column.sort()
print(location_column)
507/110:
v_df_1 = pd.DataFrame(columns=date_columns)
v_df_1.insert(loc=0, column='location',value=location_column)
v_df_1
507/111:
new_data = v_data.drop(columns=['total_boosters','daily_vaccinations_raw','daily_vaccinations','total_vaccinations_per_hundred',
                                'people_vaccinated_per_hundred','people_fully_vaccinated_per_hundred',
                                'total_boosters_per_hundred','daily_vaccinations_per_million'], axis=1)
new_data
507/112:
new_data = v_data.drop(columns=['iso_code','total_boosters','daily_vaccinations_raw','daily_vaccinations','total_vaccinations_per_hundred',
                                'people_vaccinated_per_hundred','people_fully_vaccinated_per_hundred',
                                'total_boosters_per_hundred','daily_vaccinations_per_million'], axis=1)
new_data
507/113: new_data.isnull().sum()
507/114: new_data.isnull()
507/115: new_data.fillna(0)
507/116: new_data.isnull(0).sum()
507/117: new_data.isnull().sum()
507/118: new_data.fillna(0)
507/119: new_data.isnull().sum()
507/120: new_data.fillna(0,inplace=True)
507/121: new_data.isnull().sum()
507/122:
new_data.fillna(0,inplace=True)
new_data
507/123:
v_data_1 = new_data.groupby(['date'])[["total_vaccinations","people_vaccinated","people_fully_vaccinated","location"]].sum().reset_index()
v_data_1
507/124:
v_data_1 = new_data.groupby(['date'])[["total_vaccinations","people_vaccinated","people_fully_vaccinated"]].sum().reset_index()
v_data_1
507/125: v_data_1["total_vaccinations"].iloc[-1]
507/126: v_data_1["people_vaccinated"].iloc[-1]
507/127: v_data_1["total_vaccinations"].iloc[-1]
507/128: v_data_1["people_fully_vaccinated"].iloc[-1]
507/129:
date = new_data['date'].unique()
date.sort()
date[-1]
507/130: new_data.loc[new_data['location'] == 'World' and new_data['date']== date[-1]]
507/131: new_data.loc[new_data['location'] == 'World']
507/132: new_data.loc[new_data['location'] == 'World'][-1]
507/133: globale = new_data.loc[new_data['location'] == 'World']
507/134:
globale = new_data.loc[new_data['location'] == 'World']
globale
507/135:
globale = new_data.loc[new_data['location'] == 'World']
globale[-1]
507/136:
globale = new_data.loc[new_data['location'] == 'World'].reset_index()
globale
507/137:
globale = new_data.loc[new_data['location'] == 'World'].reset_index()
globale[-1]
507/138:
globale = new_data.loc[new_data['location'] == 'World'].reset_index()
globale
507/139:
globale = new_data.loc[new_data['location'] == 'World'].reset_index()
globale['date'].iloc[-1]
507/140: new_data.loc[(new_data['location'] == 'World' and new_data['date']==date[-1])]
507/141: new_data.loc[(new_data['location'] == 'World',new_data['date']==date[-1])]
507/142: new_data.loc[new_data['location'] == 'World']
507/143: globale_date = new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]
507/144:
globale_date = new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]
globale_data
507/145:
globale_date = new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]
globale_data
507/146:
global_date = new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]
global_data
507/147: global_date = new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]
507/148: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]
507/149: new_data['location'][new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]]
507/150: new_data['location'].iloc[new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]]
507/151: new_data['location'][new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]]
507/152: new_data['location'].[new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]]
507/153: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]
507/154: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1:0]
507/155: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1:1]
507/156: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]
507/157: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,0]
507/158: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]
507/159: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,1]
507/160: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,3]
507/161: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,4]
507/162: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,5]
507/163: country_name = new_data['location'].unique()
507/164: country_name
507/165:
country_name = new_data['location'].unique()
country_dropdown = {}

for cntry in country_name:
    country_dropdown["label"]=cntry
    county_dropdown["value"]=cntry
507/166:
country_name = new_data['location'].unique()
country_dropdown = {}

for cntry in country_name:
    country_dropdown["label"]=cntry
    country_dropdown["value"]=cntry
507/167:
country_name
country_dropdown
507/168:
country_name = new_data['location'].unique()
country_dropdown = {}

for cntry in country_name:
    country_dropdown[cntry]=cntry
    country_dropdown[cntry]=cntry
507/169:
country_name
country_dropdown
507/170: date = new_data['date'].unique().sort()[-1]
507/171: date = new_data['date'].unique().sort()
507/172:
date = new_data['date'].unique().sort()
date
507/173:
date = new_data['date'].unique()
date
507/174:
date = new_data['date'].unique()
date.sort()
507/175:
date = new_data['date'].unique()
date.sort()
date
507/176:
date = new_data['date'].unique()
date.sort()
date[-1]
507/177:
date = new_data['date'].unique()
date.sort()
type(date[-1])
507/178:
date = new_data['date'].unique()
date.sort()
date = pd.to_datetime(date[-1])
type(date)
507/179:
date = new_data['date'].unique()
date.sort()
date = pd.to_datetime(date[-1])
date
507/180:
date = new_data['date'].unique()
date.sort()
date = pd.to_datetime(date[-1])
str(date[-1].strftime("%B %d, %Y"))
507/181:
date = new_data['date'].unique()
date.sort()
date = pd.to_datetime(date[-1])
str(date.strftime("%B %d, %Y"))
507/182:
covid_data_2 = new_data.groupby(["date", "location"])[["total_vaccinations",
                                                       "people_vaccinated", "people_fully_vaccinated"]].sum().reset_index()
covid_data_2
508/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
508/2:
data1 = pd.read_csv("country_wise_latest.csv")
data2 = pd.read_csv("covid_19_clean_complete.csv")
data3 = pd.read_csv("full_grouped.csv")
data4 = pd.read_csv("day_wise.csv")
508/3:
v_data = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv'
v_data = pd.read_csv(v_data)
v_data
508/4:
new_data = v_data.drop(columns=['iso_code','total_boosters','daily_vaccinations_raw','daily_vaccinations','total_vaccinations_per_hundred',
                                'people_vaccinated_per_hundred','people_fully_vaccinated_per_hundred',
                                'total_boosters_per_hundred','daily_vaccinations_per_million'], axis=1)
new_data
508/5: new_data.isnull().sum()
508/6:
new_data.fillna(0,inplace=True)
new_data
508/7: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]
508/8: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,3]
508/9: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,4]
508/10: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1,5]
508/11:
country_name = new_data['location'].unique()
country_dropdown = {}

for cntry in country_name:
    country_dropdown[cntry]=cntry
    country_dropdown[cntry]=cntry
508/12:
country_name
country_dropdown
508/13:
date = new_data['date'].unique()
date.sort()
date = pd.to_datetime(date[-1])
str(date.strftime("%B %d, %Y"))
508/14: total_vaccinations  people_vaccinated   people_fully_vaccinated
508/15:
covid_data_2 = new_data.groupby(["date", "location"])[["total_vaccinations",
                                                       "people_vaccinated", "people_fully_vaccinated"]].sum().reset_index()
covid_data_2
508/16:
covid_data_2 = new_data.groupby(["date", "location"])[["total_vaccinations",
                                                       "people_vaccinated", "people_fully_vaccinated"]].sum().reset_index()
covid_data_2.head(200)
508/17:
covid_data_2 = new_data.groupby(["date", "location"])[["total_vaccinations",
                                                       "people_vaccinated", "people_fully_vaccinated"]].sum().reset_index()
covid_data_2.head(188)
508/18: value_confirmed = covid_data_2[covid_data_2["location"] == "World"]["total_vaccinations"].iloc[-1] - covid_data_2[covid_data_2["location"] == "World"]["total_vaccinations"].iloc[-2]
508/19:
value_confirmed = covid_data_2[covid_data_2["location"] == "World"]["total_vaccinations"].iloc[-1] - covid_data_2[covid_data_2["location"] == "World"]["total_vaccinations"].iloc[-2]
value_confirmed
508/20: new_data["location"]=="World"
508/21: new_data["location"].iloc["World"]
508/22: new_data.loc[new_data['location'] == 'World']
508/23: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-1]
508/24: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-2]
508/25: 6133859009.0 - 6117325514.0
508/26:
value_confirmed = covid_data_2[covid_data_2["location"] == "World"]["total_vaccinations"].iloc[-1] - covid_data_2[covid_data_2["location"] == "World"]["total_vaccinations"].iloc[-3]
value_confirmed
508/27: new_data.loc[new_data['location'] == 'World'].reset_index().iloc[-3]
508/28: 6133859009.0 - 6101198008.0
508/29:
value_confirmed = covid_data_2[covid_data_2["location"] == "World"]["people_vaccinated"].iloc[-1] - covid_data_2[covid_data_2["location"] == "World"]["people_vaccinated"].iloc[-3]
value_confirmed
508/30:
covid_data_3 = covid_data_2[covid_data_2["location"] ==
                             "World"][["location", "date", "total_vaccinations"]].reset_index()
covid_data_3
508/31:
covid_data_3["daily_confirmed"] = covid_data_3["total_vaccinations"] - covid_data_3["total_vaccinations"].shift(1)
covid_data_3
508/32:
covid_data_3 = covid_data_2[covid_data_2["location"] ==
                             "India"][["location", "date", "total_vaccinations"]].reset_index()
covid_data_3
508/33:
covid_data_3["daily_confirmed"] = covid_data_3["total_vaccinations"] - covid_data_3["total_vaccinations"].shift(1)
covid_data_3
508/34: covid_data_3["rolling_avg"] = covid_data_3["daily_confirmed"].rolling(window=7).mean()
508/35: covid_data_3
508/36: covid_data_3["rolling_avg"] = covid_data_3["daily_confirmed"].rolling(window=2).mean()
508/37: covid_data_3
508/38: covid_data_3["rolling_avg"] = covid_data_3["daily_confirmed"].rolling(window=7).mean()
508/39: covid_data_3
508/40:
url_confirmed = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"
confirmed = pd.read_csv(url_confirmed)
confirmed.head()
508/41:
url_confirmed = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"
confirmed = pd.read_csv(url_confirmed)
confirmed.shape
508/42: len(new_data["location"].unique())
508/43: len(confirmed["Country/Region"].unique())
508/44:
list1 = list(confirmed["Country/Region"].unique())
list 2 = list(new_data["location"].unique())
508/45: list(set(list2) - set(list1)) + list(set(list1) - set(list2))
508/46:
list1 = list(confirmed["Country/Region"].unique())
list2 = list(new_data["location"].unique())
508/47: list(set(list2) - set(list1)) + list(set(list1) - set(list2))
508/48: len(list(set(list2) - set(list1)) + list(set(list1) - set(list2)))
508/49: list(set(list2) - set(list1)) + list(set(list1) - set(list2))
508/50:
test = new_data["location"]
test
508/51:
test = new_data["location"]
test.head()
508/52:
test = new_data["location"]
test
508/53:
test = new_data["location"].unique()
test
508/54:
test = new_data["location"].unique()
test.head()
508/55:
test = pd.DataFrame(new_data["location"].unique())
test.head()
508/56:
test = pd.DataFrame(new_data["location"].unique(), columns="location")
test.head()
508/57:
test = pd.DataFrame(new_data["location"].unique(), columns:"location")
test.head()
508/58:
test = pd.DataFrame(new_data["location"].unique(), columns:["location"])
test.head()
508/59:
test = pd.DataFrame(new_data["location"].unique(), columns=["location"])
test.head()
508/60:
url_confirmed = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"
confirmed = pd.read_csv(url_confirmed)
confirmed.head()
508/61:
test = confirmed.groupby(["Country/Region"])[["Lat", "Long"]].sum().reset_index()
test
508/62:
test = pd.DataFrame(new_data["location"].unique(), columns=["location"])
test
508/63:
test = confirmed.groupby(["Country/Region"])[["Lat", "Long"]].reset_index()
test
508/64:
test = confirmed.groupby(["Country/Region"])[["Lat", "Long"]]
test
508/65:
test = confirmed.groupby(["Country/Region"])[["Lat", "Long"]].sum().reset_index()
test
508/66:
url_confirmed = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"
confirmed = pd.read_csv(url_confirmed)
confirmed
508/67: test.to_dict()
508/68: confirmed.loc[confirmed["Country/Region"]=="India"]
508/69: confirmed.loc[confirmed["Country/Region"]=="India"]["lat"]
508/70: confirmed.loc[confirmed["Country/Region"]=="India"]
508/71: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[1]
508/72: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[:1]
508/73: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[:2]
508/74: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[1:]
508/75: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[2:]
508/76: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[:]
508/77: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[:-1]
508/78: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[::1]
508/79: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[::2]
508/80: confirmed.loc[confirmed["Country/Region"]=="India"].iloc["lat"]
508/81: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[]
508/82: confirmed.loc[confirmed["Country/Region"]=="India"]
508/83: confirmed["Country/Region"]=="India"
508/84: confirmed["Country/Region"]
508/85: confirmed["Country/Region"].["lat"]
508/86: confirmed["Country/Region"]["lat"]
508/87: confirmed["Country/Region"]["Lat"]
508/88: confirmed["Country/Region"].["Lat"]
508/89: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[:,3]
508/90: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[:]
508/91: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[:2]
508/92: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[:,2]
508/93: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[,2]
508/94: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[:,2]
508/95:
dic = {}

for places in confirmed["Country/Region"].unique():
    dic[places] = confirmed.loc[confirmed["Country/Region"]==places].iloc[:,2]
    
dic
508/96: confirmed.isnull().sum()
508/97: confirmed["Country/Region"].unique()
508/98: confirmed["Country/Region"]=="India".iloc[:,2]
508/99: confirmed.loc[confirmed["Country/Region"]=="India"].iloc[:,2]
508/100:
dic = {}

for places in (confirmed["Country/Region"].unique()):
    dic[places] = confirmed.loc[confirmed["Country/Region"]==places].iloc[:,2]
    
dict
508/101:
dic = {}

for places in (confirmed["Country/Region"].unique()):
    dic[places] = confirmed.loc[confirmed["Country/Region"]==places].iloc[:,2]
    
dic
508/102: confirmed.loc[confirmed["Country/Region"]=="Zimbabwe"].iloc[:,2]
508/103: confirmed.loc[confirmed["Country/Region"]=="Zimbabwe"].iloc[:,2][1]
508/104: confirmed.loc[confirmed["Country/Region"]=="Zimbabwe"].iloc[:,2]
508/105: type(confirmed.loc[confirmed["Country/Region"]=="Zimbabwe"].iloc[:,2])
508/106:
d = confirmed.loc[confirmed["Country/Region"]=="Zimbabwe"].iloc[:,2]
d
508/107:
d = confirmed.loc[confirmed["Country/Region"]=="Zimbabwe"].iloc[:,2]
d[0]
508/108:
d = confirmed.loc[confirmed["Country/Region"]=="Zimbabwe"].iloc[:,2]
d[[0]]
508/109: confirmed.loc[confirmed["Country/Region"]=="Zimbabwe"].iloc[:,2]
508/110: confirmed.iloc[:,2]
508/111: confirmed.iloc[1:,2]
508/112: confirmed.loc[India,"Lat"]
508/113: confirmed.loc["India","Lat"]
508/114: confirmed.loc["Country/Region","Lat"]
508/115: confirmed.loc[["Country/Region"],["Lat"]]
508/116: confirmed.iloc[:,2]
510/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
510/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
510/3: df.head()
510/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
510/5:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
510/6:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
510/7:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
510/8:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
510/9:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
510/10:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
510/11:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
510/12:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
510/13:
print(df["stroke"].value_counts())
sns.countplot(x=df["stroke"], data=df)
510/14: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
510/15: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
510/16: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
510/17: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
510/18: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
510/19: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
510/20: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
510/21: sns.distplot(df["age"], fit=norm)
510/22: sns.distplot(df["bmi"], fit=norm)
510/23: sns.distplot(df["avg_glucose_level"], fit=norm)
510/24: sns.boxplot(x=df["age"], data=df)
510/25: sns.boxplot(x=df["bmi"], data=df)
510/26: sns.boxplot(x=df["avg_glucose_level"], data=df)
510/27:
data = df.corr(method='pearson')
fig = plt.figure(figsize=(15,8))
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
510/28: print("Missing values in bmi before: ",df["bmi"].isnull().sum())
510/29: df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
510/30: print("Missing values in bmi after: ",df["bmi"].isnull().sum())
510/31:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound: {}, lower bound: {}".format(upper_bound,lower_bound))
510/32:
df.drop(df[df['bmi'] > upper_bound].index, inplace = True)
df.drop(df[df['bmi'] < lower_bound].index, inplace = True)
510/33:
print("After outlier removal")
fig, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x=df["bmi"], data=df, ax = axes[0])
sns.distplot(df["bmi"], fit=norm, ax = axes[1])
510/34:
# There is only one "other" category in the gender. So, we should remove it.
df.drop(df[df["gender"]=="Other"].index, inplace=True)
510/35:
# For gender column.
sex = pd.get_dummies(df["gender"], drop_first=True)
df = pd.concat([df,sex],axis=1)
510/36:
# For Ever_married colummn.
married_status = pd.get_dummies(df["ever_married"], drop_first=True)
df = pd.concat([df, married_status], axis=1)
510/37:
# For Residence type column.
residence = pd.get_dummies(df["Residence_type"], drop_first=True)
df = pd.concat([df, residence], axis=1)
510/38:
df["Work_Type"] = df["work_type"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})
df["Smoking_Status"] = df["smoking_status"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})
510/39: df.head()
510/40:
df.drop(["id","gender","ever_married","work_type","Residence_type","smoking_status"],axis=1, inplace=True)
df.head()
510/41:
df.rename(columns={"Male":"Gender","Yes":"Ever_Married","Urban":"Residence_type"}, inplace=True)
df.head()
510/42:
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state = 42)
X = df.drop(['stroke'],axis=1)
y = df['stroke']
X,y= smote.fit_resample(X,y)
y = pd.DataFrame({'stroke':y})
sns.countplot(data = y, x = 'stroke', y= None)
plt.show()
print(y.value_counts())
510/43:
df = pd.concat([X,y],axis = 1)
df.head()
510/44:
X = df.drop(["stroke"], axis=1)
y = df["stroke"]
510/45:
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
X = ss.fit_transform(X)
510/46: x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)
510/47:
lr = LogisticRegression(solver="liblinear").fit(x_train,y_train)
gnb = GaussianNB().fit(x_train,y_train)
knnc = KNeighborsClassifier().fit(x_train,y_train)
dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)
rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)
xgbc = XGBClassifier().fit(x_train,y_train)
catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)
510/48: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,catbc]
510/49:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    error = -cross_val_score(model,x_test,y_test,cv=10,scoring="neg_mean_squared_error",verbose=False).mean()
    print(name + ": ")
    print("-" * 50)
    print("Accuracy Score: ",accuracy_score(y_test,predict))
    print("Cross Validation Score: ",CV)
    print("Error: ",np.sqrt(error))
    print("R-square value: ",r2_score(y_test,predict))
    print("Confusion matrix: ")
    print(confusion_matrix(y_test,predict))
    print("-" * 100)
510/50:
df = pd.DataFrame(columns=["MODELS","Accuracy"])
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    accuracy = accuracy_score(y_test,predict)
    result = pd.DataFrame([[name,accuracy*100]],columns=["MODELS","Accuracy"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="Accuracy",y="MODELS",data=df,color="k")
plt.xlabel("ACCURACY")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL ACCURACY COMPARISON")
plt.show()
510/51:
df = pd.DataFrame(columns=["MODELS","CV"])
for model in model_names:
    name = model.__class__.__name__
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    result = pd.DataFrame([[name,CV*100]],columns=["MODELS","CV"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="CV",y="MODELS",data=df,color="k")
plt.xlabel("CV")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL CROSS VALIDATION COMPARISON")
plt.show()
510/52:
r_prob = [0 for _ in range(len(y_test))]
r_auc = roc_auc_score(y_test,r_prob)
510/53:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    auroc_score = roc_auc_score(y_test,predict)
    print(name+" score: ",auroc_score)
    print("-"*50)
510/54:
r_fpr,r_tpr,_= roc_curve(y_test,r_prob)
model_dict={}

for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    fpr,tpr,_= roc_curve(y_test,predict)
    model_dict[name]=[fpr,tpr]
510/55:
plt.figure(figsize=(15,8))
plt.plot(r_fpr,r_tpr,linestyle="--")
plt.plot(model_dict["LogisticRegression"][0],model_dict["LogisticRegression"][1],linestyle='dotted', label="LogisticRegression")
plt.plot(model_dict["GaussianNB"][0],model_dict["GaussianNB"][1],linestyle='dotted',label="GaussianNB")
plt.plot(model_dict["KNeighborsClassifier"][0],model_dict["KNeighborsClassifier"][1],linestyle='dotted', label="KNeighborsClassifier")
plt.plot(model_dict["DecisionTreeClassifier"][0],model_dict["DecisionTreeClassifier"][1],linestyle='dotted', label="DecisionTreeClassifier")
plt.plot(model_dict["RandomForestClassifier"][0],model_dict["RandomForestClassifier"][1],linestyle='dotted', label="RandomForestClassifier")
plt.plot(model_dict["XGBClassifier"][0],model_dict["XGBClassifier"][1],linestyle='dotted', label="XGBClassifier")
plt.plot(model_dict["CatBoostClassifier"][0],model_dict["CatBoostClassifier"][1],linestyle='dotted', label="CatBoostClassifier")

plt.title("ROC plot")
plt.xlabel("False positive rate.")
plt.ylabel("True positive rate.")
plt.legend()
plt.show()
511/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier


from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, roc_auc_score

from pandas_profiling import ProfileReport

from scipy.stats import norm
511/2: df = pd.read_csv("healthcare-dataset-stroke-data.csv")
511/3: df.head()
511/4:
print("Shape")
print(df.shape)
print("-"*100)
print("Columns")
columns = df.columns
print(columns)
print("-"*100)
print("Data information: ")
print(df.info())
print("-"*100)
print("Data description: ")
print(df.describe())
print("-"*100)
print("Null values count: ")
print(df.isnull().sum())
print("-"*100)
511/5:
print("Gender: ",df["gender"].unique())
print("Hypertension: ",df["hypertension"].unique())
print("Heart Disease: ",df["heart_disease"].unique())
print("Ever Married: ",df["ever_married"].unique())
print("Work Type: ",df["work_type"].unique())
print("Residence type: ",df["Residence_type"].unique())
print("Smoking status: ",df["smoking_status"].unique())
print("Stroke: ",df["stroke"].unique())
511/6:
print(df["gender"].value_counts())
sns.set(style="darkgrid")
sns.countplot(x=df["gender"],data=df)
511/7:
print(df["hypertension"].value_counts())
sns.countplot(x=df["hypertension"], data=df)
511/8:
print(df["heart_disease"].value_counts())
sns.countplot(x=df["heart_disease"], data=df)
511/9:
print(df["ever_married"].value_counts())
sns.countplot(x=df["ever_married"], data=df)
511/10:
print(df["work_type"].value_counts())
sns.countplot(x=df["work_type"], data=df)
511/11:
print(df["Residence_type"].value_counts())
sns.countplot(x=df["Residence_type"], data=df)
511/12:
print(df["smoking_status"].value_counts())
sns.countplot(x=df["smoking_status"], data=df)
511/13:
print(df["stroke"].value_counts())
sns.countplot(x=df["stroke"], data=df)
511/14: sns.countplot(x=df["hypertension"], hue=df["stroke"], data=df)
511/15: sns.countplot(x=df["gender"], hue=df["stroke"], data=df)
511/16: sns.countplot(x=df["heart_disease"], hue=df["stroke"], data=df)
511/17: sns.countplot(x=df["ever_married"], hue=df["stroke"], data=df)
511/18: sns.countplot(x=df["work_type"], hue=df["stroke"], data=df)
511/19: sns.countplot(x=df["Residence_type"], hue=df["stroke"], data=df)
511/20: sns.countplot(x=df["smoking_status"], hue=df["stroke"], data=df)
511/21: sns.distplot(df["age"], fit=norm)
511/22: sns.distplot(df["bmi"], fit=norm)
511/23: sns.distplot(df["avg_glucose_level"], fit=norm)
511/24: sns.boxplot(x=df["age"], data=df)
511/25: sns.boxplot(x=df["bmi"], data=df)
511/26: sns.boxplot(x=df["avg_glucose_level"], data=df)
511/27:
data = df.corr(method='pearson')
fig = plt.figure(figsize=(15,8))
sns.heatmap(data,annot=True,cbar=True,linewidths=1)
511/28: print("Missing values in bmi before: ",df["bmi"].isnull().sum())
511/29: df["bmi"].fillna(value=df["bmi"].mean(), inplace=True)
511/30: print("Missing values in bmi after: ",df["bmi"].isnull().sum())
511/31:
q1,q3 = np.percentile(df["bmi"],[25,75])
print(q1,q3)
iqr = q3-q1
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)
print("upper bound: {}, lower bound: {}".format(upper_bound,lower_bound))
511/32:
df.drop(df[df['bmi'] > upper_bound].index, inplace = True)
df.drop(df[df['bmi'] < lower_bound].index, inplace = True)
511/33:
print("After outlier removal")
fig, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x=df["bmi"], data=df, ax = axes[0])
sns.distplot(df["bmi"], fit=norm, ax = axes[1])
511/34:
# There is only one "other" category in the gender. So, we should remove it.
df.drop(df[df["gender"]=="Other"].index, inplace=True)
511/35:
# For gender column.
sex = pd.get_dummies(df["gender"], drop_first=True)
df = pd.concat([df,sex],axis=1)
511/36:
# For Ever_married colummn.
married_status = pd.get_dummies(df["ever_married"], drop_first=True)
df = pd.concat([df, married_status], axis=1)
511/37:
# For Residence type column.
residence = pd.get_dummies(df["Residence_type"], drop_first=True)
df = pd.concat([df, residence], axis=1)
511/38:
df["Work_Type"] = df["work_type"].map({'Private':0,'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4})
df["Smoking_Status"] = df["smoking_status"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3})
511/39: df.head()
511/40:
df.drop(["id","gender","ever_married","work_type","Residence_type","smoking_status"],axis=1, inplace=True)
df.head()
511/41:
df.rename(columns={"Male":"Gender","Yes":"Ever_Married","Urban":"Residence_type"}, inplace=True)
df.head()
511/42:
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state = 42)
X = df.drop(['stroke'],axis=1)
y = df['stroke']
X,y= smote.fit_resample(X,y)
y = pd.DataFrame({'stroke':y})
sns.countplot(data = y, x = 'stroke', y= None)
plt.show()
print(y.value_counts())
511/43:
df = pd.concat([X,y],axis = 1)
df.head()
511/44:
X = df.drop(["stroke"], axis=1)
y = df["stroke"]
511/45:
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
X = ss.fit_transform(X)
511/46: x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)
511/47:
lr = LogisticRegression(solver="liblinear").fit(x_train,y_train)
gnb = GaussianNB().fit(x_train,y_train)
knnc = KNeighborsClassifier().fit(x_train,y_train)
dtc = DecisionTreeClassifier(random_state=42).fit(x_train,y_train)
rfc = RandomForestClassifier(random_state=42,verbose=False).fit(x_train,y_train)
xgbc = XGBClassifier().fit(x_train,y_train)
catbc = CatBoostClassifier(verbose=False).fit(x_train,y_train)
511/48: model_names = [lr,gnb,knnc,dtc,rfc,xgbc,catbc]
511/49:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    error = -cross_val_score(model,x_test,y_test,cv=10,scoring="neg_mean_squared_error",verbose=False).mean()
    print(name + ": ")
    print("-" * 50)
    print("Accuracy Score: ",accuracy_score(y_test,predict))
    print("Cross Validation Score: ",CV)
    print("Error: ",np.sqrt(error))
    print("R-square value: ",r2_score(y_test,predict))
    print("Confusion matrix: ")
    print(confusion_matrix(y_test,predict))
    print("-" * 100)
511/50:
df = pd.DataFrame(columns=["MODELS","Accuracy"])
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict(x_test)
    accuracy = accuracy_score(y_test,predict)
    result = pd.DataFrame([[name,accuracy*100]],columns=["MODELS","Accuracy"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="Accuracy",y="MODELS",data=df,color="k")
plt.xlabel("ACCURACY")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL ACCURACY COMPARISON")
plt.show()
511/51:
df = pd.DataFrame(columns=["MODELS","CV"])
for model in model_names:
    name = model.__class__.__name__
    CV = cross_val_score(model,x_test,y_test,cv=10,verbose=False).mean()
    result = pd.DataFrame([[name,CV*100]],columns=["MODELS","CV"])
    df = df.append(result)
    
figure = plt.figure(figsize=(20,8))   
sns.barplot(x="CV",y="MODELS",data=df,color="k")
plt.xlabel("CV")
plt.ylabel("MODELS")
plt.xlim(0,100)
plt.title("MODEL CROSS VALIDATION COMPARISON")
plt.show()
511/52:
r_prob = [0 for _ in range(len(y_test))]
r_auc = roc_auc_score(y_test,r_prob)
511/53:
for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    auroc_score = roc_auc_score(y_test,predict)
    print(name+" score: ",auroc_score)
    print("-"*50)
511/54:
r_fpr,r_tpr,_= roc_curve(y_test,r_prob)
model_dict={}

for model in model_names:
    name = model.__class__.__name__
    predict = model.predict_proba(x_test)[:,1]
    fpr,tpr,_= roc_curve(y_test,predict)
    model_dict[name]=[fpr,tpr]
511/55:
plt.figure(figsize=(15,8))
plt.plot(r_fpr,r_tpr,linestyle="--")
plt.plot(model_dict["LogisticRegression"][0],model_dict["LogisticRegression"][1],linestyle='dotted', label="LogisticRegression")
plt.plot(model_dict["GaussianNB"][0],model_dict["GaussianNB"][1],linestyle='dotted',label="GaussianNB")
plt.plot(model_dict["KNeighborsClassifier"][0],model_dict["KNeighborsClassifier"][1],linestyle='dotted', label="KNeighborsClassifier")
plt.plot(model_dict["DecisionTreeClassifier"][0],model_dict["DecisionTreeClassifier"][1],linestyle='dotted', label="DecisionTreeClassifier")
plt.plot(model_dict["RandomForestClassifier"][0],model_dict["RandomForestClassifier"][1],linestyle='dotted', label="RandomForestClassifier")
plt.plot(model_dict["XGBClassifier"][0],model_dict["XGBClassifier"][1],linestyle='dotted', label="XGBClassifier")
plt.plot(model_dict["CatBoostClassifier"][0],model_dict["CatBoostClassifier"][1],linestyle='dotted', label="CatBoostClassifier")

plt.title("ROC plot")
plt.xlabel("False positive rate.")
plt.ylabel("True positive rate.")
plt.legend()
plt.show()
512/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
512/2: dataset = pd.read_csv("data.csv")
512/3: dataset.head()
512/4: dataset.columns()
512/5: dataset.columns
512/6:
dataset.drop(labels=['PassengerId','WikiId', 'Name_wiki',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class'], axis=1, inplce=True)
dataset.head()
512/7:
dataset.drop(labels=['PassengerId','WikiId', 'Name_wiki',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class'], axis=1, inplace=True)
dataset.head()
513/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
513/2: dataset = pd.read_csv("data.csv")
513/3: dataset.head()
513/4: dataset.columns
513/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin'
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class'], axis=1, inplace=True)
dataset.head()
513/6:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class'], axis=1, inplace=True)
dataset.head()
513/7: Now, before taking care of the Missing data fist we have to look for the missing data i.e. we will check wahat percentage of data we are missing and based on that we will make our dicision.
513/8: dataset.isnull().sum()
513/9: dataset.isna().sum()
513/10: len(dataset["Survived"])
513/11: len(dataset["Survived"])/dataset["survived"].isnull().sum()
513/12: len(dataset["Survived"])/dataset["Survived"].isnull().sum()
513/13: len(dataset["Survived"])/dataset["Survived"].isnull().sum()*100
513/14: len(dataset["Survived"])/dataset["Survived"].isnull().sum()
513/15: (len(dataset["Survived"])/dataset["Survived"].isnull().sum())*100
513/16: len(dataset["Survived"])/dataset["Survived"].isnull().sum()
513/17: dataset.shape
513/18: len(dataset["Age"])
513/19: dataset.describe
513/20: dataset.descrption
513/21: dataset.info()
513/22:
for ele in dataset.columns:
    print("{}: {}".formate(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
513/23:
for ele in dataset.columns:
    print("{}: {}".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
513/24:
for ele in dataset.columns:
    print("{}: {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
513/25:
for ele in dataset.columns:
    print("{}:\t{}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
513/26:
for ele in dataset.columns:
    print("{}: \t{}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
513/27:
for ele in dataset.columns:
    print("{}:\t\t{}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
513/28:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
514/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
514/2: dataset = pd.read_csv("data.csv")
514/3: dataset.head()
514/4: dataset.columns
514/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class'], axis=1, inplace=True)
dataset.head()
514/6: dataset.isnull().sum()
514/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
515/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
515/2: dataset = pd.read_csv("data.csv")
515/3: dataset.head()
515/4: dataset.columns
515/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
515/6: dataset.isnull().sum()
515/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
515/8:
fig = plt.figure()
ax = fig.add_subplot(111)
dataset["Age"].plot(kind='kde', ax=ax)
515/9: import seaborn as sns
515/10: sns.distplot(dataset["Age"], fit=norm)
515/11:
import seaborn as sns
from scipy.stats import norm
515/12: sns.distplot(dataset["Age"], fit=norm)
515/13: sns.distplot(dataset["Age"], fit=norm, color="Red")
515/14: temp_dataset1 = dataset
515/15:
temp_dataset1 = dataset
tem_dataset1["Age"].fillna(temp_dataset1["Age"].mean())
515/16:
temp_dataset1 = dataset
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean())
temp_dataset1.head()
515/17:
temp_dataset1 = dataset
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean())
temp_dataset1["Age"].isnull().sum()
515/18:
temp_dataset1 = dataset
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean())
temp_dataset1["Age"].isnull().sum()
515/19:
temp_dataset1 = dataset
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(),inplace=True)
temp_dataset1["Age"].isnull().sum()
515/20: dataset["Age"].isnull().sum()
516/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
516/2: dataset = pd.read_csv("data.csv")
516/3: dataset.head()
516/4: dataset.columns
516/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
516/6: dataset.isnull().sum()
516/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
516/8:
temp_dataset1 = dataset
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(),inplace=True)
temp_dataset1["Age"].isnull().sum()
516/9: dataset["Age"].isnull().sum()
516/10: dataset
516/11:
temp_dataset1 = dataset
temp_dataset1
517/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
517/2: dataset = pd.read_csv("data.csv")
517/3: dataset.head()
517/4: dataset.columns
517/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
517/6: dataset.isnull().sum()
517/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
517/8:
temp_dataset1 = dataset
temp_dataset1
517/9: dataset["Age"].isnull().sum()
517/10: dataset
517/11:
temp_dataset1 = dataset
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean())
temp_dataset1
517/12:
temp_dataset1 = dataset
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean())
temp_dataset1
517/13: dataset["Age"].isnull().sum()
517/14:
temp_dataset1 = dataset
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mea)
temp_dataset1
517/15:
temp_dataset1 = dataset
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean)
temp_dataset1
517/16:
temp_dataset1 = dataset
temp_dataset1["Age"].mean())
517/17:
temp_dataset1 = dataset
temp_dataset1["Age"].mean()
517/18:
temp_dataset1 = dataset
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean())
517/19:
temp_dataset1 = dataset
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean())
temp_dataset1
517/20:
temp_dataset1 = dataset
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(), inplace=True)
temp_dataset1
517/21: dataset["Age"].isnull().sum()
517/22: dataset
517/23:
temp_dataset1 = dataset
temp_dataset1["Age"].mean()
517/24: dataset
517/25: dataset.isnull().sum()
518/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
518/2: dataset = pd.read_csv("data.csv")
518/3: dataset.head()
518/4: dataset.columns
518/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
518/6: dataset.isnull().sum()
518/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
518/8:
temp_dataset1 = dataset
temp_dataset1["Age"].mean()
518/9:
temp_dataset1 = dataset
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(), inplace=True)
temp_dataset1
518/10: dataset["Age"].isnull().sum()
518/11: dataset
518/12: dataset.isnull().sum()
520/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
520/2: dataset = pd.read_csv("data.csv")
520/3: dataset.head()
520/4: dataset.columns
520/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
520/6: dataset.isnull().sum()
520/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
520/8:
temp_dataset1 = dataset
temp_dataset1["Age"].mean()
520/9:
temp_dataset1 = dataset
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(), inplace=True)
temp_dataset1
520/10: dataset["Age"].isnull().sum()
520/11: dataset
520/12: dataset.isnull().sum()
521/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
521/2: dataset = pd.read_csv("data.csv")
521/3: dataset.head()
521/4: dataset.columns
521/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
521/6: dataset.isnull().sum()
521/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
521/8:
temp_dataset1 = dataset.copy()
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(), inplace=True)
temp_dataset1
521/9: dataset["Age"].isnull().sum()
521/10: dataset
521/11: dataset.isnull().sum()
521/12:
temp_dataset1 = dataset.copy()
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(), inplace=True)
temp_dataset1["Age"].isnull().sum()
521/13:
from sklearn.impute import SimpleImputer

temp_dataset2 = dataset.copy()

imputer = SimpleImputer(missing_values=np.nan, strategy=mean)
imputer.fit(temp_dataset2["Age"])
temp_dataset2["Age"] = imputer.transform(temp_dataset2["Age"])
521/14:
from sklearn.impute import SimpleImputer

temp_dataset2 = dataset.copy()

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(temp_dataset2["Age"])
temp_dataset2["Age"] = imputer.transform(temp_dataset2["Age"])
521/15:
from sklearn.impute import SimpleImputer

temp_dataset2 = dataset.copy()

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(temp_dataset2[["Age"]])
temp_dataset2["Age"] = imputer.transform(temp_dataset2[["Age"]])
521/16:
from sklearn.impute import SimpleImputer

temp_dataset2 = dataset.copy()

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(temp_dataset2[["Age"]])
temp_dataset2["Age"] = imputer.transform(temp_dataset2[["Age"]])
temp_dataset2
521/17:
from sklearn.impute import SimpleImputer

temp_dataset2 = dataset.copy()

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(temp_dataset2[["Age"]])
temp_dataset2["Age"] = imputer.transform(temp_dataset2[["Age"]])
temp_dataset2["Age"].isnull().sum()
521/18: dataset["Age"].mean()
521/19: int(dataset["Age"].mean())
521/20: dataset["Age"].fillna(int(dataset["Age"].mean()),inplace=True)
521/21:
dataset["Age"].fillna(int(dataset["Age"].mean()),inplace=True)
dataset
521/22: dataset.dropna(subset=["Survived","Embarked"])
521/23: dataset.dropna(subset=["Survived","Embarked"], inplace=True)
521/24:
dataset.dropna(subset=["Survived","Embarked"], inplace=True)
dataset.isnull().sum()
521/25: dataset
521/26: temp_dataset3 = dataset.copy()
521/27:
temp_dataset3 = dataset.copy()
temp_dataset3
521/28:

##### Using pandas get_dummies ##############

temp_dataset3 = dataset.copy()
temp_dataset3

temp_column = pd.get_dummies(temp_dataset3["Embarked"])
temp_column
521/29:

##### Using pandas get_dummies ##############

temp_dataset3 = dataset.copy()
temp_dataset3

temp_column = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column
521/30:

##### Using pandas get_dummies ##############

temp_dataset3 = dataset.copy()
temp_dataset3

temp_column = pd.get_dummies(temp_dataset3["Embarked"])
temp_column
521/31:

##### Using pandas get_dummies ##############

temp_dataset3 = dataset.copy()
temp_dataset3

temp_column = pd.get_dummies(temp_dataset3["Embarked"])
temp_column

temp_dataset3 = pd.concat([temp_dataset3,temp_column],axis=1)
temp_dataset3.head()
521/32:

##### Using pandas get_dummies ##############

temp_dataset3 = dataset.copy()
temp_dataset3

temp_column = pd.get_dummies(temp_dataset3["Embarked"])
temp_column

temp_dataset3 = pd.concat([temp_dataset3,temp_column],axis=1)
temp_dataset3
521/33:
from sklearn.preprocessing import LabelEncoder

temp_dataset4 = dataset.copy()

le = LabelEncoder()
temp_dataset4["Embarked"] = le.fit_transform([temp_dataset4["Embarked"]]) 
temp_dataset4
521/34:
from sklearn.preprocessing import LabelEncoder

temp_dataset4 = dataset.copy()

le = LabelEncoder()
temp_dataset4["Embarked"] = le.fit_transform(temp_dataset4["Embarked"]) 
temp_dataset4
521/35:

##### Using pandas get_dummies ##############

temp_dataset3 = dataset.copy()
temp_dataset3

temp_column = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column

temp_dataset3 = pd.concat([temp_dataset3,temp_column],axis=1)
temp_dataset3
521/36:

##### Using pandas get_dummies ##############

temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)

temp_dataset3 = pd.concat([temp_dataset3,[temp_column1, temp_column2],axis=1)
temp_dataset3
521/37:

##### Using pandas get_dummies ##############

temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)

temp_dataset3 = pd.concat([temp_dataset3,[temp_column1, temp_column2]],axis=1)
temp_dataset3
521/38:

##### Using pandas get_dummies ##############

temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)

temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)
temp_dataset3
521/39: dataset
521/40:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm1 = ohe.fit_transform(temp_dataset4["Embarked"])

temp_dataset4["Embarked"] = pd.concat([temp_dataset4, temp_clm1],axis=1)
temp_dataset4
521/41:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm1 = ohe.fit_transform([temp_dataset4["Embarked"]])

temp_dataset4["Embarked"] = pd.concat([temp_dataset4, temp_clm1],axis=1)
temp_dataset4
521/42:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm1 = ohe.fit_transform([temp_dataset4["Embarked"]].reshape(-1,1))

temp_dataset4["Embarked"] = pd.concat([temp_dataset4, temp_clm1],axis=1)
temp_dataset4
521/43:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm1 = ohe.fit_transform(np.array(temp_dataset4["Embarked"]).reshape(-1,1))

temp_dataset4["Embarked"] = pd.concat([temp_dataset4, temp_clm1],axis=1)
temp_dataset4
521/44:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
522/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
522/2: dataset = pd.read_csv("data.csv")
522/3: dataset.head()
522/4: dataset.columns
522/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
522/6: dataset.isnull().sum()
522/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
522/8:
temp_dataset1 = dataset.copy()
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(), inplace=True)
temp_dataset1["Age"].isnull().sum()
522/9:
from sklearn.impute import SimpleImputer

temp_dataset2 = dataset.copy()

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(temp_dataset2[["Age"]])
temp_dataset2["Age"] = imputer.transform(temp_dataset2[["Age"]])
temp_dataset2["Age"].isnull().sum()
522/10: dataset["Age"].fillna(int(dataset["Age"].mean()),inplace=True)
522/11:
dataset.dropna(subset=["Survived","Embarked"], inplace=True)
dataset.isnull().sum()
522/12:
temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)

temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)
temp_dataset3
522/13:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
522/14:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_dataset4 = np.array(temp_dataset4).reshape(-1,1)

temp_dataset4
522/15:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_dataset4 = np.array(temp_dataset4)

temp_dataset4
522/16:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_dataset4 = np.array(temp_dataset4)

print(temp_dataset4)

temp_dataset4 = ohe.fit_transform(temp_dataset4)
temp_dataset4
522/17: temp_dataset4 = dataset.copy()
522/18:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"])
temp_clm
522/19:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"])
temp_clm
522/20:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"])
temp_clm = ohe.fit_transform(temp_clm)
temp_clm
522/21:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"])
#temp_clm = ohe.fit_transform(temp_clm)
temp_clm
522/22:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
#temp_clm = ohe.fit_transform(temp_clm)
temp_clm
522/23:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"])
#temp_clm = ohe.fit_transform(temp_clm)
temp_clm
522/24:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
#temp_clm = ohe.fit_transform(temp_clm)
temp_clm
522/25:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(1,-1)
#temp_clm = ohe.fit_transform(temp_clm)
temp_clm
522/26:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(1,-1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm
522/27:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm
522/28:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm, columns=["Q","S"])
522/29:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm, columns=["Q","S"])

temp_clm
522/30:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm, columns=["Q","S"])

temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4
522/31:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm, columns=["Q","S"])

temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4
522/32:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm, columns=["Q","S"])
print(temp_clm.shape)
temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4
522/33:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm)
print(temp_clm.shape)
temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4
522/34:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
#temp_clm = pd.DataFrame(data=temp_clm)
print(temp_clm.shape)
temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4
522/35:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
#temp_clm = pd.DataFrame(data=temp_clm)
print(temp_clm.shape)
#temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
#temp_dataset4
522/36:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
#temp_clm = pd.DataFrame(data=temp_clm)
print(temp_clm)
print(temp_clm.shape)
#temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
#temp_dataset4
522/37:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm)
print(temp_clm)
print(temp_clm.shape)
#temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
#temp_dataset4
522/38:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm)
print(temp_clm)
print(temp_clm.shape)
temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4
522/39:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm)
print(temp_clm)
#print(temp_clm.shape)
temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4
522/40:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
#temp_clm = pd.DataFrame(data=temp_clm)
print(temp_clm)
#print(temp_clm.shape)
temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4
522/41:
temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)
print(type(temp_column1))
temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)
temp_dataset3
522/42:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm)
print(type(temp_clm))
#print(temp_clm.shape)
temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4
522/43:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm)
print(temp_clm)
#print(temp_clm.shape)
temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4
522/44:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm, columns=["Q","S"])
print(temp_clm)
#print(temp_clm.shape)
temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4
522/45:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm, columns=["Q","S"])
print(temp_clm)
#print(temp_clm.shape)
temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4.isnull().sum()
522/46:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm, columns=["Q","S"])
print(temp_clm)
#print(temp_clm.shape)
temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4.isnull()
522/47:
pd.set_option('display.height', 1000)
pd.set_option('display.max_rows', 1000)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
522/48:

pd.set_option('display.max_rows', 1000)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
522/49:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm, columns=["Q","S"])
print(temp_clm)
#print(temp_clm.shape)
temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4.isnull()
522/50:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm, columns=["Q","S"])
print(temp_clm)
#print(temp_clm.shape)
temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4.isnull().head(100)
522/51:
from sklearn.preprocessing import OneHotEncoder

temp_dataset4 = dataset.copy()

ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm, columns=["Q","S"])
print(temp_clm)
#print(temp_clm.shape)
temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4.head(100)
522/52: dataset.head(100)
522/53:
temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)
print(type(temp_column1))
temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)
temp_dataset3.head(100)
522/54:
temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)
print(type(temp_column1))
temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)
temp_dataset3
522/55:
temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)

temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)
temp_dataset3
523/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
523/2: dataset = pd.read_csv("data.csv")
523/3: dataset.head()
523/4: dataset.columns
523/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
523/6: dataset.isnull().sum()
523/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
523/8:
temp_dataset1 = dataset.copy()
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(), inplace=True)
temp_dataset1["Age"].isnull().sum()
523/9:
from sklearn.impute import SimpleImputer

temp_dataset2 = dataset.copy()

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(temp_dataset2[["Age"]])
temp_dataset2["Age"] = imputer.transform(temp_dataset2[["Age"]])
temp_dataset2["Age"].isnull().sum()
523/10: dataset["Age"].fillna(int(dataset["Age"].mean()),inplace=True)
523/11:
dataset.dropna(subset=["Survived","Embarked"], inplace=True)
dataset.isnull().sum()
523/12:
temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)

temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)
temp_dataset3
523/13:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])], remainder='passthrough')


ohe = OneHotEncoder(drop="first", sparse=False)
temp_clm = np.array(temp_dataset4["Embarked"]).reshape(-1,1)
temp_clm = ohe.fit_transform(temp_clm)
temp_clm = pd.DataFrame(data=temp_clm, columns=["Q","S"])
print(temp_clm)
#print(temp_clm.shape)
temp_dataset4 = pd.concat([temp_dataset4, temp_clm], axis=1)
temp_dataset4.head(100)
523/14:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])], remainder='passthrough')
523/15: temp_dataset4
523/16: temp_dataset4 = np.array(temp_dataset4)
523/17:
temp_dataset4 = np.array(temp_dataset4)
temp_dataset4
523/18:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])], remainder='passthrough')

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/19:


pd.set_option('display.max_columns', 500)
523/20:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])], remainder='passthrough')

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/21:


pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
523/22:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])], remainder='passthrough')

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/23: np.set_printoptions(threshold=sys.maxsize)
523/24:
import sys
np.set_printoptions(threshold=sys.maxsize)
523/25:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])], remainder='passthrough')

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/26:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])])

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/27:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])])

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/28:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,6])], remainder='passthrough')

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/29:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), ["Sex","Embarked"])], remainder='passthrough')

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/30:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2])], remainder='passthrough')

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/31:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/32:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,:6])], remainder='passthrough')

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/33:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/34:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), slice(2,6))], remainder='passthrough')

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/35:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2]),('cat', OneHotEncoder(), [6])], remainder='passthrough')

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/36:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2]),('encode', OneHotEncoder(), [6])], remainder='passthrough')

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/37:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')

temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))
temp_dataset4
523/38:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2])], remainder='passthrough')
temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [6])], remainder='passthrough')
temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))

temp_dataset4
523/39:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')
temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))

temp_dataset4
524/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
524/2: dataset = pd.read_csv("data.csv")
524/3: dataset.head()
524/4: dataset.columns
524/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
524/6: dataset.isnull().sum()
524/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
524/8:
temp_dataset1 = dataset.copy()
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(), inplace=True)
temp_dataset1["Age"].isnull().sum()
524/9:
from sklearn.impute import SimpleImputer

temp_dataset2 = dataset.copy()

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(temp_dataset2[["Age"]])
temp_dataset2["Age"] = imputer.transform(temp_dataset2[["Age"]])
temp_dataset2["Age"].isnull().sum()
524/10: dataset["Age"].fillna(int(dataset["Age"].mean()),inplace=True)
524/11:
dataset.dropna(subset=["Survived","Embarked"], inplace=True)
dataset.isnull().sum()
524/12:
temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)

temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)
temp_dataset3
524/13:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')
temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))

temp_dataset4
524/14:
temp_dataset4 = np.array(temp_dataset4)
temp_dataset4
524/15:


pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
524/16:
import sys
np.set_printoptions(threshold=sys.maxsize)
524/17:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')
temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))

temp_dataset4
524/18:
from sklearn.preprocessing import LabelEncoder

temp_dataset5 = dataset.copy()

le = LabelEncoder()
temp_dataset5["Sex"] = le.fit_transform(temp_dataset5["Sex"])

temp_dataset5
524/19:
from sklearn.preprocessing import LabelEncoder

temp_dataset5 = dataset.copy()

le = LabelEncoder()
temp_dataset5["Sex"] = le.fit_transform(temp_dataset5["Sex","Embarked"])

temp_dataset5
524/20:
from sklearn.preprocessing import LabelEncoder

temp_dataset5 = dataset.copy()

le = LabelEncoder()
temp_dataset5["Sex"] = le.fit_transform(temp_dataset5[["Sex","Embarked"]])

temp_dataset5
524/21:
from sklearn.preprocessing import LabelEncoder

temp_dataset5 = dataset.copy()

le = LabelEncoder()
temp_dataset5["Sex"] = le.fit_transform(temp_dataset5["Sex"])
temp_dataset5["Embarked"] = le.fit_transform(temp_dataset5["Embarked"])

temp_dataset5
524/22:
from sklearn.preprocessing inport OrdinalEncoder

temp_dataset6 = dataset.copy()

oe = OrdinalEncoder()

Sex = oe.fit_transform(np.array(temp_dataset6["Sex"]).reshape(-1,1))
Embarked = oe.fit_transform(np.array(temp_dataset6["Embarked"]).reshape(-1,1))

temp_dataset6 = pd.concat([temp_dataset6,Sex,Embarked], axis=1)
temp_dataset6
524/23:
from sklearn.preprocessing import OrdinalEncoder

temp_dataset6 = dataset.copy()

oe = OrdinalEncoder()

Sex = oe.fit_transform(np.array(temp_dataset6["Sex"]).reshape(-1,1))
Embarked = oe.fit_transform(np.array(temp_dataset6["Embarked"]).reshape(-1,1))

temp_dataset6 = pd.concat([temp_dataset6,Sex,Embarked], axis=1)
temp_dataset6
524/24:
N_Embarked = pd.get_dummies(datasetset["Embarked"], drop_first=True)
N_Sex = pd.get_dummies(dataset["Sex"], drop_first=True)

dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)
temp_dataset3
524/25:
N_Embarked = pd.get_dummies(dataset["Embarked"], drop_first=True)
N_Sex = pd.get_dummies(dataset["Sex"], drop_first=True)

dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)
temp_dataset3
524/26:
N_Embarked = pd.get_dummies(dataset["Embarked"], drop_first=True)
N_Sex = pd.get_dummies(dataset["Sex"], drop_first=True)

dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)
dataset
524/27:
N_Embarked = pd.get_dummies(dataset["Embarked"], drop_first=True)
N_Sex = pd.get_dummies(dataset["Sex"], drop_first=True)

dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)
print(dataset.head())

# We, can remove the Sex and Embarked Columns as we have encoded them.
524/28:
N_Embarked = pd.get_dummies(dataset["Embarked"], drop_first=True)
N_Sex = pd.get_dummies(dataset["Sex"], drop_first=True)

dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)
print(dataset.head())

# We, can remove the Sex and Embarked Columns as we have encoded them.
525/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
525/2: dataset = pd.read_csv("data.csv")
525/3: dataset.head()
525/4: dataset.columns
525/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
525/6: dataset.isnull().sum()
525/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
525/8:
temp_dataset1 = dataset.copy()
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(), inplace=True)
temp_dataset1["Age"].isnull().sum()
525/9:
from sklearn.impute import SimpleImputer

temp_dataset2 = dataset.copy()

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(temp_dataset2[["Age"]])
temp_dataset2["Age"] = imputer.transform(temp_dataset2[["Age"]])
temp_dataset2["Age"].isnull().sum()
525/10: dataset["Age"].fillna(int(dataset["Age"].mean()),inplace=True)
525/11:
dataset.dropna(subset=["Survived","Embarked"], inplace=True)
dataset.isnull().sum()
525/12:
temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)

temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)
temp_dataset3
525/13:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')
temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))

temp_dataset4
525/14:
from sklearn.preprocessing import LabelEncoder

temp_dataset5 = dataset.copy()

le = LabelEncoder()
temp_dataset5["Sex"] = le.fit_transform(temp_dataset5["Sex"])
temp_dataset5["Embarked"] = le.fit_transform(temp_dataset5["Embarked"])

temp_dataset5
525/15:
N_Embarked = pd.get_dummies(dataset["Embarked"], drop_first=True)
N_Sex = pd.get_dummies(dataset["Sex"], drop_first=True)

dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)
print(dataset.head())

# We, can remove the Sex and Embarked Columns as we have encoded them.
525/16:


pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
525/17:
import sys
np.set_printoptions(threshold=sys.maxsize)
525/18:
N_Embarked = pd.get_dummies(dataset["Embarked"], drop_first=True)
N_Sex = pd.get_dummies(dataset["Sex"], drop_first=True)

dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)
print(dataset.head())

# We, can remove the Sex and Embarked Columns as we have encoded them.

dataset.drop(columns=["Sex","Embarked"], axis=1)
print(dataset.head())
526/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
526/2: dataset = pd.read_csv("data.csv")
526/3: dataset.head()
526/4: dataset.columns
526/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
526/6: dataset.isnull().sum()
526/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
526/8:
temp_dataset1 = dataset.copy()
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(), inplace=True)
temp_dataset1["Age"].isnull().sum()
526/9:
from sklearn.impute import SimpleImputer

temp_dataset2 = dataset.copy()

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(temp_dataset2[["Age"]])
temp_dataset2["Age"] = imputer.transform(temp_dataset2[["Age"]])
temp_dataset2["Age"].isnull().sum()
526/10: dataset["Age"].fillna(int(dataset["Age"].mean()),inplace=True)
526/11:
dataset.dropna(subset=["Survived","Embarked"], inplace=True)
dataset.isnull().sum()
526/12:
temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)

temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)
temp_dataset3
526/13:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')
temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))

temp_dataset4
526/14:
from sklearn.preprocessing import LabelEncoder

temp_dataset5 = dataset.copy()

le = LabelEncoder()
temp_dataset5["Sex"] = le.fit_transform(temp_dataset5["Sex"])
temp_dataset5["Embarked"] = le.fit_transform(temp_dataset5["Embarked"])

temp_dataset5
526/15:
N_Embarked = pd.get_dummies(dataset["Embarked"], drop_first=True)
N_Sex = pd.get_dummies(dataset["Sex"], drop_first=True)

dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)
print(dataset.head())

# We, can remove the Sex and Embarked Columns as we have encoded them.

dataset.drop(columns=["Sex","Embarked"], axis=1)
print(dataset.head())
526/16:


pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
526/17:
import sys
np.set_printoptions(threshold=sys.maxsize)
527/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
527/2: dataset = pd.read_csv("data.csv")
527/3: dataset.head()
527/4: dataset.columns
527/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
527/6: dataset.isnull().sum()
527/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
527/8:
temp_dataset1 = dataset.copy()
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(), inplace=True)
temp_dataset1["Age"].isnull().sum()
527/9:
from sklearn.impute import SimpleImputer

temp_dataset2 = dataset.copy()

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(temp_dataset2[["Age"]])
temp_dataset2["Age"] = imputer.transform(temp_dataset2[["Age"]])
temp_dataset2["Age"].isnull().sum()
527/10: dataset["Age"].fillna(int(dataset["Age"].mean()),inplace=True)
527/11:
dataset.dropna(subset=["Survived","Embarked"], inplace=True)
dataset.isnull().sum()
527/12:
temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)

temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)
temp_dataset3
527/13:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')
temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))

temp_dataset4
527/14:
from sklearn.preprocessing import LabelEncoder

temp_dataset5 = dataset.copy()

le = LabelEncoder()
temp_dataset5["Sex"] = le.fit_transform(temp_dataset5["Sex"])
temp_dataset5["Embarked"] = le.fit_transform(temp_dataset5["Embarked"])

temp_dataset5
527/15:
N_Embarked = pd.get_dummies(dataset["Embarked"], drop_first=True)
N_Sex = pd.get_dummies(dataset["Sex"], drop_first=True)

dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)
print(dataset.head())

# We, can remove the Sex and Embarked Columns as we have encoded them.

dataset.drop(columns=["Sex","Embarked"], axis=1, inplace=True)
print(dataset.head())
527/16:


pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
527/17:
import sys
np.set_printoptions(threshold=sys.maxsize)
527/18: X = dataset.drop(dataset["Survived"],axis=1)
527/19: X = dataset.drop(columns=["Survived"],axis=1)
527/20:
X = dataset.drop(columns=["Survived"],axis=1)
X
527/21:
X = dataset.drop(columns=["Survived"],axis=1)
y = dataset["Survived"]

y
527/22:
X = dataset.drop(columns=["Survived"],axis=1)
y = dataset["Survived"]
527/23: x
527/24: X
527/25: X.isnull().sum()
527/26: x
527/27: X
527/28: y
527/29: y.isnull().sum
527/30: y.isnull().sum()
527/31:
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train
527/32: X_test
527/33: y_test
527/34: y_train
527/35:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train["Age","SibSp"] = sc.fit_transform(X_train["Age","SibSp"])
X_test["Age","SibSp"] = sc.transform(X_test["Age","SibSp"])
527/36:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train[["Age"],["SibSp"]] = sc.fit_transform(X_train["Age","SibSp"])
X_test["Age","SibSp"] = sc.transform(X_test["Age","SibSp"])
527/37:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train[["Age","SibSp"]] = sc.fit_transform(X_train[["Age","SibSp"]])
X_test[["Age","SibSp"]] = sc.transform(X_test[["Age","SibSp"]])
527/38: X_train[:, 1:3]
527/39:
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)
527/40: X_train
527/41: X_train[:, 1:3]
527/42: X_train[:, 1]
527/43: X_train[:, 1:]
527/44:
np.array(X_train)
X_train[:, 1:3]
527/45:
X_train = np.array(X_train)
X_train[:, 1:3]
527/46:
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)
527/47: X_train
527/48: X_train[["Age","Sibsp"]]
527/49: X_train[["Age","SibSp"]]
527/50:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train[["Age","SibSp"]] = sc.fit_transform(X_train[["Age","SibSp"]])
X_test[["Age","SibSp"]] = sc.transform(X_test[["Age","SibSp"]])
527/51: X_train
527/52: X_train[:, 1]
527/53: X_train.iloc[:, 1]
527/54: X_train.iloc[:, 1:3]
527/55:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])
X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])
527/56: X_train
527/57:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train.loc[:, 1:3] = sc.fit_transform(X_train.loc[:, 1:3])
X_test.loc[:, 1:3] = sc.transform(X_test.loc[:, 1:3])
527/58:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])
X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])
527/59: X_train.loc[:, 1]
527/60: X_train.loc[:,1]
527/61: X_train.loc[1,1]
528/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
528/2: dataset = pd.read_csv("data.csv")
528/3: dataset.head()
528/4: dataset.columns
528/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
528/6: dataset.isnull().sum()
528/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
528/8:
temp_dataset1 = dataset.copy()
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(), inplace=True)
temp_dataset1["Age"].isnull().sum()
528/9:
from sklearn.impute import SimpleImputer

temp_dataset2 = dataset.copy()

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(temp_dataset2[["Age"]])
temp_dataset2["Age"] = imputer.transform(temp_dataset2[["Age"]])
temp_dataset2["Age"].isnull().sum()
528/10: dataset["Age"].fillna(int(dataset["Age"].mean()),inplace=True)
528/11:
dataset.dropna(subset=["Survived","Embarked"], inplace=True)
dataset.isnull().sum()
528/12:
temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)

temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)
temp_dataset3
528/13:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')
temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))

temp_dataset4
528/14:
from sklearn.preprocessing import LabelEncoder

temp_dataset5 = dataset.copy()

le = LabelEncoder()
temp_dataset5["Sex"] = le.fit_transform(temp_dataset5["Sex"])
temp_dataset5["Embarked"] = le.fit_transform(temp_dataset5["Embarked"])

temp_dataset5
528/15:
N_Embarked = pd.get_dummies(dataset["Embarked"], drop_first=True)
N_Sex = pd.get_dummies(dataset["Sex"], drop_first=True)

dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)
print(dataset.head())

# We, can remove the Sex and Embarked Columns as we have encoded them.

dataset.drop(columns=["Sex","Embarked"], axis=1, inplace=True)
print(dataset.head())
528/16:
X = dataset.drop(columns=["Survived"],axis=1)
y = dataset["Survived"]
528/17:
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)
528/18:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])
X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])
528/19: X_train
528/20: X_test
528/21:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])
X_test.iloc[:, 1:3] = sc.fit_transform(X_test.iloc[:, 1:3])
528/22: X_train
528/23: X_test
528/24:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])
X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])
528/25: X_train
528/26: X_test
528/27:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])
X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])
528/28: X_train
528/29: X_test
529/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
529/2: dataset = pd.read_csv("data.csv")
529/3: dataset.head()
529/4: dataset.columns
529/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
529/6: dataset.isnull().sum()
529/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
529/8:
temp_dataset1 = dataset.copy()
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(), inplace=True)
temp_dataset1["Age"].isnull().sum()
529/9:
from sklearn.impute import SimpleImputer

temp_dataset2 = dataset.copy()

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(temp_dataset2[["Age"]])
temp_dataset2["Age"] = imputer.transform(temp_dataset2[["Age"]])
temp_dataset2["Age"].isnull().sum()
529/10: dataset["Age"].fillna(int(dataset["Age"].mean()),inplace=True)
529/11:
dataset.dropna(subset=["Survived","Embarked"], inplace=True)
dataset.isnull().sum()
529/12:
temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)

temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)
temp_dataset3
529/13:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')
temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))

temp_dataset4
529/14:
from sklearn.preprocessing import LabelEncoder

temp_dataset5 = dataset.copy()

le = LabelEncoder()
temp_dataset5["Sex"] = le.fit_transform(temp_dataset5["Sex"])
temp_dataset5["Embarked"] = le.fit_transform(temp_dataset5["Embarked"])

temp_dataset5
529/15:
N_Embarked = pd.get_dummies(dataset["Embarked"], drop_first=True)
N_Sex = pd.get_dummies(dataset["Sex"], drop_first=True)

dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)
print(dataset.head())

# We, can remove the Sex and Embarked Columns as we have encoded them.

dataset.drop(columns=["Sex","Embarked"], axis=1, inplace=True)
print(dataset.head())
529/16:
X = dataset.drop(columns=["Survived"],axis=1)
y = dataset["Survived"]
529/17:
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)
529/18:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])
X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])
529/19: X_train
529/20: X_test
531/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
531/2: dataset = pd.read_csv("data.csv")
531/3: dataset.head()
531/4: dataset.columns
531/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
531/6: dataset.isnull().sum()
531/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
531/8: dataset["Age"].fillna(int(dataset["Age"].mean()),inplace=True)
531/9:
dataset.dropna(subset=["Survived","Embarked"], inplace=True)
dataset.isnull().sum()
531/10:
N_Embarked = pd.get_dummies(dataset["Embarked"], drop_first=True)
N_Sex = pd.get_dummies(dataset["Sex"], drop_first=True)

dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)
print(dataset.head())

# We, can remove the Sex and Embarked Columns as we have encoded them.

dataset.drop(columns=["Sex","Embarked"], axis=1, inplace=True)
print(dataset.head())
531/11:
X = dataset.drop(columns=["Survived"],axis=1)
y = dataset["Survived"]
531/12:
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)
531/13:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])
X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])
531/14: X_train
531/15: X_test
531/16: X_train.head()
532/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
532/2: dataset = pd.read_csv("data.csv")
532/3: dataset.head()
532/4: dataset.columns
532/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
532/6: dataset.isnull().sum()
532/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
532/8: dataset["Age"].fillna(int(dataset["Age"].mean()),inplace=True)
532/9:
dataset.dropna(subset=["Survived","Embarked"], inplace=True)
dataset.isnull().sum()
532/10:
N_Embarked = pd.get_dummies(dataset["Embarked"], drop_first=True)
N_Sex = pd.get_dummies(dataset["Sex"], drop_first=True)

dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)
print(dataset.head())

# We, can remove the Sex and Embarked Columns as we have encoded them.

dataset.drop(columns=["Sex","Embarked"], axis=1, inplace=True)
print(dataset.head())
532/11:
X = dataset.drop(columns=["Survived"],axis=1)
y = dataset["Survived"]
532/12:
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)
532/13:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])
X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])
532/14: X_train.head()
533/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
533/2: !pip install pandas
534/1: !pip install pandas
534/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
535/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
535/2: !pip install matplotlib, numpy
535/3: !pip install "matplotlib","numpy"
535/4: !pip install matplotlib
536/1: !pip install matplotlib
536/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
536/3:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
   1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
   2: dataset = pd.read_csv("data.csv")
   3: dataset.head()
   4: dataset.columns
   5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
   6: dataset.isnull().sum()
   7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
   8:
temp_dataset1 = dataset.copy()
temp_dataset1["Age"].fillna(temp_dataset1["Age"].mean(), inplace=True)
temp_dataset1["Age"].isnull().sum()
   9:
from sklearn.impute import SimpleImputer

temp_dataset2 = dataset.copy()

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(temp_dataset2[["Age"]])
temp_dataset2["Age"] = imputer.transform(temp_dataset2[["Age"]])
temp_dataset2["Age"].isnull().sum()
  10:
from sklearn.impute import SimpleImputer

temp_dataset2 = dataset.copy()

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(temp_dataset2[["Age"]])
temp_dataset2["Age"] = imputer.transform(temp_dataset2[["Age"]])
temp_dataset2["Age"].isnull().sum()
  11: dataset["Age"].fillna(int(dataset["Age"].mean()),inplace=True)
  12:
dataset.dropna(subset=["Survived","Embarked"], inplace=True)
dataset.isnull().sum()
  13:
temp_dataset3 = dataset.copy()
temp_dataset3

temp_column1 = pd.get_dummies(temp_dataset3["Embarked"], drop_first=True)
temp_column2 = pd.get_dummies(temp_dataset3["Sex"], drop_first=True)

temp_dataset3 = pd.concat([temp_dataset3,temp_column1, temp_column2],axis=1)
temp_dataset3
  14:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


temp_dataset4 = dataset.copy()
temp_dataset4 = np.array(temp_dataset4)
print(temp_dataset4)

ct = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2,6])], remainder='passthrough')
temp_dataset4 = np.array(ct.fit_transform(temp_dataset4))

temp_dataset4
  15:
from sklearn.preprocessing import LabelEncoder

temp_dataset5 = dataset.copy()

le = LabelEncoder()
temp_dataset5["Sex"] = le.fit_transform(temp_dataset5["Sex"])
temp_dataset5["Embarked"] = le.fit_transform(temp_dataset5["Embarked"])

temp_dataset5
  16:
N_Embarked = pd.get_dummies(dataset["Embarked"], drop_first=True)
N_Sex = pd.get_dummies(dataset["Sex"], drop_first=True)

dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)
print(dataset.head())

# We, can remove the Sex and Embarked Columns as we have encoded them.

dataset.drop(columns=["Sex","Embarked"], axis=1, inplace=True)
print(dataset.head())
  17:
X = dataset.drop(columns=["Survived"],axis=1)
y = dataset["Survived"]
  18:

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  19:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])
X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])
  20: X_train
  21: X_test
  22: dataset = pd.read_csv("train.csv")
  23: dataset.head()
  24: dataset.isnull().sum()
  25: dataset['y'].fillna(dataset["y"].mean())
  26: dataset.isnull().sum()
  27: dataset["y"].fillna(dataset["y"].mean())
  28: dataset.isnull().sum()
  29: dataset["y"].fillna(dataset["y"].mean())
  30: dataset["y"].fillna(dataset["y"].mean(), inplace=True)
  31: dataset.isnull().sum()
  32:
X = dataset.drop(columns=["y"],axis=1)
y = dataset["x"]
  33:

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  34: X_train
  35: X_test
541/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
541/2: dataset = pd.read_csv("data.csv")
541/3: dataset.head()
541/4: dataset.columns
541/5:
dataset.drop(labels=['PassengerId','WikiId','Name', 'Name_wiki','Ticket','Cabin',
       'Age_wiki', 'Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
       'Class','Fare'], axis=1, inplace=True)
dataset.head()
541/6: dataset.isnull().sum()
541/7:
for ele in dataset.columns:
    print("{}:  {}%".format(ele, ((dataset[ele].isnull().sum()/len(dataset[ele])*100))))
541/8: dataset["Age"].fillna(int(dataset["Age"].mean()),inplace=True)
541/9:
dataset.dropna(subset=["Survived","Embarked"], inplace=True)
dataset.isnull().sum()
541/10:
N_Embarked = pd.get_dummies(dataset["Embarked"], drop_first=True)
N_Sex = pd.get_dummies(dataset["Sex"], drop_first=True)

dataset = pd.concat([dataset,N_Embarked, N_Sex],axis=1)
print(dataset.head())

# We, can remove the Sex and Embarked Columns as we have encoded them.

dataset.drop(columns=["Sex","Embarked"], axis=1, inplace=True)
print(dataset.head())
541/11:
X = dataset.drop(columns=["Survived"],axis=1)
y = dataset["Survived"]
541/12:
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)
541/13:
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train.iloc[:, 1:3] = sc.fit_transform(X_train.iloc[:, 1:3])
X_test.iloc[:, 1:3] = sc.transform(X_test.iloc[:, 1:3])
541/14: X_train.head()
  36: .ipynb_checkpoints
  37: .ipynb_checkpoints/
  38: ".ipynb_checkpoints/""
  39: ".ipynb_checkpoints/"
  40: %history -g
  41: %history -g -f demo.ipyb
